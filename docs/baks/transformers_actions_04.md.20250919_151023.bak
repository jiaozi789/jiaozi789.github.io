---
title: "Transformerså®æˆ˜04-å¾®è°ƒgpt-2ç”Ÿæˆpythonä»£ç ã€‚"
date: 2025-09-18T16:55:17+08:00
# bookComments: false
# bookSearchExclude: false
---


# ç®€ä»‹
GPT-2ï¼ˆGenerative Pre-trained Transformer 2ï¼‰æ˜¯ç”±OpenAIå¼€å‘çš„ä¸€ç§åŸºäºTransformeræ¶æ„çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯å…³äºGPT-2çš„ä¸€äº›å…³é”®ç‰¹ç‚¹å’Œä¿¡æ¯ï¼š

1. **Transformeræ¶æ„**ï¼šGPT-2åŸºäºTransformeræ¨¡å‹æ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰è¾“å…¥åºåˆ—ä¸­è¯è¯­ä¹‹é—´ä¾èµ–å…³ç³»çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
    
2. **é¢„è®­ç»ƒ**ï¼šGPT-2æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ„å‘³ç€å®ƒåœ¨å¤§è§„æ¨¡çš„æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ æ–‡æœ¬æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§å’Œè¯­è¨€æ¨¡å¼ã€‚
    
3. **æ— ç›‘ç£å­¦ä¹ **ï¼šåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒGPT-2é‡‡ç”¨äº†æ— ç›‘ç£å­¦ä¹ çš„æ–¹å¼ï¼Œå³æ¨¡å‹ä»…ä»…é€šè¿‡æ–‡æœ¬æ•°æ®æœ¬èº«æ¥å­¦ä¹ ï¼Œè€Œæ²¡æœ‰ä½¿ç”¨äººå·¥æ ‡æ³¨çš„æ ‡ç­¾æˆ–ç›‘ç£ä¿¡å·ã€‚
    
4. **ç”Ÿæˆå¼ä»»åŠ¡**ï¼šGPT-2è¢«è®¾è®¡ç”¨äºç”Ÿæˆå¼ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆå’Œæ‘˜è¦ç”Ÿæˆç­‰ã€‚å®ƒå¯ä»¥æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ï¼Œå¹¶ä¸”åœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚
    
5. **å¤šå±‚æ¬¡æ¶æ„**ï¼šGPT-2å…·æœ‰å¤šå±‚çš„Transformerç¼–ç å™¨ï¼Œå…¶ä¸­åŒ…å«æ•°ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•è·å¤æ‚çš„è¯­è¨€ç»“æ„å’Œè¯­ä¹‰å…³ç³»ã€‚
    
6. **å¤§å°å˜ç§**ï¼šGPT-2æœ‰å¤šä¸ªå¤§å°çš„å˜ç§ï¼Œä»117Måˆ°1.5Bä¸ªå‚æ•°ä¸ç­‰ï¼Œæ¯ä¸ªå˜ç§éƒ½å…·æœ‰ä¸åŒçš„æ€§èƒ½å’Œèµ„æºè¦æ±‚ã€‚æ›´å¤§çš„æ¨¡å‹å¾€å¾€åœ¨ç”Ÿæˆæ›´åŠ å‡†ç¡®å’Œæµç•…çš„æ–‡æœ¬æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚
    
7. **å¼€æ”¾è®¸å¯**ï¼šGPT-2æ˜¯åœ¨OpenAIçš„ç ”ç©¶ä¸‹å¼€å‘çš„ï¼Œå…¶æ¨¡å‹å’Œç›¸å…³èµ„æºä»¥å¼€æ”¾è®¸å¯çš„å½¢å¼å‘å¸ƒï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥è‡ªç”±åœ°ä½¿ç”¨å’Œæ„å»ºåŸºäºGPT-2çš„åº”ç”¨ã€‚
    

æ€»çš„æ¥è¯´ï¼ŒGPT-2æ˜¯ä¸€ç§å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¯ç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆã€ç†è§£ã€ç¿»è¯‘ç­‰å„ç§NLPä»»åŠ¡ã€‚


# æ¡ˆä¾‹
è¯¥æ¡ˆä¾‹æ¥æºhuggingfaceå­¦ä¹ ä¸­å¿ƒ[nlp-course](https://huggingface.co/learn)ï¼Œ[Training a causal language model from scratch](https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt)
æ–‡ç« 
## æè¿°
æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç¼©å‡ç‰ˆçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼šæˆ‘ä»¬å°†ä¸“æ³¨äºä¸€è¡Œè¡¥å…¨ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å‡½æ•°æˆ–ç±»ï¼Œä½¿ç”¨Pythonä»£ç çš„ä¸€ä¸ªå­é›†ã€‚åœ¨Pythonä¸­å¤„ç†æ•°æ®æ—¶ï¼Œæ‚¨ä¼šé¢‘ç¹æ¥è§¦åˆ°Pythonæ•°æ®ç§‘å­¦æ ˆï¼ŒåŒ…æ‹¬matplotlibã€seabornã€pandaså’Œscikit-learnåº“ã€‚åœ¨ä½¿ç”¨è¿™äº›æ¡†æ¶æ—¶ï¼Œç»å¸¸éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„å‘½ä»¤ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹æ¥ä¸ºæˆ‘ä»¬å®Œæˆè¿™äº›è°ƒç”¨ï¼Œé‚£å°†æ˜¯å¾ˆå¥½çš„ã€‚

## æ”¶é›†æ•°æ®
æˆ‘ä»¬ä½¿ç”¨huggingfaceæ”¶é›†å¾—contentåŒ…å«ï¼š"pandas", "sklearn", "matplotlib", "seaborn" è¿™äº›å…³é”®å­—pythonä»£ç 
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/5289affa0aab4eec9ffcb5c434146b7d.png)
è¿™ä¸ªæ•°æ®é›†æ˜¯ä»githubå…¬å…±ä»“åº“çˆ¬å–ï¼Œæ¯”å¦‚
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/3bd924ccf8e941efa901ec62731eb0f4.png)

```
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)
```
è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ•°æ®é›†ä¸­çš„ä¾‹å­ã€‚æˆ‘ä»¬åªéœ€æ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„å‰200ä¸ªå­—ç¬¦ï¼š

```
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

è¾“å‡º

```
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```
## æ•°æ®é›†å¤„ç†
é¦–å…ˆè¦å¯¹æ•°æ®è¿›è¡Œæ ‡è®°åŒ–ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½ç”¨å®ƒè¿›è¡Œè®­ç»ƒã€‚ç”±äºæˆ‘ä»¬çš„ç›®æ ‡ä¸»è¦æ˜¯è‡ªåŠ¨è¡¥å…¨çŸ­å‡½æ•°è°ƒç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä¿æŒä¸Šä¸‹æ–‡å¤§å°ç›¸å¯¹è¾ƒå°ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯æˆ‘ä»¬å¯ä»¥æ›´å¿«åœ°è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸”éœ€è¦çš„å†…å­˜é‡æ˜æ˜¾è¾ƒå°‘ã€‚å¦‚æœä½ çš„åº”ç”¨ç¨‹åºéœ€è¦æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœä½ å¸Œæœ›æ¨¡å‹èƒ½å¤ŸåŸºäºåŒ…å«å‡½æ•°å®šä¹‰çš„æ–‡ä»¶ç¼–å†™å•å…ƒæµ‹è¯•ï¼‰ï¼Œè¯·ç¡®ä¿å¢åŠ è¯¥æ•°å­—ï¼Œä½†ä¹Ÿè¦è®°ä½è¿™ä¼šå¢åŠ GPUçš„å†…å­˜å ç”¨ã€‚ç›®å‰ï¼Œè®©æˆ‘ä»¬å°†ä¸Šä¸‹æ–‡å¤§å°å›ºå®šä¸º128ä¸ªæ ‡è®°ï¼Œè€Œä¸æ˜¯ GPT-2 æˆ– GPT-3 ä¸­åˆ†åˆ«ä½¿ç”¨çš„ 1,024 æˆ– 2,048ã€‚

### å›é¡¾é¢„å¤„ç†
#### input\_idså’Œattention_maskï¼š
input_idsæ˜¯tokenizerå¤„ç†åå¾—åˆ°çš„è¾“å…¥ç‰¹å¾ï¼Œå®ƒå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æ•°å­—åºåˆ—ã€‚æ¯ä¸ªå•è¯æˆ–è€…æ ‡è®°ï¼ˆtokenï¼‰éƒ½ä¼šè¢«æ˜ å°„æˆå¯¹åº”çš„å”¯ä¸€æ•´æ•°ã€‚è¿™äº›æ•´æ•°åºåˆ—å°±æ˜¯æ¨¡å‹çš„å®é™…è¾“å…¥ã€‚
     **ç¤ºä¾‹**ï¼šå‡è®¾åŸå§‹æ–‡æœ¬ç»è¿‡tokenizerå¤„ç†åï¼Œç”Ÿæˆçš„`input_ids`å¯èƒ½æ˜¯ä¸€ä¸ªæ•´æ•°åºåˆ—ï¼Œå¦‚`[101, 2023, 2003, 1037, 2814, 2242, 102]`ï¼Œæ¯ä¸ªæ•´æ•°å¯¹åº”ä¸€ä¸ªtokenã€‚


attention_maskç”¨äºå‘Šè¯‰æ¨¡å‹å“ªäº›éƒ¨åˆ†æ˜¯çœŸå®çš„è¾“å…¥ï¼Œå“ªäº›éƒ¨åˆ†æ˜¯å¡«å……ï¼ˆpaddingï¼‰çš„ï¼Œä»¥ä¾¿æ¨¡å‹åœ¨è®¡ç®—æ—¶èƒ½å¤Ÿæ­£ç¡®å¤„ç†ã€‚
å¯¹äºè¾“å…¥ä¸­çš„çœŸå®tokenï¼Œå¯¹åº”ä½ç½®çš„attention_maskå€¼ä¸º1ï¼›å¯¹äºå¡«å……çš„ä½ç½®ï¼Œattention_maskå€¼ä¸º0ã€‚
ç¤ºä¾‹ï¼šå¦‚æœinput_idsæ˜¯[101, 2023, 2003, 1037, 2814, 2242, 102]ï¼Œé‚£ä¹ˆå¯¹åº”çš„attention_maskå¯èƒ½æ˜¯[1, 1, 1, 1, 1, 1, 1]ï¼Œè¡¨ç¤ºæ‰€æœ‰ä½ç½®éƒ½æ˜¯çœŸå®çš„è¾“å…¥ï¼Œå¦‚æœæŸä¸ªå¥å­è¯å…ƒæ¯”ä»–å°ï¼Œå¯èƒ½å°±éœ€è¦å¡«å……ã€‚

```
#è¿™é‡Œæ¼”ç¤ºåˆ†è¯å™¨
from transformers import AutoModel, BertTokenizer
model_name="bert-base-chinese" #bert-base-uncased
model=AutoModel.from_pretrained(model_name)
tokenizer=BertTokenizer.from_pretrained(model_name)
print(type(model),type(tokenizer))
sequence = ["æˆ‘å‡ºç”Ÿåœ¨æ¹–å—å²³é˜³,æˆ‘çš„å®¶åœ¨æ·±åœ³.","æˆ‘å¾—å„¿å­æ˜¯å°è°¦è°¦"]
#è¾“å‡ºä¸­åŒ…å«ä¸¤ä¸ªé”® input_ids å’Œ attention_maskï¼Œå…¶ä¸­ input_ids å¯¹åº”åˆ†è¯ä¹‹åçš„ tokens æ˜ å°„åˆ°çš„æ•°å­—ç¼–å·åˆ—è¡¨ï¼Œè€Œ attention_mask åˆ™æ˜¯ç”¨æ¥æ ‡è®°å“ªäº› tokens #æ˜¯è¢«å¡«å……çš„ï¼ˆè¿™é‡Œâ€œ1â€è¡¨ç¤ºæ˜¯åŸæ–‡ï¼Œâ€œ0â€è¡¨ç¤ºæ˜¯å¡«å……å­—ç¬¦ï¼‰ã€‚
print(tokenizer(sequence, padding=True, truncation=True, return_tensors="pt",pair=True))

# è·å–å¡«å……tokençš„id
pad_token_id = tokenizer.pad_token_id
# è·å–å¡«å……tokençš„å­—ç¬¦ä¸²è¡¨ç¤º
pad_token = tokenizer.convert_ids_to_tokens(pad_token_id)
print(f"å®é™…å¡«å……æ˜¯id,padid={pad_token_id},padtoken={pad_token}")
#è·å–è¯æ±‡è¡¨å¤§å°
vocab = tokenizer.get_vocab()
vocab_size = len(vocab)
print("è¯æ±‡è¡¨å¤§å°:", vocab_size,len(tokenizer))
# æ‰“å°è¯æ±‡è¡¨å†…å®¹ï¼ˆå¯é€‰ï¼‰
print("è¯æ±‡è¡¨å†…å®¹:", vocab)
#å°†è¾“å…¥åˆ‡åˆ†ä¸ºè¯è¯­ã€å­è¯æˆ–è€…ç¬¦å·ï¼ˆä¾‹å¦‚æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œç»Ÿç§°ä¸º tokensï¼›
print(tokenizer.tokenize(sequence[0]),len(tokenizer.tokenize(sequence[0])))
#æˆ‘ä»¬é€šè¿‡ convert_tokens_to_ids() å°†åˆ‡åˆ†å‡ºçš„ tokens è½¬æ¢ä¸ºå¯¹åº”çš„ token IDsï¼š
print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence[0])))
#å¯ä»¥é€šè¿‡ encode() å‡½æ•°å°†è¿™ä¸¤ä¸ªæ­¥éª¤åˆå¹¶ï¼Œå¹¶ä¸” encode() ä¼šè‡ªåŠ¨æ·»åŠ æ¨¡å‹éœ€è¦çš„ç‰¹æ®Š tokenï¼Œä¾‹å¦‚ BERT åˆ†è¯å™¨ä¼šåˆ†åˆ«åœ¨åºåˆ—çš„é¦–å°¾æ·»åŠ [CLS] å’Œ [SEP]
print(tokenizer.encode(sequence[0]))
#è§£ç è¿˜åŸæ–‡å­—ï¼Œå¯ä»¥çœ‹åˆ°encodeå‰ååŠ äº†[CLS] å’Œ [SEP]
print(tokenizer.decode(tokenizer.encode(sequence[1])))
```
è¾“å‡º

```
<class 'transformers.models.bert.modeling_bert.BertModel'> <class 'transformers.models.bert.tokenization_bert.BertTokenizer'>
{'input_ids': tensor([[ 101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345,  117, 2769, 4638,
         2157, 1762, 3918, 1766,  119,  102],
        [ 101, 2769, 2533, 1036, 2094, 3221, 2207, 6472, 6472,  102,    0,    0,
            0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}
å®é™…å¡«å……æ˜¯id,padid=0,padtoken=[PAD]
è¯æ±‡è¡¨å¤§å°: 21128 21128
è¯æ±‡è¡¨å†…å®¹: {'[PAD]': 0, '[unused1]': 1, '[unused2]': 2, '[unused3]': 3, '[unused4]': 4, '[unused5]': 5, '[unused6]': 6, '[unused7]': 7, '[unused8]': 8, '[unused9]': 9, '[unused10]': 10, '[unused11]': 11,ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚
['æˆ‘', 'å‡º', 'ç”Ÿ', 'åœ¨', 'æ¹–', 'å—', 'å²³', 'é˜³', ',', 'æˆ‘', 'çš„', 'å®¶', 'åœ¨', 'æ·±', 'åœ³', '.'] 16
[2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638, 2157, 1762, 3918, 1766, 119]
[101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638, 2157, 1762, 3918, 1766, 119, 102]
[CLS] æˆ‘ å¾— å„¿ å­ æ˜¯ å° è°¦ è°¦ [SEP]
```
#### special token
Tokenizer çš„ç‰¹æ®Šæ ‡è®°ï¼ˆspecial tokensï¼‰æ˜¯åœ¨å¤„ç†æ–‡æœ¬æ•°æ®æ—¶ç»å¸¸ç”¨åˆ°çš„ä¸€äº›ç‰¹æ®Šç¬¦å·æˆ–è€…å­—ç¬¦ä¸²ï¼Œå®ƒä»¬åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­èµ·ç€é‡è¦çš„ä½œç”¨ã€‚è¿™äº›ç‰¹æ®Šæ ‡è®°é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹å‡ ç±»ï¼š

1. **Padding token (`[PAD]`)** pad_tokenï¼š  
    åœ¨è¿›è¡Œæ‰¹é‡å¤„ç†æ—¶ï¼Œåºåˆ—é•¿åº¦ä¸ä¸€è‡´æ˜¯å¾ˆå¸¸è§çš„æƒ…å†µã€‚ä¸ºäº†ä¿è¯è¾“å…¥æ•°æ®çš„ç»Ÿä¸€æ€§ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šä½¿ç”¨ `[PAD]` æ ‡è®°æ¥å¡«å……è¾ƒçŸ­çš„åºåˆ—ï¼Œä½¿å…¶ä¸å…¶ä»–åºåˆ—çš„é•¿åº¦ç›¸åŒã€‚
    
2. **Start of sequence token (`[CLS]`)**  bos_tokenï¼š  
    åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ï¼‰ä¸­ï¼Œéœ€è¦åœ¨è¾“å…¥åºåˆ—çš„å¼€å¤´æ·»åŠ ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Œä¾‹å¦‚ `[CLS]`ï¼Œç”¨äºæ¨¡å‹ç†è§£è¿™æ˜¯ä¸€ä¸ªåºåˆ—çš„èµ·å§‹ç‚¹ï¼Œgpt2çš„å¼€å§‹tokenæ˜¯ï¼š<|endoftext|>ã€‚
    
3. **End of sequence token (`[SEP]`)** eos_tokenï¼š  
    ç±»ä¼¼åœ°ï¼Œ`[SEP]` æ ‡è®°é€šå¸¸ç”¨äºè¡¨ç¤ºåºåˆ—çš„ç»“æŸï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šä¸ªå¥å­æˆ–æ–‡æœ¬å¯¹æ—¶ï¼Œå¯ä»¥ç”¨ `[SEP]` åˆ†éš”å®ƒä»¬ã€‚
    
4. **Mask token (`[MASK]`)** mask_tokenï¼š  
    åœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­ï¼Œä¸ºäº†è¿›è¡Œè¯­è¨€æ¨¡å‹çš„æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMasked Language Modelingï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å°†ä¸€äº›å•è¯æˆ–å­è¯éšæœºåœ°ç”¨ `[MASK]` æ ‡è®°æ›¿æ¢æ‰ï¼Œè®©æ¨¡å‹é¢„æµ‹è¢«æ©ç çš„éƒ¨åˆ†ã€‚
5. **unk_token** æ˜¯ tokenizer ä¸­çš„ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Œé€šå¸¸ç”¨æ¥è¡¨ç¤ºæœªç™»å½•è¯ï¼ˆUnknown Tokenï¼‰ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œæœªç™»å½•è¯æŒ‡çš„æ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„è¯æ±‡æˆ–è€…å­è¯ã€‚å½“æ¨¡å‹åœ¨å¤„ç†è¾“å…¥æ–‡æœ¬æ—¶é‡åˆ°æœªç™»å½•è¯ï¼Œå®ƒä¼šç”¨ unk_token æ¥æ›¿ä»£è¿™äº›è¯ï¼Œä»¥ä¾¿ç»§ç»­è¿›è¡Œå¤„ç†æˆ–é¢„æµ‹ã€‚
6. **sep_token** æ˜¯ tokenizer ä¸­çš„å¦ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Œé€šå¸¸ç”¨æ¥è¡¨ç¤ºåºåˆ—çš„åˆ†éš”ç¬¦ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­ï¼Œsep_token ä¸»è¦ç”¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
æŸäº›é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆå¦‚ BERTï¼‰è¦æ±‚è¾“å…¥æ•°æ®æŒ‰ç…§ç‰¹å®šæ ¼å¼ç»„ç»‡ï¼ŒåŒ…æ‹¬ä½¿ç”¨ sep_token æ¥åˆ†éš”è¾“å…¥çš„å„ä¸ªéƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œåœ¨æ–‡æœ¬å¯¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¯ä»¥ç”¨ [SEP] æ ‡è®°åˆ†éš”ä¸¤ä¸ªå¥å­ï¼š
`[CLS] Sentence A [SEP] Sentence B [SEP]`
7. **cls_token** æ˜¯ tokenizer ä¸­çš„å¦ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Œé€šå¸¸ç”¨æ¥è¡¨ç¤ºåºåˆ—çš„å¼€å¤´æˆ–è€…åˆ†ç±»ä»»åŠ¡ä¸­çš„ç‰¹æ®Šæ ‡è®°ã€‚

è¿™äº›ç‰¹æ®Šæ ‡è®°åœ¨ä¸åŒçš„ä»»åŠ¡å’Œæ¨¡å‹ä¸­å…·æœ‰ä¸åŒçš„ç”¨é€”ï¼Œä½†å®ƒä»¬çš„å…±åŒä½œç”¨æ˜¯å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å¤„ç†æ–‡æœ¬æ•°æ®ï¼Œå¤„ç†è¾“å…¥åºåˆ—çš„é•¿åº¦å˜åŒ–ï¼Œä»¥åŠåœ¨ç‰¹å®šä»»åŠ¡ä¸­å¼•å¯¼æ¨¡å‹å­¦ä¹ å’Œé¢„æµ‹ã€‚é€šè¿‡é€‚å½“ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºæ¨¡å‹å¯¹è¯­è¨€æ•°æ®çš„ç†è§£å’Œå¤„ç†èƒ½åŠ›ã€‚

```
#ç‰¹æ®Štoken
from transformers import GPT2Tokenizer,AutoTokenizer

# åˆå§‹åŒ– GPT-2 åˆ†è¯å™¨
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer1 = AutoTokenizer.from_pretrained('bert-base-chinese')
# æ‰“å°æ‰€æœ‰ç‰¹æ®Šæ ‡è®°
print("gpt2ç‰¹æ®Šæ ‡è®°:")
for token_name, token_value in tokenizer.special_tokens_map.items():
    print(f"{token_name}: {token_value}")
print("bert-base-chineseç‰¹æ®Šæ ‡è®°:")
for token_name, token_value in tokenizer1.special_tokens_map.items():
    print(f"{token_name}: {token_value}")
```
è¾“å‡º
```
gpt2ç‰¹æ®Šæ ‡è®°:
bos_token: <|endoftext|>
eos_token: <|endoftext|>
unk_token: <|endoftext|>
--------------------------
bert-base-chineseç‰¹æ®Šæ ‡è®°:
unk_token: [UNK]
sep_token: [SEP]
pad_token: [PAD]
cls_token: [CLS]
mask_token: [MASK]
```

#### chunk 
å½“ä½ æœ‰å¤šä¸ªå¥å­æˆ–æ–‡æœ¬æ®µè½éœ€è¦å¤„ç†æ—¶ï¼Œä½ å¯ä»¥å°†å®ƒä»¬åˆ’åˆ†æˆå›ºå®šé•¿åº¦çš„å°å—ï¼ˆchunksï¼‰ï¼Œä»¥ä¾¿è¾“å…¥åˆ°æ¨¡å‹ä¸­è¿›è¡Œå¤„ç†ã€‚è¿™ä¸ªè¿‡ç¨‹é€šå¸¸ç”¨äºå¤„ç†è¾ƒé•¿çš„æ–‡æœ¬ï¼Œä»¥ç¡®ä¿æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†è¾“å…¥æ•°æ®ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨Transformerç­‰æ¨¡å‹æ—¶ï¼Œå…¶è¾“å…¥é•¿åº¦é€šå¸¸æ˜¯æœ‰é™åˆ¶çš„ã€‚

chunkçš„é€»è¾‘æ˜¯ï¼Œè¾“å…¥æ•°æ®çš„æ¯ä¸€è¡Œå¥å­ï¼Œè¶…è¿‡max_length éƒ½ä¼šè¢«æˆªæ–­ï¼Œå½“å‰å¥å­è¢«æ‹†åˆ†æˆçš„chuckçš„ä¸ªæ•°ä¸ºï¼šlen(å¥å­)%max_length +1ï¼Œå½“å‰æœ‰äº›æ¨¡å‹ä¼šæ·»åŠ ä¸€äº›å¼€å§‹å’Œåˆ†å‰²å­—ç¬¦ æ¯”å¦‚[CLS][SEQ]ç­‰ä¹Ÿè¦ç®—å…¥é•¿åº¦ã€‚


>æ³¨æ„tokenizeræ‹†åˆ†å°å—çš„å¼€å¯ç”± truncation=True,å†³å®šï¼Œå¦‚æœæ˜¯False max_lengthç­‰å°±æ— æ•ˆäº†ã€‚
```
#truckçš„é—®é¢˜ã€‚
content = ["This is the first sentence. This is the second sentence.","i am a stupid man"]
from transformers import AutoTokenizer

# é€‰æ‹©ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å’Œå¯¹åº”çš„tokenizer
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# æœ€å¤§çš„å­—ç¬¦é•¿åº¦ï¼Œå› ä¸ºå­—ç¬¦çš„æœ€å‰é¢ä¼šåŠ ä¸€ä¸ª[CLS],æœ€åä¼šè¡¥ä¸€ä¸ª[SEP]ï¼Œæ¯ä¸€ä¸ªå¥å­éƒ½ä¼šè¢«æ‹†åˆ†ä¸€æ¬¡ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªtruckè¡Œåªèƒ½10ä¸ªå­—ç¬¦ï¼Œcontentã€0ã€‘å› ä¸ºè¶…è¿‡10ä¸ªå­—ç¬¦ï¼Œæ‰€ä»¥è¢«åˆ‡å‰²æˆ2ä¸ªtruckã€‚
# è¾“å‡ºçš„trucklengthæ˜¯[10,6]ï¼Œç¬¬äºŒä¸ªå¥å­ä¸æ»¡10ä¸ªåªæœ‰7ä¸ªï¼Œæœ€ålength=[10, 6, 7]
max_length = 10

# è¿›è¡Œtokenizationï¼Œå¹¶è¿”å›ç»“æœ
outputs = tokenizer(
    content,
    truncation=True,
    max_length=max_length,
    return_overflowing_tokens=True,
    return_length=True,
)
# è¾“å‡ºç»“æœ
print(outputs)
print(tokenizer.decode(outputs['input_ids'][0]))
print(tokenizer.decode(outputs['input_ids'][1]))
```
è¾“å‡º

```
{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 2023, 2003, 102], [101, 1996, 2117, 6251, 1012, 102], [101, 1045, 2572, 1037, 5236, 2158, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]], 'length': [10, 6, 7], 'overflow_to_sample_mapping': [0, 0, 1]}
[CLS] this is the first sentence. this is [SEP]
[CLS] the second sentence. [SEP]
```
æ³¨æ„overflow_to_sample_mappingä¸­æ˜¯æ ‡è¯†æ¯ä¸ªå°chuckå±äºä¹‹å‰å“ªä¸ªå¥å­ç´¢å¼•ï¼Œç¬¬1-2ä¸ªchuckæ˜¯å±äºç¬¬0ä¸ªç´¢å¼•ä¹Ÿå°±æ˜¯ç¬¬ä¸€ä¸ªå¥å­ï¼Œ3ä¸ªç¬¬äºŒä¸ªå¥å­ã€‚

å¦‚æœåŠ äº† padding=True,æ‰€æœ‰çš„å­å¥éƒ½ä¼šè‡ªåŠ¨è¡¥ä¸Špadding_idï¼Œæœ€ç»ˆlengthéƒ½ä¼šæ˜¯10ï¼Œç»“æœå°±å˜æˆ
```
{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 2023, 2003, 102], [101, 1996, 2117, 6251, 1012, 102, 0, 0, 0, 0], [101, 1045, 2572, 1037, 5236, 2158, 102, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], 'length': [10, 10, 10], 'overflow_to_sample_mapping': [0, 0, 1]}
[CLS] this is the first sentence. this is [SEP]
[CLS] the second sentence. [SEP] [PAD] [PAD] [PAD] [PAD]
```
å…¶ä»–æ›´è¯¦ç»†çš„é¢„å¤„ç†å‚è€ƒï¼šhttps://github.com/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb

#### datacollator
- DataCollatorForLanguageModeling çš„ä¸»è¦åŠŸèƒ½æ˜¯ä¸ºæ©ç è¯­è¨€æ¨¡å‹ï¼ˆMasked Language Modelingï¼ŒMLMï¼‰ä»»åŠ¡å‡†å¤‡æ•°æ®ã€‚å®ƒçš„ä¸»è¦ä½œç”¨æ˜¯éšæœºåœ°æ©ç›–è¾“å…¥ä¸­çš„ä¸€äº›æ ‡è®°ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„æ ‡ç­¾ï¼Œä»¥ä¾¿æ¨¡å‹åœ¨è®­ç»ƒæ—¶èƒ½å¤Ÿé¢„æµ‹è¿™äº›è¢«æ©ç›–çš„æ ‡è®°ã€‚
- DataCollatorWithPaddingï¼šå¯¹è¾“å…¥è¿›è¡Œå¡«å……ï¼Œä½¿å¾—è¾“å…¥å¼ é‡å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚

æ›´å¤šç›¸å…³ç±»çš„å®ç°ï¼Œè¯·[å‚è€ƒ](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)å®˜æ–¹api

ä»¥ä¸‹æ˜¯ä¸€ä¸ªä¾‹å­

```
import torch
from transformers import BertTokenizer, DataCollatorForLanguageModeling
# åˆå§‹åŒ–BERTåˆ†è¯å™¨
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# å®šä¹‰ç¤ºä¾‹æ–‡æœ¬
texts = ["Hello, how are you?", "I am fine, thank you."]
# å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)
# æ‰“å°ç¼–ç åçš„è¾“å…¥
print("Encoded inputs:", inputs)
# å°†è¾“å…¥è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œä»¥é€‚åº”DataCollatorForLanguageModelingçš„è¾“å…¥æ ¼å¼,ä»–çš„æ ¼å¼è¦æ±‚æœ‰å¤šå°‘ä¸ªå¥å­å°±å¤šå°‘è¡Œ[{'input_ids':,'attention_mask':},{'input_ids':,'attention_mask':}]
# tokenizer encodeçš„æ ¼å¼æ˜¯å­—å…¸ {'input_ids': [[],[]]æ˜¯åœ¨äºŒç»´æ•°ç»„ä½“ç°ï¼Œæ‰€ä»¥å¼ºåˆ¶è½¬ä¸€ä¸‹
batch = [{key: val[i] for key, val in inputs.items()} for i in range(len(texts))]
print("collatoréœ€è¦æ ¼å¼",batch)
# åˆå§‹åŒ–æ•°æ®æ•´ç†å™¨ï¼ŒæŒ‡å®šè¿›è¡Œæ©ç è¯­è¨€æ¨¡å‹ä»»åŠ¡
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

# å¯¹è¾“å…¥æ•°æ®è¿›è¡Œæ•´ç†
collated_inputs = data_collator(batch)

# æ‰“å°æ•´ç†åçš„è¾“å…¥,è¿™é‡Œå› ä¸ºmlm=Trueæ˜¯è‡ªåŠ¨æ©ç›–ï¼Œæœ‰15%çš„æ•°æ®è¢«æ©ç›–ï¼Œè¢«æ©ç›–çš„æ•°æ®åœ¨input_idsè¢«æ›¿æ¢æˆ103ï¼Œç„¶ååœ¨ç”Ÿæˆçš„labelsä¸Šï¼Œæ²¡æœ‰è¢«æ©ç›–çš„æ•°æ®éƒ½å˜æˆ-100ï¼Œè¢«æ©ç›–çš„æ•°æ®æ›¿æ¢ä¸ºä¹‹å‰çš„æ•°æ®
# labelsæ˜¯æœ€åçš„æ ‡ç­¾ï¼Œé€šè¿‡è®­ç»ƒåå‘å°±èƒ½å¾ˆå¥½çš„ä¼˜åŒ–æ¨¡å‹ï¼Œè¿™å°±æ˜¯maskedæ¨¡å‹æ•°æ®å¤„ç†
print("Collated inputs:", collated_inputs)
data_collator1 = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
collated_inputs = data_collator1(batch)
#mlm=Falseï¼Œä¸ä¼šäº§ç”Ÿé®ç›–ï¼Œæ‰€æœ‰çš„è¾“å…¥ç”Ÿæˆçš„æ˜¯è¾“å‡ºç›¸åŒçš„labelsï¼Œå¦‚æœæ˜¯paddingå­—ç¬¦ï¼Œlabelsæ˜¯-100
print("Collated inputs:", collated_inputs)
```
è¾“å‡º

```
Encoded inputs: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102,    0],
        [ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}
collatoréœ€è¦æ ¼å¼ [{'input_ids': tensor([ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102,    0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0])}, {'input_ids': tensor([ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012,  102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])}]
Collated inputs: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102,    0],
        [ 101, 1045, 2572, 2986,  103, 4067,  103, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, 1010, -100, 2017, -100, -100]])}
Collated inputs: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102,    0],
        [ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102, -100],
        [ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012,  102]])}
```

#### map
åœ¨ä½¿ç”¨ transformers åº“æ—¶ï¼Œdatasets ä¸­çš„ map æ–¹æ³•æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„å·¥å…·ï¼Œç”¨äºå¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ã€ç‰¹å¾æå–ã€æ•°æ®å¢å¼ºç­‰æ“ä½œã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ map æ–¹æ³•å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ä¾¿äºå°†å…¶ç”¨äºè®­ç»ƒä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ¨¡å‹ã€‚
è¯¦ç»†å¤„ç†å‚è€ƒï¼š[https://huggingface.co/docs/datasets/use_dataset](https://huggingface.co/docs/datasets/use_dataset)
`map` å‡½æ•°æ˜¯ `datasets` åº“ä¸­ä¸€ä¸ªéå¸¸å¼ºå¤§çš„å·¥å…·ï¼Œå®ƒå…è®¸ä½ å¯¹æ•°æ®é›†çš„æ¯ä¸ªæ ·æœ¬æˆ–æ‰¹æ¬¡è¿›è¡Œæ“ä½œå’Œå˜æ¢ã€‚ä»¥ä¸‹æ˜¯ `map` å‡½æ•°çš„å‡ ä¸ªå…³é”®å‚æ•°åŠå…¶è§£é‡Šï¼š

1. **`function`**

è¿™æ˜¯ä¸€ä¸ªç”¨æˆ·å®šä¹‰çš„å‡½æ•°ï¼Œå®ƒå°†åº”ç”¨äºæ•°æ®é›†çš„æ¯ä¸ªæ ·æœ¬æˆ–æ‰¹æ¬¡ã€‚å‡½æ•°å¯ä»¥æ¥å—ä¸€ä¸ªæ ·æœ¬æˆ–ä¸€ç»„æ ·æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªæˆ–å¤šä¸ªæ–°çš„å­—æ®µã€‚

`def preprocess_function(examples):     # ä½ çš„é¢„å¤„ç†é€»è¾‘     return examples`

2. **`batched`**

- ç±»å‹ï¼š`bool`
- é»˜è®¤å€¼ï¼š`False`
- è§£é‡Šï¼šå¦‚æœè®¾ç½®ä¸º `True`ï¼Œ`function` å°†ä¼šæ‰¹é‡åº”ç”¨åˆ°æ•°æ®é›†ä¸­ã€‚è¿™æ„å‘³ç€ `function` å°†æ¥æ”¶ä¸€ä¸ªåŒ…å«å¤šä¸ªæ ·æœ¬çš„å­—å…¸ä½œä¸ºè¾“å…¥ã€‚

`dataset.map(preprocess_function, batched=True)`

 3. **`batch_size`**

- ç±»å‹ï¼š`int`
- é»˜è®¤å€¼ï¼š`1000`
- è§£é‡Šï¼šæŒ‡å®šæ‰¹é‡å¤„ç†æ—¶çš„æ‰¹æ¬¡å¤§å°ã€‚ä»…å½“ `batched=True` æ—¶æœ‰æ•ˆã€‚

`dataset.map(preprocess_function, batched=True, batch_size=32)`

 4. **`remove_columns`**

- ç±»å‹ï¼š`list` or `str`
- é»˜è®¤å€¼ï¼š`None`
- è§£é‡Šï¼šæŒ‡å®šè¦ä»æ•°æ®é›†ä¸­ç§»é™¤çš„åˆ—ã€‚è¿™å¯¹äºæ¸…ç†ä¸éœ€è¦çš„å­—æ®µéå¸¸æœ‰ç”¨ã€‚

`dataset.map(preprocess_function, remove_columns=["column_name"])`
```
# å¯¼å…¥å¿…è¦çš„åº“
from datasets import Dataset

# åˆ›å»ºä¸€ä¸ªç®€å•çš„æ•°æ®é›†
data = {
    'text': [
        "This is the first sentence.",
        "Here's the second sentence.",
        "And this is the third one."
    ],
    'label': [1, 0, 1]
}

# è½¬æ¢ä¸º Dataset å¯¹è±¡
dataset = Dataset.from_dict(data)

# æ‰“å°åŸå§‹æ•°æ®é›†
print("åŸå§‹æ•°æ®é›†ï¼š")
print(dataset)

# å¯¼å…¥å¿…è¦çš„åº“
from transformers import AutoTokenizer

# åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# å®šä¹‰é¢„å¤„ç†å‡½æ•°
def preprocess_function(examples):
    print("ä¼ å…¥æ•°æ®é›†",examples)
    # ä½¿ç”¨åˆ†è¯å™¨å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
    encoded_tokenizer = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=8)
    print("åˆ†è¯æ•°æ®é›†",encoded_tokenizer)
    #è¿”å›çš„å­—å…¸æ•°æ®ä¼šè¢«ç´¯åŠ åˆ°åŸå§‹æ•°æ®é›†ä¸Šã€‚
    return encoded_tokenizer

# ä½¿ç”¨ map æ–¹æ³•åº”ç”¨é¢„å¤„ç†å‡½æ•°
encoded_dataset = dataset.map(preprocess_function, batched=True,batch_size=2)

# æ‰“å°é¢„å¤„ç†åçš„æ•°æ®é›†
print("\né¢„å¤„ç†åçš„æ•°æ®é›†ç»“æ„ï¼š",encoded_dataset)
print("\né¢„å¤„ç†åçš„æ•°æ®é›†ï¼š",encoded_dataset[0:3])

# ä½¿ç”¨ map æ–¹æ³•åº”ç”¨é¢„å¤„ç†å‡½æ•°,remove_columnsè¡¨ç¤ºåˆ é™¤æŸäº›åˆ—æ˜¯ä¸ªæ•°ç»„ã€‚
encoded_dataset = dataset.map(preprocess_function, batched=True,batch_size=2,remove_columns=dataset.features)

# æ‰“å°é¢„å¤„ç†åçš„æ•°æ®é›†
print("\né¢„å¤„ç†åçš„æ•°æ®é›†ï¼š",encoded_dataset[0:3])
```
è¾“å‡ºï¼š

```
åŸå§‹æ•°æ®é›†ï¼š
Dataset({
    features: ['text', 'label'],
    num_rows: 3
})
Map:â€‡100%
â€‡3/3â€‡[00:00<00:00,â€‡138.43â€‡examples/s]
ä¼ å…¥æ•°æ®é›† {'text': ['This is the first sentence.', "Here's the second sentence.", 'And this is the third one.'], 'label': [1, 0, 1]}
åˆ†è¯æ•°æ®é›† {'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}

é¢„å¤„ç†åçš„æ•°æ®é›†ç»“æ„ï¼š Dataset({
    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 3
})

é¢„å¤„ç†åçš„æ•°æ®é›†ï¼š {'text': ['This is the first sentence.', "Here's the second sentence.", 'And this is the third one.'], 'label': [1, 0, 1], 'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}
é¢„å¤„ç†åçš„æ•°æ®é›†ï¼š {'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}
```

### é¢„å¤„ç†
å¤§å¤šæ•°æ–‡æ¡£çš„æ ‡è®°æ•°è¿œè¶…è¿‡ 128 ä¸ªï¼Œå› æ­¤ç®€å•åœ°å°†è¾“å…¥æˆªæ–­åˆ°æœ€å¤§é•¿åº¦ä¼šæ¶ˆé™¤æˆ‘ä»¬æ•°æ®é›†çš„å¾ˆå¤§ä¸€éƒ¨åˆ†ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ return_overflowing_tokens é€‰é¡¹æ¥å¯¹æ•´ä¸ªè¾“å…¥è¿›è¡Œæ ‡è®°ï¼Œå¹¶å°†å…¶æ‹†åˆ†ä¸ºå‡ ä¸ªå—ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ return_length é€‰é¡¹è‡ªåŠ¨è¿”å›æ¯ä¸ªåˆ›å»ºå—çš„é•¿åº¦ã€‚é€šå¸¸ï¼Œæœ€åä¸€ä¸ªå—ä¼šå°äºä¸Šä¸‹æ–‡å¤§å°ï¼Œæˆ‘ä»¬å°†å»æ‰è¿™äº›éƒ¨åˆ†ä»¥é¿å…å¡«å……é—®é¢˜ï¼›å®é™…ä¸Šæˆ‘ä»¬ä¸éœ€è¦å®ƒä»¬ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰å¾ˆå¤šæ•°æ®ã€‚
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/3f5c55e1611842b7a9e191ecc1305279.png)
è®©æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹å‰ä¸¤ä¸ªä¾‹å­æ¥çœ‹çœ‹è¿™åˆ°åº•æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š

```
from transformers import AutoTokenizer

context_length = 128
#è¿™ä¸ªåˆ†è¯å™¨ä¸“é—¨ä¸ºä»£ç æœç´¢å’Œç†è§£ä»»åŠ¡è®¾è®¡ã€‚å®ƒä¸»è¦ç”¨äºå¤„ç†ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ Pythonã€JavaScriptã€Java ç­‰ï¼‰çš„æºä»£ç ã€‚
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    #è·å–0ï¼Œ1è¿™ä¸¤ä¸ªæ•°æ®é›†çš„è„šæœ¬å†…å®¹
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```
>   `huggingface-course/code-search-net-tokenizer`
>- **è®¾è®¡ç›®æ ‡**ï¼šè¿™ä¸ªåˆ†è¯å™¨ä¸“é—¨ä¸ºä»£ç æœç´¢å’Œç†è§£ä»»åŠ¡è®¾è®¡ã€‚å®ƒä¸»è¦ç”¨äºå¤„ç†ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ >Pythonã€JavaScriptã€Java ç­‰ï¼‰çš„æºä»£ç ã€‚
>- **è®­ç»ƒæ•°æ®**ï¼šè¯¥åˆ†è¯å™¨ä½¿ç”¨ CodeSearchNet æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ•°æ®é›†ä¸­åŒ…å«äº†å¤§é‡çš„ä»£ç ç¤ºä¾‹>å’Œæ³¨é‡Šã€‚
>- **åº”ç”¨é¢†åŸŸ**ï¼šé€‚ç”¨äºä»£ç æœç´¢ã€ä»£ç è¡¥å…¨ã€ä»£ç ç”Ÿæˆå’Œå…¶ä»–ä¸ä»£ç ç›¸å…³çš„ä»»åŠ¡ã€‚
>- **è¯æ±‡è¡¨**ï¼šè¯æ±‡è¡¨ä¸­åŒ…å«äº†å¤§é‡çš„ç¼–ç¨‹è¯­è¨€ç‰¹å®šçš„æ ‡è®°ï¼ˆå¦‚å…³é”®å­—ã€æ“ä½œç¬¦ã€å˜é‡åç­‰ï¼‰ï¼Œä»¥åŠ>å¸¸è§çš„ç¼–ç¨‹è¯­è¨€è¯­æ³•å’Œç»“æ„ã€‚
><font color=red>æ³¨æ„ï¼šåˆ†è¯å™¨æ¨¡å‹çš„ä½œç”¨æ˜¯å°†å•è¯è½¬æ¢ä¸ºä¸€ä¸ªä¸ªçš„æ•°å­—ï¼Œè®­ç»ƒæ—¶ä½¿ç”¨çš„æ•°å­—è®¡ç®—æ•°å­—ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ï¼Œæœ€åæ¨ç®—å¯¹åº”çš„æ•°å­—åï¼Œåå‘é€šè¿‡è¯å…¸è§£ææˆæ–‡å­—ï¼Œæ‰€ä»¥å¦‚æœéœ€è¦è®­ç»ƒä¸­æ–‡ï¼Œä½ åªéœ€è¦æœ‰ä¸€ä¸ªä¸­æ–‡åˆ†è¯æ¨¡å‹å³å¯ï¼Œè®­ç»ƒåªå’Œæ•°å­—ç›¸å…³ã€‚</font>

è¾“å‡ºï¼š
```
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä»è¿™ä¸¤ä¸ªä¾‹å­ä¸­æ€»å…±å¾—åˆ°äº† 34 ä¸ªç‰‡æ®µã€‚æŸ¥çœ‹ç‰‡æ®µé•¿åº¦ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªæ–‡æ¡£æœ«å°¾çš„ç‰‡æ®µéƒ½å°‘äº 128 ä¸ªæ ‡è®°ï¼ˆåˆ†åˆ«ä¸º 117 å’Œ 41ï¼‰ã€‚è¿™äº›ä»…å æˆ‘ä»¬æ‹¥æœ‰çš„æ€»ç‰‡æ®µçš„ä¸€å°éƒ¨åˆ†ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°ä¸¢å¼ƒå®ƒä»¬ã€‚ä½¿ç”¨ overflow_to_sample_mapping å­—æ®µï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é‡å»ºå“ªäº›ç‰‡æ®µå±äºå“ªäº›è¾“å…¥æ ·æœ¬ã€‚

é€šè¿‡è¿™ä¸ªæ“ä½œï¼Œæˆ‘ä»¬åˆ©ç”¨äº† ğŸ¤— Datasets ä¸­ Dataset.map() å‡½æ•°çš„ä¸€ä¸ªä¾¿åˆ©åŠŸèƒ½ï¼Œå³å®ƒä¸éœ€è¦ä¸€ä¸€å¯¹åº”çš„æ˜ å°„ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæ¯”è¾“å…¥æ‰¹æ¬¡å¤šæˆ–å°‘çš„å…ƒç´ æ‰¹æ¬¡ã€‚å½“è¿›è¡Œæ•°æ®å¢å¼ºæˆ–æ•°æ®è¿‡æ»¤ç­‰ä¼šæ”¹å˜å…ƒç´ æ•°é‡çš„æ“ä½œæ—¶ï¼Œè¿™éå¸¸æœ‰ç”¨ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå½“å°†æ¯ä¸ªå…ƒç´ æ ‡è®°ä¸ºæŒ‡å®šä¸Šä¸‹æ–‡å¤§å°çš„å—æ—¶ï¼Œæˆ‘ä»¬ä»æ¯ä¸ªæ–‡æ¡£ä¸­åˆ›å»ºäº†è®¸å¤šæ ·æœ¬ã€‚æˆ‘ä»¬åªéœ€è¦ç¡®ä¿åˆ é™¤ç°æœ‰åˆ—ï¼Œå› ä¸ºå®ƒä»¬çš„å¤§å°ä¸ä¸€è‡´ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¿ç•™å®ƒä»¬ï¼Œå¯ä»¥é€‚å½“é‡å¤å¹¶åœ¨ Dataset.map() è°ƒç”¨ä¸­è¿”å›å®ƒä»¬ï¼š

```
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    #è·å–å½“å‰input_idså’Œé•¿åº¦ï¼Œæœ«å°¾chuckä¸ç­‰äºcontext_lengthï¼Œå°±ä¸éœ€è¦åŠ å…¥äº†
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```
è¾“å‡ºï¼š

```
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```
æˆ‘ä»¬ç°åœ¨æœ‰ 1670 ä¸‡ä¸ªä¾‹å­ï¼Œæ¯ä¸ªä¾‹å­æœ‰ 128 ä¸ªæ ‡è®°ï¼Œæ€»å…±å¯¹åº”å¤§çº¦ 21 äº¿ä¸ªæ ‡è®°ã€‚ä¾›å‚è€ƒï¼ŒOpenAI çš„ GPT-3 å’Œ Codex æ¨¡å‹åˆ†åˆ«åœ¨ 300 å’Œ 1000 äº¿ä¸ªæ ‡è®°ä¸Šè®­ç»ƒï¼Œå…¶ä¸­ Codex æ¨¡å‹æ˜¯ä» GPT-3 æ£€æŸ¥ç‚¹åˆå§‹åŒ–çš„ã€‚æˆ‘ä»¬åœ¨è¿™ä¸€éƒ¨åˆ†çš„ç›®æ ‡ä¸æ˜¯ä¸è¿™äº›æ¨¡å‹ç«äº‰ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥ç”Ÿæˆé•¿è€Œè¿è´¯çš„æ–‡æœ¬ï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªç¼©å‡ç‰ˆæœ¬ï¼Œä¸ºæ•°æ®ç§‘å­¦å®¶æä¾›å¿«é€Ÿè‡ªåŠ¨è¡¥å…¨åŠŸèƒ½ã€‚
ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½æ•°æ®é›†ï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬è®¾ç½®æ¨¡å‹ï¼
## åˆå§‹åŒ–æ¨¡å‹
### å›é¡¾æ¨¡å‹
#### å‚æ•°è®¡ç®—
åœ¨ PyTorch ä¸­ï¼Œ`t.numel()` æ˜¯ä¸€ä¸ªå¼ é‡æ–¹æ³•ï¼Œç”¨äºè¿”å›å¼ é‡ä¸­æ‰€æœ‰å…ƒç´ çš„æ•°é‡ã€‚å®ƒç­‰ä»·äºè®¡ç®—å¼ é‡çš„å¤§å°ï¼ˆshapeï¼‰çš„æ‰€æœ‰ç»´åº¦çš„ä¹˜ç§¯ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå½¢çŠ¶ä¸º (3, 4, 5) çš„å¼ é‡æœ‰ 3 \* 4 \* 5 = 60 ä¸ªå…ƒç´ ã€‚

åœ¨ä½ æä¾›çš„ä»£ç ä¸­ï¼š

`model_size = sum(t.numel() for t in model.parameters())`

è¿™é‡Œ `model.parameters()` è¿”å›æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°çš„ä¸€ä¸ªç”Ÿæˆå™¨ã€‚é€šè¿‡ `t.numel()` è®¡ç®—æ¯ä¸ªå‚æ•°å¼ é‡ä¸­çš„å…ƒç´ æ•°é‡ï¼Œç„¶åä½¿ç”¨ `sum()` å‡½æ•°å°†æ‰€æœ‰è¿™äº›æ•°é‡åŠ èµ·æ¥ï¼Œå¾—åˆ°æ•´ä¸ªæ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°çš„æ€»å…ƒç´ æ•°é‡ï¼Œå³æ¨¡å‹çš„æ€»å¤§å°ã€‚
ç¤ºä¾‹
å‡è®¾æœ‰ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼š

```
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(10, 20)
        self.fc2 = nn.Linear(20, 30)
        
    def forward(self, x):
        x = self.fc1(x)
        x = self.fc2(x)
        return x

model = SimpleModel()
```
è®¡ç®—æ¨¡å‹å¤§å°çš„ä»£ç å¦‚ä¸‹ï¼š

```
model_size = sum(t.numel() for t in model.parameters())
print(model_size)
```
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`model.parameters()` ä¼šè¿”å› `fc1` å’Œ `fc2` çš„å‚æ•°å¼ é‡ã€‚

- `fc1` çš„æƒé‡å¼ é‡å½¢çŠ¶ä¸º (20, 10)ï¼Œæœ‰ 20 \* 10 = 200 ä¸ªå…ƒç´ ã€‚
- `fc1` çš„åç½®å¼ é‡å½¢çŠ¶ä¸º (20,)ï¼Œæœ‰ 20 ä¸ªå…ƒç´ ã€‚
- `fc2` çš„æƒé‡å¼ é‡å½¢çŠ¶ä¸º (30, 20)ï¼Œæœ‰ 30 \* 20 = 600 ä¸ªå…ƒç´ ã€‚
- `fc2` çš„åç½®å¼ é‡å½¢çŠ¶ä¸º (30,)ï¼Œæœ‰ 30 ä¸ªå…ƒç´ ã€‚

æ€»è®¡æ¨¡å‹ä¸­æœ‰ 200 + 20 + 600 + 30 = 850 ä¸ªå‚æ•°å…ƒç´ ã€‚å› æ­¤ï¼Œ`model_size` çš„å€¼å°†æ˜¯ 850ã€‚
### åˆå§‹åŒ–
æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥æ˜¯åˆå§‹åŒ–ä¸€ä¸ªGPT-2æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä¸ºæˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨ä¸å°å‹GPT-2æ¨¡å‹ç›¸åŒçš„é…ç½®ï¼Œå› æ­¤æˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒçš„é…ç½®ï¼Œç¡®ä¿æ ‡è®°å™¨å¤§å°ä¸æ¨¡å‹è¯æ±‡å¤§å°åŒ¹é…ï¼Œå¹¶ä¼ é€’boså’Œeosï¼ˆåºåˆ—å¼€å§‹å’Œç»“æŸï¼‰ä»¤ç‰ŒIDï¼š

```
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer), #è·å–è¯æ±‡è¡¨å¤§å°
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```
>å› ä¸ºä½¿ç”¨äº†ä¸åŒçš„åˆ†è¯å™¨ï¼Œæ‰€ä»¥é‡æ–°åŠ è½½é…ç½®

é€šè¿‡è¯¥é…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½ä¸€ä¸ªæ–°æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸ä½¿ç”¨ from_pretrained() å‡½æ•°ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨è‡ªå·±åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼š
```
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```
è¾“å‡º

```
GPT-2 size: 124.2M parameters
```
æˆ‘ä»¬çš„æ¨¡å‹æœ‰ 124M ä¸ªå‚æ•°éœ€è¦è°ƒä¼˜ã€‚åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®ä¸€ä¸ªæ•°æ®æ•´ç†å™¨ï¼Œæ¥å¤„ç†åˆ›å»ºæ‰¹æ¬¡çš„å·¥ä½œã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ DataCollatorForLanguageModeling æ•´ç†å™¨ï¼Œå®ƒæ˜¯ä¸“é—¨ä¸ºè¯­è¨€å»ºæ¨¡è®¾è®¡çš„ï¼ˆæ­£å¦‚å…¶åç§°å¾®å¦™åœ°æš—ç¤ºçš„é‚£æ ·ï¼‰ã€‚é™¤äº†å †å å’Œå¡«å……æ‰¹æ¬¡å¤–ï¼Œå®ƒè¿˜è´Ÿè´£åˆ›å»ºè¯­è¨€æ¨¡å‹æ ‡ç­¾â€”â€”åœ¨å› æœè¯­è¨€å»ºæ¨¡ä¸­ï¼Œè¾“å…¥ä¹Ÿä½œä¸ºæ ‡ç­¾ï¼ˆä»…åç§»ä¸€ä¸ªå…ƒç´ ï¼‰ï¼Œè¿™ä¸ªæ•°æ®æ•´ç†å™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶åˆ›å»ºå®ƒä»¬ï¼Œå› æ­¤æˆ‘ä»¬ä¸éœ€è¦é‡å¤ input_idsã€‚
è¯·æ³¨æ„ï¼ŒDataCollatorForLanguageModeling æ”¯æŒæ©ç è¯­è¨€å»ºæ¨¡ (MLM) å’Œå› æœè¯­è¨€å»ºæ¨¡ (CLM)ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒä¸º MLM å‡†å¤‡æ•°æ®ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½®å‚æ•° mlm=False åˆ‡æ¢åˆ° CLMï¼š

```
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```
è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªä¾‹å­ï¼š

```
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```
è¾“å‡º
```
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```
æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¤ºä¾‹å·²ç»è¢«å †å ï¼Œæ‰€æœ‰å¼ é‡å½¢çŠ¶ç›¸åŒã€‚

å‰©ä¸‹çš„å°±æ˜¯é…ç½®è®­ç»ƒå‚æ•°å¹¶å¯åŠ¨è®­ç»ƒå™¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦ï¼Œå¹¶è¿›è¡Œä¸€äº›é¢„çƒ­ï¼Œå®é™…æ‰¹é‡å¤§å°ä¸º256ï¼ˆper_device_train_batch_size * gradient_accumulation_stepsï¼‰ã€‚å½“å•ä¸ªæ‰¹æ¬¡æ— æ³•é€‚åº”å†…å­˜æ—¶ï¼Œä¼šä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå®ƒé€šè¿‡å¤šæ¬¡å‰å‘/åå‘ä¼ é€’é€æ­¥ç´¯ç§¯æ¢¯åº¦ã€‚å½“æˆ‘ä»¬ä½¿ç”¨ğŸ¤— Accelerate åˆ›å»ºè®­ç»ƒå¾ªç¯æ—¶ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è¿™ä¸€ç‚¹çš„å®é™…åº”ç”¨ã€‚

```
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```
ç°åœ¨æˆ‘ä»¬å¯ä»¥å¯åŠ¨è®­ç»ƒå™¨å¹¶ç­‰å¾…è®­ç»ƒå®Œæˆã€‚æ ¹æ®æ‚¨æ˜¯åœ¨å®Œæ•´çš„è®­ç»ƒé›†ä¸Šè¿è¡Œè¿˜æ˜¯åœ¨å­é›†ä¸Šè¿è¡Œï¼Œè¿™å°†åˆ†åˆ«éœ€è¦ 20 å°æ—¶æˆ– 2 å°æ—¶ï¼Œæ‰€ä»¥å‡†å¤‡å‡ æ¯å’–å•¡å’Œä¸€æœ¬å¥½ä¹¦æ¥é˜…è¯»å§ï¼

```
trainer.train()
```
## å®Œæ•´ä»£ç 

```
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)
from transformers import AutoTokenizer

context_length = 128
#è¿™ä¸ªåˆ†è¯å™¨æ¨¡å‹ä¸“é—¨ä¸ºä»£ç æœç´¢å’Œç†è§£ä»»åŠ¡è®¾è®¡ã€‚å®ƒä¸»è¦ç”¨äºå¤„ç†ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ Pythonã€JavaScriptã€Java ç­‰ï¼‰çš„æºä»£ç ï¼Œåˆ†è¯å™¨çš„ç›®çš„æ˜¯å°†å¯¹åº”è¯å…ƒè½¬æ¢ä¸ºæ•°å­—ï¼Œè®©æ¨¡å‹é€šè¿‡è®¡ç®—æ¥ç†è§£æ•°å­—å’Œæ•°å­—ä¹‹é—´çš„å…³ç³»ï¼Œé€‰æ‹©æ¨¡å‹çš„åˆ†è¯å™¨éå¸¸é‡è¦ã€‚
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    #è·å–å½“å‰input_idså’Œé•¿åº¦ï¼Œæœ«å°¾chuckä¸ç­‰äºcontext_lengthï¼Œå°±ä¸éœ€è¦åŠ å…¥äº†
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer), #è·å–è¯æ±‡è¡¨å¤§å°
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
model = GPT2LMHeadModel(config)
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="/kaggle/working",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    report_to="none",
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
trainer.train()
```
## æµ‹è¯•
>ç”±äºä½¿ç”¨kaggleçš„gpuæ— æ³•åœ¨12å°æ—¶è®­ç»ƒå®Œæˆï¼Œæ‰€ä»¥è¿™é‡Œåªèƒ½ç”¨å®˜æ–¹å·²ç»è®­ç»ƒå¥½çš„é•œåƒæµ‹è¯•äº†ã€‚

ç°åœ¨æ˜¯è§è¯ç»“æœçš„æ—¶åˆ»ï¼šè®©æˆ‘ä»¬çœ‹çœ‹è®­ç»ƒå¥½çš„æ¨¡å‹å®é™…è¡¨ç°å¦‚ä½•ï¼æˆ‘ä»¬å¯ä»¥åœ¨æ—¥å¿—ä¸­çœ‹åˆ°æŸå¤±å€¼ä¸€ç›´åœ¨ç¨³å®šä¸‹é™ï¼Œä½†ä¸ºäº†çœŸæ­£æµ‹è¯•æ¨¡å‹çš„æ•ˆæœï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å®ƒåœ¨ä¸€äº›æç¤ºä¿¡æ¯ä¸Šçš„è¡¨ç°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å°è£…åˆ°ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆç®¡é“ä¸­ï¼Œå¹¶å¦‚æœæ¡ä»¶å…è®¸çš„è¯ï¼Œå°†å…¶éƒ¨ç½²åˆ° GPU ä¸Šä»¥å®ç°å¿«é€Ÿç”Ÿæˆï¼š

```
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```
è®©æˆ‘ä»¬ä»åˆ›å»ºæ•£ç‚¹å›¾çš„ç®€å•ä»»åŠ¡å¼€å§‹ï¼š

```
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```
è¾“å‡ºï¼š

```
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter
```
ç»“æœçœ‹èµ·æ¥æ˜¯æ­£ç¡®çš„ã€‚å¯¹äº pandas çš„æ“ä½œæ˜¯å¦ä¹Ÿé€‚ç”¨å‘¢ï¼Ÿæˆ‘ä»¬æ¥çœ‹çœ‹èƒ½å¦ä»ä¸¤ä¸ªæ•°ç»„åˆ›å»ºä¸€ä¸ª DataFrameï¼š

```
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```
è¾“å‡º

```
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```
å¥½çš„ï¼Œè¿™æ˜¯æ­£ç¡®çš„ç­”æ¡ˆâ€”â€”å°½ç®¡éšååˆæ’å…¥äº†åˆ— xã€‚ç”±äºç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡æœ‰é™ï¼Œä¸‹é¢çš„ for å¾ªç¯è¢«æˆªæ–­äº†ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹èƒ½å¦åšä¸€äº›æ›´å¤æ‚çš„äº‹æƒ…ï¼Œå¹¶è®©æ¨¡å‹å¸®åŠ©æˆ‘ä»¬ä½¿ç”¨ groupby æ“ä½œï¼š

```
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```
è¾“å‡º

```
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
profession = df.groupby(['profession']).mean()

# compute the
```
è¿˜ä¸é”™ï¼›è¿™æ ·åšæ˜¯å¯¹çš„ã€‚æœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦ä¹Ÿèƒ½ç”¨å®ƒæ¥ä¸º scikit-learn è®¾ç½®ä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹ï¼š

```
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```
è¾“å‡º

```
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```
æŸ¥çœ‹è¿™å‡ ä¸ªä¾‹å­ï¼Œæ¨¡å‹ä¼¼ä¹å­¦åˆ°äº†ä¸€äº› Python æ•°æ®ç§‘å­¦å¥—ä»¶çš„è¯­æ³•ã€‚