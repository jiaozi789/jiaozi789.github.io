<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes :: liaomin416100569博客</title>
    <link>https://jiaozi789.github.io/docs/devops/kubernetes/index.html</link>
    <description></description>
    <generator>Hugo</generator>
    <language>zh</language>
    <lastBuildDate>Thu, 18 Sep 2025 16:55:17 +0800</lastBuildDate>
    <atom:link href="https://jiaozi789.github.io/docs/devops/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>K8S二次开发01-各种资源对象的理解和定义</title>
      <link>https://jiaozi789.github.io/docs/devops/kubernetes/k8s_dev_01/index.html</link>
      <pubDate>Thu, 18 Sep 2025 16:55:17 +0800</pubDate>
      <guid>https://jiaozi789.github.io/docs/devops/kubernetes/k8s_dev_01/index.html</guid>
      <description>一、Pod Kubernetes为每个Pod都分配了唯一的IP地址，称之为Pod IP，一个Pod里的多个容器共享Pod IP地址。Kubernetes要求底层网络支持集群内任意两个Pod之间的TCP/IP直接通信，这通常采用虚拟二层网络技术来实现，例如Flannel、Open vSwitch等。因此，在Kubernetes里，一个Pod里的容器与另外主机上的Pod容器能够直接通信。 Pod有两种类型：普通的Pod和静态Pod（Static Pod），静态Pod不存放在etcd存储里，而是存放在某个具体的Node上的一个具体文件中，并且只在此Node上启动运行。普通的Pod一旦被创建，就会被存储到etcd中，随后会被Kubernetes Master调度到某个具体的Node上并进行绑定（Binding），该Node上的kubelet进程会将其实例化成一组相关的Docker容器并启动起来。当Pod里的某个容器停止时，Kubernetes会自动检测到这个问题并且重新启动这个Pod（重启Pod里的所有容器）；如果Pod所在的Node宕机，则会将这个Node上的所有Pod重新调度到其他节点上运行。 Pod、容器与Node的关系如下图： Kubernetes里的所有资源对象都可以采用yaml或者JSON格式的文件来定义或描述，下面是一个简单的Pod资源定义文件：&#xA;apiVersion: v1 kind: Pod metadata: name: myweb labels: name: myweb spec: containers: - name: myweb image: kubeguide/tomcat-app: v1 ports: - containerPort: 8080 env: - name: MYSQL_SERVICE_HOST value: &#39;mysql&#39; - name: MYSQL_SERVICE_PORT value: &#39;3306&#39; kind为pod表明这是一个Pod的定义，metadata里的name属性为Pod的名字，metadata里还能定义资源对象的标签（Label），这里声明myweb拥有一个name=myweb的标签（Label）。Pod里包含的容器组的定义则在spec一节中声明，这里定义了一个名字为myweb，对应镜像为kubeguide/tomcat-app: v1的容器，该容器注入了名为MYSQL_SERVICE_HOST=‘mysql’和MYSQL_SERVICE_PORT=‘3306’的环境变量（env关键字），并且在8080端口（containerPort）上启动容器进程。Pod的IP加上这里的容器端口，就组成了一个新的概念——Endpoint，它代表着此Pod里的一个服务进程的对外通信地址。一个Pod也存在着具有多个Endpoint的情况，比如我们把Tomcat定义为一个Pod时，可以对外暴露管理端口与服务端口这两个Endpoint。 Docker里的Volume在Kubernetes里也有对应的概念——Pod Volume，Pod Volume有一些扩展，比如可以用分布式文件系统GlusterFS等实现后端存储功能；Pod Volume是定义在Pod之上，然后被各个容器挂载到自己的文件系统中的。对于Pod Volume的定义我们后面会讲到。 这里顺便提一下Event概念，Event是一个事件的记录，记录了事件的最早产生时间、最后重现时间、重复次数、发起者、类型，以及导致此事件的原因等众多信息。Event通常会关联到某个具体的资源对象上，是排查故障的重要参考信息，当我们发现某个Pod迟迟无法创建时，可以用kubectl describe pod xxx来查看它的描述信息，用来定位问题的原因。 每个Pod都可以对其能使用的服务器上的计算资源设置限额，当前可以设置限额的计算资源有CPU和Memory两种，其中CPU的资源单位为CPU（Core）的数量，是一个绝对值。 对于容器来说一个CPU的配额已经是相当大的资源配额了，所以在Kubernetes里，通常以千分之一的CPU配额为最小单位，用m来表示。通常一个容器的CPU配额被定义为100-300m，即占用0.1-0.3个CPU。与CPU配额类似，Memory配额也是一个绝对值，它的单位是内存字节数。 对计算资源进行配额限定需要设定以下两个参数：&#xA;Requests：该资源的最小申请量，系统必须满足要求。 Limits：该资源最大允许使用的量，不能超过这个使用限制，当容器试图使用超过这个量的资源时，可能会被Kubernetes Kill并重启。 通常我们应该把Requests设置为一个比较小的数值，满足容器平时的工作负载情况下的资源需求，而把Limits设置为峰值负载情况下资源占用的最大量。下面是一个资源配额的简单定义：&#xA;spec: containers: - name: db image: mysql resources: requests: memory: &#34;64Mi&#34; cpu: &#34;250m&#34; limits: memory: &#34;128Mi&#34; cpu: &#34;500m&#34; 最小0.25个CPU及64MB内存，最大0.5个CPU及128MB内存。&#xA;二、Label（标签） Label相当于我们熟悉的“标签”，给某个资源对象定义一个Label，就相当于给它打了一个标签，随后可以通过Label Selector（标签选择器）查询和筛选拥有某些Label的资源对象，Kubernetes通过这种方式实现了类似SQL的简单又通用的对象查询机制。 Label Selector相当于SQL语句中的where查询条件，例如，name=redis-slave这个Label Selector作用于Pod时，相当于select * from pod where pod’s name = ‘redis-slave’这样的语句。Label Selector的表达式有两种：基于等式的（Equality-based）和基于集合的（Set-based）。下面是基于等式的匹配例子。 name=redis-slave：匹配所有标签为name=redis-slave的资源对象。 env != production：匹配所有标签env不等于production的资源对象。 下面是基于集合的匹配例子</description>
    </item>
    <item>
      <title>K8S二次开发02-kubeadm安装k8s集群</title>
      <link>https://jiaozi789.github.io/docs/devops/kubernetes/k8s_dev_02/index.html</link>
      <pubDate>Thu, 18 Sep 2025 16:55:17 +0800</pubDate>
      <guid>https://jiaozi789.github.io/docs/devops/kubernetes/k8s_dev_02/index.html</guid>
      <description>组件概览 关于k8s整体架构，可参考：之前文章 Kubernetes主要由以下几个核心组件组成（必须安装）：&#xA;etcd保存了整个集群的状态； apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制； controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； scheduler负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上； kubelet负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理，安装每台工作节点； kube-proxy负责为Service提供cluster内部的服务发现和负载均衡，安装在每台工作节点； 除了核心组件，还有一些推荐的Add-ons（选装）： kube-dns负责为整个集群提供DNS服务，应该使用kubectl添加一个服务到容器，是一个发布的应用服务于 其中kubelet是二进制安装包，其他组件均为docker镜像，kubeadm负责拉取镜像并初始化环境。 kubectl是客户端管理工具，同样是个二进制。 所以kubelet,kubeadm,kubectl需要通过yum|apt-get安装，其他组件通过kubeadm安装。&#xA;安装k8s 最简单，成功率最高的安装方式其实是使用rke安装，这里使用kubeadmin是想知道每个组件的单独原理和作用，具体安装教程参考：https://docs.rancher.cn/docs/rke/installation/_index/。&#xA;已经安装如果需要清除，可按照此步骤处理(重置k8s，删除所有运行容易和镜像)&#xA;kubeadm reset docker ps -a | awk &#39;{if(NR&gt;1){print $1;system(&#34;docker stop &#34;$1);system(&#34;docker rm &#34;$1)}}&#39;; docker images | awk &#39;{system(&#34;docker rmi &#34;$3)}&#39; rm -rf $HOME/.kube 同时清除下面安装网络章节的网络相关文件 准备机器 这里使用debian环境，主备一主一从两台机器，最好固定ip，ip最好在同一网段&#xA;k8s-master 10.10.0.115 k8s-worker 10.10.0.116 安装kubelet，kubeadm，kubectl，两台机器均相同 确保两台机器提前安装包docker&#xA;apt-get install docker-ce 设置阿里云源 sudo vim /etc/apt/sources.list.d/kubernetes.list # 将下面的阿里源加入文件中 deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main # 也可以选择中科大的源 deb http://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main 里先运行一下 apt update, 会报错，原因是缺少相应的key，可以通过下面的命令添加(E084DAB9 为上面报错的key后8位)&#xA;gpg --keyserver keyserver.ubuntu.com --recv-keys E084DAB9 gpg --export --armor E084DAB9 | sudo apt-key add - 下载安装 apt-get update &amp;&amp; apt-get install -y kubelet kubeadm kubectl 关闭swap 如果不关闭kubernetes运行会出现错误， 即使安装成功了，node重启后也会出现kubernetes server运行错误。</description>
    </item>
    <item>
      <title>K8S二次开发03-CRD资源详解</title>
      <link>https://jiaozi789.github.io/docs/devops/kubernetes/k8s_dev_03/index.html</link>
      <pubDate>Thu, 18 Sep 2025 16:55:17 +0800</pubDate>
      <guid>https://jiaozi789.github.io/docs/devops/kubernetes/k8s_dev_03/index.html</guid>
      <description>CustomResourceDefinition简介： 在 Kubernetes 中一切都可视为资源，Kubernetes 1.7 之后增加了对 CRD 自定义资源二次开发能力来扩展 Kubernetes API，通过 CRD 我们可以向 Kubernetes API 中增加新资源类型，而不需要修改 Kubernetes 源码来创建自定义的 API server，该功能大大提高了 Kubernetes 的扩展能力。 当你创建一个新的CustomResourceDefinition (CRD)时，Kubernetes API服务器将为你指定的每个版本创建一个新的RESTful资源路径，我们可以根据该api路径来创建一些我们自己定义的类型资源。CRD可以是命名空间的，也可以是集群范围的，由CRD的作用域(scpoe)字段中所指定的，与现有的内置对象一样，删除名称空间将删除该名称空间中的所有自定义对象。customresourcedefinition本身没有名称空间，所有名称空间都可以使用。&#xA;创建crd定义 Kuberneters 官方文档 中文版本 通过crd资源创建自定义资源，即自定义一个Restful API：&#xA;apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: # 名称必须与下面的spec字段匹配，格式为: &lt;plural&gt;.&lt;group&gt; name: crontabs.stable.example.com spec: # 用于REST API的组名称: /apis/&lt;group&gt;/&lt;version&gt; group: stable.example.com # 此CustomResourceDefinition支持的版本列表 versions: - name: v1 # 每个版本都可以通过服务标志启用/禁用。 served: true # 必须将一个且只有一个版本标记为存储版本。 storage: true #使用v3定义创建容器的属性 cronSpec和image是字符串类型replicas是int型 schema: openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string description: &#34;定时任务触发时间&#34; image: type: string description: &#34;镜像&#34; replicas: type: integer description: &#34;副本数&#34; # 指定crd资源作用范围在命名空间或集群 scope: Namespaced names: # URL中使用的复数名称: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt; plural: crontabs # 在CLI(shell界面输入的参数)上用作别名并用于显示的单数名称 singular: crontab # kind字段使用驼峰命名规则. 资源清单使用如此 kind: CronTab # 短名称允许短字符串匹配CLI上的资源，意识就是能通过kubectl 在查看资源的时候使用该资源的简名称来获取。 shortNames: - ct 注意：这是只是一个资源定义，类似于java中class的定义，new class是创建的对象。 在k8s中 pod，service都是已经预先定义的资源，通过kubectl create|run创建的是实例，定义只有一个，实例可以有多个。</description>
    </item>
    <item>
      <title>K8S二次开发04-自定义operator（operator-sdk调试）</title>
      <link>https://jiaozi789.github.io/docs/devops/kubernetes/k8s_dev_04/index.html</link>
      <pubDate>Thu, 18 Sep 2025 16:55:17 +0800</pubDate>
      <guid>https://jiaozi789.github.io/docs/devops/kubernetes/k8s_dev_04/index.html</guid>
      <description>Operator 是 Kubernetes 的扩展软件，它利用 定制资源 管理应用及其组件。 Operator 遵循 Kubernetes 的理念，特别是在控制器 方面。 Operator操作那些有状态的基础设施服务，包括：组件升级、节点恢复、调整集群规模。一个理想化的运维平台必须是Operator自己维护有状态应用，并将人工干预降低到最低限度&#xA;什么是Operator？ 为了理解什么是Operator，让我们先复习一下Kubernetes。Kubernetes实际是期望状态管理器。先在Kubernetes中指定应用程序期望状态（实例数，磁盘空间，镜像等），然后它会尝试把应用维持在这种状态。Kubernetes的控制平面运行在Master节点上，它包含数个controller以调和应用达到期望状态：&#xA;检查当前的实际状态（Pod、Deployment等） 将实际状态与spec期望状态进行比较 如果实际状态与期望状态不一致，controller将会尝试协调实际状态以达到一致 比如，通过RS定义Pod拥有3个副本。当其中一个Pod down掉时，Kubernetes controller通过wacth API发现期望运行3个副本，而实际只有2个副本在运行。于是它新创建出一个Pod实例。&#xA;controller作用 如图所示，controller在Kubernetes中发挥的作用。 通过Kubectl命令发送对象spec定义（Pod，Deployment等）到Kubernetes Master节点的API服务 Master节点调度对象运行 一旦对象运行，controller会持续检查对象并根据spec协调实际情况 通过这种方式，Kubernetes非常适合维护无状态应用。但它本身的资源类型（Pod，Deployments，Namespaces，Services，DaemonSets等）比较有限。虽然每种资源类型都预定了行为及协调方式，但它们的处理方式没有多大差别。 现在，如果您的应用更复杂并需要执行自定义操作以达到期望的运行状态，应该怎么办？&#xA;举一个有状态应用的例子。假如一个数据库应用运行在多个节点上。如果超过半数的节点出现故障，则需要按照特定步骤从特定快照中加载数据。使用原生Kubernetes对象类型和controller则难以实现。或者为有状态应用程序扩展节点，升级到新版本或灾难恢复。这些类型的操作通常需要非常具体的步骤，并且通常需要手动干预。&#xA;controller系统结构 使用 CRD 定制资源后，仅仅是让 Kubernetes 能够识别定制资源的身份。创建定制资源实例后，Kubernetes 只会将创建的实例存储到数据库中，并不会触发任何业务逻辑。在 数据库保存定制资源实例是没有意义的，如果需要进行业务逻辑控制，就需要创建控制器。&#xA;Controller 的作用就是监听指定对象的新增、删除、修改等变化，并针对这些变化做出相应的响应，关于 Controller 的详细设计，可以参考 Harry (Lei) Zhang 老师在 twitter 上的分享，基本架构图如下： 图中可看出，定制资源实例的变化会通过 Informer 存入 WorkQueue，之后 Controller 会消费 WorkQueue，并对其中的数据做出业务响应。</description>
    </item>
    <item>
      <title>K8S二次开发05-使用clientgo自定义ingresscontroller</title>
      <link>https://jiaozi789.github.io/docs/devops/kubernetes/k8s_dev_05/index.html</link>
      <pubDate>Thu, 18 Sep 2025 16:55:17 +0800</pubDate>
      <guid>https://jiaozi789.github.io/docs/devops/kubernetes/k8s_dev_05/index.html</guid>
      <description>clientgo简介 client-go 作为官方维护的 go 语言实现的 client 库，提供了大量的高质量代码帮助开发者编写自己的客户端程序，来访问、操作 Kubernetes 集群&#xA;infomer简介 cient-go 是从 k8s 代码中抽出来的一个客户端工具，Informer 是 client-go 中的核心工具包，已经被 kubernetes 中众多组件所使用。所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client，本地缓存被称为 Store，索引被称为 Index。使用 informer 的目的是为了减轻 apiserver 数据交互的压力而抽象出来的一个 cache 层, 客户端对 apiserver 数据的 “读取” 和 “监听” 操作都通过本地 informer 进行。Informer 实例的Lister()方法可以直接查找缓存在本地内存中的数据。&#xA;Informer 的主要功能：&#xA;同步数据到本地缓存 根据对应的事件类型，触发事先注册好的 ResourceEventHandler infomer产生背景 随着Controller越来越多，如果Controller直接访问k8s-apiserver，那么将会导致其压力过大，于是在这样的背景下就有了Informer的概念。其发展到今天这个架构，大概可以总结出以下迭代思路： 第一阶段，Controller直接访问k8s-api-server。存在的问题：多个控制器大量访问k8s-apiserver时会对其造成巨大的压力。&#xA;第二阶段，Informer代替Controller去访问k8s-apiserver。而Controller的所有操作操作(如：查状态、对资源进行伸缩等）都和Informer进行交互。但Informer没有必要每次都去访问k8s-apiserver，它只要在需要的时候通过ListAndWatch(即通过k8s List API获取所有资源的最新状态；通过Wath API去监听这些资源状态的变化)与k8s-apiserver交互即可。&#xA;ListAndWatch的代码位置: client-go/tools/cache/reflector.go&#xA;func (r *Reflector) ListAndWatch(stopCh &lt;-chan struct{}) error{ … } 第三阶段， Informer并没有直接访问k8s-api-server，而是通过一个叫Reflector的对象进行api-server的访问。上面所说的 ListAndWatch 事实上是由Reflector`实现的。</description>
    </item>
  </channel>
</rss>