var relearn_searchindex = [
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库",
    "content": "",
    "description": "",
    "tags": [],
    "title": "transformers",
    "uri": "/docs/programming/ai/tools_libraries/transformers/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers",
    "content": "",
    "description": "",
    "tags": [],
    "title": "transformers模型详解",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 插件开发",
    "content": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "description": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "tags": [],
    "title": "vscode插件",
    "uri": "/docs/programming/plugins/vscode/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "基础理论",
    "uri": "/docs/programming/ai/machine_learning/basic/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能",
    "content": "",
    "description": "",
    "tags": [],
    "title": "工具库",
    "uri": "/docs/programming/ai/tools_libraries/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能",
    "content": "",
    "description": "",
    "tags": [],
    "title": "机器学习",
    "uri": "/docs/programming/ai/machine_learning/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 核心算法",
    "content": "一、简单k-近邻算法 本文将从k-近邻算法的思想开始讲起，使用python3一步一步编写代码进行实战训练。并且，我也提供了相应的数据集，对代码进行了详细的注释。除此之外，本文也对sklearn实现k-近邻算法的方法进行了讲解。实战实例：电影类别分类、约会网站配对效果判定、手写数字识别。\n文章中大部分文字和例题参考自https://cuijiahua.com/blog/2017/11/ml_1_knn.html 对原文很多代码进行了简化\n感谢这篇文章加速本人入门速度\n1、k-近邻法简介 k近邻法(k-nearest neighbor, k-NN)是1967年由Cover T和Hart P提出的一种基本分类与回归方法。它的工作原理是：存在一个样本数据集合，也称作为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新的数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本最相似数据(最近邻)的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。\n举个简单的例子，我们可以使用k-近邻算法分类一个电影是爱情片还是动作片。\n表1.1 每部电影的打斗镜头数、接吻镜头数以及电影类型\n表1.1 就是我们已有的数据集合，也就是训练样本集。这个数据集有两个特征，即打斗镜头数和接吻镜头数。除此之外，我们也知道每个电影的所属类型，即分类标签。用肉眼粗略地观察，接吻镜头多的，是爱情片。打斗镜头多的，是动作片。以我们多年的看片经验，这个分类还算合理。如果现在给我一部电影，你告诉我这个电影打斗镜头数和接吻镜头数。不告诉我这个电影类型，我可以根据你给我的信息进行判断，这个电影是属于爱情片还是动作片。而k-近邻算法也可以像我们人一样做到这一点，不同的地方在于，我们的经验更\"牛逼\"，而k-近邻算法是靠已有的数据。比如，你告诉我这个电影打斗镜头数为2，接吻镜头数为102，我的经验会告诉你这个是爱情片，k-近邻算法也会告诉你这个是爱情片。你又告诉我另一个电影打斗镜头数为49，接吻镜头数为51，我\"邪恶\"的经验可能会告诉你，这有可能是个\"爱情动作片\"，画面太美，我不敢想象。 (如果说，你不知道\"爱情动作片\"是什么？请评论留言与我联系，我需要你这样像我一样纯洁的朋友。) 但是k-近邻算法不会告诉你这些，因为在它的眼里，电影类型只有爱情片和动作片，它会提取样本集中特征最相似数据(最邻近)的分类标签，得到的结果可能是爱情片，也可能是动作片，但绝不会是\"爱情动作片\"。当然，这些取决于数据集的大小以及最近邻的判断标准等因素。\n2、距离度量 我们已经知道k-近邻算法根据特征比较，然后提取样本集中特征最相似数据(最邻近)的分类标签。那么，如何进行比较呢？比如，我们还是以表1.1为例，怎么判断红色圆点标记的电影所属的类别呢？ 如下图所示。\n我们可以从散点图大致推断，这个红色圆点标记的电影可能属于动作片，因为距离已知的那两个动作片的圆点更近。k-近邻算法用什么方法进行判断呢？没错，就是距离度量。这个电影分类的例子有2个特征，也就是在2维实数向量空间，可以使用我们高中学过的两点距离公式计算距离，如图1.2所示。\n通过计算，我们可以得到如下结果：\n(101,20)-\u003e动作片(108,5)的距离约为16.55 (101,20)-\u003e动作片(115,8)的距离约为18.44 (101,20)-\u003e爱情片(5,89)的距离约为118.22 (101,20)-\u003e爱情片(1,101)的距离约为128.69 通过计算可知，红色圆点标记的电影到动作片 (108,5)的距离最近，为16.55。如果算法直接根据这个结果，判断该红色圆点标记的电影为动作片，这个算法就是最近邻算法，而非k-近邻算法。那么k-近邻算法是什么呢？k-近邻算法步骤如下：\n计算已知类别数据集中的点与当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的k个点； 确定前k个点所在类别的出现频率； 返回前k个点所出现频率最高的类别作为当前点的预测分类。 比如，现在我这个k值取3，那么在电影例子中，按距离依次排序的三个点分别是动作片(108,5)、动作片(115,8)、爱情片(5,89)。在这三个点中，动作片出现的频率为三分之二，爱情片出现的频率为三分之一，所以该红色圆点标记的电影为动作片。这个判别过程就是k-近邻算法。\n其他距离公式：\n曼哈顿距离（ManhattanDistance）：设平面空间内存在两点，它们的坐标为(x1,y1)(x1,y1)，(x2,y2)(x2,y2)\n则 dis=|x1−x2|+|y1−y2|\n比如 每个小正方形距离是1 红，栏，黄色都是12个方格都是曼哈顿距离\n绿色线是欧氏距离(欧几里德距离：在二维和三维空间中的欧氏距离的就是两点之间的直线距离）\n切比雪夫距离（Chebyshev Distance ）：设平面空间内存在两点，它们的坐标为(x1,y1)(x1,y1)，(x2,y2)(x2,y2)\n则dis=max(|x1−x2|,|y1−y2|)\n闵可夫斯基距离(MinkowskiDistance)：\n两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为：\n其中p是一个变参数。\n当p=1时，就是曼哈顿距离 当p=2时，就是欧氏距离 当p→∞时，就是切比雪夫距离 根据变参数的不同，闵氏距离可以表示一类的距离。\n其他距离公式参考https://my.oschina.net/hunglish/blog/787596\n3、Python3代码实现 我们已经知道了k-近邻算法的原理，那么接下来就是使用Python3实现该算法，依然以电影分类为例。\n(1)准备数据集\n对于表1.1中的数据，我们可以使用numpy直接创建，代码如下：\nimport numpy as np; import matplotlib.pyplot as mp; import collections as c; #实现knn算法 一般用于推测 不具备学习能力 主要是比较 ' 数据集合，也就是训练样本集。这个数据集有两个特征， 即打斗镜头数和接吻镜头数。 除此之外，我们也知道每个电影的所属类型，即分类标签 电影名称 打斗镜头 接吻镜头 电影类型 神雕侠侣 100 20 动作片 毒液：致命守护者 99 10 动作片 碟中谍6：全面瓦解 67 5 动作片 热情如火 40 125 动作片 泰坦尼克号 0 10 爱情片 倩女幽魂 10 20 爱情片 大话西游之月光宝盒 10 40 爱情片 烈火如歌 1 30 爱情片 ' arr=np.array([[100,200],[99,10],[67,5],[40,125],[0,10],[10,20],[10,40],[1,30]]); tarr=np.array([1,1,1,1,0,0,0,1]); (2)k-近邻算法\n根据两点距离公式，计算距离，选择距离最小的前k个点，并返回分类结果。\nimport numpy as np; import matplotlib.pyplot as mp; import collections as c; #实现knn算法 一般用于推测 不具备学习能力 主要是比较 \" 数据集合，也就是训练样本集。这个数据集有两个特征， 即打斗镜头数和接吻镜头数。 除此之外，我们也知道每个电影的所属类型，即分类标签 电影名称 打斗镜头 接吻镜头 电影类型 神雕侠侣 100 20 动作片 毒液：致命守护者 99 10 动作片 碟中谍6：全面瓦解 67 5 动作片 热情如火 40 125 动作片 泰坦尼克号 0 10 爱情片 倩女幽魂 10 20 爱情片 大话西游之月光宝盒 10 40 爱情片 烈火如歌 1 30 爱情片 \" arr=np.array([[100,200],[99,10],[67,5],[40,125],[0,10],[10,20],[10,40],[1,30]]); tarr=np.array([1,1,1,1,0,0,0,1]); x=arr[:,:1].T[0] y=arr[:,1:].T[0] print(\"x轴数据:\",x) print(\"y轴数据:\",y) #设置字体 mp.rcParams['font.family']=['STFangsong'] mp.title(\"电影类型图\") mp.xlabel(\"打斗镜头\") mp.ylabel(\"接吻镜头\") #第三个参数 o表示使用 散点 r表示red红色 mp.plot(x,y,\"or\") mp.show(); #判断打斗镜头44 接吻镜头 12到底是哪种类型的片片了 ndata=[44,12] #计算当前这个ndata的坐标和之前所有数据的坐标的距离 放在一个jl数组中 #距离计算公式是 欧氏距离 (x-x1)**2 +(y-y1)**2 开平方根 # jl中每个下标的数据 就是ndata和对应位置xy坐标的距离 jl=[np.sqrt((ndata[0]-i[0])**2+(ndata[0]-i[1])**2) for i in arr]; print(\"未排序的数据是\",jl); #对距离进行排序 然后获取排序后的下标 # 比如数组： [10,12,8] # argsort升序 [2,0,1] jlsort=np.argsort(jl); print(\"排序的索引是\",jlsort); k=3; print(jlsort[:k]) #获取指定k 前三个值最小下标的标签 也就是前三个距离最近的都是什么类型的电影 # 比如[1,1,0] flaga=[tarr[t] for t in jlsort[:k]]; print(flaga) #统计类型集合的哪个出现的次数 会得到一个字典 #[(1,2),(0,1)] group=c.Counter(flaga); #获取到个数排序（从大到小） 值最大的前1个 #[(1,2)] [0][0]获取到1 类型就是动作片罗 print(group.most_common(1)[0][0]); #来个三目判断下 输出中文 result=(\"动作片\" if group.most_common(1)[0][0]==1 else \"爱情片\"); print(result); 运行结果:\n排序的索引是 [6 5 7 2 4 1 3 0][6 5 7][0, 0, 1]0爱情片 可以看到，分类结果根据我们的\"经验\"，是正确的，尽管这种分类比较耗时，用时1.4s。\n到这里，也许有人早已经发现，电影例子中的特征是2维的，这样的距离度量可以用两 点距离公式计算，但是如果是更高维的呢？对，没错。我们可以用欧氏距离(也称欧几里德度量)，如图1.5所示。我们高中所学的两点距离公式就是欧氏距离在二维空间上的公式，也就是欧氏距离的n的值为2的情况。\n图1.5 欧氏距离公式\n看到这里，有人可能会问：“分类器何种情况下会出错？”或者“答案是否总是正确的？”答案是否定的，分类器并不会得到百分百正确的结果，我们可以使用多种方法检测分类器的正确率。此外分类器的性能也会受到多种因素的影响，如分类器设置和数据集等。不同的算法在不同数据集上的表现可能完全不同。为了测试分类器的效果，我们可以使用已知答案的数据，当然答案不能告诉分类器，检验分类器给出的结果是否符合预期结果。通过大量的测试数据，我们可以得到分类器的错误率-分类器给出错误结果的次数除以测试执行的总数。错误率是常用的评估方法，主要用于评估分类器在某个数据集上的执行效果。完美分类器的错误率为0，最差分类器的错误率是1.0。同时，我们也不难发现，k-近邻算法没有进行数据的训练，直接使用未知的数据与已知的数据进行比较，得到结果。因此，可以说k-近邻算法不具有显式的学习过程。\n二、k-近邻算法实战之约会网站配对效果判定 上一小结学习了简单的k-近邻算法的实现方法，但是这并不是完整的k-近邻算法流程，k-近邻算法的一般流程：\n收集数据：可以使用爬虫进行数据的收集，也可以使用第三方提供的免费或收费的数据。一般来讲，数据放在txt文本文件中，按照一定的格式进行存储，便于解析及处理。 准备数据：使用Python解析、预处理数据。 分析数据：可以使用很多方法对数据进行分析，例如使用Matplotlib将数据可视化。 测试算法：计算错误率。 使用算法：错误率在可接受范围内，就可以运行k-近邻算法进行分类。 已经了解了k-近邻算法的一般流程，下面开始进入实战内容。\n1、实战背景 海伦女士一直使用在线约会网站寻找适合自己的约会对象。尽管约会网站会推荐不同的任选，但她并不是喜欢每一个人。经过一番总结，她发现自己交往过的人可以进行如下分类：\n不喜欢的人 魅力一般的人 极具魅力的人 海伦收集约会数据已经有了一段时间，她把这些数据存放在文本文件datingTestSet.txt中，每个样本数据占据一行，总共有1000行。datingTestSet.txt数据下载： 约会数据\n海伦收集的样本数据主要包含以下3种特征：\n每年获得的飞行常客里程数 玩视频游戏所消耗时间百分比 每周消费的冰淇淋公升数 这里不得不吐槽一句，海伦是个小吃货啊，冰淇淋公斤数都影响自己择偶标准。打开txt文本文件，数据格式如图2.1所示。\n图2.1 datingTestSet.txt格式\n2、准备数据：数据解析 在将上述特征数据输入到分类器前，必须将待处理的数据的格式改变为分类器可以接收的格式。分类器接收的数据是什么格式的？从上小结已经知道，要将数据分类两部分，即特征矩阵和对应的分类标签向量。创建lovesimple.py文件，创建名为dataSet的函数，以此来处理输入格式问题。 将lovedata.txt放到与py文件相同目录下，编写代码如下：\nimport numpy as np; import matplotlib.pyplot as pl import matplotlib.lines as mlines import collections as coll; \"\"\" 读取data.txt的所有数据集 前三列： 海伦收集的样本数据主要包含以下3种特征： 每年获得的飞行常客里程数 玩视频游戏所消耗时间百分比 每周消费的冰淇淋公升数 最后一列： didntLike 不喜欢的人 smallDoses 魅力一般的人 largeDoses 极具魅力的人 \"\"\" def dataSet(): arr=[]; with open(\"lovedata.txt\",\"r\") as file: for line in file: arr.append(line.strip().split(\"\\t\"));#默认删除空白符(包括'\\n','\\r','\\t',' ') arrnp=np.array(arr); #前三列是特征数据 读取出来时字符串数组， 转换成float类型 #最后一列是标签数据 转换成1维向量 return arrnp[:,:3].astype(dtype=np.float32),arrnp[:,3:].T[0]; 运行上述代码，得到的数据解析结果如图2.2所示。\n可以看到，我们已经顺利导入数据，并对数据进行解析，格式化为分类器需要的数据格式。接着我们需要了解数据的真正含义。可以通过友好、直观的图形化的方式观察数据。\n3、分析数据：数据可视化 在lovesimple.py文件中编写名为graphDataSet的函数，用来将数据可视化。编写代码如下：\ndef dataSet(): arr=[]; with open(\"lovedata.txt\",\"r\") as file: for line in file: arr.append(line.strip().split(\"\\t\"));#默认删除空白符(包括'\\n','\\r','\\t',' ') arrnp=np.array(arr); #将特征数据字符串转换成数字 return arrnp[:,:3].astype(dtype=np.float32),arrnp[:,3:].T[0]; ''' 根据数据 绘制图形 ''' def graphDataSet(feature, result): pl.rcParams['font.family'] = ['STFangsong'] #nrow=2,nclos=2时,代表fig画布被分为四个区域,axs[0][0]表示第一行第一个区域 figsize表示画布大小 fig,axs=pl.subplots(nrows=2,ncols=2);# \"\"\",figsize=(13,8)\"\"\" colorArray = [\"black\" if e == \"didntLike\" else (\"orange\" if e == \"smallDoses\" else \"red\") for e in result] drawSubPlot(axs, 0, 0, \"每年获得的飞行常客里程数和玩视频游戏所消耗时间百分比占比\" , \"每年获得的飞行常客里程数\", \"玩视频游戏所消耗时间\", feature[:, :1].T[0], feature[:, 1:2].T[0], colorArray ) #####绘制 0,1这个subplot 上面代码用于学习 drawSubPlot(axs,0,1,\"玩视频游戏所消耗时间和每周消费的冰淇淋公升数占比\" ,\"玩视频游戏所消耗时间\", \"每周消费的冰淇淋公升数\", feature[:, 1:2].T[0], feature[:, 2:3].T[0], colorArray ) drawSubPlot(axs, 1, 0, \"每年获得的飞行常客里程数和每周消费的冰淇淋公升数占比\" , \"每年获得的飞行常客里程数\", \"每周消费的冰淇淋公升数\", feature[:, 0:1].T[0], feature[:, 2:3].T[0], colorArray ) pl.show(); \"\"\" 绘制子plot的封装 \"\"\" def drawSubPlot(axs,x,y,title,xlabel,ylabel,xdata,ydata,colorArray): axs[x][y].set_title(title) axs[x][y].set_xlabel(xlabel) axs[x][y].set_ylabel(ylabel) axs[x][y].scatter(x=xdata, y=ydata, color=colorArray, s=2); didntLike = mlines.Line2D([], [], color='black', marker='.', markersize=2, label='不喜欢') smallDoses = mlines.Line2D([], [], color='orange', marker='.', markersize=2, label='魅力一般') largeDoses = mlines.Line2D([], [], color='red', marker='.', markersize=2, label='极具魅力') axs[x][y].legend(handles=[didntLike, smallDoses, largeDoses]) 运行以下代码\nfeature, result = dataSet() print(feature) print(result) graphDataSet(feature,result) ，可以看到可视化结果如图所示。\n通过数据可以很直观的发现数据的规律，比如以玩游戏所消耗时间占比与每年获得的飞行常客里程数，只考虑这二维的特征信息，给我的感觉就是海伦喜欢有生活质量的男人。为什么这么说呢？每年获得的飞行常客里程数表明，海伦喜欢能享受飞行常客奖励计划的男人，但是不能经常坐飞机，疲于奔波，满世界飞。同时，这个男人也要玩视频游戏，并且占一定时间比例。能到处飞，又能经常玩游戏的男人是什么样的男人？很显然，有生活质量，并且生活悠闲的人。我的分析，仅仅是通过可视化的数据总结的个人看法。我想，每个人的感受应该也是不尽相同。\n4、准备数据：数据归一化 以下给出了四组样本，如果想要计算样本3和样本4之间的距离，可以使用欧拉公式计算。\n计算方法如下所示。\n从上计算公式\n我们很容易发现，上面方程中数字差值最大的属性对计算结果的影响最大，也就是说，每年获取的飞行常客里程数对于计算结果的影响将远远大于表中其他两个特征-玩视频游戏所耗时间占比和每周消费冰淇淋公斤数的影响。而产生这种现象的唯一原因，仅仅是因为飞行常客里程数远大于其他特征值。但海伦认为这三种特征是同等重要的，因此作为三个等权重的特征之一，飞行常客里程数并不应该如此严重地影响到计算结果。\n在处理这种不同取值范围的特征值时，我们通常采用的方法是将数值归一化，如将取值范围处理为０到１或者-１到１之间。下面的公式可以将任意取值范围的特征值转化为０到１区间内的值：\nnewValue = (oldValue - min) / (max - min)\r其中min和max分别是数据集中的最小特征值和最大特征值。虽然改变数值取值范围增加了分类器的复杂度，但为了得到准确结果，我们必须这样做。在lovsimple.py文件中编写名为normalizing的函数，用该函数自动将数据归一化。代码如下：\ndef normalizing(feature): #graphDataSet(feature, result) #对所有的数据进行归一化 #假设 feature=np.array([[1,2],[3,4],[1.3,2.3]]) #每一列上的最小值 [1,2] minVal=np.min(feature,axis=0); #每一列上的最大值 [3,4] maxVal = np.max(feature,axis=0); # 当前数据集 -最小值 [[1,2],[3,4],[1.3,2.3]]-[1,2]是不行的 应该行和列一样 # 第一列应该-1 第二列应该减去2 # 模拟成数据 [[1,2],[3,4],[1.3,2.3]]-[[1,2],[1,2],[1,2]] 这样才行 # 有几列 就有几个 [1,2]的最小值数组 minArr=np.tile(minVal,(feature.shape[0],1)) maxArr=np.tile(maxVal,(feature.shape[0],1)) resultArr=(feature -minArr)/(maxArr-minArr); return resultArr; 添加测试代码：\nfeature, result = dataSet()print(normalizing(feature))\r运行上述代码，得到结果如图示。\n[[0.44832537 0.39805138 0.5623336 ] [0.1587326 0.34195465 0.9872441 ] [0.28542942 0.06892523 0.4744963 ] ...\r从上面运行结果可以看到，我们已经顺利将数据归一化了\n其他比较常用的归一化方法： 均值方差归一化\n这种方式给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。经过处理的数据符合标准正态分布，即均值为0，标准差为1，转化函数为：\n其中 为所有样本数据的均值， 为所有样本数据的标准差。\n5、测试算法：验证分类器 机器学习算法一个很重要的工作就是评估算法的正确率，通常我们只提供已有数据的90%作为训练样本来训练分类器，而使用其余的10%数据去测试分类器，检测分类器的正确率。需要注意的是，10%的测试数据应该是随机选择的，由于海伦提供的数据并没有按照特定目的来排序，所以我们可以随意选择10%数据而不影响其随机性。\n为了测试分类器效果，在lovsimple.py文件中创建函数datingClassTest，编写代码如下：\n\"\"\" 该函数用于返回预测当前data的label值 也就是knn算法 data 用于预测结果的数据 比如 [1000,1.1,0.8] trainData 是训练集 [[40920\t8.326976\t0.953952],[14488\t7.153469\t1.673904]] k表示预测数据最近的k个数据 labelData 表示训练集的对应的label数据 \"\"\" def knn(data,trainData,labelData,k): testData=np.tile(data,(trainData.shape[0],1)) #print(testData) #计算距离差的平方开根 sqdata=np.sqrt(np.sum((testData-trainData)**2,axis=1)); #选取与当前点距离最小的k个点的下标； kindex=np.argsort(sqdata)[:k]; #取出所有的该距离位置最近的结果 resultdata=[labelData[ki] for ki in kindex] #print(sqdata) #print(kindex) #print(resultdata) #分组获取最大的那一个 return (coll.Counter(resultdata).most_common(1)[0][0]) #knn(np.array([1000,1.1,0.8]),np.array([[40920,8.326976,0.953952],[14488,7.153469,1.673904],[35483,12.273169,1.508053]]),[\"不喜欢\",\"喜欢\",\"喜欢\"],2) \"\"\" 将所有的数据按照ratio比例拆分 90%数据用于训练 10%数据用于测试knn算法准确率 ratio 表示拆分的比例 0.1表示训练集=1-0,1 测试集是0.1 k表示knn的k \"\"\" def testData(ratio,k): feature, resultLabel = dataSet() feature=normalizing(feature); #拿到10%的数据用户测试knn算法 #获取总行数 rows=feature.shape[0]; #获取%90的实际个数 必须将float转换成int类型 ratioCount=int(rows*(1-ratio)); trainData=feature[:ratioCount,]; testData=feature[ratioCount:,]; resultI=ratioCount; #统计正确率 okCount=0; erroCount=0; for td in testData: realResult=resultLabel[resultI] calculateResult=knn(td,trainData,resultLabel,k) if realResult==calculateResult: okCount=okCount+1; else: erroCount=erroCount+1; print(\"真实结果:\",realResult,\" 预测结果:\",calculateResult) resultI=resultI+1; print(\"正确率是:\",(okCount/(okCount+erroCount))) ratio=0.1; k=5; testData(ratio,k) 运行上述代码\n正确率是: 0.9504950495049505\r算出正确率是: 0.95，这是一个想当不错的结果。我们可以改变函数testData内变量ratio和分类器k的值，检测错误率是否随着变量值的变化而增加。依赖于分类算法、数据集和程序设置，分类器的输出结果可能有很大的不同。\n6、使用算法：构建完整可用系统 我们可以给海伦一个小段程序，通过该程序海伦会在约会网站上找到某个人并输入他的信息。程序会给出她对男方喜欢程度的预测值。\n在lovsimple.py文件中创建函数classifyPerson，代码如下：\ndef classifyPerson(): precentTats= float(input(\"每年获得的飞行常客里程数:\")) ffMiles = float(input(\"玩视频游戏所耗时间百分比:\")) iceCream = float(input(\"每周消费的冰激淋公升数:\")) feature, resultLabel = dataSet() k = 10; calculateResult = knn([precentTats,ffMiles,iceCream], feature, resultLabel, k) print(\"您可能 \",calculateResult,\"这个人\") classifyPerson() 在cmd中，运行程序，并输入数据(44000,12,0.5)，预测结果是\"你可能有些喜欢这个人\"，也就是这个人魅力一般。一共有三个档次：讨厌、有些喜欢、非常喜欢，对应着不喜欢的人、魅力一般的人、极具魅力的人。\n预测结果\n以上例子换成sklearn实现,代码量大大减少\nimport numpy as np from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split; from sklearn.metrics import accuracy_score; import sklearn.preprocessing as pre; def dataSet(): arr=[]; with open(\"lovedata.txt\",\"r\") as file: for line in file: arr.append(line.strip().split(\"\\t\"));#默认删除空白符(包括'\\n','\\r','\\t',' ') arrnp=np.array(arr); #将特征数据字符串转换成数字 return arrnp[:,:3].astype(dtype=np.float32),arrnp[:,3:].T[0]; #调用sklearn测试k近邻 def testData(ratio,k): feature, result = dataSet() #使用sklearn的预处理进行归一化 使用均值方差归一化 feature=pre.StandardScaler().fit(feature).transform(feature); #train_test_split将数据拆分了 test_size的比例 传入0.1就是10%的测试集 # random_state 随即种子 可能随即抽取10%的测试集 如果random_state是某个固定的数 下次传入 获取的是相同的测试集 # 如果是0或者不填 每次获取的测试集都不是相同的数据 train_X, test_X, train_y, test_y=train_test_split(feature,result,test_size=ratio,random_state = 0) #创建一个k临近 传入距离最近的k个值 nei = KNeighborsClassifier(k) #填充 训练数据 和 训练集结果 nei.fit(train_X, train_y) #预测所有的测试集 得到预测的结果 predict_y=nei.predict(test_X) #比较预测结果和实际结果 得到得分 score=accuracy_score(test_y,predict_y) print(score) ratio=0.1; k=5; testData(ratio,k); 运行获得结果：\n0.9405940594059405\r三、k-近邻算法实战之sklearn手写数字识别 1、实战背景 对于需要识别数字的图片一般都使用图形处理软件，处理成具有相同的色彩和大小：宽高是32像素x32像素。这里将采用本文格式存储图像，但是为了方便理解，我们将图片转换为文本格式，数字的文本格式如图所示。\n与此同时，这些文本格式存储的数字的文件命名也很有特点，格式为：数字的值_该数字的样本序号，如图3.2所示。\n比如0_0.txt 记事本打开 就是一个用0或者 1 拼成的 32*32个字符的0\n比如 0_1.txt 也是0和上面的0写法优点区别 这两个文件的label就是0 有两个样本\n对于这样已经整理好的文本，我们可以直接使用Python处理，进行数字预测。数据集分为训练集和测试集，使用上小结的方法，自己设计k-近邻算法分类器，可以实现分类。数据集和实现代码下载地址：数据集下载 其中trainingDigits是训练数据\ntestDigits是测试数据\n这里不再讲解自己用Python写的k-邻域分类器的方法，因为这不是本小节的重点。接下来，我们将使用强大的第三方Python科学计算库Sklearn构建手写数字系统。\n2、sklearn简介 Scikit learn 也简称sklearn，是机器学习领域当中最知名的python模块之一。sklearn包含了很多机器学习的方式：\nClassification 分类 Regression 回归 Clustering 非监督分类 Dimensionality reduction 数据降维 Model Selection 模型选择 Preprocessing 数据与处理 使用sklearn可以很方便地让我们实现一个机器学习算法。一个复杂度算法的实现，使用sklearn可能只需要调用几行API即可。所以学习sklearn，可以有效减少我们特定任务的实现周期。\n3、sklearn安装 在安装sklearn之前，需要安装两个库，即numpy+mkl和scipy。不要使用pip直接进行安装，因为pip3默安装的是numpy，而不是numpy+mkl。第三方库下载地址：http://www.lfd.uci.edu/~gohlke/pythonlibs/\n找到对应python版本（python –version）的numpy+mkl和scipy，下载安装即可，如图3.3和图3.4所示。\n图3.3 numpy+mkl\n使用pip安装好这两个whl文件后，使用如下指令安装sklearn。\npip install -U scikit-learn\r4、sklearn实现k-近邻算法简介 官网英文文档：点我查看\nsklearn.neighbors模块实现了k-近邻算法，内容如图3.5所示。\n图3.5 sklearn.neighbors\n我们使用sklearn.neighbors.KNeighborsClassifier就可以是实现上小结，我们实现的k-近邻算法。KNeighborsClassifier函数一共有8个参数，如图3.6所示。\n图3.6 KNeighborsClassifier\nKNneighborsClassifier参数说明：\nn_neighbors：默认为5，就是k-NN的k的值，选取最近的k个点。 weights：默认是uniform，参数可以是uniform、distance，也可以是用户自己定义的函数。uniform是均等的权重，就说所有的邻近点的权重都是相等的。distance是不均等的权重，距离近的点比距离远的点的影响大。用户自定义的函数，接收距离的数组，返回一组维数相同的权重。 algorithm：快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索，brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。ball tree是为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。 leaf_size：默认是30，这个是构造的kd树和ball树的大小。这个值的设置会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需要根据问题的性质选择最优的大小。 metric：用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。 p：距离度量公式。在上小结，我们使用欧氏距离公式进行距离度量。除此之外，还有其他的度量方法，例如曼哈顿距离。这个参数默认为2，也就是默认使用欧式距离公式进行距离度量。也可以设置为1，使用曼哈顿距离公式进行距离度量。 metric_params：距离公式的其他关键参数，这个可以不管，使用默认的None即可。 n_jobs：并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。 5、sklearn小试牛刀 我们知道数字图片是32x32的二进制图像，为了方便计算，我们可以将32x32的二进制图像转换为1x1024的向量。对于sklearn的KNeighborsClassifier输入可以是矩阵，不用一定转换为向量，不过为了跟自己写的k-近邻算法分类器对应上，这里也做了向量化处理。然后构建kNN分类器，利用分类器做预测。创建numtest.py文件，编写代码如下：\nimport sklearn.neighbors as skn import numpy as np; import os \"\"\" 获取训练集的数据 \"\"\" def trainDataSet(dir): #获取目录下所有的文件名 files=os.listdir(dir); label=[]; tdata=[]; #有多少个文件 for i in range(len(files)) : fl=files[i]; with open(dir+\"/\"+fl) as file: tdata.append([]); for line in file: line=line.strip() arr=[line[e] for e in range(len(line))]; tdata[i].extend(np.array(arr).astype(np.int8)); label.append(fl.split(\"_\")[0]) return tdata,label def testData(k): #训练数据 dir = \"trainingDigits\"; tdata, label=trainDataSet(dir); ne=skn.KNeighborsClassifier(k) ne.fit(tdata,label); #测试数据 dir1=\"testDigits\"; testdata, testlabel = trainDataSet(dir1); okCount=0; errCount=0; okCount=sum(ne.predict(testdata)==testlabel) print(\"正确个数:\",okCount,\" 错误个数：\",len(testdata)-okCount); print(\"正确率：\",okCount/len(testdata)); testData(5) 运行上述代码，得到结果。\n正确个数: 934 错误个数： 12正确率： 0.9873150105708245\r上述代码使用的algorithm参数是auto，更改algorithm参数为brute，使用暴力搜索，你会发现，运行时间变长了，变为10s+。更改n_neighbors参数，你会发现，不同的值，检测精度也是不同的。自己可以尝试更改这些参数的设置，加深对其函数的理解。\n四、总结 1、kNN算法的优缺点 优点\n简单好用，容易理解，精度高，理论成熟，既可以用来做分类也可以用来做回归； 可用于数值型数据和离散型数据； 训练时间复杂度为O(n)；无数据输入假定； 对异常值不敏感 缺点\n计算复杂性高；空间复杂性高； 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）； 一般数值很大的时候不用这个，计算量太大。但是单个样本又不能太少，否则容易发生误分。 最大的缺点是无法给出数据的内在含义。 2、其他 关于algorithm参数kd_tree的原理，可以查看《统计学方法 李航》书中的讲解； 关于距离度量的方法还有切比雪夫距离、马氏距离、巴氏距离等； 下篇文章将讲解决策树，欢迎各位的捧场！ 如有问题，请留言。如有错误，还望指正，谢谢！ PS： 如果觉得本篇本章对您有所帮助，欢迎关注、评论、赞！\n参考资料：\n本文中提到的电影类别分类、约会网站配对效果判定、手写数字识别实例和数据集，均来自于《机器学习实战》的第二章k-近邻算法。 本文的理论部分，参考自《统计学习方法 李航》的第三章k近邻法以及《机器学习实战》的第二章k-近邻算法。",
    "description": "一、简单k-近邻算法 本文将从k-近邻算法的思想开始讲起，使用python3一步一步编写代码进行实战训练。并且，我也提供了相应的数据集，对代码进行了详细的注释。除此之外，本文也对sklearn实现k-近邻算法的方法进行了讲解。实战实例：电影类别分类、约会网站配对效果判定、手写数字识别。\n文章中大部分文字和例题参考自https://cuijiahua.com/blog/2017/11/ml_1_knn.html 对原文很多代码进行了简化\n感谢这篇文章加速本人入门速度\n1、k-近邻法简介 k近邻法(k-nearest neighbor, k-NN)是1967年由Cover T和Hart P提出的一种基本分类与回归方法。它的工作原理是：存在一个样本数据集合，也称作为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新的数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本最相似数据(最近邻)的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。\n举个简单的例子，我们可以使用k-近邻算法分类一个电影是爱情片还是动作片。\n表1.1 每部电影的打斗镜头数、接吻镜头数以及电影类型\n表1.1 就是我们已有的数据集合，也就是训练样本集。这个数据集有两个特征，即打斗镜头数和接吻镜头数。除此之外，我们也知道每个电影的所属类型，即分类标签。用肉眼粗略地观察，接吻镜头多的，是爱情片。打斗镜头多的，是动作片。以我们多年的看片经验，这个分类还算合理。如果现在给我一部电影，你告诉我这个电影打斗镜头数和接吻镜头数。不告诉我这个电影类型，我可以根据你给我的信息进行判断，这个电影是属于爱情片还是动作片。而k-近邻算法也可以像我们人一样做到这一点，不同的地方在于，我们的经验更\"牛逼\"，而k-近邻算法是靠已有的数据。比如，你告诉我这个电影打斗镜头数为2，接吻镜头数为102，我的经验会告诉你这个是爱情片，k-近邻算法也会告诉你这个是爱情片。你又告诉我另一个电影打斗镜头数为49，接吻镜头数为51，我\"邪恶\"的经验可能会告诉你，这有可能是个\"爱情动作片\"，画面太美，我不敢想象。 (如果说，你不知道\"爱情动作片\"是什么？请评论留言与我联系，我需要你这样像我一样纯洁的朋友。) 但是k-近邻算法不会告诉你这些，因为在它的眼里，电影类型只有爱情片和动作片，它会提取样本集中特征最相似数据(最邻近)的分类标签，得到的结果可能是爱情片，也可能是动作片，但绝不会是\"爱情动作片\"。当然，这些取决于数据集的大小以及最近邻的判断标准等因素。\n2、距离度量 我们已经知道k-近邻算法根据特征比较，然后提取样本集中特征最相似数据(最邻近)的分类标签。那么，如何进行比较呢？比如，我们还是以表1.1为例，怎么判断红色圆点标记的电影所属的类别呢？ 如下图所示。\n我们可以从散点图大致推断，这个红色圆点标记的电影可能属于动作片，因为距离已知的那两个动作片的圆点更近。k-近邻算法用什么方法进行判断呢？没错，就是距离度量。这个电影分类的例子有2个特征，也就是在2维实数向量空间，可以使用我们高中学过的两点距离公式计算距离，如图1.2所示。\n通过计算，我们可以得到如下结果：\n(101,20)-\u003e动作片(108,5)的距离约为16.55 (101,20)-\u003e动作片(115,8)的距离约为18.44 (101,20)-\u003e爱情片(5,89)的距离约为118.22 (101,20)-\u003e爱情片(1,101)的距离约为128.69 通过计算可知，红色圆点标记的电影到动作片 (108,5)的距离最近，为16.55。如果算法直接根据这个结果，判断该红色圆点标记的电影为动作片，这个算法就是最近邻算法，而非k-近邻算法。那么k-近邻算法是什么呢？k-近邻算法步骤如下：\n计算已知类别数据集中的点与当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的k个点； 确定前k个点所在类别的出现频率； 返回前k个点所出现频率最高的类别作为当前点的预测分类。 比如，现在我这个k值取3，那么在电影例子中，按距离依次排序的三个点分别是动作片(108,5)、动作片(115,8)、爱情片(5,89)。在这三个点中，动作片出现的频率为三分之二，爱情片出现的频率为三分之一，所以该红色圆点标记的电影为动作片。这个判别过程就是k-近邻算法。\n其他距离公式：\n曼哈顿距离（ManhattanDistance）：设平面空间内存在两点，它们的坐标为(x1,y1)(x1,y1)，(x2,y2)(x2,y2)\n则 dis=|x1−x2|+|y1−y2|\n比如 每个小正方形距离是1 红，栏，黄色都是12个方格都是曼哈顿距离\n绿色线是欧氏距离(欧几里德距离：在二维和三维空间中的欧氏距离的就是两点之间的直线距离）\n切比雪夫距离（Chebyshev Distance ）：设平面空间内存在两点，它们的坐标为(x1,y1)(x1,y1)，(x2,y2)(x2,y2)\n则dis=max(|x1−x2|,|y1−y2|)",
    "tags": [],
    "title": "机器学习实战教程（一）：K-近邻（KNN）算法",
    "uri": "/docs/programming/ai/machine_learning/algorithms/action_01_knn/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 基础理论",
    "content": "@TOC\n1.线性回归简介 线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w’x+e，e为误差服从均值为0的正态分布。 回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。\n1.1 正态分布 正态分布（Normal distribution），也称“常态分布”，又名高斯分布（Gaussian distribution），最早由A.棣莫弗在求二项分布的渐近公式中得到。C.F.高斯在研究测量误差时从另一个角度导出了它。P.S.拉普拉斯和高斯研究了它的性质。是一个在数学、物理及工程等领域都非常重要的概率分布，在统计学的许多方面有着重大的影响力。 以下两图来自网络 对于正态分布的理解更加简单： 高斯函数是一种常见的概率密度函数，也被称为正态分布函数。具体地说，高斯函数描述了随机变量在某个区间内取值的概率密度，其形式为： $f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$ 其中， μ 是均值，σ 是标准差。这个函数的图像呈钟形，且左右对称，最高点位于均值处，随着距离均值越远，函数值逐渐减小。\n概率密度函数是用来描述随机变量分布情况的函数，而高斯函数是其中的一种形式。当随机变量服从正态分布时，其概率密度函数就是高斯函数。因此，可以将高斯函数看作是概率密度函数的一种特殊形式。\n1.2 Linear Regression线性回归 它是最为人熟知的建模技术之一。线性回归通常是人们在学习预测模型时首选的技术之一。在这种技术中，因变量是连续的，自变量可以是连续的也可以是离散的，回归线的性质是线性的。 线性回归使用最佳的拟合直线（也就是回归线）在因变量（Y）和一个或多个自变量（X）之间建立一种关系。 多元线性回归可表示为Y=a+b1X +b2X2+ e，其中a表示截距，b表示直线的斜率，e是误差项。多元线性回归可以根据给定的预测变量（s）来预测目标变量的值。\n1.2.1 一元线程回归（简单线性回归） 在统计学中，线性回归是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。一个带有一个自变量的线性回归方程代表一条直线。我们需要对线性回归结果进行统计分析 回归线其实可以理解为一条直线，数学表示方式为： Y=b0 + b1X+e 在统计学中，假设有一系列的自变量和因变量的统计数据，可以推算出最佳拟合的b0和b1\nY - 表示因变量； X - 表示独立变量； b0 - 回归线的截距； b1 - 回归线的斜率 参考； e - 误差，预测值和真实值之间的误差； 假设我们将数据 （x1， y1），（x2， y2），（x3， y3）…….（xn， yn）的 n 个点拟合到上面的回归线上。 其中，ei 是第 i 个观测值与我们回归线预测值之间的差值。 比如 函数 y=3x+5 假设统计数据存在 (5,19),(1,9)。 其中b1=3就是斜率， b0=5就是截距， (5,19)就是第1个值 x1=5 y1=19。 如果明确了函数，当x1=5时 预测的值=3*5+5=20 。 ei=20-19=1 误差为1。 在线性回归中， 我们只有统计数据 下面蓝色的点即为统计数据。 我们需要通过这些蓝色的统计数据 计算出一条最佳的拟合线，计算出截距和斜率。 一般计算就是通过将所有数据的误差平方求和求的最小值也就是最佳的拟合方程 我们如何最小化平方误差总和（SSE）呢？ 请记住，b1 和 b0 对我们来说仍然是未知的。 在最小二乘法中，我们通过选择 b1 和 b0 的值来最小化平方误差总和（SSE），如下： 最小二乘法最容易理解解释（https://www.matongxue.com/madocs/818.html） 关于最小二分法需要掌握数学中，导数，极限，偏导概念才能实现推导出该公式 一定要弄明白，请移步梯度下降法解决线性回归问题 ： https://blog.csdn.net/liaomin416100569/article/details/84644283\n2.线性回归实践 这里因为懒于自己封装这些公式，使用sklearn已经实现的api来实现\n2.1 sklearn数据集介绍 sklearn 的数据集有好多种\n自带的小数据集（packaged dataset）：sklearn.datasets.load_ 可在线下载的数据集（Downloaded Dataset）：sklearn.datasets.fetch_ 计算机生成的数据集（Generated Dataset）：sklearn.datasets.make_ svmlight/libsvm格式的数据集:sklearn.datasets.load_svmlight_file(…) 从买了data.org在线下载获取的数据集:sklearn.datasets.fetch_mldata(…) 自带的小数据集（packageddataset）：sklearn.datasets.load_\n鸢尾花数据集：load_iris（）：用于分类任务的数据集 手写数字数据集：load_digits（）:用于分类任务或者降维任务的数据集 乳腺癌数据集load-barest-cancer（）：简单经典的用于二分类任务的数据集 糖尿病数据集：load-diabetes（）：经典的用于回归认为的数据集，值得注意的是，这10个特征中的每个特征都 已经被处理成0均值，方差归一化的特征值。 波士顿房价数据集：load-boston（）：经典的用于回归任务的数据集 体能训练数据集：load-linnerud（）：经典的用于多变量回归任务的数据集。 2.2 简单线性回归 这里使用sklearn中提供的波士顿房价 ，房价和房间数量的关系来演示简单线性回归，明显房间数量越多，面积越大，自然房价越高，成正向线性关系。\n2.2.1 加载数据集 波士顿房价数据集特征介绍 编程加载数据处理\nimport numpy as np; import matplotlib.pyplot as plot; import sklearn.linear_model as lm; import sklearn.datasets as ds; import sklearn.model_selection as ms; \"\"\" 返回一个json数据（结构） data表示房价数据 target feature_names 表示每个列的字段名称 DESCR 是描述信息 filename：存储的数据文件的位置 关于该数据所有字段： CRIM：城镇人均犯罪率。 ZN：住宅用地超过 25000 sq.ft. 的比例。 INDUS：城镇非零售商用土地的比例。 CHAS：查理斯河空变量（如果边界是河流，则为1；否则为0） NOX：一氧化氮浓度。 RM：住宅平均房间数。 AGE：1940 年之前建成的自用房屋比例。 DIS：到波士顿五个中心区域的加权距离。 RAD：辐射性公路的接近指数。 TAX：每 10000 美元的全值财产税率。 PTRATIO：城镇师生比例。 B：1000（Bk-0.63）^ 2，其中 Bk 指代城镇中黑人的比例。 LSTAT：人口中地位低下者的比例。 MEDV：自住房的平均房价，以千美元计。 target是房价 以千美元计 \"\"\" bd=ds.load_boston(); #获取波士顿房价的所有特征数据 data=bd.data; #获取每行特征对应的房价 label=bd.target; #为了演示简单线性回归 获取一个特征 #RM：住宅平均房间数。 nox=data[: ,5:6] #将数据拆分成80%的训练数据 20%的测试数据 xtrain,xtest,ytrain,ytest=ms.train_test_split(nox,label,test_size=0.2,random_state=10) #将 [[4],[2]]这样的特征矩阵转换成 [4,2]这样的向量 绘制散点图 plot.scatter(xtrain[:,-1],ytrain,c=\"red\") plot.show(); 图像效果： 2,2.2使用线程回归计算系数和截距 使用sklearn的LinearRegression类实现机器训练和预测\n#创建线程回归的类 lr=lm.LinearRegression(); lr.fit(xtrain,ytrain); #系数也就是斜率 print(lr.coef_) #截距 print(lr.intercept_) plot.scatter(xtrain[:,-1],ytrain,c=\"red\") #绘制 80%的真实数据 plot.plot(xtrain[:,-1],xtrain[:,-1]*lr.coef_+lr.intercept_); plot.show(); 得到的散点图和线性方程图下： 注意这里离线较远的点对数据的影响较大 可以选择过滤掉这些 y\u003e50以上的数据\nxtrain=xtrain[ytrain\u003c50] ytrain=ytrain[ytrain\u003c50] 得到图像 2.3 多元线性回归 上面的简单线性回归仅仅是对房间数量一个特征做了预测其实房价本身是由多个因素引起的 多元线性回归模型的一般形式为 Yi=β0+β1X1i+β2X2i+…+βkXki+μi i=1,2,…,n 多元线性回归就是求出这个i个系数 β0是截距 编程实现\nimport numpy as np; import matplotlib.pyplot as plot; import sklearn.linear_model as lm; import sklearn.datasets as ds; import sklearn.model_selection as ms; bd=ds.load_boston(); #获取波士顿房价的所有特征数据 data=bd.data; #获取每行特征对应的房价 label=bd.target; #将数据拆分成80%的训练数据 20%的测试数据 xtrain,xtest,ytrain,ytest=ms.train_test_split(data,label,test_size=0.2,random_state=10) xtrain=xtrain[ytrain\u003c50] ytrain=ytrain[ytrain\u003c50] lr=lm.LinearRegression(); lr.fit(xtrain,ytrain); np.set_printoptions(suppress=True) #不使用科学计数法 #系数也就是斜率 print(\"所有的系数:\"); print(lr.coef_) #截距 print(lr.intercept_) 2.3.1 关于系数的可解释性 执行上面的结果得到系数是：\n[ -0.12246473 0.04796854 -0.05495153 0.3323236 -11.35521528\r3.0899128 -0.00873784 -1.19747097 0.24873896 -0.0129233\r-0.72951934 0.0096858 -0.41518496] 对这些系数进行排序\n[ 4 7 10 12 0 2 9 6 11 1 8 3 5] 第4个特征是 [NOX：一氧化氮浓度。] 是 -11 是负向关系也就是说 NOX越大 房价也就会越低 第5个特征是 [ RM：住宅平均房间数。 ] 是3.08 是正向关系也就是说平均房间数越大 房价也就会越高",
    "description": "@TOC\n1.线性回归简介 线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w’x+e，e为误差服从均值为0的正态分布。 回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。\n1.1 正态分布 正态分布（Normal distribution），也称“常态分布”，又名高斯分布（Gaussian distribution），最早由A.棣莫弗在求二项分布的渐近公式中得到。C.F.高斯在研究测量误差时从另一个角度导出了它。P.S.拉普拉斯和高斯研究了它的性质。是一个在数学、物理及工程等领域都非常重要的概率分布，在统计学的许多方面有着重大的影响力。 以下两图来自网络 对于正态分布的理解更加简单： 高斯函数是一种常见的概率密度函数，也被称为正态分布函数。具体地说，高斯函数描述了随机变量在某个区间内取值的概率密度，其形式为： $f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$ 其中， μ 是均值，σ 是标准差。这个函数的图像呈钟形，且左右对称，最高点位于均值处，随着距离均值越远，函数值逐渐减小。\n概率密度函数是用来描述随机变量分布情况的函数，而高斯函数是其中的一种形式。当随机变量服从正态分布时，其概率密度函数就是高斯函数。因此，可以将高斯函数看作是概率密度函数的一种特殊形式。\n1.2 Linear Regression线性回归 它是最为人熟知的建模技术之一。线性回归通常是人们在学习预测模型时首选的技术之一。在这种技术中，因变量是连续的，自变量可以是连续的也可以是离散的，回归线的性质是线性的。 线性回归使用最佳的拟合直线（也就是回归线）在因变量（Y）和一个或多个自变量（X）之间建立一种关系。 多元线性回归可表示为Y=a+b1X +b2X2+ e，其中a表示截距，b表示直线的斜率，e是误差项。多元线性回归可以根据给定的预测变量（s）来预测目标变量的值。\n1.2.1 一元线程回归（简单线性回归） 在统计学中，线性回归是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。一个带有一个自变量的线性回归方程代表一条直线。我们需要对线性回归结果进行统计分析 回归线其实可以理解为一条直线，数学表示方式为： Y=b0 + b1X+e 在统计学中，假设有一系列的自变量和因变量的统计数据，可以推算出最佳拟合的b0和b1",
    "tags": [],
    "title": "机器学习实战教程（二）：线性回归",
    "uri": "/docs/programming/ai/machine_learning/basic/action_02_linear/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 编程语言",
    "content": "",
    "description": "",
    "tags": [],
    "title": "汇编语言",
    "uri": "/docs/programming/languages/assembly/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "深度基础",
    "uri": "/docs/programming/ai/deep_learning/basic/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客",
    "content": "",
    "description": "",
    "tags": [],
    "title": "编程开发",
    "uri": "/docs/programming/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发",
    "content": "",
    "description": "",
    "tags": [],
    "title": "编程语言",
    "uri": "/docs/programming/languages/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers",
    "content": "",
    "description": "",
    "tags": [],
    "title": "transformers实战",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发",
    "content": "",
    "description": "",
    "tags": [],
    "title": "人工智能",
    "uri": "/docs/programming/ai/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 计算机视觉",
    "content": "",
    "description": "",
    "tags": [],
    "title": "工具与框架",
    "uri": "/docs/programming/ai/computer_vision/tools/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 计算机视觉",
    "content": "",
    "description": "",
    "tags": [],
    "title": "应用案例",
    "uri": "/docs/programming/ai/computer_vision/applications/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 基础理论",
    "content": "@TOC\n梯度下降简介 梯度下降的场景假设 梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。 梯度下降原理 梯度下降的基本过程就和下山的场景很类似。 首先，我们有一个可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向(在后面会详细解释) 所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。那么为什么梯度的方向就是最陡峭的方向呢？ 其中部分文字图片来自 教程\n微分（导数|斜率） 看待微分的意义，可以有不同的角度，最常用的两种是： 函数图像中，某点的切线的斜率（导数） 导数（Derivative），也叫导函数值。又名微商，是微积分中的重要基础概念。当函数y=f（x）的自变量x在一点x0上产生一个增量Δx时，函数输出值的增量Δy与自变量增量Δx的比值在Δx趋于0时的极限a如果存在，a即为在x0处的导数，记作f’（x0）或df（x0）/dx。其实这样就是斜率。 比如函数 y=2x+1 假设有两个相邻的点 （x1,y1）,(x2,y2) Δy/Δx=(2x1+1)-(2x2+1)/x1-x2=2(x1-x2)/(x1-x2)=2 所有一元一次函数的斜率其实就是自变量x的系数 斜率你可以说他代表线的倾斜度 值越大倾斜度越大 斜率\u003e0表示是正向相关，\u003c0表示父向相关 几个微分的例子： 指数函数： 其他总结 关于单变量微分的求导 比较简单 一个复合函数的导数必须使用链式法则 所谓的复合函数，是指以一个函数作为另一个函数的自变量。 如f(x)=3x，g(x)=x+3，g(f(x))就是一个复合函数，并且g(f(x))=3x+3 链式法则(chain rule)：\n若h(x)=f(g(x))，则h'(x)=f'(g(x))g'(x)\r链式法则用文字描述，就是“由两个函数凑起来的复合函数，\r其导数等于里边函数代入外边函数的值之导数，乘以里边函数的导数 比如:\nf(x)=x²,g(x)=2x＋1, 则 f(g(x))'\r=((2x+1)²)' ×(2x+1)'\r=2(2x＋1)×2\r=8x＋4 上面的例子都是单变量的微分，当一个函数有多个变量的时候，就有了多变量的微分，即分别对每个变量进行求微分 梯度实际上就是多变量微分的一般化。 我们可以看到，梯度就是分别对每个变量进行微分，然后用逗号分割开，梯度是用\u003c\u003e包括起来，说明梯度其实一个向量。\n梯度相反的方向 为什么算出函数的微分后 往相反的方向走了 用示例来说话 y=5-x 明显导数是 -1 -1表示往左侧增大 python绘图\n#设置出现四个象限 def setXY(): # 获取当前坐标轴对象 ax = plot.gca() # 将垂直坐标刻度置于左边框 ax.yaxis.set_ticks_position('left') # 将水平坐标刻度置于底边框 ax.xaxis.set_ticks_position('bottom') # 将左边框置于数据坐标原点 ax.spines['left'].set_position(('data', 0)) # 将底边框置于数据坐标原点 ax.spines['bottom'].set_position(('data', 0)) # 将右边框和顶边框设置成无色 ax.spines['right'].set_color('none') ax.spines['top'].set_color('none') setXY() #创建 -10到10的10个线性数据 darr=np.linspace(-10,10,10); plot.plot(darr,5-darr); plot.show(); 图像显示 明显微分是-1， 明显梯度下降就需要往右侧走x坐标应该加大 x-(-1)=x+1。 再比如： y=5+x ，明显导数是 1 1表示往右侧增大 。\nsetXY() darr=np.linspace(-10,10,10); plot.plot(darr,5+darr); plot.show(); 图像显示效果： 明显梯度下降就需要往左侧走， x-(1)=x-1。 总结： 梯度下降走的方向往反方向也就是 x-(导数)走。\n梯度下降算法的数学解释 介绍梯度下降的数学公式 此公式的意义是：J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向（前面讲过-梯度），然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1这个点！ 面就这个公式的几个常见的疑问： α是什么含义？ α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！ 梯度下降算法的实例 演示 计算函数 y=(x-2)**2+2的 最小值y所在x的位置。 可以知道的是，任何数的平方都应该大于0 所有 x=2时 y最小=2。 通过t度下降法来预算。 定义梯度函数\n\"\"\" 获取每一个点的梯度 \"\"\" def gradient(x): return 2*(x-2); 定义获取每个x点对应的y值\n\"\"\" 获取每个点的dy值 \"\"\" def dy(x): return (x-2)**2+2 接下来产生一些线性随即数据\nsetXY(); x=np.linspace(-10,10,100); 绘制图形\nplot.plot(x,(x-2)**2+2) plot.show(); 图像效果 接下来随便选择一个点 比如 -7.5开始做梯度下降\n\"\"\" 模拟梯度下降 \"\"\" theta=-7.5 #表示梯度下降开始的点 dyv=0.0; #表示当前最小theta的y eta=0.1 #表示下降步长 arr=[] #记录所有下降的theta的点 方便绘图 while True: gradi=gradient(theta) #获取梯度 dyv=dy(theta); #获取当前点的y值 arr.append(theta); if np.abs(gradi)\u003c1e-8:#如果到了水平梯度就是0 基本上如果梯度到了 1e-8=0.00000001基本可以理解为平缓了 break; theta = theta - eta * gradi;#得到下一个点 print(\"最小y值的x点的坐标：\",theta); print(\"最小的y值：\",dyv); arr=np.array(arr); plot.plot(x,(x-2)**2+2) plot.plot(arr,(arr-2)**2+2,\"or\",marker=\"*\") plot.show(); 最后效果图 红点表示所有下坡的theta点，可以理解坡度越陡 走的越快。 最后输出的结果（和预测结果基本一致）：\n最小y值的x点的坐标： 1.9999999952754293\r最小的y值： 2.0 梯度下降解决线性回归实例 使用正态分布模拟在某个线附近上下波动的数据\nimport numpy as np; import matplotlib.pyplot as plot np.random.seed(100);#设置一个随机种子 让产生的随机数每次运行都想听 x=np.random.rand(100); #产生100个随机的点0-1之间 X=x.reshape(-1,1);#转换成矩阵只有一列 [0.2,0.3]转换成 [[0.2],[0.3]] #print(X) y=x*3+4+np.random.rand(100); #将x值*3+4+一个随机值 plot.plot(x,y,\"o\"); #绘制图形 显示效果: 我们将用梯度下降法来拟合出这条直线！ 首先，我们需要定义一个代价函数，在此我们选用均方误差代价函数 其中 此公示中\nm是数据集中点的个数 ½是一个常量，这样是为了在求梯度的时候，二次方乘下来就和这里的½抵消了，自然就没有多余的常数系数，- 方便后续的计算，同时对结果不会有影响 y 是数据集中每个点的真实y坐标的值 h 是我们的预测函数，根据每一个输入x，根据Θ 计算得到预测的y值，即 我们可以根据代价函数看到，代价函数中的变量有两个，分别为theta0和theta1，x和y是已知量，所以是一个多变量的梯度下降问题，求解出代价函数的梯度，也就是分别对两个变量theta0和theta1进行微分 开始使用python进行实现正太分布模拟数据的梯度下降 获取损失函数的y值： 参考图 \"\"\" 获取损失函数的y值 (y1-(theta0+theta1*x1)**2)+(y2-(theta0+theta1*x2)**2)+....+(ym-(theta0+theta1*xm)**2) \"\"\" def j(x,y,theta): return np.sum((y-(theta[0]+theta[1]*x[:,1]))**2)/len(y); 计算所有theta的梯度 我么知道有theta0和theta1两个梯度 参考图 计算theta0和theta1在每一个点的梯度(注意theta0和theta1是未知数 所有theta0和theta1是自变量 损失函数式因变量) 代码：\n\"\"\" #dj(theta0)=(y1-(theta0+theta1*x1)+(y2-(theta0+theta1*x2)+....+(ym-(theta0+theta1*xm))/m # 为了简单 将theta0*x0 x0=1 即可本来每个x是一个矩阵 比如 [[0.5], [0.5] ] 修改为 [[1,0.5], [1,0.5]] 假设传入的theta是一个向量 [2,1] 点乘就是行和列 1*2+0.5*1=theta0+theta1*x1 #dj(theta0)=np.sum((theta.*x)-yi)/m #dj(theta1)=((y1-(theta0+theta1*x1)*x1+(y2-(theta0+theta1*x2)*x2+....+(ym-(theta0+theta1*xm)*xm))/m #dj(theta1)=np.sum((theta.*x)-yi)*xi/m \"\"\" def dj(x,y,theta): djArr=np.empty((len(theta))) djArr[0]=np.sum(2*(x.dot(theta)-y)); for i in range(1,len(theta)): djArr[i] = np.sum(2 * (x.dot(theta) - y)*x[:,i]); return 2/len(x)*djArr; 初始化一个theta值，模拟梯度下降。\n\"\"\"\r梯度下降获取最佳的值\r\"\"\"\rx_b=np.hstack((np.ones((len(x),1)),X));\rinit_theta=np.array([0.1,0.5]);\reta=0.01;\rwhile True:\rjr=j(x_b,y,init_theta) #获取损失函数的y值\rdjr=dj(x_b,y,init_theta) #获取梯度值\rinit_theta=init_theta-eta*djr;#让theta按梯度下降\rif(np.all(np.abs(djr)\u003c=1e-5)):\rbreak;\r#打印获取到的两个的theta的值\rprint(init_theta);\r#打印图x和通过theta获取的y值\rplot.plot(x,init_theta[0]+init_theta[1]*x)\rplot.show(); 最后输出结果 [4.54437998 2.94672834] 最后拟合线图",
    "description": "@TOC\n梯度下降简介 梯度下降的场景假设 梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。 梯度下降原理 梯度下降的基本过程就和下山的场景很类似。 首先，我们有一个可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向(在后面会详细解释) 所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。那么为什么梯度的方向就是最陡峭的方向呢？ 其中部分文字图片来自 教程\n微分（导数|斜率） 看待微分的意义，可以有不同的角度，最常用的两种是： 函数图像中，某点的切线的斜率（导数） 导数（Derivative），也叫导函数值。又名微商，是微积分中的重要基础概念。当函数y=f（x）的自变量x在一点x0上产生一个增量Δx时，函数输出值的增量Δy与自变量增量Δx的比值在Δx趋于0时的极限a如果存在，a即为在x0处的导数，记作f’（x0）或df（x0）/dx。其实这样就是斜率。 比如函数 y=2x+1 假设有两个相邻的点 （x1,y1）,(x2,y2) Δy/Δx=(2x1+1)-(2x2+1)/x1-x2=2(x1-x2)/(x1-x2)=2 所有一元一次函数的斜率其实就是自变量x的系数 斜率你可以说他代表线的倾斜度 值越大倾斜度越大 斜率\u003e0表示是正向相关，\u003c0表示父向相关 几个微分的例子： 指数函数： 其他总结 关于单变量微分的求导 比较简单 一个复合函数的导数必须使用链式法则 所谓的复合函数，是指以一个函数作为另一个函数的自变量。 如f(x)=3x，g(x)=x+3，g(f(x))就是一个复合函数，并且g(f(x))=3x+3 链式法则(chain rule)：\n若h(x)=f(g(x))，则h'(x)=f'(g(x))g'(x)\r链式法则用文字描述，就是“由两个函数凑起来的复合函数，\r其导数等于里边函数代入外边函数的值之导数，乘以里边函数的导数 比如:",
    "tags": [],
    "title": "机器学习实战教程（三）：梯度下降",
    "uri": "/docs/programming/ai/machine_learning/basic/action_03_gradient/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "核心算法",
    "uri": "/docs/programming/ai/machine_learning/algorithms/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "框架学习",
    "uri": "/docs/programming/ai/deep_learning/frameworks/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能",
    "content": "",
    "description": "",
    "tags": [],
    "title": "深度学习",
    "uri": "/docs/programming/ai/deep_learning/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能",
    "content": "",
    "description": "",
    "tags": [],
    "title": "计算机视觉",
    "uri": "/docs/programming/ai/computer_vision/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 插件开发",
    "content": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "description": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "tags": [],
    "title": "chrome插件",
    "uri": "/docs/programming/plugins/chrome/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "卷积神经网络",
    "uri": "/docs/programming/ai/deep_learning/cnn/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发",
    "content": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "description": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "tags": [],
    "title": "插件开发",
    "uri": "/docs/programming/plugins/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 模型评估",
    "content": "泛化能力 模型泛化是指机器学习模型对新的、未见过的数据的适应能力。在机器学习中，我们通常会将已有的数据集划分为训练集和测试集，使用训练集训练模型，然后使用测试集来评估模型的性能。模型在训练集上表现得好，并不一定能在测试集或实际应用中表现得好。因此，我们需要保证模型具有良好的泛化能力，才能确保其在实际场景中的效果。\n为了提高模型的泛化能力，我们通常需要采取一系列措施，例如增加数据集的大小、特征选择、特征缩放、正则化、交叉验证等。通过这些方法可以减少模型的过拟合，提高对新数据的预测能力。\n总之，模型泛化是机器学习中非常重要的一个概念。它直接关系到模型在实际应用中的效果，并且也是评估机器学习算法和模型的重要指标之一。\n模型评价与选择 差错分析 机器预测时就好像在投飞镖，越接近靶心则预测越准。可以把差错分为两类：偏差（bias）和方差（variance）。可以用下图来形象描绘： 具体到学习任务上，若假设函数取得不够好，拟合结果可能会出现两种问题：\n欠拟合（underfit）：参数过少，假设函数太不自由，过于简单，连样本集都拟合不好，预测时容易偏向一侧，偏差大。 过拟合（overfit）：参数过多，假设函数太自由，不抗干扰，对样本集拟合得很好，但是假设函数过于畸形，预测时忽左忽右，方差大。 欠拟合与过拟合可用下图来形象地说明： 在改变模型的复杂度和训练集大小时，训练集和测试集的误差的函数图（改变模型复杂度时的误差）： 改变数据集时的误差 解决欠拟合比较简单，增加参数或增加特征就行了，麻烦的是过拟合。 解决过拟合的办法有：\n减少该模型的参数，或者改为更简单的模型。 正则化。 增大训练集，减少噪音成分等。 泛化误差 $\\theta$代表超参数，$J_{未知}$${$ $\\theta$$}$代表训练出模型$\\theta$参数后对于未知数据的误差，越小泛化能力越强，$J_{test}$${$ $\\theta$$}$代表模型对测试机的误差，越小泛化能力越强。\n我们希望我们的模型有泛化能力，即面对未训练到的、未知的情景也能发挥作用。泛化误差（generalization error）指的是模型在处理未知数据时的代价函数：$J_{未知}$${$ $\\theta$$}$ 的值，它可以量化模型的泛化能力。 然而，我们训练和测试模型时，并没有未知的数据。我们会根据模型在训练集上的表现改进模型，再进行训练与测试。但在测试集上最终算出的：$J_{test}$${$ $\\theta$$}$已经对测试集进行优化了，它明显对泛化误差的估计过于乐观，会偏低。也就是说，把模型放在实际应用中的效果，会比预想的差很多。 为了解决这个问题，人们提出了交叉验证（cross validation）的方法\n交叉验证 交叉验证的步骤 把训练集进一步分为子训练集与交叉验证集。把测试集藏好，先不用它。（测试集是对未知数据的模拟） 使用各种不同的模型在子训练集上训练，并测出各模型在交叉验证集上的 $J_{cv}$${$ $\\theta$$}$ 选择 $J_{cv}$${$ $\\theta$$}$最小的模型，认为它最佳。把子训练集和交叉验证集合并为训练集，训练出最终的模型。 交叉验证的改进方法是K折（K-fold）交叉验证（图6）：把训练集分为许多小块，每一种情况取其中一小块作为交叉验证集，其余部分合并作为子训练集，求出该模型的 $J_{cv}$${$ $\\theta$$}$，把每一种情况算遍，求出该模型的平均 $J_{cv}$${$ $\\theta$$}$，认为平均最小的模型为最佳模型。最终仍然是用整个训练集训练最佳模型，在测试集上估计泛化误差。\nK折交叉验证的优点是进一步确保交叉验证集没有特殊性，对泛化误差的估计更为准确。\nKFold拆分 在sklearn中，我们可以使用KFold类来实现k折交叉验证。 在进行k折交叉验证时，KFold对象会将原始数据集随机分成k个近似大小的子集，每个子集称为“折”（fold）。，比如10个元素数组，k=5的话会拆分为5个数据集，每个折数据集就是2个，5个折数据集都会被作为一次测试机，所以会有5个组合。\nfrom sklearn.model_selection import KFold\rimport numpy as np\r# 创建一个包含10个元素的数组作为样本数据\rX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\r# 定义K值\rk = 5\r# 创建KFold对象，并指定n_splits参数为K\rkf = KFold(n_splits=k)\r# 遍历KFold对象中的每一组训练集和测试集\rfor train_index, test_index in kf.split(X):\rprint(\"train_index:\", train_index, \"test_index:\", test_index) 输出结果如下：\ntrain_index: [2 3 4 5 6 7 8 9] test_index: [0 1]\rtrain_index: [0 1 4 5 6 7 8 9] test_index: [2 3]\rtrain_index: [0 1 2 3 6 7 8 9] test_index: [4 5]\rtrain_index: [0 1 2 3 4 5 8 9] test_index: [6 7]\rtrain_index: [0 1 2 3 4 5 6 7] test_index: [8 9] fold的值也就决定了最后使用cv数据集验证的得分个数。\ncross_val_score实战 cross_val_score函数是Scikit-learn库中用于评估模型性能的快速方法之一。它计算基于交叉验证的模型评分，并返回每个fold的测试性能得分。与KFold不同，cross_val_score不需要显示拆分数据集。您只需提供模型和数据集即可进行评估，该函数将自动处理交叉验证过程，从而使代码更加简洁和易于理解。\n数据集和模型 load_digits 是 Scikit-learn 库中的一个函数，用于加载手写数字图像数据集。这个数据集包含 8x8 像素大小的 1797 张手写数字图像，每张图像都对应一个 0 到 9 的数字标签。\nfrom sklearn.datasets import load_digits\rdigits = load_digits()\rimport matplotlib.pyplot as plt\rfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(10, 3))\rfor i, ax in enumerate(axes):\rax.imshow(digits.images[i], cmap='gray')\rax.set_title(digits.target[i])\rplt.show() 在Scikit-learn库中的KNeighborsClassifier实现了k近邻算法，其中的超参数k和p影响着模型的性能。\nn_neighbors（即k）：指定要考虑的最近邻居的个数。默认情况下，它为5，表示预测一个新样本时将使用数据集中距离其最近的5个数据点的标签,5个中最多的那个标签就是当前数据的标签。 p：用于计算距离的指标。默认情况下，使用Minkowski距离，p 为2，表示使用欧几里得距离。不同的 p 值对应不同的距离度量方式，例如，p=1 表示曼哈顿距离，p=3 可以使用一种更为复杂的曼哈顿距离度量方式。 使用数据集和测试集获取最佳k，p 将数据集拆分为训练集和测试集，然后k从1到11，p从1到6，测试训练集的得分，得到最佳的k和p。\nimport numpy as np\rfrom sklearn import datasets\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.neighbors import KNeighborsClassifier\rdigits = datasets.load_digits()\rx = digits.data\ry = digits.target\rx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=666)\rbest_score, best_p, best_k = 0, 0, 0 for k in range(2, 11):\rfor p in range(1, 6):\rknn_clf = KNeighborsClassifier(weights=\"distance\", n_neighbors=k, p=p)\rknn_clf.fit(x_train, y_train)\rscore = knn_clf.score(x_test, y_test)\rif score \u003e best_score:\rbest_score, best_p, best_k = score, p, k\rprint(\"Best K=\", best_k)\rprint(\"Best P=\", best_p)\rprint(\"Best score=\", best_score) 输出结果：\nBest K= 3\rBest P= 4\rBest score= 0.9860917941585535 使用交叉验证获取最佳k，p cross_val_score函数默认使用的交叉验证方法是3-Fold交叉验证，即将数据集分为3个相等的部分，其中2个部分用于训练，1个部分用于测试。在每个fold迭代中，使用测试集得到性能度量得分，然后将所有fold的结果平均并返回。\n需要注意的是，cross_val_score还有一个名为cv的参数，可以用来指定交叉验证的折叠数量，即k值。例如，cv=5表示5-Fold交叉验证，将数据集拆分为5个相等的部分，其中4个部分用于训练，1个部分用于测试。对于分类问题和回归问题，通常选择 3, 5 或 10 折交叉验证。通常，交叉验证的折叠数量越多，模型的评估结果越可靠，但计算成本也会增加。\n总之，在没有显式设置cv参数时，默认情况下cross_val_score使用的是3-Fold交叉验证，即默认的k值是3。\nbest_score, best_p, best_k = 0, 0, 0 for k in range(2, 11):\rfor p in range(1, 6):\rknn_clf = KNeighborsClassifier(weights=\"distance\", n_neighbors=k, p=p)\rscores = cross_val_score(knn_clf, x_train, y_train)\rscore = np.mean(scores)\rif score \u003e best_score:\rbest_score, best_p, best_k = score, p, k\rprint(\"Best K=\", best_k)\rprint(\"Best P=\", best_p)\rprint(\"Best score=\", best_score) 输出\nBest K= 2\rBest P= 2\rBest score= 0.9823599874006478 对比第一种情况，我们发现得到的最优超参数是不一样的，虽然score会稍微低一些，但是一般第二种情况更加可信。但是这个score只是说明这组参数最优，并不是指的是模型对于测试集的准确率，因此接下来看一下准确率。\nbest_knn_clf = KNeighborsClassifier(weights='distance', n_neighbors=2, p=2)\rbest_knn_clf.fit(x_train, y_train)\rbest_knn_clf.score(x_test, y_test) 输出结果：0.980528511821975，这才是模型的准确度。\n正则化（regularization） 原理 想要理解什么是正则化，首先我们先来了解上图的方程式。当训练的特征和数据很少时，往往会造成欠拟合的情况，对应的是左边的坐标；而我们想要达到的目的往往是中间的坐标，适当的特征和数据用来训练；但往往现实生活中影响结果的因素是很多的，也就是说会有很多个特征值，所以训练模型的时候往往会造成过拟合的情况，如上图所示。 以图中的公式为例，往往我们得到的模型是：\n$\\theta_{0}+\\theta_{1}x+\\theta_{2}x^2+\\theta_{3}x^3+\\theta_{4}x^4$\n为了能够得到中间坐标的图形，肯定是希望θ3和θ4越小越好，因为这两项越小就越接近于0，就可以得到中间的图形了。 对于损失函数: $$({1\\over2m}[\\sum_{i=1}^{m}{(h_\\theta(x^i)-y^i)^2}])$$ 在线性回归中，就是通过最小二乘法计算损失函数的最小值 $$min({1\\over2m}[\\sum_{i=1}^{m}{(h_\\theta(x^i)-y^i)^2}])$$ 而计算出每个特征的$\\theta$值。 如果损失函数加上一个数求最小值，那个这个数肯定越趋近于0，最小是肯定越小 那么这个值加什么了，我们是希望$\\theta$趋近于0对于损失函数的影响越小越好，也就是减少特征。 把公式通用化得： $${1\\over2m}[\\sum_{i=1}^{m}{(h_\\theta(x^i)-y^i)^2}])+\\lambda\\sum_{j=1}^{n}\\theta_{j}^2$$\n为了损失函数求得最小值，使θ值趋近于0，这就达到了我们的目的。 相当于在原始损失函数中加上了一个惩罚项(λ项) 这就是防止过拟合的一个方法，通常叫做L2正则化，也叫作岭回归。\n我们可以认为加入L2正则项后，估计参数长度变短了，这在数学上被称为特征缩减（shrinkage）。\nshrinkage方法介绍：指训练求解参数过程中考虑到系数的大小，通过设置惩罚系数，使得影响较小的特征的系数衰减到0，只保留重要特征的从而减少模型复杂度进而达到规避过拟合的目的。常用的shinkage的方法有Lasso（L1正则化）和岭回归（L2正则化）等。 Lasso（L1正则化）公式： $${1\\over2m}[\\sum_{i=1}^{m}{(h_\\theta(x^i)-y^i)^2}]+\\lambda\\sum_{j=1}^{n}|\\theta_{j}|$$\n上面的逻辑可能看出是拉格朗日乘子法的应用\n采用shrinkage方法的主要目的包括两个：\n一方面因为模型可能考虑到很多没必要的特征，这些特征对于模型来说就是噪声，shrinkage可以通过消除噪声从而减少模型复杂度； 另一方面模型特征存在多重共线性（变量之间相互关联）的话可能导致模型多解，而多解模型的一个解往往不能反映模型的真实情况，shrinkage可以消除关联的特征提高模型稳定性。 对应图形 我们可以简化L2正则化的方程： $J=J_{0}+\\lambda\\sum_ww^2$ J0表示原始的损失函数，咱们假设正则化项为： 假设是2个特征w有两个值w1和w2 $L=\\lambda(w_{1}^2+w_{2}^2)$ 我们不妨回忆一下圆形的方程： $(x-a)^2+(y-b)^2=r^2$ 其中(a,b)为圆心坐标，r为半径。那么经过坐标原点的单位元可以写成： 正和L2正则化项一样，同时，机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。 此时我们的任务变成在L约束下求出J0取最小值的解(拉格朗日乘子法)。\n求解J0的过程可以画出等值线。同时L2正则化的函数L也可以在w1w2的二维平面上画出来。如下图： L表示为图中的黑色圆形，随着梯度下降法的不断逼近，与圆第一次产生交点，而这个交点很难出现在坐标轴上。\n这就说明了L2正则化不容易得到稀疏矩阵，同时为了求出损失函数的最小值，使得w1和w2无限接近于0，达到防止过拟合的问题。\n岭回归（Ridege Regression） 就是L2正则化 测试用例：\nimport numpy as np\rimport matplotlib.pyplot as plt\rnp.random.seed(42)\rx = np.random.uniform(-3.0, 3.0, size=100)\rX = x.reshape(-1, 1)\ry = 0.5 * x + 3 + np.random.normal(0, 1, size=100)\rplt.scatter(x, y)\rplt.show() 使用20项式来进行拟合（模拟过拟合）\nfrom sklearn.linear_model import LinearRegression\rfrom sklearn.pipeline import Pipeline\rfrom sklearn.preprocessing import PolynomialFeatures\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.metrics import mean_squared_error\rdef PolynomiaRegression(degree):\rreturn Pipeline([\r('poly', PolynomialFeatures(degree=degree)),\r('std_scale', StandardScaler()),\r('lin_reg', LinearRegression()),\r])\rnp.random.seed(666)\rx_train, x_test, y_train, y_test = train_test_split(X, y)\rpoly_reg = PolynomiaRegression(degree=20)\rpoly_reg.fit(x_train, y_train)\ry_poly_predict = poly_reg.predict(x_test)\rprint(mean_squared_error(y_test, y_poly_predict))\r# 167.9401085999025\rimport matplotlib.pyplot as plt\rx_plot = np.linspace(-3, 3, 100).reshape(100, 1)\ry_plot = poly_reg.predict(x_plot)\rplt.scatter(x, y)\rplt.plot(x_plot[:,0], y_plot, color='r')\rplt.axis([-3, 3, 0, 6])\rplt.show() 封装一个函数生成测试集并测试模型\ndef plot_model(model):\rx_plot = np.linspace(-3, 3, 100).reshape(100, 1)\ry_plot = model.predict(x_plot)\rplt.scatter(x, y)\rplt.plot(x_plot[:,0], y_plot, color='r')\rplt.axis([-3, 3, 0, 6])\rplt.show() 使用岭回归：\nfrom sklearn.linear_model import Ridge\rdef RidgeRegression(degree, alpha):\rreturn Pipeline([\r('poly', PolynomialFeatures(degree=degree)),\r('std_scale', StandardScaler()),\r('lin_reg', Ridge(alpha=alpha)),\r])\rridege1_reg = RidgeRegression(20, alpha=0.0001)\rridege1_reg.fit(x_train, y_train)\ry1_predict = ridege1_reg.predict(x_test)\rprint(mean_squared_error(y_test, y1_predict))\r# 跟之前的136.相比小了很多\rplot_model(ridege1_reg) 输出误差：1.3233492754136291 调整 $\\alpha$=1\nridege2_reg = RidgeRegression(20, alpha=1)\rridege2_reg.fit(x_train, y_train)\ry2_predict = ridege2_reg.predict(x_test)\rprint(mean_squared_error(y_test, y2_predict))\rplot_model(ridege2_reg) 输出：1.1888759304218461 调整 $\\alpha$=100\nridege2_reg = RidgeRegression(20, alpha=100)\rridege2_reg.fit(x_train, y_train)\ry2_predict = ridege2_reg.predict(x_test)\rprint(mean_squared_error(y_test, y2_predict))\r# 1.3196456113086197\rplot_model(ridege2_reg) 输出：1.3196456113086197 调整 $\\alpha$=1000000\nridege2_reg = RidgeRegression(20, alpha=1000000)\rridege2_reg.fit(x_train, y_train)\ry2_predict = ridege2_reg.predict(x_test)\rprint(mean_squared_error(y_test, y2_predict))\r# 1.8404103153255003\rplot_model(ridege2_reg) 输出：1.8404103153255003 通过上面几种alpha的取值可以看出我们可以在1-100进行更加细致的搜索，找到最合适的一条相对比较平滑的曲线去拟合。这就是L2正则。\nLASSO Regularization 封装\n#%%\rimport numpy as np\rimport matplotlib.pyplot as plt\rfrom skimage.metrics import mean_squared_error\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.pipeline import Pipeline\rfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\rnp.random.seed(42)\rx = np.random.uniform(-3.0, 3.0, size=100)\rX = x.reshape(-1, 1)\ry = 0.5 * x + 3 + np.random.normal(0, 1, size=100)\rnp.random.seed(666)\rx_train, x_test, y_train, y_test = train_test_split(X, y)\rplt.scatter(x, y)\rplt.show()\r#%%\rfrom sklearn.linear_model import Lasso\rdef plot_model(model):\rx_plot = np.linspace(-3, 3, 100).reshape(100, 1)\ry_plot = model.predict(x_plot)\rplt.scatter(x, y)\rplt.plot(x_plot[:,0], y_plot, color='r')\rplt.axis([-3, 3, 0, 6])\rplt.show()\rdef LassoRegression(degree, alpha):\rreturn Pipeline([\r('poly', PolynomialFeatures(degree=degree)),\r('std_scale', StandardScaler()),\r('lin_reg', Lasso(alpha=alpha)),\r])\rdef TestRegression(degree, alpha):\rlasso1_reg = LassoRegression(degree, alpha) #这里相比Ridge的alpha小了很多，这是因为在Ridge中是平方项\rlasso1_reg.fit(x_train, y_train)\ry1_predict = lasso1_reg.predict(x_test)\rprint(mean_squared_error(y_test, y1_predict))\r# 1.149608084325997\rplot_model(lasso1_reg) 使用lasso回归： 调整 $\\alpha$=0.01\nTestRegression(20,0.01) 输出：1.149608084325997 调整 $\\alpha$=0.1\nTestRegression(20,0.1) 输出：1.1213911351818648 调整 $\\alpha$=1\nTestRegression(20,1) 输出：1.8408939659515595 解释Ridge和LASSO 通过这两幅图进行对比发现，LASSO拟合的模型更倾向于是一条直线，而Ridge拟合的模型更趋向与一条曲线。这是因为两个正则的本质不同，Ridge是趋向于使所有 的加和尽可能的小，而Lasso则是趋向于使得一部分 的值变为0，因此可作为特征选择用，这也是为什么叫Selection Operation的原因。",
    "description": "泛化能力 模型泛化是指机器学习模型对新的、未见过的数据的适应能力。在机器学习中，我们通常会将已有的数据集划分为训练集和测试集，使用训练集训练模型，然后使用测试集来评估模型的性能。模型在训练集上表现得好，并不一定能在测试集或实际应用中表现得好。因此，我们需要保证模型具有良好的泛化能力，才能确保其在实际场景中的效果。\n为了提高模型的泛化能力，我们通常需要采取一系列措施，例如增加数据集的大小、特征选择、特征缩放、正则化、交叉验证等。通过这些方法可以减少模型的过拟合，提高对新数据的预测能力。\n总之，模型泛化是机器学习中非常重要的一个概念。它直接关系到模型在实际应用中的效果，并且也是评估机器学习算法和模型的重要指标之一。\n模型评价与选择 差错分析 机器预测时就好像在投飞镖，越接近靶心则预测越准。可以把差错分为两类：偏差（bias）和方差（variance）。可以用下图来形象描绘： 具体到学习任务上，若假设函数取得不够好，拟合结果可能会出现两种问题：\n欠拟合（underfit）：参数过少，假设函数太不自由，过于简单，连样本集都拟合不好，预测时容易偏向一侧，偏差大。 过拟合（overfit）：参数过多，假设函数太自由，不抗干扰，对样本集拟合得很好，但是假设函数过于畸形，预测时忽左忽右，方差大。 欠拟合与过拟合可用下图来形象地说明： 在改变模型的复杂度和训练集大小时，训练集和测试集的误差的函数图（改变模型复杂度时的误差）： 改变数据集时的误差 解决欠拟合比较简单，增加参数或增加特征就行了，麻烦的是过拟合。 解决过拟合的办法有：\n减少该模型的参数，或者改为更简单的模型。 正则化。 增大训练集，减少噪音成分等。 泛化误差 $\\theta$代表超参数，$J_{未知}$${$ $\\theta$$}$代表训练出模型$\\theta$参数后对于未知数据的误差，越小泛化能力越强，$J_{test}$${$ $\\theta$$}$代表模型对测试机的误差，越小泛化能力越强。\n我们希望我们的模型有泛化能力，即面对未训练到的、未知的情景也能发挥作用。泛化误差（generalization error）指的是模型在处理未知数据时的代价函数：$J_{未知}$${$ $\\theta$$}$ 的值，它可以量化模型的泛化能力。 然而，我们训练和测试模型时，并没有未知的数据。我们会根据模型在训练集上的表现改进模型，再进行训练与测试。但在测试集上最终算出的：$J_{test}$${$ $\\theta$$}$已经对测试集进行优化了，它明显对泛化误差的估计过于乐观，会偏低。也就是说，把模型放在实际应用中的效果，会比预想的差很多。 为了解决这个问题，人们提出了交叉验证（cross validation）的方法\n交叉验证 交叉验证的步骤 把训练集进一步分为子训练集与交叉验证集。把测试集藏好，先不用它。（测试集是对未知数据的模拟） 使用各种不同的模型在子训练集上训练，并测出各模型在交叉验证集上的 $J_{cv}$${$ $\\theta$$}$ 选择 $J_{cv}$${$ $\\theta$$}$最小的模型，认为它最佳。把子训练集和交叉验证集合并为训练集，训练出最终的模型。 交叉验证的改进方法是K折（K-fold）交叉验证（图6）：把训练集分为许多小块，每一种情况取其中一小块作为交叉验证集，其余部分合并作为子训练集，求出该模型的 $J_{cv}$${$ $\\theta$$}$，把每一种情况算遍，求出该模型的平均 $J_{cv}$${$ $\\theta$$}$，认为平均最小的模型为最佳模型。最终仍然是用整个训练集训练最佳模型，在测试集上估计泛化误差。",
    "tags": [],
    "title": "机器学习实战教程（九）：模型泛化",
    "uri": "/docs/programming/ai/machine_learning/evaluation/action_09_generalize/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "模型评估",
    "uri": "/docs/programming/ai/machine_learning/evaluation/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 基础理论",
    "content": "@[toc]\n概述 PyTorch是一个基于Python的开源机器学习框架，由Facebook的人工智能研究团队开发并维护。它提供了丰富的工具和接口，用于构建和训练深度神经网络模型。\nPyTorch的主要特点和优势包括：\n动态图：PyTorch使用动态图机制，即在运行时构建计算图。这使得模型的构建和调试更加直观和灵活，能够更好地处理复杂的计算流程和动态控制流。\n简洁明了：PyTorch的API设计简洁明了，易于学习和使用。它提供了一系列高级接口，使得模型的构建、训练和评估变得更加简单和高效。\n强大的GPU加速支持：PyTorch能够利用GPU进行张量运算和模型训练，从而加快计算速度。它提供了简单易用的接口，使得在GPU上进行加速变得更加方便。\n灵活扩展：PyTorch支持自定义操作符和扩展，使得用户可以方便地实现和使用自己的模型组件和功能。\n相比之下，TensorFlow是由Google开发的另一个流行的深度学习框架。与PyTorch相比，TensorFlow的主要特点和优势包括：\n静态图：TensorFlow使用静态图机制，即在编译时构建计算图。这使得TensorFlow在模型运行时能够进行更多的优化和性能提升，适用于大规模的、计算密集型的任务。\n跨平台支持：TensorFlow可以在多种硬件和操作系统上运行，并且具有广泛的部署支持。它提供了TensorFlow Serving、TensorFlow Lite和TensorFlow.js等工具，使得模型的部署和移植更加方便。\n分布式训练支持：TensorFlow提供了分布式训练的功能，可以在多个设备和计算节点上进行模型训练，从而加快训练速度。\n生态系统和社区：TensorFlow具有庞大的生态系统和活跃的社区，提供了丰富的资源和支持，包括模型库、教程和论坛等。\n总的来说，PyTorch和TensorFlow都是优秀的深度学习框架，各有其特点和适用场景。PyTorch适合于快速原型开发、动态计算流程和小规模任务，而TensorFlow适合于大规模、计算密集型的任务和分布式训练。选择哪个框架取决于具体的需求和个人偏好。\n对于初学接触神经网络，建议先学pytorch，它提供的api接近理论概念，有动态图，方便调试，适合做研究使用，，由于最近chargpt的大火，Hugging Face的transforms是使用PyTorch的。Hugging Face是一个提供自然语言处理（NLP）模型和工具的平台，他们的Transformers库主要基于PyTorch实现，他的入门pytorch必须要有基础。这个库提供了一系列用于数据预处理和后处理的函数，可以方便地对文本数据进行转换和处理。\n环境准备 安装cuda和cudnn 一般pc电脑或者服务器都有nvida显卡，可以通过nvidia-smi命令查看。 其中python环境（3.8+版本），cuda和cudnn安装请参考：https://blog.csdn.net/liaomin416100569/article/details/130532993 安装后可以看到我的cuda version是11.2\n安装pytorch 考虑到版本向下兼容，不一定非要下载cuda=11.2对应的那个版本的torch，或许低于这个版本就可以。所以我就选择下载cuda11.1的版本。 以下是pytorch对应的稳定版的网址下载链接，可以根据需要找到对应的torch版本下载。cu版本就是gpu版本，不带cu的是cpu版本，https://download.pytorch.org/whl/torch_stable.html，搜索cu111 直接选择\npip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html 编写测试代码\nimport torch\rprint(torch.__version__)\r#cuda是否可用，如果返回True，表示正常可用gpu\rprint(torch.cuda.is_available())\rprint(torch.cuda.device_count())\rx1=torch.rand(5,3)\r#把x1转换gpu0的tensor\rx1=x1.cuda(0)\rprint(x1) 测试运行\n1.9.1+cu111\rTrue 1\rtensor([[0.5761, 0.7046, 0.2004],\r[0.6030, 0.3285, 0.5602],\r[0.6852, 0.6602, 0.0033],\r[0.4213, 0.7174, 0.0591],\r[0.5276, 0.4181, 0.8665]], device='cuda:0') 使用colab 如果自己沒有gpu的環境，可以使用cpu進行學習，但是學到模型训练还是要gpu，如果有外网环境，可以考虑使用google提供的colab，主要是免费，gpu能给到16GB，系统磁盘100gb，googledrive15gb,非常良心了，注意：colab不支持和本地pycharm远程调试，模型比较大时磁盘是个大问题，根据Colab的官方规定，每个用户每天可以使用Colab资源的总时间为12小时。这意味着，一旦你的Colab会话运行了12小时，你将无法继续使用Colab的计算资源。当然，你可以重新启动一个新的Colab会话来继续使用。\n申请colab 首先你先需要申请一个googledrive（类似百度网盘），准备一个gmail邮箱就可以申请，申请完成后默认有15GB的存储空间，如果需要更多就需要购买了。 在我的云端云盘右侧空白的地方点击邮件，关联更多应用，搜索colab安装，安装后右键就会多了 一个google colaboratory，点击进去，会弹出一个notebook的开发环境。 可以点击左侧的文件夹，自动分配一个计算资源 查看分配的机器 点击目录..可以进入到系统根目录,点击工具栏第三个图标挂载googledrive 在content notepadbook中执行命令查看资源信息，和右侧的资源坐下对比，可以确认系统是ubuntu，内存是12.7gb，磁盘107gb，没有gpu 你的notepad文件内容，默认是新建在googledrive的根目录下，你可以双击文件直接进入notebook 挂载googledrive 挂载完成后/content多了一个drive目录，MyDrive内容就和googledrive是一致的。 colab申请gpu 点击右上角的view resouce 选择change runtime type,选择免费的GPU或者TPU 再次使用nvdia-smi确认 google的另一款免费的实验免费gpu是kaggle，也可以注册使用，比colab更简单方便，同时可直接引用其他开源模型。\nnotebook语法 python语法 可直接在快中执行python语法。 使用！执行shell命令。 比如使用 !bash进入一个交互shell行，exit退出 可以使用 ！任意shell命令执行， 基础 张量 在PyTorch中，除了张量（Tensor）之外，还有很多其他的数据类型和类。以下是一些常见的PyTorch数据类型和类：\nTensor（张量）：张量是PyTorch的核心数据结构，类似于数组，可以存储和操作多维数据。\nVariable（变量）：Variable是对张量的封装，用于自动求导。\nnn.Module：nn.Module是PyTorch中用于构建神经网络模型的基类，可以包含多个层和操作。\nnn.Parameter：nn.Parameter是Variable的子类，用于定义模型中需要进行学习的参数。\nDataLoader：DataLoader是一个用于加载数据的实用类，可以方便地对数据进行批量处理和迭代。\nOptimizer（优化器）：优化器是用于更新模型参数的算法，例如SGD、Adam等。\nLoss Function（损失函数）：损失函数用于衡量模型预测结果与真实标签之间的差异，例如交叉熵损失、均方误差等。\n这些是PyTorch中常用的一些数据类型和类，它们提供了丰富的功能来支持深度学习任务的实现和训练。\n定义 import torch as t\rimport numpy as np\r#构建5*3数组,只是分配了空间未初始化\rresult=t.Tensor(5,3)\rprint(result)\r#这里产生个0-1之间的tensor张量，并且初始化\rx1=t.rand(5,3)\ry1=t.rand(5,3)\rprint(x1)\rprint(x1.size())\rresult=x1+y1\rprint(result) numpy转换 #产生5个1的一维数组tensor转换成numpy\rprint(t.ones(5).numpy())\r#将numpy数组转换为tensor\rprint(t.from_numpy(np.array([2,2,2,]))) 数学函数 随机数 下面是一些常见的PyTorch函数，可以用于生成随机数：\ntorch.randn(size, dtype=None, device=None) - 从标准正态分布中返回随机样本。返回一个具有给定大小的张量，其中每个元素独立地从标准正态分布中抽取。可以通过指定size参数来指定张量的形状。 例子：\nx = torch.randn(3, 3)\rprint(x) torch.rand(size, dtype=None, device=None) - 从均匀分布中返回随机样本。返回一个具有给定大小的张量，其中每个元素独立地从一个均匀分布上抽取。可以通过指定size参数来指定张量的形状。 例子：\nx = torch.rand(3, 3)\rprint(x)` torch.randint(low, high, size, dtype=None, device=None) - 从离散均匀分布中返回随机整数。返回一个具有给定大小的张量，其中每个元素独立地从一个均匀分布上抽取。可以通过指定low和high参数来指定取值范围。 例子：\nx = torch.randint(0, 10, (3, 3))\rprint(x) torch.normal(mean, std, size, dtype=None, device=None) - 从正态分布中返回随机样本。返回一个具有给定大小的张量，其中每个元素独立地从一个正态分布中抽取。可以通过指定mean和std参数来指定正态分布的均值和标准差。 例子：\nx = torch.normal(0, 1, (3, 3))\rprint(x) 这些函数可以帮助您在PyTorch中生成随机数。请根据您的需求选择适当的函数。\n计算函数 常用的数学计算函数 当然，下面是PyTorch中一些常用的数学函数的清单，每个都附有简短的描述和一个调用小例子：\ntorch.abs(input): 返回输入张量的绝对值。示例：torch.abs(torch.tensor([-1, 2, -3]))。 torch.sqrt(input): 返回输入张量的平方根。示例：torch.sqrt(torch.tensor([4, 9, 16]))。 torch.exp(input): 计算输入张量的指数函数。示例：torch.exp(torch.tensor([1, 2, 3]))。 torch.log(input): 计算输入张量的自然对数。示例：torch.log(torch.tensor([1, 10, 100]))。 torch.sin(input): 计算输入张量的正弦值。示例：torch.sin(torch.tensor([0, math.pi/2, math.pi]))。 torch.cos(input): 计算输入张量的余弦值。示例：torch.cos(torch.tensor([0, math.pi/2, math.pi]))。 torch.tan(input): 计算输入张量的正切值。示例：torch.tan(torch.tensor([0, math.pi/4, math.pi/2]))。 torch.sigmoid(input): 计算输入张量的Sigmoid函数。示例：torch.sigmoid(torch.tensor([0, 1, 2]))。 torch.relu(input): 应用ReLU激活函数，即max(0, input)。示例：torch.relu(torch.tensor([-1, 0, 1]))。 torch.softmax(input, dim): 计算输入张量在指定维度上的Softmax函数。示例：torch.softmax(torch.tensor([[1, 2], [3, 4]]), dim=1)。 torch.mean(input): 计算输入张量的均值。示例：torch.mean(torch.tensor([1, 2, 3]))。 torch.sum(input): 计算输入张量的总和。示例：torch.sum(torch.tensor([1, 2, 3]))。 torch.max(input): 返回输入张量中的最大值。示例：torch.max(torch.tensor([1, 2, 3]))。 torch.min(input): 返回输入张量中的最小值。示例：torch.min(torch.tensor([1, 2, 3]))。 torch.argmax(input): 返回输入张量中最大值的索引。示例：torch.argmax(torch.tensor([1, 2, 3]))。 torch.argmin(input): 返回输入张量中最小值的索引。示例：torch.argmin(torch.tensor([1, 2, 3]))。 torch.sort(input): 对输入张量进行排序。示例：torch.sort(torch.tensor([3, 1, 2]))。 torch.clamp(input, min, max): 将输入张量的值限制在指定范围内。示例：torch.clamp(torch.tensor([1, 2, 3]), min=2, max=3)。 torch.round(input): 对输入张量进行四舍五入。示例：torch.round(torch.tensor([1.1, 2.4, 3.6]))。 torch.floor(input): 向下取整，返回不大于输入张量的最大整数。示例：torch.floor(torch.tensor([1.1, 2.4, 3.6]))。 矩阵处理函数 以下是PyTorch中常用的20个矩阵处理函数的清单及其描述：\ntorch.mm(): 计算两个矩阵的乘积。 示例：torch.mm(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5], [6]]))返回tensor([[17], [39]])\ntorch.matmul(): 计算两个张量的矩阵乘积。 示例：torch.matmul(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5], [6]]))返回tensor([[17], [39]])\ntorch.transpose(): 返回输入张量的转置。 示例：torch.transpose(torch.tensor([[1, 2], [3, 4]]), 0, 1)返回tensor([[1, 3], [2, 4]])\ntorch.mm(): 计算一个矩阵和一个向量的乘积。 示例：torch.mm(torch.tensor([[1, 2], [3, 4]]), torch.tensor([5, 6]))返回tensor([17, 39])\ntorch.trace(): 返回矩阵的迹。 示例：torch.trace(torch.tensor([[1, 2], [3, 4]]))返回tensor(5)\ntorch.det(): 计算矩阵的行列式。 示例：torch.det(torch.tensor([[1, 2], [3, 4]]))返回tensor(-2)\ntorch.svd(): 对矩阵进行奇异值分解。 示例：torch.svd(torch.tensor([[1, 2], [3, 4]]))返回(tensor([[-0.4046, -0.9145], [-0.9145, 0.4046]]), tensor([5.4645, 0.3650]), tensor([[-0.5760, -0.8174], [-0.8174, 0.5760]]))\ntorch.eig(): 计算矩阵的特征值和特征向量。 示例：torch.eig(torch.tensor([[1, 2], [3, 4]]))返回(tensor([[0.3723, 0.0000], [5.6277, 0.0000]]), tensor([]))\ntorch.inverse(): 计算矩阵的逆。 示例：torch.inverse(torch.tensor([[1, 2], [3, 4]]))返回tensor([[-2.0000, 1.0000], [ 1.5000, -0.5000]])\ntorch.diag(): 返回矩阵的对角线元素。 示例：torch.diag(torch.tensor([[1, 2], [3, 4]]))返回tensor([1, 4])\ntorch.diag_embed(): 将一维张量转化为对角矩阵。 示例：torch.diag_embed(torch.tensor([1, 2, 3]))返回tensor([[[1, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 2, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 3]]])\ntorch.einsum(): 执行爱因斯坦求和约定。 示例：torch.einsum(‘ij,jk-\u003eik’, torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))返回tensor([[19, 22], [43, 50]])\ntorch.flatten(): 对输入张量进行扁平化操作。 示例：torch.flatten(torch.tensor([[1, 2], [3, 4]]))返回tensor([1, 2, 3, 4])\ntorch.cat(): 沿指定维度拼接张量。 示例：torch.cat((torch.tensor([[1, 2]]), torch.tensor([[3, 4]])), dim=0)返回tensor([[1, 2], [3, 4]])\ntorch.stack(): 沿新维度拼接张量。 示例：torch.stack((torch.tensor([1, 2]), torch.tensor([3, 4])), dim=0)返回tensor([[1, 2], [3, 4]])\ntorch.split(): 沿指定维度分割张量。 示例：torch.split(torch.tensor([[1, 2, 3, 4]]), 2, dim=1)返回(tensor([[1, 2]]), tensor([[3, 4]]))\ntorch.chunk(): 将张量分割成指定数量的块。 示例：torch.chunk(torch.tensor([[1, 2, 3, 4]]), 2, dim=1)返回(tensor([[1, 2]]), tensor([[3, 4]]))\ntorch.reshape(): 改变张量的形状。 示例：torch.reshape(torch.tensor([[1, 2, 3, 4]]), (2, 2))返回tensor([[1, 2], [3, 4]])\ntorch.squeeze(): 压缩张量中尺寸为1的维度。 示例：torch.squeeze(torch.tensor([[[1], [2]]]))返回tensor([1, 2])\ntorch.unsqueeze(): 在指定位置插入尺寸为1的新维度。 示例：torch.unsqueeze(torch.tensor([1, 2]), dim=1)返回tensor([[1], [2]]\ntorch.view是PyTorch中的一个函数，用于改变张量的形状，即对张量进行重塑操作。它的作用类似于NumPy中的reshape函数。 x = torch.tensor([1, 2, 3, 4, 5, 6]) y = x.view(2, 3)\ntorch.permute函数是PyTorch中的一个函数，用于重新排列张量的维度顺序。它的作用是交换或重新组织张量的维度。 在下述示例中，原始张量x的维度顺序为(2, 3, 4)，通过使用permute(2, 0, 1)，将维度顺序重新排列为(4, 2, 3)，得到了新的张量 也就是维度2换成函数索引0个维度,0维度的换成1,1维度的换成2\nimport torch\rx = torch.randn(2, 3, 4) # 创建一个形状为(2, 3, 4)的张量\rx_permuted = x.permute(2, 0, 1) # 将维度顺序重新排列为(4, 2, 3)\rprint(x_permuted.shape) # 输出: torch.Size([4, 2, 3]) 自动梯度 深度学习的算法本质上是通过反向传播求导数，PyTorch的Autograd模块实现了此功能。在Tensor上的所有操作，Autograd都能为它们自动提供微分，避免手动计算导数的复杂过程。\n在PyTorch中，Tensor和Variable都可以求梯度，但是它们有一些区别。\n在旧版本的PyTorch中，Variable是一个Tensor的封装，它包含了Tensor的数据以及关于这个Tensor的梯度信息。在新版本的PyTorch中，Variable已经被弃用，官方建议直接使用Tensor。\nPyTorch中的Tensor对象有一个属性.requires_grad，默认为False。当你将其设置为True时，表示希望计算这个Tensor的梯度。在进行反向传播计算梯度时，所有具有.requires_grad=True的Tensor都会被保留梯度信息。\n当你使用Tensor进行计算时，可以调用.backward()方法来计算相对于这个Tensor的梯度。梯度信息会保存在.grad属性中。\n所以，Variable的作用可以用Tensor的.requires_grad属性来代替，而且在新版本的PyTorch中，官方建议直接使用Tensor进行梯度计算。 Variable和Tensor主要包含三个属性。\ndata：保存计算后结果对应的的Tensor。 grad：保存data对应的梯度，是Tensor，它和data的形状一样。 grad fn：指向一个Function对象，这个Function用来反向传播计算输入的梯度,requires_grad=True x=Variable(t.from_numpy(np.array([[1,2],[2,4]],dtype=float)),requires_grad=True)\rprint(\"张量x=\",x)\ry=x.sum()\rprint(\"输出y\",y)\rprint(\"输出y的梯度\",y.grad) #注意结果是y，所以y是没有梯度的，y进行反向传播，可以求导x的导数\rprint(\"y的反向梯度函数\",y.grad_fn)\rprint(\"y的数据\",y.data)\r# 因为y=x[0][0]+x[0][1]+x[1][0]++x[1][1],可以认为四个数是四个变量，比如求每个变量的导数\r# 假设是y=x1+x2+x3+x4 x1是自变量，x1的导数就是1，同理x2的导数也是1，最后就得到了4个1\r# 注意每个点都有个梯度\ry.backward() #反向传播计算梯度\rprint(x.grad) 输出\n张量x= tensor([[1., 2.],\r[2., 4.]], dtype=torch.float64, requires_grad=True)\r输出y tensor(9., dtype=torch.float64, grad_fn=\u003cSumBackward0\u003e)\r输出y的梯度 None\ry的反向梯度函数 \u003cSumBackward0 object at 0x0000025F8F2C8C10\u003e\ry的数据 tensor(9., dtype=torch.float64)\rx的梯度 tensor([[1., 1.],\r[1., 1.]], dtype=torch.float64) 案例 案例1:计算$x^2*e^x$导数\n#计算x**2*e^x导数\r#dx=2*x*e^x+x**2*e^x\r#定义fx的函数逻辑\rdef f(x):\rreturn x**2*t.exp(x)\r#我们预先知道他的梯度函数是\rdef graddx(x):\rreturn 2*x*t.exp(x)+x**2*t.exp(x)\r#生成一个3*3随机矩阵，求梯度\rx=Variable(t.rand(3,3),requires_grad=True)\rprint(graddx(x))\r#使用反向传播求梯度\ry=f(x)\ry.backward(t.ones(y.size()))\rprint(x.grad) 案例2: 使用自动梯度梯度下降拟合最佳直线\n#使用autograd计算梯度，来实现线性回归\rimport torch as t\rfrom torch.autograd import Variable as V\rimport matplotlib.pyplot as plot\rt.manual_seed(42)\r# 使用自动梯度实现线性回归\rx=t.randn(100,1)\ry=3*x+2+t.randn(100,1) #实际值上加上一些随机噪点\rplot.plot(x,y,'.')\rplot.show()\rw=V(t.randn(1,1),requires_grad=True)\rb=V(t.randn(1),requires_grad=True)\rdef fx(x):\rreturn t.mm(x,w)+b\r#损失函数 def lossf(y_pre,y):\rreturn t.mean((y_pre-y)**2)\r#训练100次，100次梯度下降，计算到最小损失时的w和b\rw_gra_last,b_gra_last=0,0\rfor epoch in range(100):\ry_pre=fx(x)\rloss=lossf(y_pre,y)\rloss.backward()\rw_gra=w.grad.data\rb_gra=b.grad.data\rw_gra_last=w_gra.clone()\rb_gra_last=b_gra.clone()\r#如果梯度小于某个值直接退出\rif t.abs(w_gra)\u003c=1e-8 and t.abs(b_gra)\u003c=1e-8:\rbreak;\rlearn_rate=0.01\r#注意w.sub_是不行的因为w是requires_grad=True，需要后面的参数都是设置为：requires_grad=True\r#所以只能是更新他的data\rw.data.sub_(w_gra*learn_rate)\rb.data.sub_(b_gra*learn_rate)\r#注意梯度清零，否则会累加\rw.grad.data.zero_()\rb.grad.data.zero_()\r# w_gra_last是张量，item输出标量\rprint(epoch,w_gra_last.item(),b_gra_last.item()) y_pre=fx(x) plot.plot(x,y,'.')\rplot.plot(x.data.numpy(),y_pre.data.numpy())\rplot.show() 计算图 PyTorch的计算图是一种用于描述计算操作的有向无环图(Directed Acyclic Graph, DAG)。在PyTorch中，计算图是动态的，它会随着代码的执行而构建。\n计算图的主要作用是记录和管理计算操作的流程，以便进行自动微分和梯度优化。通过构建计算图，PyTorch能够追踪和记录所有的计算操作，从而实现自动求导。这使得在深度学习中，我们可以方便地进行反向传播和优化模型的参数。\n使用计算图的好处有：\n自动求导：PyTorch可以根据计算图自动生成反向传播所需的梯度计算代码，简化了手动求导的过程。 动态图灵活性：计算图是动态构建的，可以根据需要进行动态修改和调整，使得模型的结构和计算过程更加灵活和可变。 可视化和调试：计算图可以可视化，帮助我们理解和调试模型的运行过程，更好地理解和解释模型的行为。 总之，PyTorch的计算图是一种强大的工具，它为我们提供了灵活、高效的自动求导功能，使得深度学习模型的训练和优化更加方便和快捷。\n#打印计算图\rimport torch\rfrom torchviz import make_dot\r# 定义一个简单的计算图\rw= torch.randn(1, requires_grad=True)\rb= torch.randn(1, requires_grad=True)\rx = torch.randn(1, requires_grad=True)\ry = w*x + b\r# 使用make_dot函数绘制计算图,图上的数字只是代表数据的维度\rdot = make_dot(y, params={'x': x, 'w': w, 'b': b}, show_attrs=True, show_saved=True)\rdot.render(filename='compute_graph', format='png') 当前运行的目录出现一个compute_graph.png 剖析下反向求导的过程 如表达式z=wx+b可分解为y=wx和z=y+b，其计算图如图3-5所示，图中的MUL和ADD都是算子，w、x、b为变量。 如上有向无环图中，X 和b是叶子节点（leaf node），这些节点通常由用户自己创建，不依赖于其他变量。z称为根节点，是计算图的最终目标。利用链式法则很容易求得各个叶子节点的梯度。 而有了计算图，上述链式求导即可利用计算图的反向传播自动完成，其过程如图所示。 torchvision模块 torchvision是PyTorch的一个扩展库，提供了许多用于计算机视觉任务的实用函数和预训练模型。它包含了常用的数据集、数据转换、模型架构和图像处理方法等功能。\ntorchvision的主要特点包括：\n数据集：torchvision提供了许多常用的计算机视觉数据集，例如MNIST、CIFAR-10、ImageNet等。这些数据集可以方便地用于训练和测试模型。\n数据转换：torchvision提供了一系列用于数据预处理和增强的转换函数，例如对图像进行裁剪、缩放、翻转、归一化等操作。这些转换函数可以灵活地应用于数据集中的样本，以满足模型训练的需求。\n预训练模型：torchvision中集成了一些经典的计算机视觉模型，例如AlexNet、VGG、ResNet等。这些预训练模型可以直接用于特定任务的迁移学习，也可以作为基准模型进行性能比较。\n图像处理：torchvision还提供了一些常用的图像处理方法，例如图像滤波、边缘检测、颜色转换等。这些方法可以用于图像处理和增强的任务。\n总之，torchvision为PyTorch提供了丰富的计算机视觉功能和工具，可以极大地简化计算机视觉任务的开发和实现过程。\nTransforms torchvision提供了一些用于数据增强的常用transforms，如随机裁剪、翻转、旋转、归一化等。这些transforms可以在数据加载时应用于图像，以提高模型的泛化能力和鲁棒性。 以下是torch中所有的transforms：\nCompose: 将多个transforms组合在一起。 ToTensor: 将PIL图像或NumPy数组转换为张量。 ToPILImage: 将张量转换为PIL图像对象。 Normalize: 标准化张量，将每个通道的值减去均值，然后除以标准差。 Resize: 调整图像的大小。 CenterCrop: 中心裁剪图像的一部分。 RandomCrop: 随机裁剪图像的一部分。 RandomResizedCrop: 随机裁剪并调整大小图像。 FiveCrop: 对图像进行五个不同位置的裁剪。 TenCrop: 对图像进行十个不同位置的裁剪。 RandomHorizontalFlip: 随机水平翻转图像。 RandomVerticalFlip: 随机垂直翻转图像。 RandomRotation: 随机旋转图像。 RandomAffine: 随机仿射变换图像。 ColorJitter: 随机调整图像的亮度、对比度、饱和度和色调。 RandomGrayscale: 随机将图像转换为灰度图像。 RandomErasing: 随机擦除图像的一部分。 RandomChoice: 随机选择一个transform进行应用。 RandomApply: 随机应用一个transform。 RandomOrder: 随机打乱transforms的顺序。 这是torch中所有的transforms，你可以根据需要选择适合的transforms来处理图像数据。 下面是一些常用的transforms功能和示例代码：\nResize：调整图像大小 from PIL import Image\r# 定义一个Resize变换，将图像调整为指定大小\rresize = transforms.Resize((256, 256))\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行Resize变换\rresized_image = resize(image) ToTensor：将图像转换为Tensor类型 from PIL import Image\r# 定义一个ToTensor变换，将图像转换为Tensor类型\rto_tensor = transforms.ToTensor()\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行ToTensor变换\rtensor_image = to_tensor(image) Normalize：对图像进行归一化 from PIL import Image\r# 定义一个Normalize变换，将图像进行归一化\rnormalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行Normalize变换\rnormalized_image = normalize(image) transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) 的作用是将输入数据标准化到均值为0，标准差为1的范围内，而不是将值标准化到-1和1之间。标准化的目的是为了使数据具有相似的尺度，以便更好地进行模型训练和优化。 对于给定的某个点(100, 150, 200)，标准化的过程如下： 1.计算每个通道的均值：(100 + 150 + 200) / 3 = 150 2.计算每个通道的标准差：sqrt(((100-150)^2 + (150-150)^2 + (200-150)^2) / 3) = sqrt((2500 + 0 + 2500) / 3) ≈ 50 3.对每个通道的值进行标准化：(100-150)/50 = -1, (150-150)/50 = 0, (200-150)/50 = 1 所以，标准化后的点为(-1, 0, 1)。 需要注意的是，这只是一个简单的例子，实际上在计算标准差时使用的是整个数据集的均值和标准差，而不是单个点的均值和标准差。\nRandomCrop：随机裁剪图像 from PIL import Image\r# 定义一个RandomCrop变换，随机裁剪图像\rrandom_crop = transforms.RandomCrop((224, 224))\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行RandomCrop变换\rcropped_image = random_crop(image) RandomHorizontalFlip：随机水平翻转图像 from PIL import Image\r# 定义一个RandomHorizontalFlip变换，随机水平翻转图像\rrandom_horizontal_flip = transforms.RandomHorizontalFlip(p=0.5)\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行RandomHorizontalFlip变换\rflipped_image = random_horizontal_flip(image) 通过使用transforms模块中的这些函数，我们可以方便地对图像进行预处理和增强，以便于在训练模型时使用。需要注意的是，transforms函数通常需要作为参数传递给torchvision.transforms.Compose函数，以便将多个transforms组合在一起应用到图像上，如：\ntransform = transforms.Compose([\rtransforms.ToTensor(),\rtransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\r]) 更详细的图像增强处理例子参考：https://github.com/lzeqian/deeplearn/blob/master/learn_rnn/pytorch/4.nn%E6%A8%A1%E5%9D%97/3.%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86.ipynb\nDataSet torchvision库中提供了许多常用的计算机视觉数据集。以下是torchvision库中支持的一些常见数据集的列表：\nMNIST：手写数字图片数据集。 FashionMNIST：时尚商品图片数据集。 CIFAR10：包含10个类别的彩色图片数据集。 CIFAR100：包含100个细分类别的彩色图片数据集。 SVHN：包含数字图片的街景数据集。 ImageNet：包含超过100万个物体类别的彩色图片数据集。 COCO：包含多个物体类别的彩色图片数据集，用于目标检测和图像分割任务。 除了上述数据集，torchvision还提供了一些辅助函数和类，用于加载和预处理数据集，如DataLoader、ImageFolder等。\nDataLoader 以下是对CIFAR10的加载例子\nimport torchvision.datasets as datasets\rimport torchvision.transforms as transforms\rfrom torch.utils.data import Dataset,DataLoader\rimport torch as t\rimport numpy as np\r#加载训练数据50000条\rtrain_dataset=datasets.CIFAR10(root=\"./data\",train=True,transform=transforms.ToTensor(),download=True)\r#测试数据集10000条\rtest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\r#打印数据集的维度\rprint(train_dataset.data.shape,test_dataset.data.shape)\r#打印数据集的标签\rprint(len(train_dataset.targets))\r#torchvision.datasets.cifar.CIFAR10\rprint(type(train_dataset))\r#torchvision.datasets.vision.VisionDataset\rprint(type(train_dataset).__bases__) 注意datasets.CIFAR10在root指定的目录没有数据集会自动下载，如果下载很慢，可以将控制台打印的路径下载下来丢到./data目录即可离线加载。\nDataLoader是PyTorch中用于数据加载的实用工具类。它可以将自定义的数据集包装成一个可迭代的数据加载器，方便进行批处理、洗牌和并行加载等操作。以下是DataLoader的一些常用参数的详细解释：\ndataset：要加载的数据集。可以是继承自torch.utils.data.Dataset的自定义数据集类的实例，也可以是已有的PyTorch数据集类（如torchvision.datasets.ImageFolder）的实例。\nbatch_size：每个批次中样本的数量。默认值为1。通常会根据模型和设备的内存情况选择合适的批量大小。\nshuffle：是否在每个epoch开始前对数据进行洗牌（随机打乱顺序）。默认值为False。洗牌可以提高训练的随机性，有助于模型更好地学习数据中的模式。\nsampler：用于定义数据采样策略的采样器。如果指定了sampler，则忽略shuffle参数。常用的采样器包括torch.utils.data.RandomSampler（随机采样）和torch.utils.data.SequentialSampler（顺序采样）。\nbatch_sampler：用于定义批次级别的数据采样策略的采样器。如果指定了batch_sampler，则忽略batch_size、shuffle和sampler参数。常用的批次采样器包括torch.utils.data.BatchSampler。\nnum_workers：用于数据加载的子进程数量。默认值为0，表示在主进程中加载数据。可以根据计算机的CPU核心数和数据加载的性能需求来选择合适的数值。\ncollate_fn：用于将样本列表转换为批次张量的函数。默认情况下使用默认的collate函数，它假定样本是Tensor或Numpy数组，并将它们堆叠成批次。如果数据集返回的样本具有不同的类型或形状，可以自定义collate函数来处理。\npin_memory：是否将数据加载到CUDA固定内存中。默认值为False。当使用GPU进行训练时，设置pin_memory为True可以加速数据传输，但会占用额外的内存。\ndrop_last：如果数据集的大小不能被批次大小整除，是否丢弃最后一个不完整的批次。默认值为False。在训练过程中，通常会设置为True，以避免不完整的批次导致的错误。\ntimeout：数据加载器在等待数据时的超时时间（以秒为单位）。默认值为0，表示无超时限制。如果数据加载时间较长，可以设置一个较大的超时时间。\nworker_init_fn：用于每个数据加载器子进程的初始化函数。可以用来设置每个子进程的随机种子或其他初始化操作。\n这些参数可以根据具体的需求进行调整和配置，以实现更高效、方便的数据加载 DataLoader会将加载的数据集转换为（批量，通道，高度，宽度）的形式。在PyTorch中，图像数据一般采用CHW（通道，高度，宽度）的顺序。而DataLoader则会将加载的图像数据转换为（批量，通道，高度，宽度）的形式， 其中批量表示一次加载的图像数量。这样的数据形式符合PyTorch中卷积神经网络的输入要求。 torchvision.datasets.vision.VisionDataset复杂处理这些。\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\rtest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\r# 使用数据加载器进行迭代,一批次64条，64条一个循环\rfor batch in train_loader:\rinput_data, labels = batch\rprint(input_data.shape)\rbreak; 输出：torch.Size([64, 3, 32, 32])\n自定义数据集 自己创建的数据集没有做任何维度的转换。\nclass MyDs(Dataset):\rdef __init__(self,data,label):\rself.data=data\rself.label=label\rdef __len__(self):\rreturn len(self.data)\rdef __getitem__(self, index):\rreturn self.data[index],self.label[index]\rds=MyDs([1,2,3,4],[0,1,1,1])\rdsLoader=DataLoader(ds,batch_size=2,shuffle=True)\rfor input,label in dsLoader: #四条数据分成了2批，循环两次\rprint(input,label) nn模块 nn.Module nn.Module是PyTorch中所有神经网络模块的基类。它是构建自定义神经网络模块的核心组件，提供了一些基本功能和属性。\n下面是nn.Module的一些重要属性和方法：\nparameters()：返回模块中需要训练的参数的迭代器。 named_parameters()：返回模块中需要训练的参数及其名称的迭代器。 children()：返回模块中所有子模块的迭代器。 named_children()：返回模块中所有子模块及其名称的迭代器。 to(device)：将模块移动到指定的设备（如CPU或GPU）。 train()：将模块设置为训练模式，启用BatchNorm和Dropout等层的训练行为。 eval()：将模块设置为评估模式，禁用BatchNorm和Dropout等层的训练行为。 forward(input)：定义模块的前向传播逻辑，接收输入并返回输出。 此外，nn.Module还提供了一些方法用于模块的初始化和参数管理：\n__init__()：构造函数，用于初始化模块的参数和子模块。 zero_grad()：将模块中所有参数的梯度置零。 apply(fn)：递归地对模块和子模块应用指定的函数。 state_dict()：返回模块的当前状态字典，包含所有参数和缓冲区。 load_state_dict(state_dict)：加载给定的状态字典，用于恢复模块的参数和缓冲区。 通过继承nn.Module类，可以方便地构建自定义的神经网络模块，并使用PyTorch提供的许多功能来管理模块的参数、状态和计算逻辑。\n使用module自定义一个全连接层\nimport torch as t;\rimport torch.nn as nn\rclass Linear(nn.Module):\rdef __init__(self,input_feature,out_feature):\rnn.Module.__init__(self)\r#nn.Prameter是自动算梯度的\rself.w=nn.Parameter(t.randn(input_feature,out_feature))\rself.b=nn.Parameter(t.randn(out_feature))\rdef forward(self,x):\rreturn x.mm(self.w)+self.b\rlayer=Linear(4,1)\rrtn=layer(t.randn(3,4))\rrtn.backward(t.ones(rtn.size())) # 计算梯度\rprint(layer.w.grad) # 获取w的梯度\rprint(layer.b.grad) # 获取b的梯度 CNN 在神经网络处理中，图片矩阵的通道通常是在宽高之前。这种表示方式被称为“通道优先”（channel-first）或“NCHW”表示法。在这种表示法中，矩阵的维度顺序为（批量大小，通道数，高度，宽度）。\n例如，对于一个RGB彩色图像，它的矩阵表示将具有维度顺序为（1，3，H，W），其中1是批量大小（表示一次处理的图像数量，就是行数），3是通道数（表示RGB三个通道），H是图像的高度，W是图像的宽度，pytorch使用这种方式。\n另一种表示方式是“宽高优先”（channel-last）或“NHWC”表示法，其中矩阵的维度顺序为（批量大小，高度，宽度，通道数）。但是，通道优先的表示法更常见，因为它与卷积操作的计算方式更契合，tensorflow使用这种方式。\n图像处理层 PyTorch提供了一系列用于图像处理的层和函数。以下是一些常用的图像处理层：\nnn.Conv2d：卷积层，用于提取图像中的特征。 nn.MaxPool2d：最大池化层，用于降低特征图的空间维度。 nn.AvgPool2d：均值池化层，用于降低特征图的空间维度 nn.BatchNorm2d：批量归一化层，用于加速训练并提高模型的鲁棒性。 nn.ReLU：ReLU激活函数层，用于引入非线性性。 nn.Linear：全连接层，用于将卷积层的输出映射到最终的分类或回归结果。 nn.Dropout2d：二维Dropout层，用于减少过拟合。 nn.Upsample：上采样层，用于增加特征图的空间维度。 nn.Softmax：Softmax函数层，用于多类别分类问题中的概率计算。 除了这些层，PyTorch还提供了一些用于图像处理的函数，例如卷积操作torch.nn.functional.conv2d，池化操作torch.nn.functional.max_pool2d，以及其他常用的图像处理函数如裁剪、旋转、缩放等。\n这些层和函数可以用来构建卷积神经网络（CNN）等图像处理模型,torch.nn.functional只是用于计算结果而nn包的函数可以用于计算梯度，如果在构建神经网络时必须用nn包。\n卷积神经网络的各层的概念请参考：https://blog.csdn.net/liaomin416100569/article/details/130597944?spm=1001.2014.3001.5501\nnn.Conv2d nn.Conv是PyTorch中用于定义卷积层的类，它的参数如下：\nin_channels：输入张量的通道数。 out_channels：卷积层输出的通道数，也是卷积核的数量。 kernel_size：卷积核的大小，可以是一个整数或一个元组，如(3, 3)。 stride：卷积操作的步长，默认为1。 padding：在输入张量的边缘周围填充0的层数，默认为0。 dilation：卷积核元素之间的间隔，默认为1。 groups：将输入张量分成几组进行卷积，默认为1。 bias：是否使用偏置项，默认为True。 以下是一个示例：\nimport torch.nn as nn\r# 创建一个卷积层\rconv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\r## 打印卷积层的参数\rprint(conv) 输出结果如下：\nConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n上述代码创建了一个输入通道数为3，输出通道数（神经元个数）为64的卷积层，卷积核大小为3x3，步长为1，填充层数为1。\nin_channels代表输入张量的通道数，也可以理解为输入张量的维度。在卷积神经网络中，输入张量的维度通常是指图像的通道数。例如，对于RGB图像，通道数为3，因为图像由红、绿、蓝三个通道组成。对于灰度图像，通道数为1，因为图像只有一个通道。\n在使用nn.Conv创建卷积层时，需要根据输入张量的通道数来设置in_channels参数，以确保卷积层与输入张量的维度匹配。\nConv2d的步长（stride）参数表示卷积核在进行滑动时的步幅大小。步长的作用是控制输出特征图的尺寸。具体来说，如果步长为1， 则卷积核每次滑动一个像素；如果步长为2，则卷积核每次滑动两个像素，以此类推。 步长的两个维度分别表示在图像的行方向和列方向上的步幅大小。在您提供的示例中，步长为(1, 1)，表示卷积核在图像的行和列方向上每次滑动一个像素。\nConv2d的padding参数表示在输入图像的周围添加填充（padding）的大小。填充的作用是在卷积操作中保持输出特征图的尺寸与输入特征图的尺寸相同，或者根据需要进行调整。\n#################学习卷积\rimport torchvision.datasets as datasets\rimport torchvision.transforms as transforms\rimport matplotlib.pyplot as plt\rimport torch.nn as nn\rimport torch as t\r#预处理模块\rfrom PIL import Image\rimage=Image.open(\"./images/2023_6_30.jpg\")\r# plt.imshow(image)\r# plt.show()\r\"\"\"\r这是一个用于边缘检测的卷积核。在这个卷积核中，中心元素是1，\r表示当前位置的像素值对边缘检测有贡献，而周围的元素都是-0.1111，\r表示对边缘检测没有贡献。这样的卷积核可以帮助我们提取图像中的垂直边缘特征。\r\"\"\"\rkernel=t.Tensor(\r[[-0.1111, -0.1111, -0.1111],\r[-0.1111, 1.0000, -0.1111],\r[-0.1111, -0.1111, -0.1111]],\r)\rkernel=t.ones(3,3)/-9\rkernel[1][1]=1\r#转换成灰度图，通道数变成1了\rimage=image.convert(\"L\")\r#转换成张量\rimageTensor=transforms.ToTensor()(image)\rprint(imageTensor.shape)\r#在第0个维度添加一个一维表示批次数据\rinput=imageTensor.unsqueeze(0)\rprint(\"输入形状\",input.shape)\rlayer=nn.Conv2d(1,1,(3,3),bias=False)\r# 定义输入张量shape为(batch_size, channels, height, width)\rlayer.weight.data=kernel.view(1,1,3,3)\routput=layer(input)\rplt.imshow(transforms.ToPILImage()(output.squeeze(0)),cmap=\"gray\")\rplt.show()\r#每个卷积核（3×3）与原始的输入图像（480×479）进行卷积，这样得到的 feature map（特征图）大小为（480-3+1）×（479-3+1）= 478×477\rprint(\"输出形状\",output.shape) 原始图 输出： torch.Size([1, 480, 479]) 输入形状 torch.Size([1, 1, 480, 479]) 输出形状 torch.Size([1, 1, 478, 477]) nn.AvgPool2d和nn.MaxPool2d 上面的卷积图在经过池化\nplt.rcParams['font.sans-serif'] = ['SimHei'] # 设置全局字体为SimHei\r#平均池化（AvgPool）\rpool=nn.AvgPool2d(kernel_size=2, stride=2)\r#池化层478×477经过(2,2)池化后=(478/2=239,477/2=238)\rpoolOuput=pool(output)\rprint(poolOuput.shape)\rplt.title(\"平均池化\")\rplt.imshow(transforms.ToPILImage()(poolOuput.squeeze(0)),cmap=\"gray\")\rplt.show()\r#最大化池\rpool=nn.MaxPool2d(kernel_size=2, stride=2)\r#池化层478×477经过(2,2)池化后=(478/2=239,477/2=238)\rpoolOuput=pool(output)\rprint(poolOuput.shape)\rplt.title(\"最大池化\")\rplt.imshow(transforms.ToPILImage()(poolOuput.squeeze(0)),cmap=\"gray\")\rplt.show() 输出 nn.Linear nn.Linear是PyTorch中用于定义线性变换的类。它是nn.Module的子类，用于构建神经网络的层。\nnn.Linear接受两个参数：in_features和out_features，分别表示输入特征的大小和输出特征的大小。它会自动创建一个可学习的权重矩阵，形状为( in_features，out_features)，以及一个可学习的偏置向量，形状为(out_features,)。\n#注意全连接是特征连接是是改变最后一维的特征数的，在pytorch图片批量处理后最后需要进行view操作来降低维度到二维。\rarr=t.randn((3,4)) print(arr)\rresult=nn.Linear(4,5)\r#全连接就是一个输入数据点乘(输入数据维度,输出数据维度)最后得到一个（输入数据行数，输出数据维度）的数组\rprint(result(arr)) nn.BatchNorm2d BatchNorm2d是用于对二维卷积层的输出进行批量归一化的操作。它的计算过程如下所示：\n假设输入的维度为 [batch_size, num_channels, height, width]，其中 batch_size 表示批量大小，num_channels 表示通道数，height 和 width 表示特征图的高度和宽度。\n计算每个通道的均值和方差：\n对于每个通道，计算当前批次中所有样本的特征图在该通道上的均值和方差。 均值的计算：mean = sum(x) / N，其中 x 是当前通道上的特征图值，N 是批次大小。 方差的计算：var = sum((x - mean)^2) / N。 对于每个通道，进行归一化：\n对于每个样本，在当前通道上，将特征图的值减去均值，然后除以标准差（方差的平方根），以实现归一化。 归一化后的特征图为：y = (x - mean) / sqrt(var + eps)，其中 eps 是一个很小的数，以避免除以零的情况。 对于每个通道，进行缩放和平移：\n对于每个归一化后的特征图，通过乘以一个可学习的缩放因子（scale）和加上一个可学习的平移因子（shift），对特征图进行缩放和平移。 缩放和平移后的特征图为：y = gamma * y + beta，其中 gamma 和 beta 是可学习的参数。 最后，BatchNorm2d操作的输出为归一化、缩放和平移后的特征图。\n这样做的好处是可以加快神经网络的训练速度，提高模型的收敛性和泛化能力，并减少对学习率的敏感性。\n\"\"\"\r具体来说，nn.BatchNorm2d是应用在卷积层之后、激活函数之前的操作，其目的是对每个特征通道的数据进行归一化。\r它通过对每个特征通道的数据进行标准化，使得数据的均值为0，方差为1。这样做的好处是可以防止梯度消失或爆炸的问题，\r并且有助于加速模型的收敛速度。\r除此之外，nn.BatchNorm2d还具有正则化的效果，可以减少模型的过拟合。它通过引入额外的可学习参数，实现了对每个特征通道的平移和缩放操作，以便网络可以自行学习数据的适当分布。\r\"\"\"\rarr=t.randint(0,10,(1,1,2,2)).float()#一批次一个通道，高是2，宽是2\rprint(arr)\rresult=nn.BatchNorm2d(num_features=1)\r#全连接就是一个输入数据点乘(输入数据维度,输出数据维度)最后得到一个（输入数据行数，输出数据维度）的数组\rprint(result(arr)) 输出：\ntensor([[[[9., 1.],\r[8., 9.]]]])\rtensor([[[[ 0.6727, -1.7191],\r[ 0.3737, 0.6727]]]], grad_fn=\u003cNativeBatchNormBackward\u003e) nn.Relu nn.ReLU是PyTorch中的一个激活函数，它将输入中的所有负值变为零，保持正值不变。具体来说，对于输入张量x，nn.ReLU函数的计算公式为：\nReLU(x) = max(0, x) 例子\narr=t.randint(0,10,(1,1,2,2)).float()#一批次一个通道，高是2，宽是2\r#首先进行归一化，归一化后会有负数的部分\rbatchNorm2d=nn.BatchNorm2d(num_features=1)\rresult=batchNorm2d(arr)\rprint(\"归一化\",result)\rrelu=nn.ReLU()\r#全连接就是一个输入数据点乘(输入数据维度,输出数据维度)最后得到一个（输入数据行数，输出数据维度）的数组\rprint(\"relu结果\",relu(result)) 输出：\n归一化 tensor([[[[ 0.2773, -1.3867],\r[ 1.3867, -0.2773]]]], grad_fn=\u003cNativeBatchNormBackward\u003e)\rrelu结果 tensor([[[[0.2773, 0.0000],\r[1.3867, 0.0000]]]], grad_fn=\u003cReluBackward0\u003e) nn.Dropout2d nn.Dropout2d会在训练过程中，对输入张量的每个通道的每个元素按照给定的概率进行丢弃。被丢弃的元素会被设置为零，而保留的元素则会按比例进行缩放，以保持期望值不变。 这种随机丢弃的操作有助于在训练过程中减少过拟合现象，增强模型的泛化能力。丢弃的概率可以通过nn.Dropout2d的参数进行控制。 需要注意的是，在测试过程中，所有的元素都会被保留，不会进行丢弃操作。nn.Dropout2d通常用于卷积神经网络中，可以放在卷积层或者全连接层之后，帮助网络更好地适应数据。 例子\narr=t.randint(0,10,(1,1,4,4)).float()#一批次一个通道，高是2，宽是2\rdrop=nn.Dropout2d()\rnewArr=drop(arr)\rprint(newArr) 输出\ntensor([[[[ 8., 8., 12., 12.],\r[10., 12., 6., 12.],\r[ 0., 4., 12., 16.],\r[ 4., 4., 18., 10.]]]]) nn.Softmax nn.Softmax是PyTorch中的一个函数，它用于计算softmax函数的输出。softmax函数通常用于多分类问题的神经网络中，它将原始的类别分数转化为概率分布。\n在PyTorch中，nn.Softmax可以被应用于一维或二维张量。对于一维张量，它会对张量中的每个元素进行softmax操作，并返回一个与输入张量相同形状的张量。对于二维张量，它会在指定维度上对每行进行softmax操作。\nsoftmax函数的计算公式如下：\n$softmax(x_i) = exp(x_i) / sum(exp(x_j))$\n其中，$x_i$是原始的类别分数，exp是指数函数，sum是对所有类别分数的求和。\nsoftmax函数的输出是一个概率分布，每个类别的概率值介于0和1之间，并且所有类别的概率之和为1。这样可以方便地用于多分类问题中，根据概率选择最可能的类别。\n在PyTorch中，可以使用nn.Softmax函数对网络的输出进行处理，以得到分类结果。 例子\narr=t.randint(0,10,(1, 1, 4, 4)).float()#一批次一个通道，高是2，宽是2\rprint(arr)\r#注意在哪个维度上的和等于1，比如一个4维的（维度从0开始），(1, 1, 4, 4)如果你从0维上，取出0维第一行数据/0维上所有数据行，因为只有一行所有永远都是1\r#如果是第3维上，总共有4个数据，也就是这四个数之和等于1\r#Softmax2D==nn.Softmax(dim=1)\rsoftmax2d=nn.Softmax(dim=3)\rnewArr=softmax2d(arr)\rprint(newArr)\rt.manual_seed(10)\rarr=t.randint(0,10,(1, 2, 4, 4)).float()#一批次2个通道，高是2，宽是2\rsoftmax2d=nn.Softmax2d()\rnewArr=softmax2d(arr)\rprint(arr)\rprint(newArr) 输出\ntensor([[[[7., 5., 2., 0.],\r[3., 0., 8., 1.],\r[6., 8., 8., 4.],\r[2., 6., 3., 5.]]]])\rtensor([[[[8.7490e-01, 1.1841e-01, 5.8950e-03, 7.9781e-04],\r[6.6846e-03, 3.3281e-04, 9.9208e-01, 9.0466e-04],\r[6.2840e-02, 4.6433e-01, 4.6433e-01, 8.5045e-03],\r[1.2755e-02, 6.9639e-01, 3.4671e-02, 2.5619e-01]]]])\rtensor([[[[7., 5., 2., 7.],\r[2., 5., 7., 2.],\r[1., 5., 6., 3.],\r[1., 0., 6., 3.]],\r[[4., 0., 6., 2.],\r[8., 9., 2., 0.],\r[9., 9., 4., 4.],\r[9., 4., 4., 5.]]]])\rtensor([[[[9.5257e-01, 9.9331e-01, 1.7986e-02, 9.9331e-01],\r[2.4726e-03, 1.7986e-02, 9.9331e-01, 8.8080e-01],\r[3.3535e-04, 1.7986e-02, 8.8080e-01, 2.6894e-01],\r[3.3535e-04, 1.7986e-02, 8.8080e-01, 1.1920e-01]],\r[[4.7426e-02, 6.6929e-03, 9.8201e-01, 6.6929e-03],\r[9.9753e-01, 9.8201e-01, 6.6929e-03, 1.1920e-01],\r[9.9966e-01, 9.8201e-01, 1.1920e-01, 7.3106e-01],\r[9.9966e-01, 9.8201e-01, 1.1920e-01, 8.8080e-01]]]]) nn.Sequential nn.Sequential和nn.ModuleList是PyTorch中用于组合神经网络模块的两种容器。\nnn.Sequential：\nnn.Sequential是一个按照顺序排列的容器，其中的模块按照它们被添加到容器中的顺序依次执行。 可以通过在Sequential对象的构造函数中传递模块列表来创建Sequential容器，或者通过.add_module()方法逐个添加模块。 nn.Sequential适用于简单的顺序模型，其中每个模块只有一个输入和一个输出。 nn.ModuleList：\nnn.ModuleList是一个可以包含任意数量模块的容器，模块之间没有特定的顺序。 可以通过在ModuleList对象的构造函数中传递模块列表来创建ModuleList容器，或者通过.append()方法逐个添加模块。 nn.ModuleList适用于自定义连接和复杂的模型结构，其中模块之间可能存在多个输入和输出。 总而言之，nn.Sequential适用于简单的顺序模型，而nn.ModuleList适用于自定义连接和复杂的模型结构。在实际使用中，可以根据模型的结构和需要选择合适的容器。 使用nn.Sequential自定义一个多层感知器\nimport torch as t\rimport torch.nn as nn\r#实现一个多层感知器,多层感知器（Multilayer Perceptron, MLP）的隐藏层的特征数就是神经元的个数\rclass MulPerceptron(nn.Module):\rdef __init__(self,input_feature,hidden_feature,out_feature):\rnn.Module.__init__(self)\r#Sequential会将上一层的输出作为下层的输入\rself.model=nn.Sequential(\rnn.Linear(input_feature,hidden_feature),\rnn.ReLU(),\rnn.Linear(hidden_feature,out_feature)\r)\rdef forward(self,x):\r#隐藏层进行一次全连接得到（行，hidden_feature）数据矩阵\rreturn self.model(x);\rmp=MulPerceptron(784,512,1)\rresult=mp(t.randn(200,784))\rprint(result) 最后输出(200,1)的结果。\n损失函数 PyTorch提供了一系列常用的损失函数，下面是其中一些常见的损失函数及其用法举例：\nnn.MSELoss（均方误差损失函数）：\n用于回归任务，计算预测值与真实值之间的均方误差。\nloss_fn = nn.MSELoss()\rloss = loss_fn(output, target)\nnn.CrossEntropyLoss（交叉熵损失函数）：\n用于多分类任务，计算预测类别与真实类别之间的交叉熵损失。\nloss_fn = nn.CrossEntropyLoss()\rloss = loss_fn(output, target)\nnn.BCELoss（二分类交叉熵损失函数）：\n用于二分类任务，计算预测概率与真实标签之间的二分类交叉熵损失。\nloss_fn = nn.BCELoss()\rloss = loss_fn(output, target)\nnn.BCEWithLogitsLoss（二分类交叉熵损失函数，结合了Sigmoid函数）：\n用于二分类任务，结合了Sigmoid函数的操作，可以在计算二分类交叉熵损失时避免数值稳定性问题。\nloss_fn = nn.BCEWithLogitsLoss()\rloss = loss_fn(output, target)\nnn.NLLLoss（负对数似然损失函数）：\n用于多分类任务，计算预测类别的负对数似然损失。\nloss_fn = nn.NLLLoss()\rloss = loss_fn(output, target)\n这只是一小部分PyTorch中提供的损失函数，还有其他损失函数可用于不同的任务和应用场景。你可以根据具体的需求选择合适的损失函数来进行模型训练和优化。\n均方误差 均方误差（Mean Squared Error，MSE）是一种常用的回归问题的损失函数。它衡量了预测值与真实值之间的差异的平方的平均值。\n对于给定的预测值和真实值，MSE的计算公式如下：\nMSE = (1/n) * Σ(y_pred - y_true)^2\n其中，n是样本数量，y_pred是预测值，y_true是真实值。\nMSE的值越小，表示预测值和真实值之间的差异越小，模型的性能越好。常用的优化算法，如梯度下降法，通过最小化MSE来调整模型的参数，以提高模型的准确性。\nx=t.randn(100,1)\ry=3*x+2+t.randn(100,1) #实际值上加上一些随机噪点\ry_pre=3*x+2\rplot.plot(x,y,'.')\rplot.plot(x,y_pre)\rplot.show()\r#使用均方误差计算损失值\rcriterion=nn.MSELoss()\rloss=criterion(y,y_pre)\rprint(loss) 交叉熵 下面是一个使用nn.CrossEntropyLoss的例子，并对交叉熵的计算过程进行详细解释：\nimport torch.nn as nn\r# 假设有4个样本，每个样本有3个类别的预测结果\r# 真实标签为[2, 1, 0, 2]\r# 预测结果为一个3维张量，每一维表示对应类别的预测概率\routputs = torch.tensor([[0.1, 0.2, 0.7],\r[0.6, 0.3, 0.1],\r[0.8, 0.1, 0.1],\r[0.3, 0.5, 0.2]])\rlabels = torch.tensor([2, 1, 0, 2])\r# 创建交叉熵损失函数\rloss_fn = nn.CrossEntropyLoss()\r# 计算损失\rloss = loss_fn(outputs, labels)\rprint(loss) 输出结果为：\ntensor(0.8025)\n交叉熵是一种常用的损失函数，用于衡量两个概率分布之间的相似性。在分类任务中，我们通常将模型的预测结果视为一个概率分布，其中每个类别都有一个对应的概率。\n在上面的例子中，我们有4个样本，每个样本有3个类别的预测结果。outputs是一个3维张量，每一维表示对应类别的预测概率。例如，outputs[0]表示第一个样本对三个类别的预测概率，分别为0.1、0.2和0.7。\nlabels是一个1维张量，表示每个样本的真实类别标签。例如，labels[0]表示第一个样本的真实类别标签为2。\n交叉熵损失函数的计算过程如下：\n首先，对于每个样本，我们需要根据预测概率和真实标签计算出对应类别的预测概率。\n在上面的例子中，对于第一个样本，预测概率为[0.1, 0.2, 0.7]，真实标签为2。我们只需要取出预测概率中对应真实标签的值，即0.7。\n接下来，我们对每个样本的预测概率进行对数转换，即计算每个预测概率的自然对数。\n在上面的例子中，对于第一个样本，对数转换后的预测概率为log(0.7)。\n然后，我们将对数转换后的预测概率求和，并除以样本的数量，得到平均交叉熵损失。\n在上面的例子中，我们有4个样本，因此将对数转换后的预测概率求和，并除以4，得到平均交叉熵损失。\n最后，我们将平均交叉熵损失作为模型的损失，并用于模型的训练和优化过程。在PyTorch中，我们可以使用nn.CrossEntropyLoss函数来计算交叉熵损失，它会自动进行softmax操作和对数转换的计算。\n优化器 torch.optim是PyTorch中用于优化算法的模块。它提供了各种优化算法的实现，用于更新神经网络模型的参数以最小化损失函数。\n在PyTorch中，我们通过创建一个优化器对象来使用torch.optim模块。该优化器对象将被用于更新神经网络模型的参数。\n以下是torch.optim模块中常用的优化算法：\nSGD (Stochastic Gradient Descent): 随机梯度下降算法是最基本的优化算法之一。它通过计算损失函数对参数的梯度，并根据学习率更新参数。可以通过torch.optim.SGD类来实现。\nAdam (Adaptive Moment Estimation): Adam是一种自适应的优化算法，它结合了Momentum和RMSprop的优点。它使用动量和平方梯度的指数加权移动平均来自适应地调整学习率。可以通过torch.optim.Adam类来实现。\nAdagrad (Adaptive Gradient): Adagrad是一种自适应的优化算法，它为每个参数分配一个学习率，并根据参数的历史梯度的平方和来自适应地调整学习率。可以通过torch.optim.Adagrad类来实现。\nRMSprop (Root Mean Square Propagation): RMSprop也是一种自适应的优化算法，它使用指数加权移动平均来自适应地调整学习率。它通过除以梯度的平方和的平方根来缩放学习率。可以通过torch.optim.RMSprop类来实现。\n这些优化算法都可以通过创建相应的优化器对象，并传递神经网络模型的参数和其他超参数来使用。例如，下面的代码演示了如何使用SGD优化算法(伪代码)：\nimport torch\rimport torch.optim as optim\r# 创建神经网络模型\rmodel = MyModel()\r# 创建优化器对象，学习率为0.001\roptimizer = optim.SGD(model.parameters(), lr=0.001)\r# 在每个训练迭代中，使用优化器更新模型的参数\roptimizer.zero_grad() # 清零梯度\routput = model(input) # 前向传播\rloss = criterion(output, target) # 计算损失\rloss.backward() # 反向传播\roptimizer.step() # 更新参数 在上面的代码中，model.parameters()返回神经网络模型的所有可学习参数，这些参数将被优化器更新。optimizer.zero_grad()方法用于将参数的梯度清零，loss.backward()方法用于计算梯度，optimizer.step()方法用于更新参数。\n除了上述常用的优化算法之外，torch.optim模块还提供了其他一些优化算法，如Adadelta、AdamW等。您可以根据自己的需求选择适合的优化算法来训练模型。\nLetnet5分类CIFAR10 #%%\rimport torch\rimport torch.nn as nn\rimport torch.optim as optim\rimport torchvision.transforms as transforms\rfrom torch.utils.data import DataLoader\rfrom torchvision.datasets import CIFAR10\r#定义LeNet5模型，模型计算过程参考：https://blog.csdn.net/liaomin416100569/article/details/130677530?spm=1001.2014.3001.5501\rclass LeNet5(nn.Module):\rdef __init__(self):\rsuper(LeNet5, self).__init__()\rself.features = nn.Sequential(\r#C1 层（卷积层）：6@28×28 该层使用了 6 个卷积核，每个卷积核的大小为 5×5，这样就得到了 6 个 feature map（特征图）。\rnn.Conv2d(3, 6, kernel_size=5),\rnn.ReLU(inplace=True),\r#S2 层（下采样层，也称池化层）：6@14×14,池化单元为 2×2，因此，6 个特征图的大小经池化后即变为 14×14\rnn.MaxPool2d(kernel_size=2, stride=2),\r#C3 层（卷积层）：16@10×10 C3 层有 16 个卷积核，卷积模板大小为 5×5 C3 层的特征图大小为（14-5+1）×（14-5+1）= 10×10。\rnn.Conv2d(6, 16, kernel_size=5),\rnn.ReLU(inplace=True),\r#S4（下采样层，也称池化层）：16@5×5,与 S2 的分析类似，池化单元大小为 2×2，因此，该层与 C3 一样共有 16 个特征图，每个特征图的大小为 5×5。\rnn.MaxPool2d(kernel_size=2, stride=2)\r)\rself.classifier = nn.Sequential(\r#LeNet-5模型中的C5层是一个全连接层。在LeNet-5模型中，前两个卷积层（C1和C3）之后是一个池化层（S2和S4），\r# 然后是一个全连接层（C5），最后是输出层（F6）。全连接层C5的输入是S4层的输出，它将这个输入展平为一个向量，\r# 并将其连接到输出层F6。因此，C5层是一个全连接层，而不是卷积层，这里和文中有些冲突。\r#C5 层（卷积层）：120 该层有 120 个卷积核，每个卷积核的大小仍为 5×5，因此有 120 个特征图,特征图大小为（5-5+1）×（5-5+1）= 1×1。这样该层就刚好变成了全连接\rnn.Linear(16 * 5 * 5, 120),\rnn.ReLU(inplace=True),\r#F6 层（全连接层）：84,该层有 84 个特征图，特征图大小与 C5 一样都是 1×1\rnn.Linear(120, 84),\rnn.ReLU(inplace=True),\r# OUTPUT 层（输出层）：10\rnn.Linear(84, 10)\r)\rdef forward(self, x):\rx = self.features(x)\r#张量x在第0个维度上的大小，因为第0个维度是数据批次数（行数），s4层后的维度是(批次数，16,5,5)\r#转换成2维就是(行数,16*5*5)，-1表示自动计算合并成最后一个维度\rx = x.view(x.size(0), -1)\rx = self.classifier(x)\rreturn x\rmodel = LeNet5()\r\"\"\"\rtransforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])是一种数据预处理操作，用于对图像数据进行归一化处理。这个操作会将每个像素的数值减去均值(mean)并除以标准差(std)。\r在这个例子中，mean=[0.5, 0.5, 0.5]表示将每个通道的像素值减去0.5，std=[0.5, 0.5, 0.5]表示将每个通道的像素值除以0.5。这样处理后，图像的像素值会在-1到1之间。\r归一化可以帮助提高模型的训练效果和稳定性，因为它可以使输入数据的分布更加接近标准正态分布。此外，对于不同的数据集，可能需要不同的均值和标准差进行归一化操作，以使数据的分布更加合理。\r在使用PyTorch的transforms.Normalize时，通常需要将其与其他数据预处理操作一起使用，例如transforms.ToTensor()将图像转换为张量。可以通过transforms.Compose将多个预处理操作组合在一起，形成一个数据预处理管道。\r\"\"\"\rtransform = transforms.Compose([\rtransforms.ToTensor(),\rtransforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\r])\r#下载训练集,data是数据数组，target是标签\rtrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\r#下载测试集\rtest_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\r#数据批处理和打乱，一次64条数据\rtrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\rtest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\r#使用交叉熵损失函数\rcriterion = nn.CrossEntropyLoss()\r#使用随机梯度下降法优化参数，梯度下降的学习率是0.001\roptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\r#判断是否有gpu如果有的话讲模型附加到cuda设备上\r#momentum参数通过累积之前的梯度信息，使得参数更新具有一定的惯性，从而在参数空间中更快地找到全局最优解或局部最优解。\rdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\rmodel.to(device)\r#模型对数据集进行10次epoch\rnum_epochs = 10\rfor epoch in range(num_epochs):\rmodel.train()\repoch_loss = 0.0\rfor images, labels in train_loader:\rimages = images.to(device)\rlabels = labels.to(device)\roptimizer.zero_grad()\routputs = model(images)\rloss = criterion(outputs, labels)\rloss.backward()\roptimizer.step()\repoch_loss += loss.item()\rprint(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\r\"\"\"\rmodel.eval()是PyTorch中用于将模型设置为评估模式的函数。当调用model.eval()时，模型的行为会发生变化，包括：\r1. Batch Normalization和Dropout等具有随机性的层会固定住，不再产生随机变化。\r2. 模型的参数不会被更新，即不会进行梯度计算和反向传播。\r3. 在推断阶段，模型会根据输入数据生成输出，而不会进行训练。\r通常，在测试或评估模型时，需要调用model.eval()来确保模型的行为与训练时保持一致。\r这样可以避免由于Batch Normalization和Dropout等层的随机性而导致结果不稳定。在调用model.train()之前，\r应该使用model.eval()将模型切换回训练模式,要将模型切换回训练模式，可以使用model.train()方法。\r\"\"\" model.eval()\rcorrect = 0\rtotal = 0\r#torch.no_grad()是一个上下文管理器，将其包裹的代码块中的所有操作都不会计算梯度。\r# 通常用于在不需要计算梯度的情况下进行推理或评估。\rwith torch.no_grad():\rfor images, labels in test_loader:\r#数据加载到显存\rimages = images.to(device)\rlabels = labels.to(device)\routputs = model(images)\r#获取输出数据中概率最高的那一个\r_, predicted = torch.max(outputs.data, 1)\r#总共数据行\rtotal += labels.size(0)\r#正确的数据行\rcorrect += (predicted == labels).sum().item()\raccuracy = 100 * correct / total\rprint(f\"Test Accuracy: {accuracy:.2f}%\") RNN nn.RNN nn.RNN是PyTorch中的一个循环神经网络模块，用于处理序列数据。下面是nn.RNN的常用参数和解释：\ninput_size：输入的特征维度。 hidden_size：隐藏层的特征维度。 num_layers：RNN的层数。 nonlinearity：激活函数，默认为\"tanh\"。可以是\"tanh\"、“relu\"等。 bias：是否使用偏置，默认为True。 batch_first：是否输入数据的第一个维度为batch大小，默认为False。 dropout：是否在输出层应用dropout操作，默认为0，即不使用dropout。 bidirectional：是否使用双向RNN，默认为False。 这些参数可以在创建nn.RNN时进行设置。例如：\nimport torch.nn as nn\rinput_size = 10\rhidden_size = 20\rnum_layers = 2\rrnn = nn.RNN(input_size, hidden_size, num_layers) 这样就创建了一个具有输入特征维度为10、隐藏层特征维度为20、2层的RNN模型。\n传入数据格式 nn.RNN的输入数据格式通常为三维张量，具体格式为：\n如果batch_first=False（默认值），则输入数据的形状为(sequence_length, batch_size, input_size)。 如果batch_first=True，则输入数据的形状为(batch_size, sequence_length, input_size)。 其中，\nsequence_length表示序列的长度，即时间步的数目。 batch_size表示每个batch的样本数量。 input_size表示输入特征的维度。 例如，假设我们有一个batch包含3个样本，每个样本的序列长度为4，输入特征维度为5，那么输入数据的形状可以是(4, 3, 5)或(3, 4, 5)。\n可以使用torch.randn()函数生成随机输入数据进行测试，例如：\nimport torch.nn as nn\rbatch_size = 3\rsequence_length = 4\rinput_size = 5\rinput_data = torch.randn(sequence_length, batch_size, input_size)\rrnn = nn.RNN(input_size, hidden_size, num_layers)\routput, hidden = rnn(input_data) 其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\n案例 \"\"\"\rPyTorch中实现了如今最常用的三种RNN：RNN（vanilla RNN）、LSTM和GRU。此外还有对应的三种RNNCell。\rRNN和RNNCell层的区别在于前者能够处理整个序列，而后者一次只处理序列中一个时间点的数据，\r前者封装更完备更易于使用，后者更具灵活性。RNN层可以通过组合调用RNNCell来实现。\r理论参考：https://blog.csdn.net/liaomin416100569/article/details/131380370?spm=1001.2014.3001.5501\r输入参数和RNN参数解释参考readme.md\r\"\"\"\rimport torch as t\rimport torch.nn as nn\r#注意默认（时间步，批次数，数据维度）\rsequence_length =3\rbatch_size =2\rinput_size =4\rinput=t.randn(sequence_length,batch_size,input_size)\rprint(\"输入数据\",input)\rrnnModel=nn.RNN(input_size,3,1)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, hidden=rnnModel(input)\rprint(\"RNN最后时间步隐藏层\",hidden)\rprint(\"RNN最后时间步隐藏层维度\",hidden.shape)\rprint(\"RNN所有隐藏层\",output)\rprint(\"RNN所有隐藏层维度\",output.shape) 输出：\n输入数据 tensor([[[ 0.5364, -0.5291, 0.3117, -0.0282],\r[-0.2012, 0.9933, 1.5328, -0.8234]],\r[[ 1.3270, -1.2367, 0.5925, 1.0894],\r[-1.8035, 0.3598, -0.4404, 0.4921]],\r[[-0.6487, -0.0487, -0.9728, 0.7563],\r[ 1.2929, 0.5146, 1.2296, 1.0124]]])\rRNN最后时间步隐藏层 tensor([[[0.2800, 0.8572, 0.3759],\r[0.5901, 0.4742, 0.9417]]], grad_fn=\u003cStackBackward\u003e)\rRNN最后时间步隐藏层维度 torch.Size([1, 2, 3])\rRNN所有隐藏层 tensor([[[ 0.5862, 0.7417, 0.8068],\r[ 0.9564, 0.5668, 0.6112]],\r[[-0.1729, 0.7310, 0.9879],\r[ 0.6202, 0.7824, 0.3075]],\r[[ 0.2800, 0.8572, 0.3759],\r[ 0.5901, 0.4742, 0.9417]]], grad_fn=\u003cStackBackward\u003e)\rRNN所有隐藏层维度 torch.Size([3, 2, 3]) nn.LSTM nn.LSTM是PyTorch中的一个循环神经网络模块，它基于长短期记忆（Long Short-Term Memory，LSTM）的架构。LSTM是一种特殊类型的循环神经网络，通过使用门控机制来解决传统循环神经网络中的梯度消失和梯度爆炸问题，从而能够更好地处理长期依赖关系。\nnn.LSTM的主要参数包括：\ninput_size：输入数据的特征维度。 hidden_size：隐藏层的维度，也是LSTM单元输出的维度。 num_layers：LSTM的层数，默认为1。 bias：是否使用偏置，默认为True。 batch_first：输入数据的维度顺序是否为(batch, seq, feature)，默认为False。 dropout：是否应用dropout，用于防止过拟合，默认为0，表示不使用dropout。 bidirectional：是否使用双向LSTM，默认为False。 nn.LSTM的输入数据格式通常是一个三维张量，具体格式取决于batch_first参数的设置。如果batch_first为False（默认值），输入数据的维度应为(seq_len, batch, input_size)，其中seq_len表示序列的长度，batch表示批次的大小，input_size表示输入数据的特征维度。如果batch_first为True，输入数据的维度应为(batch, seq_len, input_size)。\nnn.LSTM的前向传播过程会根据输入数据的时间步长和层数进行迭代计算，并返回最后一个时间步的输出以及最后一个时间步的隐藏状态和记忆细胞状态。这些输出可以用于下游任务，如分类或回归。\n使用nn.LSTM时，可以通过调整参数来适应不同的任务和数据。此外，还可以使用nn.LSTMCell来构建自定义的LSTM网络。\nnn.LSTM的返回值是一个元组，包含两个元素：output和(hidden_state, cell_state)。\noutput：表示LSTM模型的隐藏状态输出。它是一个元组，包含了模型在每个时间步的输出结果。具体来说，output的形状是(seq_len, batch, num_directions * hidden_size)，其中：\nseq_len表示输入序列的长度； batch表示输入数据的批次大小； num_directions表示LSTM模型的方向数，通常为1或2（双向LSTM）； hidden_size表示隐藏状态的维度。 (hidden_state, cell_state)：表示LSTM模型的最后一个时间步的隐藏状态和细胞状态。它们的形状都是(num_layers * num_directions, batch, hidden_size)，其中：\nnum_layers表示LSTM模型的层数； num_directions表示LSTM模型的方向数，通常为1或2（双向LSTM）； batch表示输入数据的批次大小； hidden_size表示隐藏状态的维度。 这两个返回值可以用于进一步的处理和分析，比如用于序列标注、语言建模等任务。 也就是hidden_state是output最后一个值，每个时间步都有一个cell_state 案例\nlstmModel=nn.LSTM(input_size,3,1)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, (h, c) =lstmModel(input)\rprint(\"LSTM隐藏层输出的维度\",output.shape)\rprint(\"LSTM隐藏层最后一个时间步输出的维度\",h.shape)\rprint(\"LSTM隐藏层最后一个时间步细胞状态\",c.shape) 输出\nLSTM隐藏层输出的维度 torch.Size([3, 2, 3])\rLSTM隐藏层最后一个时间步输出的维度 torch.Size([1, 2, 3])\rLSTM隐藏层最后一个时间步细胞状态 torch.Size([1, 2, 3]) nn.GRU nn.GRU是PyTorch中的一个循环神经网络（Recurrent Neural Network，RNN）模块，它实现了门控循环单元（Gated Recurrent Unit，GRU）的功能。GRU是一种用于处理序列数据的RNN变体，它具有比传统的循环神经网络更强大的建模能力。\nGRU通过引入两个门控机制，即更新门（Update Gate）和重置门（Reset Gate），来控制信息的流动。这些门控机制使得GRU能够学习长期依赖关系，并且在处理长序列时能够更好地捕捉到序列中的重要信息。\n在nn.GRU模块中，可以通过设置参数来定义GRU的输入维度、隐藏状态维度、层数等。以下是nn.GRU的一些常用参数：\ninput_size：输入的特征维度。 hidden_size：隐藏状态的维度。 num_layers：GRU的层数。 bias：是否使用偏置。 batch_first：如果为True，则输入数据的形状应为（batch_size，sequence_length，input_size）；如果为False，则输入数据的形状应为（sequence_length，batch_size，input_size）。 dropout：dropout比例，用于控制输入数据的随机丢弃比例。 bidirectional：是否使用双向GRU。 除了上述参数之外，nn.GRU还提供了其他一些方法和功能，如forward方法用于前向传播计算，reset_parameters方法用于重置模型的参数等。 案例\n# gru没有细胞状态\rgruModel=nn.GRU(input_size,3,1)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, h =gruModel(input)\rprint(\"GRU隐藏层输出的维度\",output.shape)\rprint(\"GRU隐藏层最后一个时间步输出的维度\",h.shape) 输出\nGRU隐藏层输出的维度 torch.Size([3, 2, 3])\rGRU隐藏层最后一个时间步输出的维度 torch.Size([1, 2, 3]) models checkpoints 在深度学习中，checkpoints是训练期间保存模型参数的文件。它们是在每个训练周期或某个特定时间间隔保存的，以便在训练过程中出现问题时可以恢复训练。通过保存checkpoints，我们可以在训练过程中随时停止并重新开始，而无需从头开始训练。\n“.pt\"是PyTorch中用于保存模型参数的文件扩展名。当我们训练一个模型时，我们可以将模型的参数保存在.pt文件中，以便以后在其他地方使用或加载到其他模型中。这些文件包含了模型在训练期间学到的权重和偏差等参数。在PyTorch中，我们可以使用torch.save()函数将模型参数保存为.pt文件，并使用torch.load()函数加载.pt文件中的参数。\nimport torch\rimport torch.nn as nn\r#模型的保存和加载\rmodel=nn.Linear(3,1)\r#修改权重和偏置后保存模型\rnew_weight = torch.tensor([[1.0, 2.0, 3.0]])\rnew_bias = torch.tensor([4.0])\rmodel.weight = nn.Parameter(new_weight)\rmodel.bias = nn.Parameter(new_bias)\rtorch.save(model.state_dict(),\"./model.pt\")\rnewModel=nn.Linear(3,1)\rprint(\"默认参数\",newModel.weight,newModel.bias)\rnewModel.load_state_dict(torch.load(\"./model.pt\"))\rprint(\"加载后\",newModel.weight,newModel.bias) 输出\n默认参数 Parameter containing:\rtensor([[-0.4357, -0.0781, 0.0136]], requires_grad=True) Parameter containing:\rtensor([-0.2013], requires_grad=True)\r加载后 Parameter containing:\rtensor([[1., 2., 3.]], requires_grad=True) Parameter containing:\rtensor([4.], requires_grad=True) 内置models 在PyTorch的torchvision.models模块中，提供了一些已经实现好的经典神经网络模型，包括：\nAlexNet VGG ResNet SqueezeNet DenseNet Inception GoogLeNet MobileNet ShuffleNet ResNeXt Wide ResNet MNASNet 这些模型可以通过torchvision.models模块的函数进行实例化，以便在自己的项目中使用。每个模型都有预训练的权重，也可以在自定义数据集上进行微调。你可以根据自己的需求选择适合的模型进行使用。\n以下使用models.resnet18分类datasets.CIFAR10\n#%%\rimport torch\rimport torch.nn as nn\rimport torch.optim as optim\rimport torchvision.transforms as transforms\rimport torchvision.datasets as datasets\rimport torchvision.models as models\r# 定义数据预处理\rtransform = transforms.Compose([\rtransforms.RandomCrop(32, padding=4),\rtransforms.RandomHorizontalFlip(),\rtransforms.ToTensor(),\rtransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\r])\r# 加载CIFAR-10数据集\rtrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\rtest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\r# 定义模型\r#参数pretrained表示是否加载预训练的权重。如果pretrained为True，那么模型将加载在ImageNet数据集上预训练的权重。\r# 这些预训练的权重可以提供更好的初始权重，有助于模型在其他任务上进行迁移学习。如果pretrained为False，\r# 则使用随机初始化的权重进行训练。\rmodel = models.resnet18(pretrained=False)\rnum_classes = 10\rmodel.fc = nn.Linear(512, num_classes)\r# 定义损失函数和优化器\rcriterion = nn.CrossEntropyLoss()\roptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\r# 训练模型\rbatch_size = 64\rtrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\rtest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\rnum_epochs = 10\rdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\rmodel.to(device)\rfor epoch in range(num_epochs):\rmodel.train()\repoch_loss = 0.0\rfor images, labels in train_loader:\rimages = images.to(device)\rlabels = labels.to(device)\r# 前向传播和计算损失\routputs = model(images)\rloss = criterion(outputs, labels)\r# 反向传播和优化\roptimizer.zero_grad()\rloss.backward()\roptimizer.step()\repoch_loss += loss.item()\rprint(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\r# 在测试集上评估模型\rmodel.eval()\rwith torch.no_grad():\rcorrect = 0\rtotal = 0\rfor images, labels in test_loader:\rimages = images.to(device)\rlabels = labels.to(device)\routputs = model(images)\r_, predicted = torch.max(outputs.data, 1)\rtotal += labels.size(0)\rcorrect += (predicted == labels).sum().item()\raccuracy = 100 * correct / total\rprint(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy}%')\rtorch.cuda.empty_cache() torch.hub torch.hub是PyTorch中一个用于加载预训练模型的工具。它提供了一个简单的接口，可以方便地从互联网上获取训练好的模型并加载到您的代码中使用。通过使用torch.hub，您可以轻松地使用各种预训练模型，如图像分类、目标检测、语义分割等模型。\ntorch.hub的使用非常简单，您只需要提供模型的命名空间和模型名称，它将自动下载并加载预训练模型。例如，要加载一个名为\"pytorch/vision\"的模型，您可以使用以下代码：\nimport torch\rmodel = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\n上述代码将下载并加载名为\"resnet50\"的预训练模型，并将其存储在model变量中。您可以使用model变量进行推理、特征提取等操作。\ntorch.hub还支持本地模型缓存，这意味着当您多次运行相同的代码时，它将自动从本地缓存中加载模型，而不是重新下载。这样可以提高代码的运行效率。\n总之，torch.hub是一个非常方便的工具，使您能够轻松地使用各种预训练模型，并将它们集成到您的代码中，从而加速您的深度学习项目开发。 官网模型搜索地址：https://pytorch.org/hub/research-models 以下是最火的6个model yolov5目标检测 实战使用yolov5目标检测,参考官方模型文档：https://pytorch.org/hub/ultralytics_yolov5/ 注意 Python\u003e=3.8 PyTorch\u003e=1.7 安装ultralytics\npip install -U ultralytics 编写程序\n#%%\rimport torch\r# Model，加载模型中的参数\rmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\r# Images\rimgs = ['https://ultralytics.com/images/zidane.jpg'] # batch of images\r# Inference\rresults = model(imgs)\r# Results\rresults.print()\rresults.save() # or .show()\rresults.xyxy[0] # img1 predictions (tensor) 会在当前运行的目录上生成一个runs/detect/exp/zidane.jpg 生成动漫图像 github项目地址：https://github.com/bryandlee/animegan2-pytorch 可以将人物图像转换为动漫效果。\nfrom PIL import Image\rimport torch\rfrom matplotlib import pyplot\rmodel = torch.hub.load(\"bryandlee/animegan2-pytorch:main\", \"generator\",pretrained=\"celeba_distill\").eval()\rface2paint = torch.hub.load(\"bryandlee/animegan2-pytorch:main\", \"face2paint\", size=512)\rimage=Image.open(\"./images/lyf.png\")\rout = face2paint(model, image)\rpyplot.imshow(out)\rpyplot.show() 原始图像 转换后 可视化监控 在 TensorFlow 中，最常使用的可视化工具是Tensorboard ,TensorboardX 工具使得 PyTorch 也享受到 Tensorboard 的便捷功能。 pytorch1.8之后已经包含了tensorboardx工具，在torch.utils.tensorboard包中。 FaceBook 也为 PyTorch 开发了一款交互式可视化工具 Visdom，它可以对实时数据进行丰富的可视化，帮助实时监控实验过程。\ntensorboard Tensorboard 是 TensorFlow 的一个附加工具，用于记录训练过程的模型的参数、评价指标与图像等细节内容，并通过 Web 页面提供查看细节与过程的功能，用浏览器可视化的形式展现，帮助我们在实验时观察神经网络的训练过程，把握训练趋势。既然 Tensorboard 工具这么方便，TensorFlow 外的其它深度学习框架自然也想获取 Tensorboard 的便捷功能，于是，TensorboardX 应运而生。 先安装Tensorboard\npip install tensorboard 我这里tensorboard要求的setuptools版本较低，在使用过程中报错\nAttributeError: module 'distutils' has no attribute 'version' 降级版本即可\npip uninstall setuptools\rmicromamba install setuptools==59.5.0\r或者用pip install setuptools==59.5.0 工具使用规范 1、创建SummaryWriter 的实例：\nfrom torch.utils.tensorboard import SummaryWriter\r# 创建一个SummaryWriter的实例\rwriter = SummaryWriter(log_dir=None) 其中的 log_dir 表示保存日志的路径，默认会保存在“runs/ 当前时间 _ 主机名”文件夹中。\nadd_scalar 2、add_scalar方法，这个方法用来记录数字常量（比如损失函数值），它的定义如下：\nadd_scalar(tag, scalar_value, global_step=None, walltime=None) tag：字符串类型，表示对应要监控的数据名称，是任意自定义的，不同名称的数据会使用不同曲线展示； scalar_value：浮点型，表示要监控及保存的数值； global_step：整型，表示训练的 step 数，作为横坐标； walltime：浮点型，表示记录发生的时间，默认为 time.time()。 一般会使用add_scalar方法来记录训练过程的 loss、accuracy、learning rate 等数值的变化，这样就能直观地监控训练过程。每监控一个指标，就需要使用一个add_scalar方法。（如果要看x个指标就使用x次add_scalar方法） 3、add_image方法用来记录单个图像数据（需要 Pillow 库的支持），它的定义如下 add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW') tag、global_step 和 walltime 的含义跟add_scalar方法里一样 img_tensor：PyTorch 的 Tensor 类型或 NumPy 的 array 类型，表示图像数据； dataformats：字符串类型，表示图像数据的格式，默认为“CHW”，即 Channel x Height x Width，还可以是“CHW”、“HWC”或“HW”等。 这里演示一个线性回归的例子 ，演示将epoch次数作为x，损失作为y值的的scalar图 #%%\rimport os\rimport shutil\rdef delete_directory_contents(directory):\rfor filename in os.listdir(directory): # 遍历目录下的所有文件和子目录\rfile_path = os.path.join(directory, filename) # 构建文件或子目录的完整路径\rif os.path.isfile(file_path): # 如果是文件，则直接删除\ros.remove(file_path)\relif os.path.isdir(file_path): # 如果是子目录，则递归调用删除子目录中的内容\rshutil.rmtree(file_path)\r#删除runs目录下的所有文件和目录 delete_directory_contents(\"./runs/\") import torch as t\rimport matplotlib.pyplot as plot\rimport torch.nn as nn\rfrom torch.utils.tensorboard import SummaryWriter\r#########例子演示梯度下降损失（每个epoch的损失）\r#其中的 log_dir 表示保存日志的路径，默认会保存在“runs/ 当前时间 _ 主机名”文件夹中。\rwriter=SummaryWriter(log_dir=None)\rt.manual_seed(42)\r# 使用自动梯度实现线性回归\rx=t.randn(100,1)\rx_test=t.randn(20,1)\ry=3*x+2+t.randn(100,1) #实际值上加上一些随机噪点\ry_test=3*x+2+t.randn(100,1)\rclass LinearModel(nn.Module):\rdef __init__(self):\rnn.Module.__init__(self)\rself.w=nn.Parameter(t.randn(1,1))\rself.b=nn.Parameter(t.randn(1))\rdef forward(self,x):\rreturn t.mm(x,self.w)+self.b\rmodel=LinearModel()\rlossf=nn.MSELoss()\r#定义优化器,第一个参数为模型的参数，参数传入后,自动获取他的梯度并且-梯度*学习率\roptim=t.optim.SGD(model.parameters(),lr=0.01)\r#训练100次，100次梯度下降，计算到最小损失时的w和b\repochCount=100\rfor epoch in range(epochCount):\ry_pre=model(x)\r#注意梯度清零，否则会累加\roptim.zero_grad() loss=lossf(y_pre,y)\rwriter.add_scalar(\"Loss/train\",loss,epoch)\rloss.backward()\r#更新参数w和b\roptim.step()\rplot.plot(x,y,'.')\rplot.plot(x.data.numpy(),y_pre.data.numpy())\rplot.show()\rwriter.close() 运行后在runs目录下生成了日志，切换到当前安装tensorboard的环境执行命令：tensorboard –logdir=runs，\ntensorboard 是热加载的，上面的代码比如调整epoch次数，重新运行，是实时刷新的。\n(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\u003etensorboard --logdir=runs\rTensorFlow installation not found - running with reduced feature set.\rServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\rTensorBoard 2.13.0 at http://localhost:6006/ (Press CTRL+C to quit) 访问：http://localhost:6006/ 可以看到epoch到达80左右基本损失就很小了 我们把代码的epochCount调整到20 可以看到损失梯度下降还没有达到平缓，在看下拟合的图形 再把epochCount调整到10000 可以看到在100左右基本就平缓了，后面的训练是多余的了，所以我们可以观察到epoch到100是最合适的\nadd_histogram 使用 add_histogram 方法来记录一组数据的直方图。\nadd_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None) 参数\ntag (string): 数据名称 values (torch.Tensor, numpy.array, or string/blobname): 用来构建直方图的数据 global_step (int, optional): 训练的 step bins (string, optional): 取值有 ‘tensorflow’、‘auto’、‘fd’ 等, 该参数决定了分桶的方式，详见这里。 walltime (float, optional): 记录发生的时间，默认为 time.time() max_bins (int, optional): 最大分桶数 我们可以通过观察数据、训练参数、特征的直方图，了解到它们大致的分布情况，辅助神经网络的训练过程。 我们来观察下假设10次产生均值是0方差是1的1000条数据，每一次的波动 import numpy as np\rfrom torch.utils.tensorboard import SummaryWriter\rwriter = SummaryWriter()\rflag = 1\rif flag :\rfor x in range(10):\rdata_1 = np.arange(1000)\rdata_2 = np.random.normal(size=1000)\r#直方图的结构是y轴是第多少次，x轴显示value的波动\rwriter.add_histogram(\"data1\",data_1,x)\rwriter.add_histogram('data2',data_2,x)\rwriter.close() 右侧的坐标表示循环的次数，下方的坐标表示这1000个数的分布情况\n运行图 (graph) 使用 add_graph 方法来可视化一个神经网络。\nadd_graph(model, input_to_model=None, verbose=False, **kwargs) 参数\nmodel (torch.nn.Module): 待可视化的网络模型 input_to_model (torch.Tensor or list of torch.Tensor, optional): 待输入神经网络的变量或一组变量 在add_scalar线性回归的代码中我们打印线性模型输入x的计算图 model=LinearModel()\r#加入代码\rwriter.add_graph(model,model.w) 图片add_image 使用 add_image 方法来记录单个图像数据。注意，该方法需要 pillow 库的支持。\nadd_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW') 参数\ntag (string): 数据名称 img_tensor (torch.Tensor / numpy.array): 图像数据 global_step (int, optional): 训练的 step walltime (float, optional): 记录发生的时间，默认为 time.time() dataformats (string, optional): 图像数据的格式，默认为 'CHW'，即 Channel x Height x Width，还可以是 'CHW'、'HWC' 或 'HW' 等 我们一般会使用 add_image 来实时观察生成式模型的生成效果，或者可视化分割、目标检测的结果，帮助调试模型。\nVisdom 后续补",
    "description": "@[toc]\n概述 PyTorch是一个基于Python的开源机器学习框架，由Facebook的人工智能研究团队开发并维护。它提供了丰富的工具和接口，用于构建和训练深度神经网络模型。\nPyTorch的主要特点和优势包括：\n动态图：PyTorch使用动态图机制，即在运行时构建计算图。这使得模型的构建和调试更加直观和灵活，能够更好地处理复杂的计算流程和动态控制流。\n简洁明了：PyTorch的API设计简洁明了，易于学习和使用。它提供了一系列高级接口，使得模型的构建、训练和评估变得更加简单和高效。\n强大的GPU加速支持：PyTorch能够利用GPU进行张量运算和模型训练，从而加快计算速度。它提供了简单易用的接口，使得在GPU上进行加速变得更加方便。\n灵活扩展：PyTorch支持自定义操作符和扩展，使得用户可以方便地实现和使用自己的模型组件和功能。\n相比之下，TensorFlow是由Google开发的另一个流行的深度学习框架。与PyTorch相比，TensorFlow的主要特点和优势包括：\n静态图：TensorFlow使用静态图机制，即在编译时构建计算图。这使得TensorFlow在模型运行时能够进行更多的优化和性能提升，适用于大规模的、计算密集型的任务。\n跨平台支持：TensorFlow可以在多种硬件和操作系统上运行，并且具有广泛的部署支持。它提供了TensorFlow Serving、TensorFlow Lite和TensorFlow.js等工具，使得模型的部署和移植更加方便。\n分布式训练支持：TensorFlow提供了分布式训练的功能，可以在多个设备和计算节点上进行模型训练，从而加快训练速度。\n生态系统和社区：TensorFlow具有庞大的生态系统和活跃的社区，提供了丰富的资源和支持，包括模型库、教程和论坛等。\n总的来说，PyTorch和TensorFlow都是优秀的深度学习框架，各有其特点和适用场景。PyTorch适合于快速原型开发、动态计算流程和小规模任务，而TensorFlow适合于大规模、计算密集型的任务和分布式训练。选择哪个框架取决于具体的需求和个人偏好。\n对于初学接触神经网络，建议先学pytorch，它提供的api接近理论概念，有动态图，方便调试，适合做研究使用，，由于最近chargpt的大火，Hugging Face的transforms是使用PyTorch的。Hugging Face是一个提供自然语言处理（NLP）模型和工具的平台，他们的Transformers库主要基于PyTorch实现，他的入门pytorch必须要有基础。这个库提供了一系列用于数据预处理和后处理的函数，可以方便地对文本数据进行转换和处理。\n环境准备 安装cuda和cudnn 一般pc电脑或者服务器都有nvida显卡，可以通过nvidia-smi命令查看。 其中python环境（3.8+版本），cuda和cudnn安装请参考：https://blog.csdn.net/liaomin416100569/article/details/130532993 安装后可以看到我的cuda version是11.2\n安装pytorch 考虑到版本向下兼容，不一定非要下载cuda=11.2对应的那个版本的torch，或许低于这个版本就可以。所以我就选择下载cuda11.1的版本。 以下是pytorch对应的稳定版的网址下载链接，可以根据需要找到对应的torch版本下载。cu版本就是gpu版本，不带cu的是cpu版本，https://download.pytorch.org/whl/torch_stable.html，搜索cu111 直接选择\npip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html 编写测试代码\nimport torch\rprint(torch.__version__)\r#cuda是否可用，如果返回True，表示正常可用gpu\rprint(torch.cuda.is_available())\rprint(torch.cuda.device_count())\rx1=torch.rand(5,3)\r#把x1转换gpu0的tensor\rx1=x1.cuda(0)\rprint(x1) 测试运行",
    "tags": [],
    "title": "深度学习06-pytorch从入门到精通",
    "uri": "/docs/programming/ai/machine_learning/basic/action_06_pytorch/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "实践工具",
    "uri": "/docs/programming/ai/machine_learning/tools/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "循环神经网络",
    "uri": "/docs/programming/ai/deep_learning/rnn/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 核心算法",
    "content": "1. 协方差 概念 方差和标准差的原理和实例演示，请参考\n方差 方差（Variance）是度量一组数据的分散程度。方差是各个样本与样本均值的差的平方和的均值： 标准差 标准差是数值分散的测量。 标准差的符号是 σ （希腊语字母 西格马，英语 sigma） 公式很简单：方差的平方根。 协方差 通俗理解 可以通俗的理解为：两个变量在变化过程中是同方向变化？还是反方向变化？同向或反向程度如何？ 你变大，同时我也变大，说明两个变量是同向变化的，这时协方差就是正的。 你变大，同时我变小，说明两个变量是反向变化的，这时协方差就是负的。 从数值来看，协方差的数值越大，两个变量同向程度也就越大。反之亦然。 通俗易懂的理解看知乎文章 或者 gitlab转载\n协方差矩阵 协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。 这个解释摘自维基百科，看起来很是抽象，不好理解。其实简单来讲，协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减）。而协方差矩阵，只是将所有变量的协方差关系用矩阵的形式表现出来而已。通过矩阵这一工具，可以更方便地进行数学运算。 数学定义 回想概率统计里面关于方差的数学定义： 协方差的数学定义异曲同工： 这里的 x和y表示两个变量空间。用机器学习的话讲，就是样本有 x和 y两种特征， 而 X 就是包含所有样本的 x特征的集合， Y就是包含所有样本的 y特征的集合。 用一个例子来解释会更加形象。 用一个矩阵表示为： 现在，我们用两个变量空间X ，Y 来表示这两个特征： 由于协方差反应的是两个变量之间的相关性，因此，协方差矩阵表示的是所有变量之间两两相关的关系，具体来讲，一个包含两个特征的矩阵，其协方差矩阵应该有 2*2 大小： 接下来，就来逐一计算 Cov(Z)的值。 首先，我们需要先计算出 X，Y 两个特征空间的平均值： AVG(X)=3.25,AVG(Y)=3 ， 。 然后，根据协方差的数学定义，计算协方差矩阵的每个元素： 所以协方差矩阵 好了，虽然这只是一个二维特征的例子，但我们已经可以从中总结出协方差矩阵 的「计算套路」： python协方差原理\n# 协方差主要是多个特征\rpa=np.array([\r[1,2] ,\r[3,6] ,\r[4,2] ,\r[5,2] ])\r'''\rx应该是个二维矩阵表示\r[\r[1] ,\r[3] ,\r[4] ,\r[5] ]\r'''\rx=np.array([pa[:,0]]).reshape((4,1))\r'''\ry应该是个二维矩阵表示\r[\r[2] ,\r[6] ,\r[2] ,\r[2] ]\r'''\ry=np.array([pa[:,1]]).reshape((4,1))\rprint(\"分别获取X和Y:\",x,y)\rx_mean=np.mean(x)\ry_mean=np.mean(y)\rprint(\"x和y特征的均值\",x_mean,y_mean)\r'''\r这里只是求第一个特征x（第一列）和第二个特征(第二列)的方差cov(x,y)，\r，实际还有cov(x,x),cov(y,x),cov(y,y)\rx-x_mean转置T就变成了\r[\r[1-xmean,3-xmean,4-xmean,5-xmean] ]\ry-ymean是\r[\r[2-ymean] ,\r[6-ymean] ,\r[2-ymean] ,\r[2-ymean] ]\r(x-x_mean).T.dot(y-y_mean)就变成了矩阵乘法了\r（1-xmean）*（2-ymean）+（3-xmean）*（6-ymean）+（4-xmean）*（2-ymean）+（5-xmean）*（2-ymean）\r然后除以n-1就是协方差了cov(x,y)\r'''\rprint(\"cov(x,y)=\",np.sum((x-x_mean).T.dot(y-y_mean))/(len(pa)-1))\r#数学表示横表示列，竖表示行，默认行横表示特征\r'''\r[[1 3 4 5]\r[2 6 2 2]]\r'''\rprint(pa.T)\rprint(\"conv\",np.cov(pa.T))\r# 使用rowvar=False，表示列是变量是特征\rprint(\"conv\",np.cov(pa,rowvar=False)) 输出结果可查看：github\n方差是一种特殊的协方差，协方差=cov(x,x) 熟悉协方差概念方便理解矩阵构造\n2. PCA 概念 主成分分析(Principal Component Analysis):\n一个非监督的机器学习算法 主要用于数据的降维 通过降维，可以发现更便于人类理解的特征 其他应用：可视化，降噪 假设存在一根直线，将所有的点都映射在该条指直线上，这样的话点的整体分布和原来的点的分布就没有很大的差异（点和点的距离比映射到x轴或者映射到y轴都要大，区分度就更加明显），与此同时所有的点都在一个轴上（理解成一个维度），虽然这个轴是斜着的。用这种方式将二维降到了一维度\n下表1是某些学生的语文、数学、物理、化学成绩统计：\n学生姓名 语文 数学 英文 张三 90 87 75 李四 90 50 76 王五 90 99 70 赵六 90 60 80 首先，假设这些科目成绩不相关，那么怎么区分谁的成绩好了，明显语文和英文大家都差不多，数学上就拉开了差距，数学就可以理解为主成分。\n理论 先看下面这幅图： 先假定特征只有二维，即只有两个变量，它们由横坐标和纵坐标所代表；因此每个观测值都有相应于这两个坐标轴的两个坐标值；如果这些数据形成一个椭圆形状的点阵，那么这个椭圆有一个长轴和一个短轴。在短轴方向上，数据变化很少；在极端的情况，短轴如果退化成一点，那只有在长轴的方向才能够解释这些点的变化了；这样，由二维到一维的降维就自然完成了。\n上图中，u1就是主成分方向，然后在二维空间中取和u1方向正交的方向，就是u2的方向。则n个数据在u1轴的离散程度最大（方差最大），数据在u1上的投影代表了原始数据的绝大部分信息，即使不考虑u2，信息损失也不多。而且，u1、u2不相关。只考虑u1时，二维降为一维。\n椭圆的长短轴相差得越大，降维也越有道理。\n公式推导 那么如何找到这个让样本间距最大的轴？\n如何定义样本间间距？ 使用方差(Variance) 方差越大代表样本之间越稀疏，方差越小代表样本之间越紧密。 移动坐标轴，使得样本在每一个维度均值都为0：\n创建一个3x+4线性附近20个随机样本，样例和结果：https://github.com/lzeqian/machinelearn/blob/master/sklean_pca/demean.ipynb\nimport numpy as np;\rimport matplotlib.pyplot as plot\rfrom commons.common import setXY\r#生成一个 3x+4附近的点\rnp.random.seed(100);\r# 获取randc个随机点\rrandc=20\rx1=np.random.rand(randc);\rx2=x1*3+4+np.random.rand(randc);\rplot.plot(x1,x2,\"o\"); 转换为矩阵表示\n#使用矩阵数组表示,x1,x2 x1和x2是两个特征\r'''\r[ [x1,x2]\r[1,2],\r[3,4]\r]\rX=x1.reshape(-1,1); 等价于x1.reshape(len(x1),1)\r[1,2]转换为\rx1=[\r[1],\r[2]\r]\rx2=[\r[4],\r[5]\r]\rx1.hstack(x2)\r[\r[1,4],\r[2,5]\r]\r'''\rX=np.hstack((x1.reshape(len(x1),1),x2.reshape(len(x2),1)))\rprint(X)\r[[0.54340494 6.06191901]\r[0.27836939 5.77513797]\r[0.42451759 6.09120215]\r[0.84477613 6.87044035]\r[0.00471886 4.18956702]\r[0.12156912 4.73753941]\r[0.67074908 6.01793576]\r[0.82585276 6.72998462]\r[0.13670659 5.20578228]\r[0.57509333 5.74053496]\r[0.89132195 7.27280924]\r[0.20920212 5.23141091]\r[0.18532822 4.66113234]\r[0.10837689 4.70707412]\r[0.21969749 4.69556853]\r[0.97862378 7.82628292]\r[0.81168315 7.4159703 ]\r[0.17194101 4.57576503]\r[0.81622475 7.33922019]\r[0.27407375 5.39912274]] demean，均值归0处理\nnp.set_printoptions(suppress=True) #不使用科学计数法\rsetXY()\rdef demean(X):\rreturn X-np.mean(X,axis=0) #取对应列的均值\r#均值归零的算法是x1-xmean，x2-x2mean\rX_demean=demean(X)\rplot.plot(X_demean[:,0],X_demean[:,1],\"o\"); demean之后的方差最大其实就是求映射后每个点到(0,0)的距离最大再求和，假设降维后轴的方向是w=(w1, w2) ，Xi是映射前的向量，Xi(project)是映射后的向量，这里注意w向量是单位向量 |w|=1 以上是推导公式 目标函数是： 通过公司可知 注意i是样本索引，下标1，2是特征数，x1是特征1，x2是特征2 xj是特征j，每个特征xj对应一个维度wj\n目标函数即： 对目标函数求梯度： 转化为： 由于最终转换的结果是一个1行m列的矩阵，而我们想要得到一个n行1列的矩阵，所以还要进行一次转置\n编程实现（第一主成分） 产生一个 3x+4附近的点\nimport numpy as np;\rimport matplotlib.pyplot as plot\rfrom commons.common import setXY\r#生成一个 3x+4附近的点\r#np.random.seed(100);\r# 获取randc个随机点\rrandc=100\r# 0-1的数*多少倍，注意太小的样本点，abs(f(w=w, X=X) - f(w=last_w, X=X)) 的值增量过小可能导致循环次数后，还没有到\u003cepsilon导致拟合不准确\r# 如果是1，建议循环次数加大100000\r# 如果是100 可以设置为100\rblow=100\rx1=np.random.rand(randc)*blow;\rx2=x1*3+4+np.random.rand(randc)*blow;\rplot.plot(x1,x2,\"o\");\rX=np.hstack((x1.reshape(len(x1),1),x2.reshape(len(x2),1)))\rnp.set_printoptions(suppress=True) #不使用科学计数法 定义目标函数\n# 这是目标函数 np.sum（（X*W）**2)/M\r# 注意目标函数是传入X是已知的数据样本，w是个2个特征向量 ，f(w1,w2)是个三位空间\rdef f(X,w):\rreturn np.sum((X.dot(w))**2)/len(X) 获取w在各个特征的导数\n# 获取各个维度的导数 def df_w(X,w):\rreturn X.T.dot(X.dot(w))*2/len(X) 也可以用这个通用的方法求导数\n'''\r通用计算某个点的斜率的方法\r为了验证我们的这个是正确的，使用这个df_debug这个函数，\r和线性下降法一样，使两个点之间连成的直线不断的靠近应得的直线，\r使其斜率相当，注意的是，这里的epsilon取值比较小，是因为在PCA的梯度上升法中，\rw是一个方向向量，其模为1，所以w的每一个维度其实都很小，那么为了适应，相应的epsilon也要小一些\r'''\rdef df_debug(w,X,epsilon=0.0001):\rres = np.empty(len(w))\rfor i in range(len(w)):\rw_1 = w.copy()\rw_1[i] += epsilon\rw_2 = w.copy()\rw_2[i] -= epsilon\rres[i] = (f(w=w_1,X=X) - f(w=w_2,X=X)) / (2*epsilon) 将w向量转换为单位向量\n# 将任意向量转换为单位向量 np.linalg.norm(w)是 x**2+x1**2开根号\r# (3,4)/5=(3/5,4/5)就是单位向量，模是1\rdef direction(w):\rreturn w / np.linalg.norm(w) 梯度上升测试\nwapp=np.array([])\rdef gradient_ascent(df, X, initial_w, eta, n_iters=1e4, epsilon=1e-8):\rw = direction(initial_w)\ri_iter = 1\rglobal wapp\rwapp=np.append(wapp,w).reshape((1,len(w)))\rwhile i_iter \u003c n_iters:\rgradient = df(w=w, X=X)\rlast_w = w\r# gradient是对每个维度求偏导得到的列表，如果偏导数为负则w的这个维度加上一个负值，降维后的方差趋于变大\r# 如果偏导数为正，则w的这个维度加上一个正值，降维后的方差趋于变大，因此w加上导数值，降维后的方差趋于变大\r# 在eta合适的情况下，随着循环进行，导数值逐渐趋近0，eta是常数，降维后的方差的变化量会越来越小\rw = w + eta * gradient\rw = direction(w) # 注意1，每次求一个单位向量\rwapp=np.vstack((wapp,np.array([w])))\r# abs求绝对值\rif (abs(f(w=w, X=X) - f(w=last_w, X=X)) \u003c epsilon):\rprint(\"精度：\",abs(f(w=w, X=X) - f(w=last_w, X=X)),epsilon)\rprint(\"梯度\",gradient)\rbreak\ri_iter += 1\rreturn w\rinitial_w = np.random.random(X.shape[1]) # 注意2：不能用0向量开始\reta = 0.0001\r# print(gradient_ascent(df_debug, X=X_demean, initial_w=initial_w, eta=eta))\rw=(gradient_ascent(df_w, X=X_demean, initial_w=initial_w, eta=eta, n_iters=100))\rsetXY()\rplot.plot(X_demean[:,0],X_demean[:,1],\"o\");\rprint(w)\r# 单位向量乘同一个，方向是相同的\rplot.plot([0,w[0]*(blow)],[0,w[1]*blow])\r#plot.plot(X_demean[:,0],w[1]/w[0]*X_demean[:,0])\r#%%\rfig = plot.figure()\r#创建梯度上升的过程\rax = plot.axes(projection='3d')\rwappy=[f(w=w_t, X=X) for w_t in wapp]\rax.plot3D(wapp[:,0],wapp[:,1],wappy, 'red')\rprint(\"w1值\",wapp[:,0])\rprint(\"w2值\",wapp[:,1])\rprint(\"方差：\",wappy)\rax.set_title('3D line plot')\rplot.show() 拟合的直线 梯度上升过程",
    "description": "1. 协方差 概念 方差和标准差的原理和实例演示，请参考\n方差 方差（Variance）是度量一组数据的分散程度。方差是各个样本与样本均值的差的平方和的均值： 标准差 标准差是数值分散的测量。 标准差的符号是 σ （希腊语字母 西格马，英语 sigma） 公式很简单：方差的平方根。 协方差 通俗理解 可以通俗的理解为：两个变量在变化过程中是同方向变化？还是反方向变化？同向或反向程度如何？ 你变大，同时我也变大，说明两个变量是同向变化的，这时协方差就是正的。 你变大，同时我变小，说明两个变量是反向变化的，这时协方差就是负的。 从数值来看，协方差的数值越大，两个变量同向程度也就越大。反之亦然。 通俗易懂的理解看知乎文章 或者 gitlab转载\n协方差矩阵 协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。 这个解释摘自维基百科，看起来很是抽象，不好理解。其实简单来讲，协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减）。而协方差矩阵，只是将所有变量的协方差关系用矩阵的形式表现出来而已。通过矩阵这一工具，可以更方便地进行数学运算。 数学定义 回想概率统计里面关于方差的数学定义： 协方差的数学定义异曲同工： 这里的 x和y表示两个变量空间。用机器学习的话讲，就是样本有 x和 y两种特征， 而 X 就是包含所有样本的 x特征的集合， Y就是包含所有样本的 y特征的集合。 用一个例子来解释会更加形象。 用一个矩阵表示为： 现在，我们用两个变量空间X ，Y 来表示这两个特征： 由于协方差反应的是两个变量之间的相关性，因此，协方差矩阵表示的是所有变量之间两两相关的关系，具体来讲，一个包含两个特征的矩阵，其协方差矩阵应该有 2*2 大小： 接下来，就来逐一计算 Cov(Z)的值。 首先，我们需要先计算出 X，Y 两个特征空间的平均值： AVG(X)=3.25,AVG(Y)=3 ， 。 然后，根据协方差的数学定义，计算协方差矩阵的每个元素： 所以协方差矩阵 好了，虽然这只是一个二维特征的例子，但我们已经可以从中总结出协方差矩阵 的「计算套路」： python协方差原理",
    "tags": [],
    "title": "机器学习实战教程（四）：从特征分解到协方差矩阵：详细剖析和实现PCA算法",
    "uri": "/docs/programming/ai/machine_learning/algorithms/action_04_pca/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 核心算法",
    "content": "1.引言 在互联网大数据场景下，我们经常需要面对高维数据，在对这些数据做分析和可视化的时候，我们通常会面对「高维」这个障碍。在数据挖掘和建模的过程中，高维数据也同样带来大的计算量，占据更多的资源，而且许多变量之间可能存在相关性，从而增加了分析与建模的复杂性。\n我们希望找到一种方法，在对数据完成降维「压缩」的同时，尽量减少信息损失。由于各变量之间存在一定的相关关系，因此可以考虑将关系紧密的变量变成尽可能少的新变量，使这些新变量是两两不相关的，那么就可以用较少的综合指标分别代表存在于各个变量中的各类信息。机器学习中的降维算法就是这样的一类算法。\n主成分分析（Principal Components Analysis，简称PCA）是最重要的数据降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。本篇我们来展开讲解一下这个算法。\n2.相关概念 协方差矩阵 协方差(Covariance)目的是度量两个变量(只能为两个)线性相关的程度。 cov=0为可以说明两个变量线性无关，但不能证明两个变量相互独立，当cov\u003e0时，二者呈正相关，cov\u003c0时，二者呈负相关。\n协方差矩阵可以处理多维度问题。 协方差矩阵是一个对称的矩阵，而且对角线是各个维度上的方差。 协方差矩阵计算的是不同维度之间的协方差，而不是不同样本之间的。 样本矩阵中若每行是一个样本，则每列为一个维度。 假设数据是3维的，那么对应协方差矩阵为： 这里简要概括一下协方差矩阵是怎么求得的，假设一个数据集有3维特征、每个特征有m个变量，这个数据集对应的数据矩阵如下： 若假设他们的均值都为0，可以得到下面等式： 可以看到对角线上为每个特征方差，其余位置为两个特征之间的协方差， 求得的就为协方差矩阵。 推导： 如果列是特征，公式为： '''\r假设列是矩阵特征，代数里面是行表示特征\r[\r[x1,y2]\r[x2，y2]\r]\r求协方差是\r[\r[cov(x,x),cov(x,y)],\r[cov(y,x),cov(y,y)],\r]\r'''\rpc=np.array([[-1,4],\r[-2,8],\r[-7,2]\r]);\rmean_pa=np.mean(pc,axis=0)\rprint(\"均值\",mean_pa)\rpc_zero=pc-mean_pa\rprint(pc_zero)\rprint(pc_zero.T.dot(pc_zero)/(len(pc_zero)-1)) #注意样本的话是n-1啊，全部数据集是n，否则和np.cov对不上\rprint(\"conv\",np.cov(pc,rowvar=False)) #rowvar=False表示列是特征，默认行是特征 结果为：\n均值 [-3.33333333 4.66666667]\r[[ 2.33333333 -0.66666667]\r[ 1.33333333 3.33333333]\r[-3.66666667 -2.66666667]]\r[[10.33333333 6.33333333]\r[ 6.33333333 9.33333333]]\rconv [[10.33333333 6.33333333]\r[ 6.33333333 9.33333333]] 协方差矩阵一定是方形矩阵，第一矩阵列数n（特征数）决定了是（n,n）方形矩阵。\n矩阵的行列式 行列式等于零可以得出结论\nA的行向量线性相关； A的列向量线性相关； 方程组Ax=0有非零解； A的秩小于n。（n是A的阶数） A不可逆 比如，如果行列式=0，说明x1y2-x2y1=0, x1=y1/y2 * x2 说明行线性相关。\n[x1,x2 y1,y2] [1,5 3,15] 该矩阵就是行列式=0例子。\n矩阵的行列式是一个可以从方形矩阵（方阵）计算出来的特别的数。 这矩阵的行列式是（待会儿会解释计算方法）：\n3×6 − 8×4 = 18 − 32 = −14\n行列式告诉我们矩阵的一些特性，这些特性对解线性方程组很有用，也可以帮我们找逆矩阵，并且在微积分及其他领域都很有用. 2×2 矩阵 2×2 矩阵 （2行和2列）： 行列式是：\n|A| = ad - bc “A 的行列式等于 a 乘 d 减 b 乘 c”\n3×3 矩阵 行列式是：\n|A| = a(ei - fh) - b(di - fg) + c(dh - eg) “A 的行列式等于。。。。。。”\n乍看很复杂，但这是有规律的：\n求 3×3 矩阵的行列式：\n把 a 乘以不在 a 的行或列上的 2×2 矩阵的行列式。 以 b 和 c 也做相同的计算 把结果加在一起，不过 b 前面有个负号！ 公式是（记着两边的垂直线 || 代表 “的行列式”）： 更多维计算参考： shuxuele github numpy求行列式\n#行列式一定是个方形矩阵\rp_pet=np.array([[-1,4],\r[-2,8]\r]);\rprint(np.linalg.det(p_pet)) 输出：0\n特征向量和特征值 得到了数据矩阵的协方差矩阵，下面就应该求协方差矩阵的特征值和特征向量，先了解一下这两个概念，如果一个向量v是矩阵A的特征向量，那么一定存在下列等式：\n其中A可视为数据矩阵对应的协方差矩阵，是特征向量v的特征值。数据矩阵的主成分就是由其对应的协方差矩阵的特征向量，按照对应的特征值由大到小排序得到的。最大的特征值对应的特征向量就为第一主成分，第二大的特征值对应的特征向量就为第二主成分，依次类推，如果由n维映射至k维就截取至第k主成分。\n求矩阵特征值的例子 如果入=4 numpy\n#求特征值和特征向量\rp_eig=np.array([[1.2,0.8],\r[0.8,1.2]\r]);\reigenvalue, featurevector =(np.linalg.eig(p_eig))\rprint(eigenvalue)\r# 这里特征向量是进行单位化（除以所有元素的平方和的开方）的形式\r# 比如入=0.4时特征向量是[-1,1],单位化[-1/开根(1**2+1**2)=-0.70710678 , -1/开根(1**2+1**2)=0.70710678]\r# 比如入=2时特征向量是[1,1],单位化[1/开根(1**2+1**2)=0.70710678 , 1/开根(1**2+1**2)=0.70710678]\r# 所以注意特征值2是第一列，对应的是特征向量第一列的值作为向量\rprint(featurevector)\r#组合特征值和特征向量\reig=[(eigenvalue[i],featurevector[:,i])for i in range(len(eigenvalue))]\rprint(eig) 输出\n[2. 0.4]\r[[ 0.70710678 -0.70710678]\r[ 0.70710678 0.70710678]]\r[(2.0, array([0.70710678, 0.70710678])), (0.3999999999999997, array([-0.70710678, 0.70710678]))] 可以理解为特征向量就是原始特征的一个基础向量，最后生成一个和原始数据相同维度，行是降维维度值的矩阵，比如一个50行60列（特征）的矩阵需要降维到30个特征，特征向量会是一个(30,60)的矩阵\n3. 降维实现 通过上述部分总结一下PCA降维操作的步骤：\n去均值化 依据数据矩阵计算协方差矩阵 计算协方差矩阵的特征值和特征向量 将特征值从大到小排序 保留前k个特征值对应的特征向量 将原始数据的n维映射至k维中 公式手推 原始数据集矩阵，每行代表一个特征: 对每个特征去均值化： 计算对应的协方差矩阵： 依据协方差矩阵计算特征值和特征向量，套入公式： 拆开计算如下： 可以求得两特征值： 当 时，对应向量应该满足如下等式： 对应的特征向量可取为： 同理当 时，对应特征向量可取为： 这里我就不对两个特征向量进行标准化处理了，直接合并两个特征向量可以得到矩阵P： 选取大的特征值对应的特征向量乘以原数据矩阵后可得到降维后的矩阵A： 综上步骤就是通过PCA手推公式实现二维降为一维的操作。\n手推转自：https://juejin.cn/post/6844904177571758088\nnumpy实现降维 输出结果，查看gitlab\n#%%\rimport numpy as np;\rimport matplotlib.pyplot as plot;\r# 数学中行是表示特征\rX=np.array([\r[1,1,2,4,2],\r[1,3,3,4,4]\r])\r# 转置为列为特征\rX=X.T\rprint(X)\r#%%\r#均值归零\rx_demean=np.mean(X,axis=0)\rX_Zero=X-x_demean\rprint(\"均值：\",x_demean)\rprint(X_Zero)\r#%%\r#计算对应的协方差矩阵,手撕或者使用np.conv\rcon=X_Zero.T.dot(X_Zero)/(len(X_Zero)-1)\rcon1=np.cov(X_Zero,rowvar=False)\rprint(\"协方差：\",con,\"\\n\",con1)\r#%%\r#依据协方差矩阵计算特征值和特征向量,转换参考 矩阵计算.ipynb\rf_value,f_vector=np.linalg.eig(con)\reig=[(f_value[i],f_vector[:,i])for i in range(len(f_value))]\rprint(\"特征值-特征向量\",eig)\r#%%\r#获取最大值的索引\rmax_f_value_index=np.argsort(f_value)[::-1][0]\rprint(np.array([eig[max_f_value_index][1]]).dot(X_Zero.T))\rprint(X_Zero.dot(np.array([eig[max_f_value_index][1]]).T)) 代码部分是公式的套用，每一步后都有注释，不再过多解释。可以看到得到的结果和上面手推公式得到的有些出入，上文曾提过特征向量是可以随意缩放的，这也是导致两个结果不同的原因，eig方法计算的特征向量是归一化后的。\nsklean实现降维 #%%\rfrom sklearn.decomposition import PCA\rimport numpy as np\rX = [[1, 1], [1, 3], [2, 3], [4, 4], [2, 4]]\rX = np.array(X)\rpca = PCA(n_components=1)\rPCA_mat = pca.fit_transform(X)\rprint(PCA_mat) 这里只说一下参数n_components，如果输入的是整数，代表数据集需要映射的维数，比如输入3代表最后要映射至3维；如果输入的是小数，则代表映射的维数为原数据维数的占比，比如输入0.3，如果原数据20维，就将其映射至6维。\n4. pca人脸数据降维 fetch_lfw_people人脸识别数据集是由n个人不同时间、不同角度、不同表情等图像组成的数据集； 从我这统计目录结构有5760个人 这是小布什部分图(530个） 下载并抓取图库\n#读取人脸数据\rfrom sklearn.decomposition import PCA\rfrom sklearn.datasets import fetch_lfw_people\rimport matplotlib.pyplot as plt\rfaces=fetch_lfw_people(data_home=\"d:/test/face\",min_faces_per_person=60)\rprint(\"图片数据维度：\",faces.images.shape) #数据维度：1348张照片，每张照片是一个62*47=2914的矩阵\rprint(\"图片二维数据维度：\",faces.data.shape)\rX=faces.data #sklearn降维算法只接受二维特征矩阵，把数据换成特征矩阵，维度是1348*2914，\rX = faces.data min_faces_per_person 提取的数据集将仅保留具有至少min_faces_per_person不同图片的人的图片 比如小布什的图片超过了60就加载出来，比如Abdullah只有3张该用户就不会加载。\n如果数据403无法下载请百度，下载其他用户上传到类似百度盘数据解压到data_home指定的目录即可 输出\n图片数据维度： (1348, 62, 47)\r图片二维数据维度： (1348, 2914) 绘制原始图片，绘制前32个\n# 在matplotlib中，整个图像为一个Figure对象。在Figure对象中可以包含一个或者多个Axes对象\r# figsize代表画布的大小 3行8列表示子图axes大小\rfig, axes = plt.subplots(3,8 #创建一个画布有3*8个子图\r,figsize = (8,4) #创建一个大小为8*4的黄布\r,subplot_kw = {\"xticks\":[],\"yticks\":[]} # 每个子图都不显示坐标轴\r)\rfor i,ax in enumerate(axes.flat):\rax.imshow(faces.images[i,:,:], cmap = \"gray\") pca降维到150维\npca=PCA(n_components=150)\rV1 = pca.fit_transform(X)\rx_inv = pca.inverse_transform(V1)\rprint(\"逆转升维维度：\",x_inv.shape)\rV = pca.components_\rprint(\"降维后特征向量：\",V.shape)\rprint(\"降维后数据：\",V1.shape) 显示特征向量\nfig, axes = plt.subplots(3,8 #创建一个画布有3*8个子图\r,figsize = (8,4) #创建一个大小为8*4的黄布\r,subplot_kw = {\"xticks\":[],\"yticks\":[]} # 每个子图都不显示坐标轴\r)\rfor i,ax in enumerate(axes.flat):\rax.imshow(V[i,:].reshape(62,47), cmap = \"gray\") 显示降维数据\nfig, axes = plt.subplots(3,8 #创建一个画布有3*8个子图\r,figsize = (8,4) #创建一个大小为8*4的黄布\r,subplot_kw = {\"xticks\":[],\"yticks\":[]} # 每个子图都不显示坐标轴\r)\rfor i,ax in enumerate(axes.flat):\rax.imshow(x_inv[i].reshape(62, 47), cmap='binary_r') 4. pca+knn识别手写数据 MNIST是一个手写体数字的图片数据集，该数据集来由美国国家标准与技术研究所（National Institute of Standards and Technology (NIST)）发起整理，一共统计了来自250个不同的人手写数字图片，其中50%是高中生，50%来自人口普查局的工作人员。该数据集的收集目的是希望通过算法，实现对手写数字的识别。 sklearn.datasets中提供了fetch_openml的方法抓取：https://www.openml.org/search?type=data\u0026sort=runs\u0026status=active 免费的数据，其中mnist_784就是手写数据。 knn预测 抓取数据集\n#%%\rfrom sklearn.decomposition import PCA\rimport numpy as np\rimport matplotlib.pyplot as plt\rimport matplotlib as mpl\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.datasets import fetch_openml\r## https://www.openml.org/可以搜索到对应数据集\rmnist = fetch_openml(data_home=\"d:/test/face\",name='mnist_784')\rX, y = mnist['data'], mnist['target']\rX_train, X_test, y_train, y_test=train_test_split(X,y,train_size=0.9)\r#共有7万张图片，每张图片有784个特征 784开方是：28*28。\rprint(X.shape, y.shape) 输出：(70000, 784) (70000,) 绘制25张图片\nfig, axes = plt.subplots(5,5 #创建一个画布有3*8个子图\r,figsize = (8,4) #创建一个大小为8*4的黄布\r,subplot_kw = {\"xticks\":[],\"yticks\":[]} # 每个子图都不显示坐标轴\r)\rfor i,ax in enumerate(axes.flat):\rax.imshow(X[i].reshape(28, 28), cmap = \"gray\") 使用knn训练后进行预测\n#%%\rfrom sklearn.neighbors import KNeighborsClassifier\rknn=KNeighborsClassifier()\r#导入time模块 训练数据将近一分钟左右\r%time knn.fit(X_train,y_train)\r#%%\r#获取第几个模型的测试数据，用来预测\rpreindex=101;\rplt.imshow(X_test[preindex,].reshape(28, 28), cmap = \"gray\");\r%time print(\"预测的数字：\",knn.predict(X_test[preindex:preindex+1,]))\rplt.show() 注意由于knn计算量大，fit使用时间为： Wall time: 1min 2s 计算准确率\n#%%\r#通过测试数据获取该模型的得分。\r%time print(knn.score(X_test,y_test)) 输出：\n0.9738571428571429 准确率\rWall time: 9min 9s 用时 用于score是使用剩余的测试数据来测试准确性，用时很长\npca降维 使用pca从784降维成100，会发现训练和求score时间大幅下降 完整代码参考：\nfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.decomposition import PCA\r#保留多少个主成分维度，如果是数组是保留多少个 如果是比例 用0-1的数字，比如0.9保留90%主成分\rpca = PCA(n_components=100)\r#注意要transform多个 一定要调用fit方法，而不是调用两次fit_transform否则导致两次的维度不一致，fit会根据数据行算出特征的。\rpca.fit(X_train,y_train)\rPCA_trainmat = pca.transform(X_train)\rPCA_testmat = pca.transform(X_test)\rprint(PCA_trainmat.shape,PCA_testmat.shape)\rknn1=KNeighborsClassifier()\r%time knn1.fit(PCA_trainmat,y_train)\rpreindex1=101;\rx_inv = pca.inverse_transform(PCA_testmat) plt.imshow(x_inv[preindex1,].reshape(28, 28), cmap = \"gray\");\rplt.show()\r%time print(\"预测的数字：\",knn1.predict(PCA_testmat[preindex1:preindex1+1,]))\r#%%\r%time print(knn1.score(PCA_testmat,y_test))",
    "description": "1.引言 在互联网大数据场景下，我们经常需要面对高维数据，在对这些数据做分析和可视化的时候，我们通常会面对「高维」这个障碍。在数据挖掘和建模的过程中，高维数据也同样带来大的计算量，占据更多的资源，而且许多变量之间可能存在相关性，从而增加了分析与建模的复杂性。\n我们希望找到一种方法，在对数据完成降维「压缩」的同时，尽量减少信息损失。由于各变量之间存在一定的相关关系，因此可以考虑将关系紧密的变量变成尽可能少的新变量，使这些新变量是两两不相关的，那么就可以用较少的综合指标分别代表存在于各个变量中的各类信息。机器学习中的降维算法就是这样的一类算法。\n主成分分析（Principal Components Analysis，简称PCA）是最重要的数据降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。本篇我们来展开讲解一下这个算法。\n2.相关概念 协方差矩阵 协方差(Covariance)目的是度量两个变量(只能为两个)线性相关的程度。 cov=0为可以说明两个变量线性无关，但不能证明两个变量相互独立，当cov\u003e0时，二者呈正相关，cov\u003c0时，二者呈负相关。\n协方差矩阵可以处理多维度问题。 协方差矩阵是一个对称的矩阵，而且对角线是各个维度上的方差。 协方差矩阵计算的是不同维度之间的协方差，而不是不同样本之间的。 样本矩阵中若每行是一个样本，则每列为一个维度。 假设数据是3维的，那么对应协方差矩阵为： 这里简要概括一下协方差矩阵是怎么求得的，假设一个数据集有3维特征、每个特征有m个变量，这个数据集对应的数据矩阵如下： 若假设他们的均值都为0，可以得到下面等式： 可以看到对角线上为每个特征方差，其余位置为两个特征之间的协方差， 求得的就为协方差矩阵。 推导： 如果列是特征，公式为： '''\r假设列是矩阵特征，代数里面是行表示特征\r[\r[x1,y2]\r[x2，y2]\r]\r求协方差是\r[\r[cov(x,x),cov(x,y)],\r[cov(y,x),cov(y,y)],\r]\r'''\rpc=np.array([[-1,4],\r[-2,8],\r[-7,2]\r]);\rmean_pa=np.mean(pc,axis=0)\rprint(\"均值\",mean_pa)\rpc_zero=pc-mean_pa\rprint(pc_zero)\rprint(pc_zero.T.dot(pc_zero)/(len(pc_zero)-1)) #注意样本的话是n-1啊，全部数据集是n，否则和np.cov对不上\rprint(\"conv\",np.cov(pc,rowvar=False)) #rowvar=False表示列是特征，默认行是特征 结果为：",
    "tags": [],
    "title": "机器学习实战教程（五）：使用PCA实战人脸降维",
    "uri": "/docs/programming/ai/machine_learning/algorithms/action_05_pcaface/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "生成对抗网络",
    "uri": "/docs/programming/ai/deep_learning/gans/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 核心算法",
    "content": "决策树 决策树是什么？决策树(decision tree)是一种基本的分类与回归方法。举个通俗易懂的例子，如下图所示的流程图就是一个决策树，长方形代表判断模块(decision block)，椭圆形成代表终止模块(terminating block)，表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作为分支(branch)，它可以达到另一个判断模块或者终止模块。我们还可以这样理解，分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。蒙圈没？？如下图所示的决策树，长方形和椭圆形都是结点。长方形的结点属于内部结点，椭圆形的结点属于叶结点，从结点引出的左右箭头就是有向边。而最上面的结点就是决策树的根结点(root node)。这样，结点说法就与模块说法对应上了，理解就好。\n本文大部分文字转载自https://cuijiahua.com/blog/2017/11/ml_2_decision_tree_1.html，代码和部分原创\n我们回到这个流程图，对，你没看错，这就是一个假想的相亲对象分类系统。它首先检测相亲对方是否有房。如果有房，则对于这个相亲对象可以考虑进一步接触。如果没有房，则观察相亲对象是否有上进心，如果没有，直接Say Goodbye，此时可以说：“你人很好，但是我们不合适。“如果有，则可以把这个相亲对象列入候选名单，好听点叫候选名单，有点瑕疵地讲，那就是备胎。\n不过这只是个简单的相亲对象分类系统，只是做了简单的分类。真实情况可能要复杂得多，考虑因素也可以是五花八门。脾气好吗？会做饭吗？愿意做家务吗？家里几个孩子？父母是干什么的？天啊，我不想再说下去了，想想都可怕。\n我们可以把决策树看成一个if-then规则的集合，将决策树转换成if-then规则的过程是这样的：由决策树的根结点(root node)到叶结点(leaf node)的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：互斥并且完备。这就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。这里所覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。\n使用决策树做预测需要以下过程：\n收集数据：可以使用任何方法。比如想构建一个相亲系统，我们可以从媒婆那里，或者通过采访相亲对象获取数据。根据他们考虑的因素和最终的选择结果，就可以得到一些供我们利用的数据了。 准备数据：收集完的数据，我们要进行整理，将这些所有收集的信息按照一定规则整理出来，并排版，方便我们进行后续处理。 分析数据：可以使用任何方法，决策树构造完成之后，我们可以检查决策树图形是否符合预期。 训练算法：这个过程也就是构造决策树，同样也可以说是决策树学习，就是构造一个决策树的数据结构。 测试算法：使用经验树计算错误率。当错误率达到了可接收范围，这个决策树就可以投放使用了。 使用算法：此步骤可以使用适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。 决策树的构建的准备工作 使用决策树做预测的每一步骤都很重要，数据收集不到位，将会导致没有足够的特征让我们构建错误率低的决策树。数据特征充足，但是不知道用哪些特征好，将会导致无法构建出分类效果好的决策树模型。从算法方面看，决策树的构建是我们的核心内容。\n决策树要如何构建呢？通常，这一过程可以概括为3个步骤：特征选择、决策树的生成和决策树的修剪。\n特征选择 特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率，如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的标准是信息增益(information gain)或信息增益比，为了简单，本文使用信息增益作为选择特征的标准。那么，什么是信息增益？在讲解信息增益之前，让我们看一组实例，贷款申请样本数据表。\n希望通过所给的训练数据学习一个贷款申请的决策树，用于对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。\n特征选择就是决定用哪个特征来划分特征空间。比如，我们通过上述数据表得到两个可能的决策树，分别由两个不同特征的根结点构成。 图(a)所示的根结点的特征是年龄，有3个取值，对应于不同的取值有不同的子结点。图(b)所示的根节点的特征是工作，有2个取值，对应于不同的取值有不同的子结点。两个决策树都可以从此延续下去。问题是：究竟选择哪个特征更好些？这就要求确定选择特征的准则。直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益就能够很好地表示这一直观的准则。\n什么是信息增益呢？在划分数据集之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。 信息增益=整个数据的不确定性-某个特征条件的不确定=这个特征增强了多少确定性\n那怎么确定数据的不确定性了，引出了香农熵的概念\n香农熵 在可以评测哪个数据划分方式是最好的数据划分之前，我们必须学习如何计算信息增益。集合信息的度量方式称为香农熵或者简称为熵(entropy)，这个名字来源于信息论之父克劳德·香农。\n如果看不明白什么是信息增益和熵，请不要着急，因为他们自诞生的那一天起，就注定会令世人十分费解。克劳德·香农写完信息论之后，约翰·冯·诺依曼建议使用\"熵\"这个术语，因为大家都不知道它是什么意思。\n如果想彻底理解信息熵原理参考 图解原理： 其中推导用到的拉格朗日乘子法 同时理解对数函数的特点：\n如果ax =N（a\u003e0，且a≠1），那么数x叫做以a为底N的对数，记作x=logaN，读作以a为底N的对数，其中a叫做对数的底数，N叫做真数。 如果Y=logaX 表示为Y个a相乘等于X 底数a是0-1之间是单调递减 大于1是单调递增, 如果a\u003e1 x在0-1之间y负数 x=1 y=0 x\u003e1时 y为正数 ln为一个算符，意思是求自然对数，即以e为底的对数。 e是一个常数，等于2.71828183… lnx可以理解为ln(x)，即以e为底x的对数，也就是求e的多少次方等于x。 lnx=loge^x， logeE=Ine=1 熵定义为信息的期望值。在信息论与概率统计中，熵是表示随机变量不确定性的度量。如果待分类的事物可能划分在多个分类之中，则符号xi的信息定义为 ： 其中p(xi)是选择该分类的概率，比如一个团队10人中性别有男生（3）和女生（7）两个分类，上述式中的对数以2为底，也可以e为底(自然对数)。\n男生熵=-log2 3/10 = 1.7369655941662063 女生熵=-log2 7/10 = 0.5145731728297583\r整个团队的熵=男生熵+女生熵 =2.2515387669959646 注意因为log2 p(X) 因为p(x)是概率所以是在0-1之间是负数 所以在前面加个-转换成+数 由于男生的熵明显大于女生，说明整个团队是男生的概率要低于女生。\n通过上式，我们可以得到所有类别的信息。为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值(数学期望)，通过下面的公式得到： 期中n是分类的数目。熵越大，随机变量的不确定性就越大。\n当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵(empirical entropy)。什么叫由数据估计？比如有10个数据，一共有两个类别，A类和B类。其中有7个数据属于A类，则该A类的概率即为十分之七。其中有3个数据属于B类，则该B类的概率即为十分之三。浅显的解释就是，这概率是我们根据数据数出来的。我们定义贷款申请样本数据表中的数据为训练数据集D，则训练数据集D的经验熵为H(D)，|D|表示其样本容量，及样本个数。设有K个类Ck, = 1,2,3,…,K,|Ck|为属于类Ck的样本个数，因此经验熵公式就可以写为 ： 根据此公式计算经验熵H(D)，分析贷款申请样本数据表中的数据。最终分类结果只有两类，即放贷和不放贷。根据表中的数据统计可知，在15个数据中，9个数据的结果为放贷，6个数据的结果为不放贷。所以数据集D的经验熵H(D)为： 经过计算可知，数据集D的经验熵H(D)的值为0.971。\n编写代码计算经验熵 在编写代码之前，我们先对数据集进行属性标注。\n年龄：0代表青年，1代表中年，2代表老年； 有工作：0代表否，1代表是； 有自己的房子：0代表否，1代表是； 信贷情况：0代表一般，1代表好，2代表非常好； 类别(是否给贷款)：no代表否，yes代表是。 确定这些之后，我们就可以创建数据集，并计算经验熵了，代码编写如下： #%%\r#数据集，yes表示放贷，no表示不放贷\r'''\r具体参考：案例图\r特征1表示年龄 0表示青年，1表示中间，2表示老年\r特征2表示是否有工作 0表示否，1表示有\r特征3表示是否有自己的房子 0表示否 1表示有\r特征4是信贷情况 0表示一般 1表示好 2表示非常好。\r'''\rimport numpy as np\rdataSet = np.array([[0, 0, 0, 0, 'no'], [0, 0, 0, 1, 'no'],\r[0, 1, 0, 1, 'yes'],\r[0, 1, 1, 0, 'yes'],\r[0, 0, 0, 0, 'no'],\r[1, 0, 0, 0, 'no'],\r[1, 0, 0, 1, 'no'],\r[1, 1, 1, 1, 'yes'],\r[1, 0, 1, 2, 'yes'],\r[1, 0, 1, 2, 'yes'],\r[2, 0, 1, 2, 'yes'],\r[2, 0, 1, 1, 'yes'],\r[2, 1, 0, 1, 'yes'],\r[2, 1, 0, 2, 'yes'],\r[2, 0, 0, 0, 'no']])\rlabels = ['不放贷', '放贷']\r'''\r计算经验熵\rD代表传入的数据集\r'''\rdef ShannonEnt(D):\r#去除重复元素的结论的分类:[yes,no]\rkArray=np.unique(D[:,4].reshape(1,len(D)))\r#计算出最终分类的个数\rk=len(kArray)\r#获取整个样本集的个数\rD_count=len(D)\r#经验熵\rHD=0;\r#循环多个分类，计算这个分类的熵，最后求和\rfor i in range(k):\r#获取等于当前分类的数据行\rck=[row for row in D if row[4]==kArray[i]]\rHD-=len(ck)/D_count *np.log2(len(ck)/D_count) return HD;\rHD_=ShannonEnt(dataSet)\rprint(\"整个数据经验熵：\",HD_) 输出结果： 整个数据经验熵： 0.9709505944546686\n条件熵 熵我们知道是什么，条件熵又是个什么鬼？条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵(conditional entropy)H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望： 设特征A有n个不同的取值{a1,a2,···,an}，根据特征A的取值将D划分为n个子集{D1,D2，···,Dn}，|Di|为Di的样本个数。记子集Di中属于Ck的样本的集合为Dik，即Dik = Di ∩ Ck，|Dik|为Dik的样本个数。于是经验条件熵的公式可以些为： 其实就是：求和（获取当前特征相同数据集在整个数据的概率 * 这个数据集中最终分类的熵）\n'''\r计算条件熵\rH(D|0) 计算某个特征列的条件熵，当年龄特征的情况下，是否房贷不确定的情况，越大越不确定\r''' def calcConditionShannon(D,index):\r#去除重复元素的index列的数组\rfeatureType=np.unique(D[:,index].reshape(1,len(D)))\rfeatureTypeCount=len(featureType)\r#获取整个样本集的个数\rD_count=len(D)\rHDA=0;\rfor i in range(featureTypeCount):\rDi=np.array([row for row in D if row[index]==featureType[i]])\rHDA+=len(Di)/D_count*ShannonEnt(Di)\rreturn HDA;\rprint(\"年龄特征条件熵\",calcConditionShannon(dataSet,0)) 输出：年龄特征条件熵 0.8879430945988998\n信息增益 信息增益=整个数据的不确定性-某个特征条件的不确定=这个特征增强了多少确定性 信息增益=经验熵-当前特征条件熵 信息增益是相对于特征而言的，信息增益越大，特征对最终的分类结果影响也就越大，我们就应该选择对最终分类结果影响最大的那个特征作为我们的分类特征。\n明确了条件熵和经验条件熵的概念。接下来，让我们说说信息增益。前面也提到了，信息增益是相对于特征而言的。所以，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即： 说了这么多概念性的东西，没有听懂也没有关系，举几个例子，再回来看一下概念，就懂了。\n以贷款申请样本数据表为例进行说明。看下年龄这一列的数据，也就是特征A1，一共有三个类别，分别是：青年、中年和老年。我们只看年龄是青年的数据，年龄是青年的数据一共有5个，所以年龄是青年的数据在训练数据集出现的概率是十五分之五，也就是三分之一。同理，年龄是中年和老年的数据在训练数据集出现的概率也都是三分之一。现在我们只看年龄是青年的数据的最终得到贷款的概率为五分之二，因为在五个数据中，只有两个数据显示拿到了最终的贷款，同理，年龄是中年和老年的数据最终得到贷款的概率分别为五分之三、五分之四。所以计算年龄的信息增益，过程如下： 同理，计算其余特征的信息增益g(D,A2)、g(D,A3)和g(D,A4)。分别为： 最后，比较特征的信息增益，由于特征A3(有自己的房子)的信息增益值最大，所以选择A3作为最优特征。\n我们已经学会了通过公式计算信息增益，接下来编写代码，计算信息增益。\n'''\r计算某个特征的信息增益\r信息增益=整个数据的不确定性-某个特征条件的不确定=这个特征增强了多少确定性\r'''\rdef calaInfoGrain(D,index):\rreturn ShannonEnt(dataSet)-calcConditionShannon(D,index)\rprint(\"年龄的信息增益\",HD_-calcConditionShannon(dataSet,0))\rprint(\"工作的信息增益\",calaInfoGrain(dataSet,1))\rfeature_count=len(dataSet[0])\rfor i in range(feature_count-1):\rprint(\"第\"+str(i)+\"个特征的信息增益\",HD_-calcConditionShannon(dataSet,i)) 输出： 年龄的信息增益 0.08300749985576883 工作的信息增益 0.32365019815155627 第0个特征的信息增益 0.08300749985576883 第1个特征的信息增益 0.32365019815155627 第2个特征的信息增益 0.4199730940219749 第3个特征的信息增益 0.36298956253708536\n对比我们自己计算的结果，发现结果完全正确！最优特征的索引值为2，也就是特征A3(有自己的房子)。\n决策树的生成 我们已经学习了从数据集构造决策树算法所需要的子功能模块，包括经验熵的计算和最优特征的选择，其工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据集被向下传递到树的分支的下一个结点。在这个结点上，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。\n构建决策树的算法有很多，比如C4.5、ID3和CART，这些算法在运行时并不总是在每次划分数据分组时都会消耗特征。由于特征数目并不是每次划分数据分组时都减少，因此这些算法在实际使用时可能引起一定的问题。目前我们并不需要考虑这个问题，只需要在算法开始运行前计算列的数目，查看算法是否使用了所有属性即可。\n决策树生成算法递归地产生决策树，直到不能继续下去未为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。\n决策树构建 ID3算法的核心是在决策树各个结点上对应信息增益准则选择特征，递归地构建决策树。具体方法是：从根结点(root node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。\nID3算法 在使用ID3构造决策树之前，我们再分析下数据。 按照第三列排好序的数据\nprint(dataSet[np.argsort(dataSet[:,2])])\r输出：\r[['0' '0' '0' '0' 'no']\r['0' '0' '0' '1' 'no']\r['0' '1' '0' '1' 'yes']\r['0' '0' '0' '0' 'no']\r['1' '0' '0' '0' 'no']\r['1' '0' '0' '1' 'no']\r['2' '1' '0' '1' 'yes']\r['2' '1' '0' '2' 'yes']\r['2' '0' '0' '0' 'no']\r['0' '1' '1' '0' 'yes']\r['1' '1' '1' '1' 'yes']\r['1' '0' '1' '2' 'yes']\r['1' '0' '1' '2' 'yes']\r['2' '0' '1' '2' 'yes']\r['2' '0' '1' '1' 'yes']] 由于特征A3(有自己的房子)的信息增益值最大，所以选择特征A3作为根结点的特征。它将训练集D划分为两个子集D1(A3取值为\"是”)和D2(A3取值为\"否”)。由于D1只有同一类的样本点，所以它成为一个叶结点，结点的类标记为“是”。 其中D1就是\n['0' '1' '1' '0' 'yes']\r['1' '1' '1' '1' 'yes']\r['1' '0' '1' '2' 'yes']\r['1' '0' '1' '2' 'yes']\r['2' '0' '1' '2' 'yes']\r['2' '0' '1' '1' 'yes']] 由于D1=1的时候只有一个分类结论yes，所以他是一个叶子节点，没有分叉 D2就是\n['0' '0' '0' '0' 'no']\r['0' '0' '0' '1' 'no']\r['0' '1' '0' '1' 'yes']\r['0' '0' '0' '0' 'no']\r['1' '0' '0' '0' 'no']\r['1' '0' '0' '1' 'no']\r['2' '1' '0' '1' 'yes']\r['2' '1' '0' '2' 'yes']\r['2' '0' '0' '0' 'no'] 对D2则需要从特征A1(年龄)，A2(有工作)和A4(信贷情况)中选择新的特征，计算各个特征的信息增益： 根据计算，选择信息增益最大的特征A2(有工作)作为结点的特征。由于A2有两个可能取值，从这一结点引出两个子结点：一个对应\"是\"(有工作)的子结点，包含3个样本，它们属于同一类，所以这是一个叶结点，类标记为\"是\"；另一个是对应\"否\"(无工作)的子结点，包含6个样本，它们也属于同一类，所以这也是一个叶结点，类标记为\"否\"。 剩余数据按照A2排序\n[['0' '0' '0' '0' 'no']\r['0' '0' '0' '1' 'no']\r['0' '0' '0' '0' 'no']\r['1' '0' '0' '0' 'no']\r['1' '0' '0' '1' 'no']\r['2' '0' '0' '0' 'no']\r['0' '1' '0' '1' 'yes']\r['2' '1' '0' '1' 'yes']\r['2' '1' '0' '2' 'yes']] 发现A2=0的结果全是no，A1等于1的全部为yes，所以没有其他分类了，到工作这里节点就结束了 可以理解为叶子节点就是结论是否贷款，分叉就是特征的值。\n编写代码构建决策树 我们使用字典存储决策树的结构，比如上小节我们分析出来的决策树，用字典可以表示为：\n{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}} 代码实现如下\n#%%\r'''\r将数据按照值指定特征列分组，，比如有房子=1的数据行和无房子=0的数据行\r{\r0:[[]]\r1:[[]]\r}\r'''\rcolLabels=[\"年龄\",\"有工作\",\"有自己的房子\",\"信贷情况\"]\rdef splitData(D,index):\rkArray=np.unique(D[:,index].reshape(1,len(D)))\r#循环多个分类，计算这个分类的熵，最后求和\rreturnJSon={};\rfor i in range(len(kArray)):\r#获取等于当前分类的数据行\rck=[row for row in D if row[index]==kArray[i]]\rreturnJSon[i]=np.array(ck)\rreturn returnJSon;\rdef createDecisionTree(D):\rbuildTree=None\r#如果传入的D没有数据或者第5列（是否贷款）只有一个分类值，就说明已经是叶子节点了,直接返回结论值\rresultUniqueArray=np.unique(D[:,4].reshape(1,len(D)))\rprint(resultUniqueArray,len(D),len(resultUniqueArray))\rif(len(D)==0 or len(resultUniqueArray)==1):\rreturn resultUniqueArray[0]\r#获取特征数\rfeature_count=D.shape[1]\r#算出每个特征的信息增益\rgrain=[calaInfoGrain(D,i)for i in range(feature_count-1)]\r#获取信息增益最大的特征值\rmaxFeatureIndex=np.argmax(grain);\r#创建一个json对象，里面有个当前特征名称的对象:比如{'有自己的房子': {}}\rbuildTree={colLabels[maxFeatureIndex]:{}};\r#循环每个独立的特征值 featureGroup=splitData(D,maxFeatureIndex)\rfor featureValue in featureGroup:\rbuildTree[colLabels[maxFeatureIndex]][featureValue]=createDecisionTree(featureGroup[featureValue])\rreturn buildTree;\rprint(createDecisionTree(dataSet)) 决策树可视化 以内graphviz简单易懂，这里使用graphviz来进行可视化 下载graphviz，选择将环境加入到PATH中。 python安装组件\npip install graphviz 代码绘制\nfrom graphviz import Digraph\rimport uuid\rdef graphDecisionTree(dot,treeNode,parentName,lineName):\rfor key in treeNode:\rif type(key)==int:\rif type(treeNode[key])==str or type(treeNode[key])==np.str_:\r#因为会出现两个yes，所以可能不能出现一个分叉而直接指向了，所以名字加上个uuid区分\rnode_name=str(treeNode[key])+str(uuid.uuid1())\rdot.node(name=node_name, label=str(treeNode[key]), color='red',fontname=\"Microsoft YaHei\")\rdot.edge(str(parentName),str(node_name), label=str(key), color='red')\relse:\rgraphDecisionTree(dot,treeNode[key],parentName,key)\relif type(treeNode[key])==dict:\rgraphDecisionTree(dot,treeNode[key],key,None)\rif type(key)==str or type(treeNode[key])==str:\rdot.node(name=key, label=str(key), color='red',fontname=\"Microsoft YaHei\")\rif parentName is not None and lineName is not None:\rdot.edge(parentName,key, label=str(lineName), color='red')\rdot = Digraph(name=\"pic\", comment=\"测试\", format=\"png\")\rgraphDecisionTree(dot,decisionTreeJson,None,None)\rdot.render(filename='my_pic',\rdirectory='.', # 当前目录\rview=True) 输出流程图 使用决策树执行分类 依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类。在执行数据分类时，需要决策树以及用于构造树的标签向量。然后，程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子结点；最后将测试数据定义为叶子结点所属的类型。在构建决策树的代码，可以看到，有个featLabels参数。它是用来干什么的？它就是用来记录各个分类结点的，在用决策树做预测的时候，我们按顺序输入需要的分类结点的属性值即可。举个例子，比如我用上述已经训练好的决策树做分类，那么我只需要提供这个人是否有房子，是否有工作这两个信息即可，无需提供冗余的信息。\n用决策树做分类的代码很简单，编写代码如下：\n#%%\r'''\r在决策树中判断传入的特征是否贷款\r'''\rdef classfiy(decisionTreeJson,featureLabel,vecTest,index):\rif type(decisionTreeJson)==str or type(decisionTreeJson)==np.str_:\rreturn decisionTreeJson\relif type(decisionTreeJson[featureLabel[index]])==dict :\rreturn classfiy(decisionTreeJson[featureLabel[index]][vecTest[index]],featureLabel,vecTest,index+1)\relse :\rreturn decisionTreeJson\rprint(\"是\" if classfiy(decisionTreeJson,featureLabel,[1,0],0)=='yes' else \"否\") 决策树的存储 使用序列化的方式即可\nimport pickle\r#写入\rwith open(filename, 'wb') as fw:\rpickle.dump(inputTree, fw)\r#读取\rfr = open(filename, 'rb')\rjson=pickle.load(fr) Sklearn之使用决策树预测隐形眼睛类型 实战背景 进入本文的正题：眼科医生是如何判断患者需要佩戴隐形眼镜的类型的？一旦理解了决策树的工作原理，我们甚至也可以帮助人们判断需要佩戴的镜片类型。\n隐形眼镜数据集是非常著名的数据集，它包含很多换着眼部状态的观察条件以及医生推荐的隐形眼镜类型。隐形眼镜类型包括硬材质(hard)、软材质(soft)以及不适合佩戴隐形眼镜(no lenses)。数据来源与UCI数据库，数据集下载地址：https://github.com/lzeqian/machinelearntry/blob/master/sklearn_decisiontree/lenses.txt\n一共有24组数据，数据的Labels依次是age、prescript、astigmatic、tearRate、class，也就是第一列是年龄，第二列是症状，第三列是是否散光，第四列是眼泪数量，第五列是最终的分类标签。数据如下图所示： 可以使用已经写好的Python程序构建决策树，不过出于继续学习的目的，本文使用Sklearn实现。\n使用Sklearn构建决策树 官方英文文档地址：http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\nsklearn.tree模块提供了决策树模型，用于解决分类问题和回归问题。方法如下图所示： 本次实战内容使用的是DecisionTreeClassifier和export_graphviz，前者用于决策树构建，后者用于决策树可视化。\nDecisionTreeClassifier构建决策树：\n让我们先看下DecisionTreeClassifier这个函数，一共有12个参数：\n参数说明如下：\ncriterion：特征选择标准，可选参数，默认是gini，可以设置为entropy。gini是基尼不纯度，是将来自集合的某种结果随机应用于某一数据项的预期误差率，是一种基于统计的思想。entropy是香农熵，也就是上篇文章讲过的内容，是一种基于信息论的思想。Sklearn把gini设为默认参数，应该也是做了相应的斟酌的，精度也许更高些？ID3算法使用的是entropy，CART算法使用的则是gini。 splitter：特征划分点选择标准，可选参数，默认是best，可以设置为random。每个结点的选择策略。best参数是根据算法选择最佳的切分特征，例如gini、entropy。random随机的在部分划分点中找局部最优的划分点。默认的\"best\"适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐\"random\"。 -max_features：划分时考虑的最大特征数，可选参数，默认是None。寻找最佳切分时考虑的最大特征数(n_features为总共的特征数)，有如下6种情况： 如果max_features是整型的数，则考虑max_features个特征； 如果max_features是浮点型的数，则考虑int(max_features * n_features)个特征； 如果max_features设为auto，那么max_features = sqrt(n_features)； 如果max_features设为sqrt，那么max_featrues = sqrt(n_features)，跟auto一样； 如果max_features设为log2，那么max_features = log2(n_features)； 如果max_features设为None，那么max_features = n_features，也就是所有特征都用。 一般来说，如果样本特征数不多，比如小于50，我们用默认的\"None\"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。 max_depth：决策树最大深，可选参数，默认是None。这个参数是这是树的层数的。层数的概念就是，比如在贷款的例子中，决策树的层数是2层。如果这个参数设置为None，那么决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。或者如果设置了min_samples_slipt参数，那么直到少于min_smaples_split个样本为止。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。 min_samples_split：内部节点再划分所需最小样本数，可选参数，默认是2。这个值限制了子树继续划分的条件。如果min_samples_split为整数，那么在切分内部结点的时候，min_samples_split作为最小的样本数，也就是说，如果样本已经少于min_samples_split个样本，则停止继续切分。如果min_samples_split为浮点数，那么min_samples_split就是一个百分比，ceil(min_samples_split * n_samples)，数是向上取整的。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 min_samples_leaf：叶子节点最少样本数，可选参数，默认是1。这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。叶结点需要最少的样本数，也就是最后到叶结点，需要多少个样本才能算一个叶结点。如果设置为1，哪怕这个类别只有1个样本，决策树也会构建出来。如果min_samples_leaf是整数，那么min_samples_leaf作为最小的样本数。如果是浮点数，那么min_samples_leaf就是一个百分比，同上，celi(min_samples_leaf * n_samples)，数是向上取整的。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 min_weight_fraction_leaf：叶子节点最小的样本权重和，可选参数，默认是0。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。 max_leaf_nodes：最大叶子节点数，可选参数，默认是None。通过限制最大叶子节点数，可以防止过拟合。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。 class_weight：类别权重，可选参数，默认是None，也可以字典、字典列表、balanced。指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。类别的权重可以通过{class_label：weight}这样的格式给出，这里可以自己指定各个样本的权重，或者用balanced，如果使用balanced，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。当然，如果你的样本类别分布没有明显的偏倚，则可以不管这个参数，选择默认的None。 random_state：可选参数，默认是None。随机数种子。如果是证书，那么random_state会作为随机数生成器的随机数种子。随机数种子，如果没有设置随机数，随机出来的数与当前系统时间有关，每个时刻都是不同的。如果设置了随机数种子，那么相同随机数种子，不同时刻产生的随机数也是相同的。如果是RandomState instance，那么random_state是随机数生成器。如果为None，则随机数生成器使用np.random。 min_impurity_split：节点划分最小不纯度,可选参数，默认是1e-7。这是个阈值，这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。 presort：数据是否预排序，可选参数，默认为False，这个值是布尔值，默认是False不排序。一般来说，如果样本量少或者限制了一个深度很小的决策树，设置为true可以让划分点选择更加快，决策树建立的更加快。如果样本量太大的话，反而没有什么好处。问题是样本量少的时候，我速度本来就不慢。所以这个值一般懒得理它就可以了。 除了这些参数要注意以外，其他在调参时的注意点有：\n当样本数量少但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比特征数多一些会比较容易建立健壮的模型 如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度会大大减小。再来拟合决策树模型效果会好。 推荐多用决策树的可视化，同时先限制决策树的深度，这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。 在训练模型时，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，就要考虑用class_weight来限制模型过于偏向样本多的类别。 决策树的数组使用的是numpy的float32类型，如果训练数据不是这样的格式，算法会先做copy再运行。 如果输入的样本矩阵是稀疏的，推荐在拟合前调用csc_matrix稀疏化，在预测前调用csr_matrix稀疏化。 sklearn.tree.DecisionTreeClassifier()提供了一些方法供我们使用，如下图所示： 了解到这些，我们就可以编写代码了。 注意： 因为在fit()函数不能接收string类型的数据，通过打印的信息可以看到，数据都是string类型的。在使用fit()函数之前，我们需要对数据集进行编码，这里可以使用两种方法：\nLabelEncoder ：将字符串转换为增量值 OneHotEncoder：使用One-of-K算法将字符串转换为整数 为了对string类型的数据序列化，需要先生成pandas数据，这样方便我们的序列化工作。这里我使用的方法是，原始数据-\u003e字典-\u003epandas数据，编写代码如下：\n#%%\rimport numpy as np\rimport pandas as pd\rfr = open('lenses.txt')\rlenses = np.array([inst.strip().split('\\t') for inst in fr.readlines()])\rprint(lenses)\r#四个特征一列是：年龄，第二列是症状，第三列是是否散光，第四列是眼泪数量\r#，第五列是最终的分类标签，隐形眼镜类型包括硬材质(hard)、软材质(soft)以及不适合佩戴隐形眼镜(no lenses)\rlensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']\r#最终分类在最后一列\rlenses_target = [each[-1] for each in lenses] print(lenses_target)\r#%%\r#组装成带有表头的数据格式\rlensesDataFrame=np.concatenate((np.array([lensesLabels]),lenses[:,0:4]))\r'''\r注意dataframe的用法\rdf['a']#取a列\rdf[['a','b']]#取a、b列\r默认的表头是0，1，2这样的序号，如果需要自定义表头需要定义json\r{\r\"age\":[young,pre],\r\"prescript\":[\"myope\",\"myope\"]\r}\r'''\rjsonData= {l:lenses[:,i]for i,l in enumerate(lensesLabels)}\rlenses_pd = pd.DataFrame(jsonData) #生成pandas.DataFrame\rprint(lenses_pd)\r#%%\rfrom sklearn.preprocessing import LabelEncoder\r# 将所有的label 比如young转换成0，pre转为成1这样的数字编码\rle = LabelEncoder() #传入一个一维的数字，在这个数组里，相同的字符串转换为相同的数字\rfor i in lenses_pd.columns:\rlenses_pd[i]=le.fit_transform(lenses_pd[i])\rprint(lenses_pd) 使用Graphviz可视化决策树 graphviz之前已经安装过了，安装一个pydotplus库\npip3 install pydotplus 编写代码\n#使用sklearn决策树\rfrom sklearn import tree\rimport pydotplus\rfrom io import StringIO\rclf = tree.DecisionTreeClassifier(max_depth = 4) #创建DecisionTreeClassifier()类\rclf = clf.fit(lenses_pd.values.tolist(), lenses_target) #使用数据，构建决策树\rdot_data = StringIO()\rtree.export_graphviz(clf, out_file = dot_data, #绘制决策树\rfeature_names = lenses_pd.keys(),\rclass_names = clf.classes_,\rfilled=True, rounded=True,\rspecial_characters=True)\rgraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\rgraph.write_pdf(\"tree.pdf\") 运行代码，在该python文件保存的相同目录下，会生成一个名为tree的PDF文件，打开文件，我们就可以看到决策树的可视化效果图了。 确定好决策树之后，我们就可以做预测了。可以根据自己的眼睛情况和年龄等特征，看一看自己适合何种材质的隐形眼镜。使用如下代码就可以看到预测结果：\nprint(clf.predict([[1,1,1,0]])) 总结 决策树的一些优点：\n易于理解和解释。决策树可以可视化。 几乎不需要数据预处理。其他方法经常需要数据标准化，创建虚拟变量和删除缺失值。决策树还不支持缺失值。 使用树的花费（例如预测数据）是训练数据点(data points)数量的对数。 可以同时处理数值变量和分类变量。其他方法大都适用于分析一种变量的集合。 可以处理多值输出变量问题。 使用白盒模型。如果一个情况被观察到，使用逻辑判断容易表示这种规则。相反，如果是黑盒模型（例如人工神经网络），结果会非常难解释。 即使对真实模型来说，假设无效的情况下，也可以较好的适用。 决策树的一些缺点：\n决策树学习可能创建一个过于复杂的树，并不能很好的预测数据。也就是过拟合。修剪机制（现在不支持），设置一个叶子节点需要的最小样本数量，或者数的最大深度，可以避免过拟合。 决策树可能是不稳定的，因为即使非常小的变异，可能会产生一颗完全不同的树。这个问题通过decision trees with an ensemble来缓解。 概念难以学习，因为决策树没有很好的解释他们，例如，XOR, parity or multiplexer problems。 如果某些分类占优势，决策树将会创建一棵有偏差的树。因此，建议在训练之前，先抽样使样本均衡。 其他：",
    "description": "决策树 决策树是什么？决策树(decision tree)是一种基本的分类与回归方法。举个通俗易懂的例子，如下图所示的流程图就是一个决策树，长方形代表判断模块(decision block)，椭圆形成代表终止模块(terminating block)，表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作为分支(branch)，它可以达到另一个判断模块或者终止模块。我们还可以这样理解，分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。蒙圈没？？如下图所示的决策树，长方形和椭圆形都是结点。长方形的结点属于内部结点，椭圆形的结点属于叶结点，从结点引出的左右箭头就是有向边。而最上面的结点就是决策树的根结点(root node)。这样，结点说法就与模块说法对应上了，理解就好。\n本文大部分文字转载自https://cuijiahua.com/blog/2017/11/ml_2_decision_tree_1.html，代码和部分原创\n我们回到这个流程图，对，你没看错，这就是一个假想的相亲对象分类系统。它首先检测相亲对方是否有房。如果有房，则对于这个相亲对象可以考虑进一步接触。如果没有房，则观察相亲对象是否有上进心，如果没有，直接Say Goodbye，此时可以说：“你人很好，但是我们不合适。“如果有，则可以把这个相亲对象列入候选名单，好听点叫候选名单，有点瑕疵地讲，那就是备胎。\n不过这只是个简单的相亲对象分类系统，只是做了简单的分类。真实情况可能要复杂得多，考虑因素也可以是五花八门。脾气好吗？会做饭吗？愿意做家务吗？家里几个孩子？父母是干什么的？天啊，我不想再说下去了，想想都可怕。\n我们可以把决策树看成一个if-then规则的集合，将决策树转换成if-then规则的过程是这样的：由决策树的根结点(root node)到叶结点(leaf node)的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：互斥并且完备。这就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。这里所覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。\n使用决策树做预测需要以下过程：\n收集数据：可以使用任何方法。比如想构建一个相亲系统，我们可以从媒婆那里，或者通过采访相亲对象获取数据。根据他们考虑的因素和最终的选择结果，就可以得到一些供我们利用的数据了。 准备数据：收集完的数据，我们要进行整理，将这些所有收集的信息按照一定规则整理出来，并排版，方便我们进行后续处理。 分析数据：可以使用任何方法，决策树构造完成之后，我们可以检查决策树图形是否符合预期。 训练算法：这个过程也就是构造决策树，同样也可以说是决策树学习，就是构造一个决策树的数据结构。 测试算法：使用经验树计算错误率。当错误率达到了可接收范围，这个决策树就可以投放使用了。 使用算法：此步骤可以使用适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。 决策树的构建的准备工作 使用决策树做预测的每一步骤都很重要，数据收集不到位，将会导致没有足够的特征让我们构建错误率低的决策树。数据特征充足，但是不知道用哪些特征好，将会导致无法构建出分类效果好的决策树模型。从算法方面看，决策树的构建是我们的核心内容。\n决策树要如何构建呢？通常，这一过程可以概括为3个步骤：特征选择、决策树的生成和决策树的修剪。\n特征选择 特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率，如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的标准是信息增益(information gain)或信息增益比，为了简单，本文使用信息增益作为选择特征的标准。那么，什么是信息增益？在讲解信息增益之前，让我们看一组实例，贷款申请样本数据表。\n希望通过所给的训练数据学习一个贷款申请的决策树，用于对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。\n特征选择就是决定用哪个特征来划分特征空间。比如，我们通过上述数据表得到两个可能的决策树，分别由两个不同特征的根结点构成。 图(a)所示的根结点的特征是年龄，有3个取值，对应于不同的取值有不同的子结点。图(b)所示的根节点的特征是工作，有2个取值，对应于不同的取值有不同的子结点。两个决策树都可以从此延续下去。问题是：究竟选择哪个特征更好些？这就要求确定选择特征的准则。直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益就能够很好地表示这一直观的准则。\n什么是信息增益呢？在划分数据集之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。 信息增益=整个数据的不确定性-某个特征条件的不确定=这个特征增强了多少确定性\n那怎么确定数据的不确定性了，引出了香农熵的概念",
    "tags": [],
    "title": "机器学习实战教程（六）：决策树",
    "uri": "/docs/programming/ai/machine_learning/algorithms/action_06_decidetree/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 核心算法",
    "content": "一 简介 朴素贝叶斯算法是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题。该算法的优点在于简单易懂、学习效率高、在某些领域的分类问题中能够与决策树、神经网络相媲美。但由于该算法以自变量之间的独立（条件特征独立）性和连续变量的正态性假设为前提，就会导致算法精度在某种程度上受影响。\n二 朴素贝叶斯理论 把样本空间划分成容易研究的几种情况。\n全概率公式（由原因到结果）考察在每一种情况下事件B发生的概率，计算B的概率。 Bayes公式（由结果到原因）在事件B发生的条件下，考察每种情况出现的条件概率。 条件概率 公式推导 我们需要了解什么是条件概率(Conditional probability)，就是指在事件B发生的情况下，事件A发生的概率，用P(A|B)来表示。 根据文氏图，可以很清楚地看到在事件B发生的情况下，事件A发生的概率就是P(A∩B)除以P(B)。 因此， 同理根据条件概率知道A发生时B的概率 转换下 所以 即 这就是条件概率的计算公式。\n计算案例 引例.掷一枚质地均匀的骰子，\n向上的点数是偶数”的概率是多少? 向上的点数是偶数并且大于4”的概率是多少? 例2.某种动物出生之后活到20岁的概率为0.7，活到25岁的概率为0.56求现年为20岁的这种动物活到25岁的概率。 全概率公式 首先要理解什么是“样本空间的划分”【又称“完备事件群”。】 我们将满足（假定样本空间Ω，是两个事件A与A’的和）\nA1,A2,…,An是一组两两互斥的事件 A1 U A2 U,…,An=Ω 这样的一组事件称为一个“完备事件群”。简而言之，就是事件之间两两互斥，所有事件的并集是整个样本空间（必然事件）。 B在整个Ω中发生的概率是： 公式推导 假定样本空间S，是两个事件A与A’的和。 上图中，红色部分是事件A，绿色部分是事件A’，它们共同构成了样本空间S。 在这种情况下，事件B可以划分成两个部分。 即 在上一节的推导当中，我们已知 所以， 就是全概率公式。它的含义是，如果A和A’构成样本空间的一个划分，那么事件B的概率，就等于A和A’的概率分别乘以B对这两个事件的条件概率之和。\n计算案例 例1：有一批同一型号的产品，已知其中由一厂生产的占30%，二厂生产的占50%，三厂生产的占20%，又知这三个厂的产品次品率分别为2%，1%，1%，问从这批产品中任取一件是次品的概率是多少？ 例2：有某电子设备制造厂所用的元件是由三家元件制造厂提供的。根据以往的记录，有以下的数据： 设这三家工厂的产品在仓库中是均匀混合的，且无区别的标志。\n在仓库中随机地取一只元件，求它是次品的概率； 在仓库中随机地取一只元件，若已知取到的是次品，分析此次品出自何厂，需求出此次品有三家工厂生产的概率分别是多少。试求这些概率。 贝叶斯 贝叶斯决策 假设现在我们有一个数据集，它由两类数据组成，数据分布如下图所示： 我们现在用p1(x,y)表示数据点(x,y)属于类别1(图中红色圆点表示的类别)的概率，用p2(x,y)表示数据点(x,y)属于类别2(图中蓝色三角形表示的类别)的概率，那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：\n如果p1(x,y)\u003ep2(x,y)，那么类别为1 如果p1(x,y)\u003cp2(x,y)，那么类别为2 也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。已经了解了贝叶斯决策理论的核心思想，贝叶斯的实现就是如何计算p1和p2概率。 贝叶斯推导 对条件概率公式进行变形，可以得到如下形式： 通过全概率公式 得到条件概率的另外一种变形 在条件概率 我们把P(A)称为\"先验概率\"（Prior probability），即在B事件发生之前，我们对A事件概率的一个判断。 P(A|B)称为\"后验概率\"（Posterior probability），即在B事件发生之后，我们对A事件概率的重新评估。 P(B|A)/P(B)称为\"可能性函数\"（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。 所以，条件概率可以理解成下面的式子：\n后验概率　＝　先验概率 ｘ 调整因子 这就是贝叶斯推断的含义。我们先预估一个\"先验概率\"，然后加入实验结果，看这个实验到底是增强还是削弱了\"先验概率\"，由此得到更接近事实的\"后验概率\"。\n在这里，如果\"可能性函数\"P(B|A)/P(B)\u003e1，意味着\"先验概率\"被增强，事件A的发生的可能性变大；如果\"可能性函数\"=1，意味着B事件无助于判断事件A的可能性；如果\"可能性函数\"\u003c1，意味着\"先验概率\"被削弱，事件A的可能性变小。\n计算案例 为了加深对贝叶斯推断的理解，我们举一个例子。 两个一模一样的碗，一号碗有30颗水果糖和10颗巧克力糖，二号碗有水果糖和巧克力糖各20颗。现在随机选择一个碗，从中摸出一颗糖，发现是水果糖。请问这颗水果糖来自一号碗的概率有多大？\n我们假定，H1表示一号碗，H2表示二号碗。由于这两个碗是一样的，所以P(H1)=P(H2)，也就是说，在取出水果糖之前，这两个碗被选中的概率相同。因此，P(H1)=0.5，我们把这个概率就叫做\"先验概率\"，即没有做实验之前，来自一号碗的概率是0.5。\n再假定，E表示水果糖，所以问题就变成了在已知E的情况下，来自一号碗的概率有多大，即求P(H1|E)。我们把这个概率叫做\"后验概率\"，即在E事件发生之后，对P(H1)的修正。\n根据条件概率公式，得到 已知，P(H1)等于0.5，P(E|H1)为一号碗中取出水果糖的概率，等于30÷(30+10)=0.75，那么求出P(E)就可以得到答案。根据全概率公式， 所以， 将数字代入原方程，得到 这表明，来自一号碗的概率是0.6。也就是说，取出水果糖之后，H1事件的可能性得到了增强。\n同时再思考一个问题，在使用该算法的时候，如果不需要知道具体的类别概率，即上面P(H1|E)=0.6，只需要知道所属类别，即来自一号碗，我们有必要计算P(E)这个全概率吗？要知道我们只需要比较 P(H1|E)和P(H2|E)的大小，找到那个最大的概率就可以。既然如此，两者的分母都是相同的，那我们只需要比较分子即可。即比较P(E|H1)P(H1)和P(E|H2)P(H2)的大小，所以为了减少计算量，全概率公式在实际编程中可以不使用。 对以往数据分析结果表明，当机器调整得良好时，产品的合格率为90%，而当机器发生某一故障时，其合格率为30%。每天早上机器开动时，机器调整良好的概率为75%，试求已知某日早上第一件产品是合格品时，机器调整良好的概率是多少？\n某地区居民的肝癌发病率为0.0004，现用甲胎蛋白法进行普查。医学研究表明，化验结果是有错检的可能的。已知患有肝癌的人其化验结果99%呈阳性（有病），而没患肝癌的人其化验结果99.9%呈阴性（无病） 现某人的检查结果呈阳性，问他真的患有肝癌的概率是多少？。 朴素贝叶斯 朴素贝叶斯是一种简单但极为强大的预测建模算法，之所以称为朴素贝叶斯，是因为他假设的每个特征都是独立的。 如果有多个特征条件下预测某个分类，因为假设是每个特征都是独立的所以可以分解为单个特征下分类的概率计算的结果。 比如，收到了一份垃圾邮件，\n是房地产的概率， 是贷款的概率 是房地产和贷款的概率 简化为： 朴素贝叶斯模型由两种类型的概率组成 1、每个类别的概率P(CJ) 2、每个属性的条件概率P(AI|CJ) 公式推导 根据贝叶斯公式(假设特征X（多个X1,X2….Xn）,对应的分类结果Y) P(Y|X)=P(Y) * (P(X|Y) / P(X)) 因为特征X是多维 P(Y|X)=P(Y) * (P((X1,X2,….Xn)|Y) / P(X1,X2,….Xn)) 独立性拆分 P(Y|(X1,X2…Xn))=P(Y) * (P(X1|Y)P(X2|Y)….P(Xn|Y) / P(X1)P(X2)….P(Xn)) 假设 某个分类的结果Y=男|女，X特征表示（身高，体重） 此时如果给出某个人的X特征要判断到底是男和女，实际上就是比较 给出的具体身高和体重对应是男和女的概率谁大 即可 由于公式 P(Y|(X1,X2…Xn))=P(Y) * (P(X1|Y)P(X2|Y)….P(Xn|Y) / P(X1)P(X2)….P(Xn)) 男或者女情况下 P(Y) 【男和女的概率预测的就是0.5和0.5】 和 P(X1)P(X2)….P(Xn))都是相同的，实际上就只需要比较 P(X1|Y)P(X2|Y)….P(Xn|Y） 的概率大小即可，谁大就是谁的分类结果。\n计算案例 离散数据如下： 计算：身高为高，体重为中，鞋码为中这个人是男还是女？ X1:表示身高 X2:表示体重 X3:表示鞋码 Y:代表类别 Y1表示男，Y2表示女。未知表示Yj P(Yj|X1,X2,X3) = P(Yj) * (P(X1|Yj)P(X2|Yj)….P(Xn|Yj) / P(X1)P(X2)….P(Xn)) 由于先验概率和分母两个分类都相同，只比较分子\nP(X1X2X3|Yj) = P(X1|Yj)P(X2|Yj)….P(Xn|Yj)\n假设类别为j=1 Y1是男\nP(X1|Y1) = 2/4 就是男生（1-4行）中身高是高（1-2行）的概率\rP(X2|Y1) = 2/4\rP(X3|Y1) = 1/4\rP(X1|Y1)P(X2|Y1) P(X3|Y1) = 2/4*2/4*1/4 = 1/16 假设类别为j=2 Y1是女\nP(X1|Y2) = 0 就是女生（5-8行）中身高是高（没有）的概率\rP(X2|Y2) = 2/4\rP(X3|Y2) = 2/4\rP(X1|Y2) P(X2|Y2)P(X3|Y2)) = 0*2/4*2/4 = 0\rC1\u003eC2 由此推论：身高为高，体重为中，鞋码为中这个人是男 该案例中由于给定的身高都是确定的数据，判断起来比较简单，如果是连续性数据\n1、离散型：有些随机变量它全部可能取到的不相同的值是有限个或可列无限多个，也可以说概率1以一定的规律分布在各个可能值上。 2、连续型：随机变量X的取值不可以逐个列举，只可取数轴某一区间内的任一点。\n需求：身高180，体重120，鞋码41该人是男还是女？ 公式还是上面的公式，但身高，体重，鞋码是连续变量，不能采用离散型方法计算概率。假设身高，体重，鞋码是正态分布通过样本计算出均值和方差，也就得到了正态分布的密度函数，有了密度函数，可以算出一点的密度涵数值。如男性平均身高179.5,标准差3.697正态分布，高180的概率是0.1069\npython实现\n#%%\rimport numpy as np\rimport pandas as pd\rdf = pd.read_excel('连续性.xlsx',sheet_name=\"Sheet1\",index_col=0)\r# 计算男女在每个特征维度的方差和均值\rdf2 = df.groupby(\"性别\").agg([np.mean, np.var])\rprint(df2)\r#%%\rmale_high_mean = df2.loc[\"男\",\"身高\"][\"mean\"]\rmale_high_var = df2.loc[\"男\",\"身高\"][\"var\"]\rmale_weight_mean = df2.loc[\"男\",\"体重\"][\"mean\"]\rmale_weight_var = df2.loc[\"男\",\"体重\"][\"var\"]\rmale_code_mean = df2.loc[\"男\",\"鞋码\"][\"mean\"]\rmale_code_var = df2.loc[\"男\",\"鞋码\"][\"var\"]\rfrom scipy import stats\r# pdf ——概率密度函数标准形式是,算出在男性中身高180的概率\rmale_high = stats.norm.pdf(180,male_high_mean,male_high_var)\r# 算出在男性中体重120的概率\rmale_weight = stats.norm.pdf(120, male_weight_mean, male_weight_var)\r# 算出在男性中鞋码41的概率\rmale_code = stats.norm.pdf(41, male_code_mean, male_code_var)\rfz=(male_high*male_weight*male_code)\rprint(fz)\rfemale_high_mean = df2.loc[\"女\",\"身高\"][\"mean\"]\rfemale_high_var = df2.loc[\"女\",\"身高\"][\"var\"]\rfemale_weight_mean = df2.loc[\"女\",\"体重\"][\"mean\"]\rfemale_weight_var = df2.loc[\"女\",\"体重\"][\"var\"]\rfemale_code_mean = df2.loc[\"女\",\"鞋码\"][\"mean\"]\rfemale_code_var = df2.loc[\"女\",\"鞋码\"][\"var\"]\r#计算在女性分类中的三种特征的概率\rfemale_high = stats.norm.pdf(180, female_high_mean, female_high_var)\rfemale_weight = stats.norm.pdf(120, female_weight_mean, female_weight_var)\rfemale_code = stats.norm.pdf(41, female_code_mean, female_code_var)\rffz=female_high*female_weight*female_code\rprint(ffz)\rif fz\u003effz:\rprint(\"男性\")\relse :\rprint(\"女性\") 三 实践言论过滤器 TF-IDF特征向量 TF-IDF原理 F-IDF特征向量是一种将文本数据转换为数值型表示的方式，其中每个维度代表一个单词，每个样本（也即一个文本）都被表示为一个向量。\nTF-IDF是一种用于信息检索与文本挖掘的常用加权技术\nTF-IDF（Term Frequency-Inverse Document Frequency）是一种常用于信息检索与文本数据分析的算法，用于衡量一个词语对于一个文档或一个文本集合中的其中一份文本的重要程度。\nTF（Term Frequency）指的是词频，代表该词在某个文本中出现的次数。IDF（Inverse Document Frequency）指的是逆文档频率，用于衡量该词在整个文本集合中出现的频率，即该词在多少份文本中出现过。如果一个词在文本集合中越常见，它的逆文档频率就越低，说明该词对于区分不同文本的重要性就越小。\n因此，TF-IDF值是通过将词频（TF）与逆文档频率（IDF）相乘得到的。对于单个文本而言，TF-IDF值越高，则代表该词对于该文本的重要性越大，越能够代表该文本所表示的主题；而在整个文本集合中，TF-IDF值越高，则代表该词能够很好地区分不同的文本，越能够代表该文本集合所表示的主题。\nTF-IDF算法在信息检索、文本分类、关键词提取等领域有着广泛的应用\n假设有以下两个文本：\nThe quick brown fox jumps over the lazy dog. The brown fox is quick and the blue dog is lazy. 首先，需要将这些文本进行预处理，包括去除标点符号、停用词（the ，is and）等，并将每个文本转换为词语列表。针对这两个文本，可能得到以下词语列表：\n['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\r['brown', 'fox', 'quick', 'blue', 'dog', 'lazy'] 接下来，需要计算每个词语在每个样本中出现的次数，即词频（TF, term frequency）。这个过程可以使用CountVectorizer类实现。以第一个样本为例，其词频向量为：\n[1, 1, 1, 1, 1, 1] 即’quick’、‘brown’、‘fox’、‘jumps’、’lazy’、‘dog’在第一个样本中均出现了1次。 接着，需要计算逆文档频率（IDF, inverse document frequency），用于衡量每个词语的重要性。IDF的计算公式为： 其中，N表示文档总数，这里就是2个文本，df(t)表示包含词语t的文档数量。将以上面两个文本为例进行计算，得到各个词语的IDF值：\n[0.0, 0.0, 0.0, 0.6931471805599453, 0.0, 0.0]\r[0.0, 0.0, 0.0, 0.6931471805599453, 0.0, 0.0] 其中，‘jumps’在第一个样本中只出现了1次，在所有文档中也只出现了1次，因此其IDF值为 log（2/1）=0.6931。而’quick’、‘brown’、‘fox’、’lazy’、‘dog’在两个文档中都出现了，因此它们的IDF值为0。\n最后，需要将每个文本的TF向量与对应的IDF向量相乘，得到TF-IDF特征向量。以第一个文本为例，其TF-IDF向量为：\n[0.0, 0.0, 0.0, 0.6931471805599453, 0.0, 0.0] 是因为’jumps’在第一个文本中出现了1次，而且在所有文档中也只出现了1次，因此其TF-IDF值为 1*log(2/1)其他词语的TF-IDF值均为0。\n以此类推，可以得到所有文本的TF-IDF特征向量。需要注意的是，每个文本的特征向量维度通常是一样的，因此在计算TF-IDF时需要遍历所有文本。\nTfidfVectorizer和CountVectorizer区别 TfidfVectorizer计算的是词语在文本中的重要程度，即TF-IDF值。 CountVectorizer只计算词语在文本中出现的次数。\n以下是使用实际数据来说明 TfidfVectorizer 和 CountVectorizer 的区别：\n假设我们有以下三篇文档：\n文档A：天气晴朗，温度适宜，阳光明媚。 文档B：天气多云，温度适宜，偶有小雨。 文档C：天气阴天，温度偏低，有雨。 我们可以使用 TfidfVectorizer 和 CountVectorizer 对这三篇文档进行特征向量化，得到它们的词频矩阵。\n具体来说，使用 CountVectorizer 可以得到以下的词频矩阵：\n词汇 文档A 文档B 文档C 天气 1 1 1 温度 1 1 1 适宜 1 1 0 阳光明媚 1 0 0 多云 0 1 0 偶有小雨 0 1 0 阴天 0 0 1 偏低 0 0 1 有雨 0 0 1 而使用 TfidfVectorizer 可以得到以下的词频矩阵：\n词汇 文档A 文档B 文档C 天气 0.00 0.00 0.58 温度 0.42 0.42 0.42 适宜 0.58 0.58 0.00 阳光明媚 0.81 0.00 0.00 多云 0.00 0.81 0.00 偶有小雨 0.00 0.81 0.00 阴天 0.00 0.00 0.58 偏低 0.00 0.00 0.81 有雨 0.00 0.00 0.58 从上面的词频矩阵可以看出，CountVectorizer 只考虑了每种词汇在当前文档中出现的频率，而 TfidfVectorizer 则同时考虑了某一词汇在当前训练文本中出现的频率以及包含这个词汇的其它训练文本数目的倒数，因此 TfidfVectorizer 更能够反映出不同文档之间的差异性。\nTfidfVectorizer和CountVectorizer实例 CountVectorizer统计词频\n#%%\rpostingList=['my my dog has flea problems help please', #切分的词条\r'maybe not take him to dog park stupid',\r'my dalmation is so cute I love him',\r'stop posting stupid worthless garbage',\r'mr licks ate my steak how to stop him',\r'quit buying worthless dog food stupid']\rclassVec = [0,1,0,1,0,1] #类别标签向量，1代表侮辱性词汇，0代表不是\rfrom sklearn.feature_extraction.text import CountVectorizer\r# 初始化CountVectorizer并进行文本特征提取\rvectorizer = CountVectorizer()\rX = vectorizer.fit_transform(postingList)\r# 显示特征向量和对应的单词\rprint(X.toarray())\rprint(vectorizer.get_feature_names()) 输出\n[[0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 2 0 0 1 0 1 0 0 0 0 0 0 0 0]\r[0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0]\r[0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0]\r[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1]\r[1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0]\r[0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1]]\r['ate', 'buying', 'cute', 'dalmation', 'dog', 'flea', 'food', 'garbage', 'has', 'help', 'him', 'how', 'is', 'licks', 'love', 'maybe', 'mr', 'my', 'not', 'park', 'please', 'posting', 'problems', 'quit', 'so', 'steak', 'stop', 'stupid', 'take', 'to', 'worthless']\r12 注意，是将所有的单词去重复后作为特征列，数据行就是当前的文档，行中的数据就是在这个特征单词上出现的次数\nTfidfVectorizer统计tf-idf\n#%%\rfrom sklearn.feature_extraction.text import TfidfVectorizer\r# 初始化TfidfVectorizer\rtvectorizer = TfidfVectorizer(stop_words='english')\r# 转换文本数据到词袋模型\rX_train = tvectorizer.fit_transform(postingList)\r# 显示特征向量和对应的单词\rprint(X_train.toarray())\rprint(vectorizer.get_feature_names()) 输出\n[[0. 0. 0. 0. 0.37115593 0.53611046\r0. 0. 0.53611046 0. 0. 0.\r0. 0. 0. 0.53611046 0. 0.\r0. 0. 0. ]\r[0. 0. 0. 0. 0.40249409 0.\r0. 0. 0. 0. 0. 0.58137639\r0. 0.58137639 0. 0. 0. 0.\r0. 0.40249409 0. ]\r[0. 0. 0.57735027 0.57735027 0. 0.\r0. 0. 0. 0. 0.57735027 0.\r0. 0. 0. 0. 0. 0.\r0. 0. 0. ]\r[0. 0. 0. 0. 0. 0.\r0. 0.51136725 0. 0. 0. 0.\r0. 0. 0.51136725 0. 0. 0.\r0.41932846 0.3540259 0.41932846]\r[0.46262479 0. 0. 0. 0. 0.\r0. 0. 0. 0.46262479 0. 0.\r0.46262479 0. 0. 0. 0. 0.46262479\r0.37935895 0. 0. ]\r[0. 0.46468841 0. 0. 0.32170956 0.\r0.46468841 0. 0. 0. 0. 0.\r0. 0. 0. 0. 0.46468841 0.\r0. 0.32170956 0.38105114]]\r['ate', 'buying', 'cute', 'dalmation', 'dog', 'flea', 'food', 'garbage', 'has', 'help', 'him', 'how', 'is', 'licks', 'love', 'maybe', 'mr', 'my', 'not', 'park', 'please', 'posting', 'problems', 'quit', 'so', 'steak', 'stop', 'stupid', 'take', 'to', 'worthless'] 训练朴素贝叶斯分类器 我们先得到词条向量\nimport numpy as np\rpostingList=['my dog has flea problems help please', #切分的词条\r'maybe not take him to dog park stupid',\r'my dalmation is so cute I love him',\r'stop posting stupid worthless garbage',\r'mr licks ate my steak how to stop him',\r'quit buying worthless dog food stupid']\rclassVec = np.array([0,1,0,1,0,1]) from sklearn.feature_extraction.text import CountVectorizer\r# 初始化CountVectorizer并进行文本特征提取\rvectorizer = CountVectorizer()\rX = vectorizer.fit_transform(postingList)\r# 显示特征向量和对应的单词\rv=np.array(X.toarray())\rprint(v)\rfn=np.array(vectorizer.get_feature_names())\rprint(fn) 输出\n[[0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0]\r[0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0]\r[0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0]\r[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1]\r[1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0]\r[0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1]]\r['ate' 'buying' 'cute' 'dalmation' 'dog' 'flea' 'food' 'garbage' 'has'\r'help' 'him' 'how' 'is' 'licks' 'love' 'maybe' 'mr' 'my' 'not' 'park'\r'please' 'posting' 'problems' 'quit' 'so' 'steak' 'stop' 'stupid' 'take'\r'to' 'worthless'] 接下来，我们就可以通过词条向量训练朴素贝叶斯分类器。\n\"\"\"\r通过传入单词向量和分类结果训练数据集获取到每个特征在不同分类下的条件概率，以及对应分类的先验概率。\r利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，\r即计算p(w0|1)p(w1|1)p(w2|1)。\r如果其中有一个概率值为0，那么最后相乘的结果也为0\r这样是不合理的，为了降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。这种做法就叫做拉普拉斯平滑(Laplace Smoothing)又被称为加1平滑，\r是比较常用的平滑方法，它就是为了解决0概率问题，具体参考拉普拉斯平滑目录。\r\"\"\"\rdef trainData(vecList,classVec):\r#获取先验概率P(侮辱类), P(非侮辱类)=1-P(侮辱类)\rPϹ侮辱类先验Ͻ=np.sum(classVec)/len(classVec)\r#找到所有classVec==0非侮辱类索引行并取得数据行。\rvec0=vecList[np.where(classVec==0)]\r#找到所有classVec==1侮辱类索引行并取得数据行。\rvec1=vecList[np.where(classVec==1)]\r#设置拉普拉斯平滑因子为1: ,分类的种类是2中所有分子+1，分母+2\ra=1\r#让分子都加上1\rvec0=np.add(vec0,a)\rvec1=np.add(vec1,a)\r#计算每个特征在对应分类下的条件概率，分母加上2\rPϹ特征l非侮辱类Ͻ=np.sum(vec0,axis=0)/(np.sum(vec0)+a*2)\rPϹ特征l侮辱类Ͻ=np.sum(vec1,axis=0)/(np.sum(vec1)+a*2)\rreturn PϹ特征l侮辱类Ͻ,PϹ特征l非侮辱类Ͻ,PϹ侮辱类先验Ͻ\rPϹ特征l侮辱类Ͻ,PϹ特征l非侮辱类Ͻ,PϹ侮辱类先验Ͻ=(trainData(v,classVec))\rprint(PϹ特征l非侮辱类Ͻ,PϹ特征l侮辱类Ͻ,PϹ侮辱类先验Ͻ) 输出：\n[0.03389831 0.02542373 0.03389831 0.03389831 0.03389831 0.03389831\r0.02542373 0.02542373 0.03389831 0.03389831 0.04237288 0.03389831\r0.03389831 0.03389831 0.03389831 0.02542373 0.03389831 0.05084746\r0.02542373 0.02542373 0.03389831 0.02542373 0.03389831 0.02542373\r0.03389831 0.03389831 0.03389831 0.02542373 0.02542373 0.03389831\r0.02542373]\r[0.02631579 0.03508772 0.02631579 0.02631579 0.04385965 0.02631579\r0.03508772 0.03508772 0.02631579 0.02631579 0.03508772 0.02631579\r0.02631579 0.02631579 0.02631579 0.03508772 0.02631579 0.02631579\r0.03508772 0.03508772 0.02631579 0.03508772 0.02631579 0.03508772\r0.02631579 0.02631579 0.03508772 0.05263158 0.03508772 0.03508772\r0.04385965] 0.5 拉普拉斯平滑概念和例子参考：https://github.com/lzeqian/machinelearntry/tree/master/sklearn_bayes/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91\nP(特征l非侮辱类) 是非侮辱类下某个特征的概率，也就是P(非侮辱类)条件概率的分子，其中第五个特征是dog,也就是P(dog|非侮辱类) 概率是0.03389831 。 P(特征l侮辱类) 是侮辱类下某个特征的概率，也就是P(非侮辱类)条件概率的分子，其中第五个特征是dog,也就是P(dog|侮辱类) 概率是0.04385965 。 P(侮辱类先验)就是侮辱类的先验概率。 使用训练数据分类 获取到P(特征l非侮辱类) ， P(特征l侮辱类) 和P(侮辱类先验)，P(非侮辱类先验)=1-P(侮辱类先验)，后就可以使用这些数据和传入的新词汇来判断归属的分类了。\n'''\r注意求条件概率是找到对应的单词下在对应的分类的乘积，比如\ryou are a dog as b\r0.001 0.0005 0.03 0.666 0.3 0.99\r传入的矩阵就是\r1 1 1 1 0 0\r实际条件侮辱类概率就是\rP(you|侮辱类)*P(are|侮辱类)*P(a|侮辱类)*P(dog|侮辱类)\r乘积小数位太多就可能导致小数位溢出，需要使用两个乘数的log来防止溢出\rlog(P(you|侮辱类)*P(are|侮辱类)*P(a|侮辱类)*P(dog|侮辱类))=log(P(you|侮辱类))+log(P(are|侮辱类))+log(P(a|侮辱类))+log(P(dog|侮辱类))\r为了通过计算直接获取到对应的这些特征单词的和，可以先求出所有特征的log值和传入的矩阵乘积在求和就是上面的结果\r'''\rdef classResult(wordVec,PϹ特征l侮辱类Ͻ,PϹ特征l非侮辱类Ͻ,PϹ侮辱类先验Ͻ):\rPϹ侮辱类Ͻ=np.sum(np.log(PϹ特征l侮辱类Ͻ)*wordVec)+np.log(PϹ侮辱类先验Ͻ)\rPϹ非侮辱类Ͻ=np.sum(np.log(PϹ特征l非侮辱类Ͻ)*wordVec)+np.log(1-PϹ侮辱类先验Ͻ)\rreturn 1 if PϹ侮辱类Ͻ\u003ePϹ非侮辱类Ͻ else 0\r#测试的词汇\rtext=[\"you are a dog\"]\rtestX = vectorizer.transform(text)\rtestV=np.array(testX.toarray())\rprint(classResult(testV,PϹ特征l侮辱类Ͻ,PϹ特征l非侮辱类Ͻ,PϹ侮辱类先验Ͻ)) 注意多个小数相乘，使用log函数防止小数位溢出理论参考：https://github.com/lzeqian/machinelearntry/blob/master/sklearn_bayes/%E4%B8%8B%E6%BA%A2%E5%87%BA/%E4%B9%98%E7%A7%AF%E7%BB%93%E6%9E%9C%E5%8F%96%E8%87%AA%E7%84%B6%E5%AF%B9%E6%95%B0%E9%98%B2%E6%AD%A2%E4%B8%8B%E6%BA%A2%E5%87%BA.png\n四 朴素贝叶斯之数据归类（sklearn） 朴素贝叶斯分类器是对于特征维数较小而训练样本数比较多的分类问题而使用的分类器，其假设所有特征在类别已知的条件下相互独立。在构建分类器时，只需要逐个估计出每个类别的训练样本在每一维特征上的分布，就可以得到每个类别的条件概率密度，大大减少了需要估计参数的数量。也就是说，在给定样本的目标特征值的情况下观察到特征x1,x2,…,xn的联合概率等于每个单独的特征的概率的乘积。\nscikit-learn中，一共有3个朴素贝叶斯的分类算法类。分别是GaussianNB，MultinomialNB和BernoulliNB。\nGaussianNB就是先验为高斯分布的朴素贝叶斯， MultinomialNB就是先验为多项式分布的朴素贝叶斯， BernoulliNB就是先验为伯努利分布的朴素贝叶斯。 如果样本特征的分布大部分是连续值，使用GaussianNB会比较好。 如果如果样本特征的分大部分是多元离散值，使用MultinomialNB比较合适。 如果样本特征是二元离散值或者很稀疏的多元离散值，应该使用BernoulliNB。 前面讲解的先验概率模型就是先验概率为多项式分布的朴素贝叶斯。\n对于新闻分类，属于多分类问题。我们可以使用MultinamialNB()完成我们的新闻分类问题。另外两个函数的使用暂且不再进行扩展，可以自行学习。MultinomialNB假设特征的先验概率为多项式分布，即如下式： 其中， P(Xj = Xjl | Y = Ck)是第k个类别的第j维特征的第l个取值条件概率。mk是训练集中输出为第k类的样本个数。λ为一个大于0的常数，常常取值为1，即拉普拉斯平滑，也可以取其他值。\n接下来，我们看下MultinamialNB这个函数，只有3个参数：\n参数说明如下：\nalpha：浮点型可选参数，默认为1.0，其实就是添加拉普拉斯平滑，即为上述公式中的λ ，如果这个参数设置为0，就是不添加平滑； fit_prior：布尔型可选参数，默认为True。布尔参数fit_prior表示是否要考虑先验概率，如果是false,则所有的样本类别输出都有相同的类别先验概率。否则可以自己用第三个参数class_prior输入先验概率，或者不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率，此时的先验概率为P(Y=Ck)=mk/m。其中m为训练集样本总数量，mk为输出为第k类别的训练集样本数。 class_prior：可选参数，默认为None。 总结如下： 除此之外，MultinamialNB也有一些方法供我们使用： MultinomialNB一个重要的功能是有partial_fit方法，这个方法的一般用在如果训练集数据量非常大，一次不能全部载入内存的时候。这时我们可以把训练集分成若干等分，重复调用partial_fit来一步步的学习训练集，非常方便。GaussianNB和BernoulliNB也有类似的功能。 在使用MultinomialNB的fit方法或者partial_fit方法拟合数据后，我们可以进行预测。此时预测有三种方法，包括predict，predict_log_proba和predict_proba。predict方法就是我们最常用的预测方法，直接给出测试集的预测类别输出。predict_proba则不同，它会给出测试集样本在各个类别上预测的概率。容易理解，predict_proba预测出的各个类别概率里的最大值对应的类别，也就是predict方法得到类别。predict_log_proba和predict_proba类似，它会给出测试集样本在各个类别上预测的概率的一个对数转化。转化后predict_log_proba预测出的各个类别对数概率里的最大值对应的类别，也就是predict方法得到类别。具体细节不再讲解，可参照官网手册。\n使用skearn分类新浪新闻 数据加载 例子来源于：https://cuijiahua.com/blog/2017/11/ml_5_bayes_2.html 以下是新闻分类的类别\nC000008\t财经\rC000010\tIT\rC000013\t健康\rC000014\t体育\rC000016\t旅游\rC000020\t教育\rC000022\t招聘\rC000023\t文化\rC000024\t军事 文章数据位于每个分类目录下的多篇文章 数据集下载：https://github.com/lzeqian/machinelearntry/tree/master/sklearn_bayes/%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE 加载数据集（文章中的单词需要单独作为特征，需要分词，这里使用jieba）\n分词整理 数据集已经准备好，接下来，让我们直接进入正题。切分中文语句，编写如下代码：\nimport os\rimport jieba\r'''\r判断字符串是否为数字，清理包括:1,1.5,023,34%等特别的数字字符串\r'''\rdef isNumber(num):\rif(num.isdigit() or num.isnumeric() or num.isdecimal()):\rreturn True\rif num.endswith('%'):\rnum_str = num[:-1] # 去掉百分号\rreturn isNumber(num_str)\rtry:\r_ = float(num)\rreturn True\rexcept ValueError:\rreturn False\rreturn False\r'''\r将某个字符串通过jieba分词后通过空格拼接，因为CountVectorizer统计词频传入的是带空格的字符串\r'''\rdef wordToVec(word):\rword_cut = jieba.cut(word, cut_all = False) filtered_words = filter(lambda w: w is not None and len(w.strip()) \u003e 0 and not isNumber(w.strip()), list(word_cut)) # 过滤掉空字符串\rword_list=\" \".join(filtered_words)\rreturn word_list\r'''\r读取新闻分类数据/Sample目录下的所有数据\r'''\rdef TextProcessing(folder_path):\rfolder_list = os.listdir(folder_path) #查看folder_path下的文件\rdata_list = [] #训练集\rclass_list = []\r#遍历每个子文件夹\rfor folder in folder_list:\rnew_folder_path = os.path.join(folder_path, folder) #根据子文件夹，生成新的路径\rfiles = os.listdir(new_folder_path) #存放子文件夹下的txt文件的列表\rj = 1\r#遍历每个txt文件\rfor file in files:\rif j \u003e 100: #每类txt样本数最多100个\rbreak\rwith open(os.path.join(new_folder_path, file), 'r', encoding = 'utf-8') as f: #打开txt文件\rraw = f.read()\rword_list=wordToVec(raw)\rdata_list.append(word_list)\rclass_list.append(folder)\rj += 1\rprint(\"词条行:\",data_list)\rprint(\"分类：\",class_list)\rreturn data_list,class_list\r使用CountVectorizer向量化，并且打印出现次数最多的此的前50\nimport numpy as np\rfrom sklearn.feature_extraction.text import CountVectorizer\rfrom sklearn.naive_bayes import MultinomialNB\rif __name__ == '__main__':\r#文本预处理\rfolder_path = './新闻分类数据/Sample' #训练集存放地址\rdata_list1,class_list1=TextProcessing(folder_path)\rstop_words=\"\";\rwith open(os.path.join(\"./新闻分类数据\", \"stopwords_cn.txt\"), 'r', encoding = 'utf-8') as f: #打开txt文件\rstop_words = f.read()\rstop_words_array=stop_words.split(\"\\n\") #除了停止词外，单个字母的都会被自动过滤掉\rvectorizer = CountVectorizer(stop_words=stop_words_array)\rX = vectorizer.fit_transform(data_list1)\rfn=np.array(vectorizer.get_feature_names())\rprint(\"特征列：\",fn)\rv=np.array(X.toarray())\rprint(\"词条向量：\\n\",v)\rtop=50\rwordcount=v.sum(axis=0)[0:top]\rprint(\"获取单词出现次数:\",wordcount)\rprint(\"排序索引:\",np.argsort(wordcount)[::-1])\rprint(\"排序特征:\",fn[np.argsort(wordcount)[::-1]])\rprint(\"排序词频:\",wordcount[np.argsort(wordcount)[::-1]]) 输出：\n特征列： ['04vs' '110min' '125min' ... '龙岗' '龙江' '龙珠']\r词条向量：\r[[0 0 0 ... 0 0 0]\r[0 0 0 ... 0 0 0]\r[0 0 0 ... 0 0 0]\r...\r[0 0 0 ... 0 0 0]\r[0 0 0 ... 0 0 0]\r[0 0 0 ... 0 0 0]]\r获取单词出现次数: [ 1 1 1 1 1 1 2 1 1 1 1 1 3 2 1 2 6 6 5 1 1 1 1 1\r3 1 1 1 1 2 4 2 1 1 1 7 2 1 1 1 2 1 1 1 1 1 1 1\r10 5]\r排序索引: [48 35 16 17 49 18 30 12 24 40 6 15 13 31 29 36 4 5 3 7 44 8 9 10\r11 47 2 14 1 46 45 20 19 39 38 37 41 34 33 32 42 28 27 26 25 43 23 22\r21 0]\r排序特征: ['ceo' 'bbc' 'ak' 'an' 'cfo' 'and' 'ax' 'ac' 'armed' 'bittorrent' '3g'\r'ah' 'academic' 'a股' 'aw' 'bbn' '3d' '3dmax' '16i' '5140i' 'brings'\r'80mb' '95min' 'ab' 'abc' 'cbs' '125min' 'adj' '110min' 'career'\r'brothers' 'anti' 'answer' 'bennett' 'begins' 'be' 'bjeea' 'band' 'b09'\r'b06' 'bot' 'availwidth' 'availheight' 'assessment' 'army' 'bravo' 'area'\r'are' 'applications' '04vs']\r排序词频: [10 7 6 6 5 5 4 3 3 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1\r1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r1 1] 将数据拆分成训练集和测试集（注意数据集要先打乱，因为现在的数据集都是通过分类读取的，就是按分配来排序的，可能抽取的20%的数据集把某个分类下的所有数据都抽走了，就导致这个分类下没有训练，导致准确性不高）\nfrom sklearn.utils import shuffle\rfrom sklearn.model_selection import train_test_split\rX, y = shuffle(v, class_list1, random_state=42)\rX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 测试下 该训练器的准确率\nclassifier = MultinomialNB().fit(X_train, y_train)\rtest_accuracy = classifier.score(X_test, y_test)\rprint(test_accuracy) 输出：0.7222222222222222\n随便输入一个某个字符串预测下\nv1=vectorizer.transform([wordToVec(\"身体是革命的本钱\")]).toarray();\rprint(classifier.predict(v1)) 输出：[‘C000020’] 也就是：教育",
    "description": "一 简介 朴素贝叶斯算法是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题。该算法的优点在于简单易懂、学习效率高、在某些领域的分类问题中能够与决策树、神经网络相媲美。但由于该算法以自变量之间的独立（条件特征独立）性和连续变量的正态性假设为前提，就会导致算法精度在某种程度上受影响。\n二 朴素贝叶斯理论 把样本空间划分成容易研究的几种情况。\n全概率公式（由原因到结果）考察在每一种情况下事件B发生的概率，计算B的概率。 Bayes公式（由结果到原因）在事件B发生的条件下，考察每种情况出现的条件概率。 条件概率 公式推导 我们需要了解什么是条件概率(Conditional probability)，就是指在事件B发生的情况下，事件A发生的概率，用P(A|B)来表示。 根据文氏图，可以很清楚地看到在事件B发生的情况下，事件A发生的概率就是P(A∩B)除以P(B)。 因此， 同理根据条件概率知道A发生时B的概率 转换下 所以 即 这就是条件概率的计算公式。",
    "tags": [],
    "title": "机器学习实战教程（七）：朴素贝叶斯",
    "uri": "/docs/programming/ai/machine_learning/algorithms/action_07_bays/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 核心算法",
    "content": "多项式回归 概念 线性回归研究的是一个因变量与一个自变量之间的回归问题。 多项式回归是指在线性回归的基础上，通过增加非线性特征来拟合非线性数据的方法。多项式回归模型可以用一个 n 次多项式函数来近似描述目标变量和输入变量之间的关系。例如，对于只有一个自变量 x 的情况，可以将拟合函数写作： 其中 y 表示目标变量，x 表示自变量， 是模型的参数。模型的目标是通过调整参数来使预测值与真实值的误差最小化。\n多项式回归可以通过 Scikit-Learn 的 PolynomialFeatures 类来实现，它可以将原始的自变量数据转化为包含了多项式特征的新自变量数据。这样，我们就可以使用线性回归算法来处理增广后的非线性特征，从而得到多项式回归模型。\n拟合实例 生成一个多项式的模拟数据 y=3x+2x**2\nimport numpy as np\rimport matplotlib.pyplot as plt\rx = np.random.uniform(-3, 3, size=100)\rX = x.reshape(-1, 1)\ry = 3*x+ 2*x**2+ np.random.normal(0, 1, size=100)\rplt.scatter(x, y)\rplt.show() 如果直接使用线性回归，看一下效果：\nfrom sklearn.linear_model import LinearRegression\rlin_reg = LinearRegression()\rlin_reg.fit(X, y)\ry_predict = lin_reg.predict(X)\rplt.scatter(x, y)\rplt.plot(x, y_predict, color='r')\rplt.show() 很显然，拟合效果并不好。那么解决呢？ 解决方案：添加一个特征。 x**2\nX2 = np.hstack([X, X**2])\rlin_reg2 = LinearRegression()\rlin_reg2.fit(X2, y)\ry_predict2 = lin_reg2.predict(X2)\rplt.scatter(x, y)\rplt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')\rplt.show() 这样就比直线拟合要好很多,斜率和截距是。\n[2.9391452 1.94366894]\r0.04332751905483523 scikit-learn中的多项式回归 polynomialFeatures polynomialFeatures是Scikit-Learn中的一个函数，用于将输入数据转化为多项式特征集合。其作用是在对非线性数据进行线性回归时，通过增加非线性特征来拟合非线性数据。\n具体地，PolynomialFeatures函数将原始的特征向量转化为包含了所有多项式组合的新特征向量。例如，如果原始特征向量为 [a,b]，且使用degree=2，那么通过PolynomialFeatures生成的新特征向量为 [1,a,b,a^2, ab,b^2]。如果原始特征向量为 [x]，且使用degree=2，那么通过PolynomialFeatures生成的新特征向量为 [1,x,x^2]\n这样，由于新特征向量包含了原始特征向量的所有多项式，可以更好地拟合非线性函数。\nPolynomialFeatures主要有以下参数：\ndegree：表示多项式的次数，决定了到多少次项的多项式将被生成。 interaction_only：默认为False，表示新特征向量包含交叉项和高次项，如a×b、a^2等等。 include_bias：默认为True，表示是否创建偏差列。 总之，PolynomialFeatures是一个非常有用的函数，可以帮助我们更好地处理非线性数据，从而提高模型的预测能力。\nfrom sklearn.preprocessing import PolynomialFeatures\r# 这个degree表示我们使用多少次幂的多项式\rpoly = PolynomialFeatures(degree=2) poly.fit(X)\rX2 = poly.transform(X)\rprint(X2.shape)\rprint(X2) 输出结果(第一列是常量1，第二列是之前的x，第三列是x**2)：\n(100, 3)\r[[ 1.00000000e+00 -2.37462045e+00 5.63882230e+00]\r[ 1.00000000e+00 -7.90962247e-01 6.25621276e-01]\r[ 1.00000000e+00 -7.02888543e-01 4.94052304e-01]\r[ 1.00000000e+00 -6.54589498e-01 4.28487411e-01]] 使用线性回归拟合\nfrom sklearn.linear_model import LinearRegression\rreg = LinearRegression() reg.fit(X2, y)\ry_predict = reg.predict(X2) plt.scatter(x, y) plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')\rplt.show()\rprint(lin_reg2.coef_)\r# array([0.90802935, 1.04112467])\rprint(lin_reg2.intercept_) 输出截距和斜率\n[3.02468873 1.94228967]\r0.41539122650325755 之前使用的都是1维数据，如果使用2维3维甚至更高维呢？ 生成一个二维数据（1到10，转换为5行2列）\nimport numpy as np\rx = np.arange(1, 11).reshape(5, 2)\rprint(x) 输出\n[[ 1 2]\r[ 3 4]\r[ 5 6]\r[ 7 8]\r[ 9 10]] 使用PolynomialFeatures构造\nfrom sklearn.preprocessing import PolynomialFeatures\rpoly = PolynomialFeatures()\rpoly.fit(x)\rx2 = poly.transform(x)\rprint(x2) 输出\n[[ 1. 1. 2. 1. 2. 4.]\r[ 1. 3. 4. 9. 12. 16.]\r[ 1. 5. 6. 25. 30. 36.]\r[ 1. 7. 8. 49. 56. 64.]\r[ 1. 9. 10. 81. 90. 100.]] 此时，可以看出当数据维度是2维是，经过多项式预处理生成了6维数据，第一列很显然是0次项系数，第二列和第三列也很好理解，分别是x1，x2，第四列和第六列分别是 x12和x22 ,还有一列，其实是x1*x2,这就是第5列，总共6列。由此可以猜想一下如果数据是3维的时候是什么情况？\npoly = PolynomialFeatures(degree=3)\rpoly.fit(x)\rx3 = poly.transform(x)\rprint(x3) 输出\n[[ 1. 1. 2. 1. 2. 4. 1. 2. 4. 8.]\r[ 1. 3. 4. 9. 12. 16. 27. 36. 48. 64.]\r[ 1. 5. 6. 25. 30. 36. 125. 150. 180. 216.]\r[ 1. 7. 8. 49. 56. 64. 343. 392. 448. 512.]\r[ 1. 9. 10. 81. 90. 100. 729. 810. 900. 1000.]] 那么这10列，分别对应着什么？通过PolynomiaFeatures，将所有的可能组合，升维的方式呈指数型增长。这也会带来一定的问题。 如何解决这种爆炸式的增长？如果不控制一下，试想x和x[^100]相比差异就太大了。这就是传说中的过拟合。\nsklearn中的Pipeline sklearn中的Pipeline是一个工具，可以将多个数据预处理步骤（transformer）和一个机器学习模型（estimator）串联在一起，形成一个完整的机器学习流程。Pipeline 中的每个步骤都是一个包含 fit 和 transform 方法的对象，其中 fit 方法用于拟合训练数据，transform 方法用于对数据进行转换。\n通过 Pipeline 可以将多个预处理算法和机器学习算法组合在一起，使得整个流程变得规范化和简化，同时可以方便地进行交叉验证和参数调节等操作。Pipeline 中的每个步骤都可以使用一个字符串来标识，这个字符串可以用于对模型中的超参数进行调节。\nPipeline 通常用于机器学习中的特征工程，在数据预处理的过程中构建完整的机器学习流程，并将其应用于训练集和测试集中。通过将多个步骤组合在一起，避免了手动地进行特征工程和模型选择的各种组合操作，同时也提高了代码复用性和可维护性。\n一般情况下多项式回归，我们会对数据进行归一化，然后进行多项式升维，再接着进行线性回归。因为sklearn中并没有对多项式回归进行封装，不过可以使用Pipeline对这些操作进行整合。\n#%%\rimport numpy as np\rimport matplotlib.pyplot as plt\rx = np.random.uniform(-3, 3, size=100)\rX = x.reshape(-1, 1)\ry = 3*x+ 2*x**2+ np.random.normal(0, 1, size=100)\rplt.scatter(x, y)\rplt.show()\r#%%\rfrom sklearn.pipeline import Pipeline\rfrom sklearn.preprocessing import PolynomialFeatures\rfrom sklearn.linear_model import LinearRegression\rfrom sklearn.preprocessing import StandardScaler\rpoly_reg = Pipeline([\r('poly', PolynomialFeatures(degree=2)), ('std_scale', StandardScaler()),\r('lin_reg', LinearRegression())\r]) poly_reg.fit(X, y)\ry_predict = poly_reg.predict(X)\rplt.scatter(x, y)\rplt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')\rplt.show() 过拟合和欠拟合 ​ 多项式回归的最大优点就是可以通过增加x的高次项对实测点进行逼近，直至满意为止。但是这也正是它最大的缺点，因为通常情况下试过过高的维度对数据进行拟合，在训练集上会有很好的表现，但是测试集可能就不那么理想了，这也正是解决过拟合的一种办法。\n均方误差 mean_squared_error 是一个用于计算两个数组之间均方误差（Mean Squared Error，简称MSE）的函数。它是评价回归模型准确度的一种常见指标。 该函数的输入参数包括：\ny_true: 真实值数组； y_pred: 预测值数组； sample_weight: 用于对样本赋权重的数组，可以不传入，默认值为 None 函数返回的是一个数值，表示两个数组之间的均方误差。\n均方误差是回归模型中使用广泛的一种衡量模型预测能力的指标。均方误差越小说明模型的预测越准确。它的定义是：将每个样本预测值与真实值之间的偏差进行平方后求和，再除以样本总数得到的平均值，即 均方误差越小就意味着模型的预测能力越准确\n拟合效果 以下使用同一个方式生成数据集之后，使用不同的拟合方式，并使用均方误差来对比几种拟合的效果。\n线性拟合 from sklearn.linear_model import LinearRegression\rfrom sklearn.metrics import mean_squared_error\rlin_reg = LinearRegression()\rlin_reg.fit(X, y)\ry_predict = lin_reg.predict(X)\rplt.scatter(x, y)\rplt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')\rplt.show()\rprint(\"线性均方误差\",mean_squared_error(y, y_predict)) 输出：3.0750025765636577 显然，直接使用简单的一次线性回归，拟合的结果就是欠拟合(underfiting)， 二次多项式拟合 from sklearn.pipeline import Pipeline\rfrom sklearn.preprocessing import PolynomialFeatures\rfrom sklearn.linear_model import LinearRegression\rfrom sklearn.preprocessing import StandardScaler\rdef PolynomialRegression(degree):\rreturn Pipeline([\r('poly', PolynomialFeatures(degree=degree)),\r('std_scale', StandardScaler()),\r('lin_reg', LinearRegression())\r]) poly_reg = PolynomialRegression(degree=2)\rpoly_reg.fit(X, y)\rPipeline(memory=None,\rsteps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('std_scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lin_reg', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\rnormalize=False))])\ry_predict = poly_reg.predict(X)\rprint(\"2次多项式均方误差\",mean_squared_error(y, y_predict))\rplt.scatter(x, y)\rplt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')\rplt.show() 输出：1.0987392142417856 二次多项式回归的拟合程度要高于线性回归。 十次多项式拟合 poly10_reg = PolynomialRegression(degree=10)\rpoly10_reg.fit(X, y)\ry10_predict = poly10_reg.predict(X)\rprint(\"10次多项式均方误差\",mean_squared_error(y, y10_predict))\rplt.scatter(x, y)\rplt.plot(np.sort(x), y10_predict[np.argsort(x)], color='r')\rplt.show() 输出：1.0508466763764202 百次多项式拟合 poly10_reg = PolynomialRegression(degree=100)\rpoly10_reg.fit(X, y)\ry10_predict = poly10_reg.predict(X)\rprint(\"100次多项式均方误差\",mean_squared_error(y, y10_predict))\rplt.scatter(x, y)\rplt.plot(np.sort(x), y10_predict[np.argsort(x)], color='r')\rplt.show() 输出：0.6870911922673567 百次多项式生成数据集测试 从上面的图形看出uniform生成的数据预测的y值都在-1到10之间，我们使用相同的模型预测下 x=3的值\ny_plot = poly100_reg.predict([[3]])\rprint(y_plot) 输出：[-2.49133715e+06 -6.32965634e+24] 转换下：-2.49133715e+06==-2491337.16790313\n发现\u003e=3后，如果按照这个弯月形的图形，明显是不正常的。 我们生成一个等差数列作为测试集\nfrom sklearn.preprocessing import PolynomialFeatures\rx_plot = np.linspace(-3, 3, 100).reshape(100, 1)\ry_plot = poly100_reg.predict(x_plot)\rplt.scatter(x, y)\rplt.plot(x_plot[:,0], y_plot, color='r')\r# plt.axis([-3, 3, -1, 10])\rplt.show() 这样因为x=3图形都乱了，我们截图图形x从-3到3 y从-1到10\nfrom sklearn.preprocessing import PolynomialFeatures\rx_plot = np.linspace(-3, 3, 100).reshape(100, 1)\ry_plot = poly100_reg.predict(x_plot)\rplt.scatter(x, y)\rplt.plot(x_plot[:,0], y_plot, color='r')\rplt.axis([-3, 3, -1, 10])\rplt.show() 说明通过训练集训练出的模型对测试集没有很好的表现能力\n解决过拟合问题 通常在机器学习的过程中，主要解决的都是过拟合问题，因为这牵涉到模型的泛化能力。所谓泛化能力，就是模型在验证训练集之外的数据时能够给出很好的解答。只是对训练集的数据拟合的有多好是没有意义的，我们需要的模型的泛化能力有多好。\n为什么要训练数据集与测试数据集？\n通常情况下我们会将数据集分为训练集和测试集，通过训练数据训练出来的模型如果能对测试集具有较好的表现，才有意义。\n以下使用train_test_split将生成的数据拆分为训练集和测试集，重新生成模型并计算均方误差 使用线性回归\nfrom sklearn.model_selection import train_test_split\rimport numpy as np\rimport matplotlib.pyplot as plt\rnp.random.seed(666)\rnp.random.seed(666)\rx = np.random.uniform(-3.0, 3.0, size=100)\rX = x.reshape(-1, 1)\ry = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)\rx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=666)\rlin_reg = LinearRegression()\rlin_reg.fit(x_train, y_train)\ry_predict = lin_reg.predict(x_test)\rmean_squared_error(y_test, y_predict) 输出结果：2.2199965269396573\n使用二项式\nfrom sklearn.pipeline import Pipeline\rfrom sklearn.preprocessing import PolynomialFeatures\rfrom sklearn.linear_model import LinearRegression\rfrom sklearn.preprocessing import StandardScaler\rdef PolynomialRegression(degree):\rreturn Pipeline([\r('poly', PolynomialFeatures(degree=degree)),\r('std_scale', StandardScaler()),\r('lin_reg', LinearRegression())\r])\rpoly2_reg = PolynomialRegression(degree=2)\rpoly2_reg.fit(x_train, y_train)\ry2_predict = poly2_reg.predict(x_test)\rmean_squared_error(y_test, y2_predict) 输出结果： 0.8035641056297901\n使用10项式\npoly10_reg = PolynomialRegression(degree=10)\rpoly10_reg.fit(x_train, y_train)\ry10_predict = poly10_reg.predict(x_test)\rmean_squared_error(y_test, y10_predict) 输出结果：0.9212930722150781\n通过上面的例子可以发现，当degree=2的时候在测试集上的均方误差和直线拟合相比好了很多，但是当degree=10的时候再测试集上的均方误差相对degree=2的时候效果差了很多，这就说名训练出来的模型已经过拟合了。\n100项式\npoly100_reg = PolynomialRegression(degree=100)\rpoly100_reg.fit(x_train, y_train)\ry100_predict = poly100_reg.predict(x_test)\rmean_squared_error(y_test, y100_predict) 输出结果：14440175276.314638 小结：对于模型复杂度与模型准确率中寻找泛化能力最好的地方。\n欠拟合：underfitting，算法所训练的模型不能完整表述数据关系。 过拟合：overfitting，算法所训练的模型过多地表达数据间的噪音关系。 学习曲线 机器学习的学习曲线是一种图形化表示机器学习算法在训练数据上表现的方式，通常以训练数据集大小或训练迭代次数为横轴，以模型性能指标（如准确率、误差等）为纵轴。这条曲线可以帮助我们了解算法的学习过程，评估学习效果和调整模型。\n随着训练数据集的增加，我们希望看到模型的性能不断提高；而如果在训练集上性能较好但在测试集上表现欠佳，则说明出现了过拟合（overfitting）问题；相反，如果在训练集和测试集上的表现都不好，则可能需要重新考虑数据预处理、特征工程和模型结构等问题。\n与学习曲线相关的概念还包括偏差（bias）和方差（variance），它们通常被用来对模型进行诊断和调整。当模型的偏差较大时，说明模型太简单，不能准确地拟合训练集和测试集，需要增加模型复杂度；而当模型的方差较大时，说明模型过于复杂，出现了过拟合问题，需要缩减模型复杂度或增加训练数据集的大小。\n我们尝试将整个数据集拆分为训练集和测试机，训练集的大小从1到len（训练集）依次增加，生成训练集对应的模型（使用线性回归，2次多项式，100次多项式），使用相同的测试集测试对应的模型，并且绘制成x=训练集的个数，y=均方误差来看下欠拟合（线性回归），最佳拟合（二项式），过拟合（20项式）\n下面绘制学习曲线的函数封装一下，方便后面调用\ndef plot_learning_curve(algo, x_train, x_test, y_train, y_test):\rtrain_score = []\rtest_score = []\rfor i in range(1, len(x_train)+1):\ralgo.fit(x_train[:i], y_train[:i])\ry_train_predict = algo.predict(x_train[:i])\rtrain_score.append(mean_squared_error(y_train[:i], y_train_predict))\ry_test_predict = algo.predict(x_test)\rtest_score.append(mean_squared_error(y_test, y_test_predict))\rplt.plot([i for i in range(1, len(x_train)+1)], np.sqrt(train_score), label='train')\rplt.plot([i for i in range(1, len(x_train)+1)], np.sqrt(test_score), label='test')\rplt.legend()\rplt.axis([0, len(x_train)+1, 0, 4])\rplt.show()\rplot_learning_curve(LinearRegression(), x_train, x_test, y_train, y_test) 生成数据集\nnp.random.seed(666)\rnp.random.seed(666)\rx = np.random.uniform(-3.0, 3.0, size=100)\rX = x.reshape(-1, 1)\ry = 0.5 * x**2 + x + 2 + np.random.normal(0, 1, size=100)\rx_train, x_test, y_train, y_test = train_test_split(X, y, random_state=10) 使用线性回归，图像是欠拟合（欠拟合指模型不能在训练集上获得足够低的误差），训练数据误差都达到2.0了 使用二项式回顾，图像是最佳拟合\ndef PolynomialRegression(degree):\rreturn Pipeline([\r('poly', PolynomialFeatures(degree=degree)),\r('std_scale', StandardScaler()),\r('lin_reg', LinearRegression())\r])\rpoly2_reg = PolynomialRegression(degree=2)\rplot_learning_curve(poly2_reg, x_train, x_test, y_train, y_test) 使用20项式，过拟合（过拟合则是指模型在训练集上表现很好，但在测试集上却表现很差）\npoly2_reg = PolynomialRegression(degree=20)\rplot_learning_curve(poly2_reg, x_train, x_test, y_train, y_test)",
    "description": "多项式回归 概念 线性回归研究的是一个因变量与一个自变量之间的回归问题。 多项式回归是指在线性回归的基础上，通过增加非线性特征来拟合非线性数据的方法。多项式回归模型可以用一个 n 次多项式函数来近似描述目标变量和输入变量之间的关系。例如，对于只有一个自变量 x 的情况，可以将拟合函数写作： 其中 y 表示目标变量，x 表示自变量， 是模型的参数。模型的目标是通过调整参数来使预测值与真实值的误差最小化。\n多项式回归可以通过 Scikit-Learn 的 PolynomialFeatures 类来实现，它可以将原始的自变量数据转化为包含了多项式特征的新自变量数据。这样，我们就可以使用线性回归算法来处理增广后的非线性特征，从而得到多项式回归模型。\n拟合实例 生成一个多项式的模拟数据 y=3x+2x**2\nimport numpy as np\rimport matplotlib.pyplot as plt\rx = np.random.uniform(-3, 3, size=100)\rX = x.reshape(-1, 1)\ry = 3*x+ 2*x**2+ np.random.normal(0, 1, size=100)\rplt.scatter(x, y)\rplt.show() 如果直接使用线性回归，看一下效果：",
    "tags": [],
    "title": "机器学习实战教程（八）：多项式回归",
    "uri": "/docs/programming/ai/machine_learning/algorithms/action_08_multinomial/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 核心算法",
    "content": "概述 逻辑回归（Logistic Regression）是一种用于解决二分类或多分类问题的统计学习方法。它以自变量线性组合的形式进行建模，并使用Sigmoid函数将结果映射到[0, 1]的值域内，表示样本属于某个类别的概率。 Logistic Regression是最广泛使用的一种算法，逻辑回归常年在机器学习算法排名前10。\n逻辑回归推导 线性回归 线性回归的表达式： $f(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+….+\\theta_nx_n$ 转换为矩阵乘法： $[[x_1,x_2….,x_n]]$点乘$[[\\theta_1,\\theta_2…..,\\theta_n]]^T$ 矩阵演示： 首先，假设我们有一个包含3个样本、每个样本有2个特征的训练集X：\nX = [[1, 2], [3, 4], [5, 6]] 其中，每个样本都有两个特征。接着，我们随机初始化参数向量θ：\nθ = [[0.5,0.5]]\rθ.T=[[0.5],[0.5]]\rX * θ = [[1, 2], [3, 4], [5, 6]] * [[0.5], [0.5]] = [[10.5+20.5], [30.5+40.5], [50.5+60.5]] = [[1.5], [3.5], [5.5]] 所以： $f(x)=\\theta_0+\\theta^Tx$ 如果在x数据集加上一列常量1，$\\theta_0$加入到$\\theta$矩阵中，也就能再次缩写 $f(x)=\\theta^Tx$\n$\\theta$是权值,它与输出y之间的关系强度。如果权值越大，则输入特征对输出的影响就越大；如果权值越小，则输入特征对输出的影响就越小。。\n逻辑回归 逻辑回归(Logistic Regression, LR)模型其实仅在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数，使得逻辑回归模型成为了机器学习领域一颗耀眼的明星，更是计算广告学的核心。 通常线性方程的值域为 $(-\\infty，+\\infty)$,而概率的值域为[0, 1]，因此我们在这个基础上做一个变形，完成从 $(-\\infty，+\\infty)$,到[0,1]的转换。 逻辑回归的假设函数可以表示为 $$h_\\theta(x)=g(\\theta^Tx)$$ 这个转换函数g就叫做Sigmoid函数，函数的表达式： $$g(z)={1\\over(1+e^{-z})}$$ 我们来看下Sigmoid函数的图形\nimport numpy as np\rimport matplotlib.pyplot as plt\rdef sigmoid(t):\rreturn 1 / (1 + np.exp(-t))\rx = np.linspace(-10, 10, 500)\ry = sigmoid(x)\rplt.plot(x, y)\rplt.show() 于是我们得到了这样的关系式：\nX（也就是$\\theta^Tx$）\u003e0时，Y的值（概率）大于0.5，label也就是1 X\u003c0时，Y的值（概率）小于0.5，label也就是0 X=0时，Y的值=0.5 二分类问题，比如是否有肿瘤，概率大于0.5预测就是明确的1，概率小于0.5预测就是明确的0\n决策边界 下面再举一个例子，假设我们有许多样本，并在图中表示出来了，并且假设我们已经通过某种方法求出了LR模型的参数（如下图）。 根据上面得到的关系式，我们可以得到： 我们再图像上画出得到： 这时，直线上方所有样本都是正样本y=1，直线下方所有样本都是负样本y=0。因此我们可以把这条直线成为**决策边界**。 以下代码暂时只做参考,绘制一个x1+x2=3附近的随机点，使用sklearn的逻辑回归训练，并绘制边界（安装mlxtend库）\n#%%\rimport numpy as np;\rimport matplotlib.pyplot as plt\rfrom mlxtend.plotting import plot_decision_regions\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.model_selection import train_test_split\r#产生一个x+y在3附近的随机点。\rnp.random.seed(100)\rx1=np.random.uniform(0,6,100)\rx2=x1-3+ np.random.normal(0, 3, size=100)\rX=np.hstack((x1.reshape(-1,1),x2.reshape(-1,1)))\ry=np.array([1 if i1+i2\u003e=3 else 0 for i1,i2 in zip(x1,x2)])\rcolor=np.array(['red' if i1+i2\u003e=3 else 'blue' for i1,i2 in zip(x1,x2)])\rplt.scatter(x1, x2,c=color)\rplt.show()\r#使用逻辑回归训练模型\rlr_model = LogisticRegression(max_iter=2100)\rX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\rlr_model.fit(X_train, y_train)\r#绘制边界\rplot_decision_regions(X, y, clf=lr_model, legend=2) 输出： 同理，对于非线性可分的情况，我们只需要引入多项式特征就可以很好的去做分类预测，如下图： 产生一个x2+y2=1附近的随机点。\nimport numpy as np;\rimport matplotlib.pyplot as plt\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.model_selection import train_test_split\r#产生一个x**2+y**2=1附近的随机点。\rnp.random.seed(100)\rx1=np.random.uniform(-1,1,100)\rx2=np.sqrt(1-x1**2)+ np.random.normal(-1, 1, size=100)\rX=np.hstack((x1.reshape(-1,1),x2.reshape(-1,1)))\ry=np.array([1 if i1**2+i2**2\u003e=1 else 0 for i1,i2 in zip(x1,x2)])\r#下面同上\r#y = np.where(x1**2 + x2**2 \u003c 1, 0, 1)\r#使用逻辑回归训练模型\rlr_model = LogisticRegression(max_iter=2100)\rX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\rlr_model.fit(X_train, y_train)\r#绘制边界\rprint(\"截距\",lr_model.intercept_)\rprint(\"斜率\",lr_model.coef_)\r# 绘制圆形决策边界\rcircle = plt.Circle((0, 0), radius=1, color='black', fill=False)\rfig, ax = plt.subplots()\rax.add_artist(circle)\rax.scatter(x1[y == 0], x2[y == 0], color='blue', s=5)\rax.scatter(x1[y == 1], x2[y == 1], color='red', s=5)\rax.set_xlim([-1, 1])\rax.set_ylim([-1, 1])\rplt.axis([-2, 2, -2, 2])\r#plt.axis('equal')\rplt.show() 输出 得注意的一点，决策边界并不是训练集的属性，而是假设本身和参数的属性。因为训练集不可以定义决策边界，它只负责拟合参数；而只有参数确定了，决策边界才得以确定。\n损失函数 损失函数就是用来衡量模型的输出与真实输出的差别。 假设只有两个标签1和0，$y_n\\in{0,1}$ 。我们把采集到的任何一组样本看做一个事件的话，那么这个事件发生的概率假设为p。我们的模型y的值等于标签为1的概率也就是p。\n交叉熵（Cross-Entropy，CE）和均方误差（Mean Squared Error，MSE）是机器学习中经常使用的两种损失函数。它们在许多领域都被广泛应用，例如分类、回归等任务。它们的区别在于对于不同类型的问题，理论依据和优化效果不同。\nMSE通常用于回归问题，用于衡量预测值与真实值之间的差异 而CE则通常用于分类问题中，它可以衡量预测结果与真实标签之间的差异。对于二分类问题，它的公式为： MSE(均方误差) 为什么损失函数不用最小二乘？即逻辑斯蒂回归损失函数为什么使用交叉熵（下一节）而不是MSE？ 从逻辑的角度出发，我们知道逻辑斯蒂回归的预测值是一个概率，而交叉熵又表示真实概率分布与预- 测概率分布的相似程度，因此选择使用交叉熵。 从MSE的角度来说，预测的概率与欧氏距离没有任何关系，并且在分类问题中，样本的值不存在大小关系，与欧氏距离更无关系，因此不适用MSE。\n原因一：损失函数的凸性（使用MSE可能会陷入局部最优） 前面我们介绍线性回归模型时，给出了线性回归的代价函数的形式（误差平方和函数），具体形式如下： $$J(\\theta)={1\\over m}\\sum_{i=1}^m{1\\over2}(h_\\theta(x^{(j)}) -y^{(j)})$$ 这里我们想到逻辑回归也可以视为一个广义的线性模型，那么线性模型中应用最广泛的代价函数-误差平方和函数，可不可以应用到逻辑回归呢？首先告诉你答案：是不可以的！ 那么为什么呢? 这是因为LR的假设函数的外层函数是Sigmoid函数，Sigmoid函数是一个复杂的非线性函数，这就使得我们将逻辑回归的假设函数 $$h_\\theta(x)={1\\over(1+e^{-\\theta^Tx})}$$ 带入上式时，我们得到的 $$J(\\theta)={1\\over m}\\sum_{i=1}^m{1\\over2}( {1\\over(1+e^{-\\theta^Tx^{(j)}})}-y^{(j)})$$\n是一个非凸函数，如下图： 这样的函数拥有多个局部极小值，这就会使得我们在使用梯度下降法求解函数最小值时，所得到的结果并非总是全局最小，而有更大的可能得到的是局部最小值\nMSE 为损失函数的逻辑斯蒂回归就是一个非凸函数，如何证明这一点呢，要证明一个函数的凸性，只要证明其二阶导恒大于等于0即可，如果不是恒大于等于0，则为非凸函数。\n凸（Convex）：在该区间函数图象上的任意两点所连成的线段上的每一个点都位于函数图象的下方(或上方)。 一个典型的凸函数$y=-x^2$ ，任意两点连线上所有的点都在函数图像的下方，如下图： 非凸（Non-Convex）：函数在该区间上有多个极值,即系统有多个稳定的平衡态。 非凸函数$y=sin(x)$，两点连线上的点可能分布在函数图像的两侧，如下图： 这里只需要证明上面函数不是恒大于等于0即可，这里就不去证明了，自行百度。\nCE（交叉熵） 逻辑回归损失函数就是用来衡量模型的输出与真实输出的差别。 假设只有两个标签1和0，$y_n \\in{0, 1}$。我们把采集到的任何一组样本看做一个事件的话，那么这个事件发生的概率假设为p。我们的模型y的值等于标签为1的概率也就是p。 $$P_{y=1}=\\frac{1}{1+e^{-\\bm{\\theta}^T\\bm{x}}} = p$$ 因为标签不是1就是0，因此标签为0的概率就是：$P_{y=0} = 1-p$ 我们把单个样本看做一个事件，那么这个事件发生的概率就是： $$P(y|\\bm{x})=\\left{ \\begin{aligned} p, y=1 \\ 1-p,y=0 \\end{aligned} \\right.$$ 这个函数不方便计算，它等价于: $$P(y_i|\\bm{x}_i) = p^{y_i}(1-p)^{1-{y_i}}$$ 解释下这个函数的含义，我们采集到了一个样本$(\\bm{x_i},y_i)$对这个样本，它的标签是$y_i$的概率是 $p^{y_i}(1-p)^{1-{y_i}}$ （当y=1，结果是p；当y=0，结果是1-p）。 如果我们采集到了一组数据一共N个，${(\\bm{x}_1,y_1),(\\bm{x}_2,y_2),(\\bm{x}_3,y_3)…(\\bm{x}N,y_N)}$，这个合成在一起的合事件发生的总概率怎么求呢？其实就是将每一个样本发生的概率相乘就可以了，即采集到这组样本的概率： $$\\begin{aligned} P{总} \u0026= P(y_1|\\bm{x}_1)P(y_2|\\bm{x}_2)P(y_3|\\bm{x}_3)….P(y_N|\\bm{x}N) \\ \u0026= \\prod{n=1}^{N}p^{y_n}(1-p)^{1-y_n} \\end{aligned}$$\n注意$P_总$ 是一个函数，并且未知的量只有 $\\theta$ （在p里面）。 由于连乘很复杂，我们通过两边取对数来把连乘变成连加的形式,对数可以防止下溢出，不会影响单调性，同时和原函数在相同的点求极限值，也就是找到的是相同$\\theta$值，即： $$\\begin{aligned} F(\\bm{\\theta})=ln(P_{总} ) \u0026= ln(\\prod_{n=1}^{N}p^{y_n}(1-p)^{1-y_n} ) \\ \u0026= \\sum_{n=1}^{N}ln (p^{y_n}(1-p)^{1-y_n}) \\ \u0026= \\sum_{n=1}^{N}(y_n ln (p) + (1-y_n)ln(1-p)) \\end{aligned} $$\n其中，$p = \\frac{1}{1+e^{-\\bm{\\theta}^T\\bm{x}}}$ 这个函数$F(\\bm{w})$的值等于事件发生的总概率，我们希望它越大越好。但是跟损失的含义有点儿违背，因此也可以在前面取个负号，负数就是越小越好，$-F(\\bm{w})$又叫做它的损失函数。损失函数可以理解成衡量我们当前的模型的输出结果，跟实际的输出结果之间的差距的一种函数。 损失函数也就是交叉熵\n$$\\begin{aligned} J(\\bm{\\theta})=-ln(P_{总} ) = - \\sum_{n=1}^{N}(y_n ln (p) + (1-y_n)ln(1-p)) \\end{aligned} $$\n在逻辑回归中，我们通常使用交叉熵作为损失函数来评估模型预测结果和实际标签之间的差异。损失函数的目标是尽可能地缩小模型的预测误差，使模型能够更准确地预测数据的标签。\n在实际训练中，我们通常会将整个训练集分为若干个小批次（batch），利用每个小批次的数据对模型参数进行更新。这样做的好处是可以提高模型训练的速度，减少模型在训练过程中的波动性。\n为了避免样本数量对损失函数大小的影响，通常在计算损失函数时会将所有样本损失函数值的平均作为最终的损失函数值。也就是说，用上面的公式计算出来的是训练集中所有样本的平均损失函数。 $$\\begin{aligned} J(\\bm{\\theta})= -\\dfrac{1}n\\sum_{n=1}^{N}(y_n ln (p) + (1-y_n)ln(1-p)) \\end{aligned} $$ 而除以样本数量m，其实就是将平均损失函数转化为单个样本的损失函数，即用每个样本的损失函数值的平均值来衡量模型在单个样本上的错误程度。这样做可以使优化算法更加稳定和可靠。 好的，假设我们的训练集中有1000个样本，我们将其分成10个小批次（每个小批次包含100个样本）进行训练。模型对每个小批次的样本进行预测\n那么，使用如下的公式计算这个模型的交叉熵损失：\n$$\\text{CE}(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) =J(\\bm{\\theta})= -\\dfrac{1}{10 \\times 100} \\sum_{i=1}^{10 \\times 100} \\bigg[ y_i \\log_e \\hat{y}_i + (1 - y_i) \\log_e (1 - \\hat{y}_i) \\bigg]$$\n1000个样本的平均损失函数值。在实际训练中，优化算法通常是按照小批次的形式进行迭代更新的，因此使用平均损失函数可以对单个小批次的数据完整反映模型对整个训练集数据的学习程度。\n而如果我们不将损失函数除以样本数量m，那么不同样本数量的训练集会对损失函数大小产生影响，从而使得优化算法在训练时产生不必要的波动和不稳定。\nCE梯度推导 我们指导入到需要使用梯度下降法求出最小值的$\\theta$，需要先获取到损失函数的梯度。 首先，我们需要知道向量是如何求导的。具体的推导过程以及原理请参见梯度下降\n首先我们知道 $$p=\\frac{1}{1+e^{-\\bm{\\theta}^T\\bm{x}}}$$ 那么 $$1-p=1-\\frac{1}{1+e^{-\\bm{\\theta}^T\\bm{x}}}=\\frac{1+e^{-\\bm{\\theta}^T\\bm{x}}}{1+e^{-\\bm{\\theta}^T\\bm{x}}}-\\frac{1}{1+e^{-\\bm{\\theta}^T\\bm{x}}}=\\frac{e^{-\\bm{\\theta}^T\\bm{x}}}{1+e^{-\\bm{\\theta}^T\\bm{x}}}$$ p是一个关于变量 $\\theta$ 的函数，我们对p求导，通过链式求导法则，慢慢展开可以得： $\\begin{aligned} p’ = f’(\\bm{\\theta})\u0026= (\\frac{1}{1+e^{-\\bm{\\theta}^T\\bm{x}}} )’ \\ \u0026= -\\frac{1}{ (1+e^{-\\bm{\\theta}^T\\bm{x}} )^2} · ( 1+e^{-\\bm{\\theta}^T\\bm{x}})’ \\ \u0026= -\\frac{1}{ (1+e^{-\\bm{\\theta}^T\\bm{x}} )^2} · e^{-\\bm{\\theta}^T\\bm{x}} · (-\\bm{\\theta}^T\\bm{x})’ \\ \u0026= -\\frac{1}{ (1+e^{-\\bm{\\theta}^T\\bm{x}} )^2} · e^{-\\bm{\\theta}^T\\bm{x}} · (-\\bm{x} ) \\ \u0026= \\frac{e^{-\\bm{\\theta}^T\\bm{x}} }{ (1+e^{-\\bm{\\theta}^T\\bm{x}} )^2} · \\bm{x} \\ \u0026= \\frac{1}{ 1+e^{-\\bm{\\theta}^T\\bm{x}} } · \\frac{e^{-\\bm{\\theta}^T\\bm{x}} }{ 1+e^{-\\bm{\\theta}^T\\bm{x}} } · \\bm{x} \\ \u0026= p(1-p)\\bm{x} \\end{aligned}$\n上面都是我们做的准备工作，总之我们得记住： $p’ = p(1-p)\\bm{x}$ 那么 $(1-p)’=1’-p’=-p’= -p(1-p)\\bm{x}$ 接下来我们对$J_\\theta求导$，求导的时候请始终记住，我们的变量只有 $\\theta$，其他的什么 都是已知的，可以看做常数。 $\\begin{aligned} \\nabla J（\\bm{\\theta}）\u0026 = \\nabla （ \\sum_{n=1}^{N}(y_n ln (p) + (1-y_n)ln(1-p)) ）\\ \u0026= \\sum ( y_n ln’(p) + (1-y_n) ln’(1-p)) \\ \u0026= \\sum( (y_n \\frac{1}{p}p’)+(1-y_n)\\frac{1}{1-p}(1-p)’) \\ \u0026= \\sum(y_n(1-p)\\bm{x}_n - (1-y_n)p\\bm{x}n) \\ \u0026= \\sum{n=1}^{N}{(y_n-p)\\bm{x}_n} \\end{aligned}$\n终于，我们求出了梯度$J（\\bm{\\theta}）$的表达式了，现在我们再来看看它长什么样子： $\\begin{aligned} \\nabla J（\\bm{\\theta}）\u0026= \\sum_{n=1}^{N}{(y_n-p)\\bm{x}_n} \\end{aligned}$ 它是如此简洁优雅，这就是我们选取sigmoid函数的原因之一。当然我们也能够把p再展开，即：\n$\\begin{aligned} \\nabla J（\\bm{\\theta}）\u0026= \\sum_{n=1}^{N}{(y_n- \\frac{1}{1+e^{-\\bm{\\theta}^T\\bm{x}_n}} )\\bm{x}_n} \\end{aligned}$\n梯度下降法（GD）与随机梯度下降法（SGD） 现在我们已经解出了损失函数 $J_\\theta$在任意 $\\theta$处的梯度 $\\nabla J（\\bm{\\theta}）$，可是我们怎么算出来 $\\theta*$ 呢？ 回到之前的问题，我们现在要求损失函数取最小值时候的的$\\theta*$值： $\\bm{\\theta^*} = arg\\min_{w}J(\\bm{\\theta})$，\n梯度下降法(Gradient Descent)，可以用来解决这个问题。核心思想就是先随便初始化一个$\\theta_0$ 然后给定一个步长$\\eta$ 通过不断地修改$\\bm{\\theta}$，从而最后靠近到达取得最小值的点，即不断进行下面的迭代过程，直到达到指定次数，或者梯度等于0为止。 $\\bm{\\theta}_{t+1} = \\bm{\\theta}_t + \\eta\\nabla F（\\bm{\\theta}）$\n随机梯度下降法（Stochastic Gradient Descent），如果我们能够在每次更新过程中，加入一点点噪声扰动，可能会更加快速地逼近最优值。在SGD中，我们不直接使用$\\nabla F（\\bm{\\theta}）$,，而是采用另一个输出为随机变量的替代函数 $G(\\bm{\\theta})$ $\\bm{\\theta}_{t+1} = \\bm{\\theta}_t + \\eta G(\\bm{\\theta})$ 当然，这个替代函数 $G(\\bm{\\theta})$需要满足它的期望值等于$\\nabla F（\\bm{\\theta}）$，相当于这个函数围绕着$\\nabla F（\\bm{\\theta}）$的输出值随机波动。\n在这里我先解释一个问题：为什么可以用梯度下降法？\n因为逻辑回归的损失函数L是一个连续的凸函数（conveniently convex）。这样的函数的特征是，它只会有一个全局最优的点，不存在局部最优。对于GD跟SGD最大的潜在问题就是它们可能会陷入局部最优。然而这个问题在逻辑回归里面就不存在了，因为它的损失函数的良好特性，导致它并不会有好几个局部最优。当我们的GD跟SGD收敛以后，我们得到的极值点一定就是全局最优的点，因此我们可以放心地用GD跟SGD来求解。\n好了，那我们要怎么实现学习算法呢？其实很简单，注意我们GD求导每次都耿直地用到了所有的样本点，从1一直到N都参与梯度计算。 $\\begin{aligned} \\nabla J（\\bm{\\theta}）\u0026= -\\sum_{n=1}^{N}{(y_n- \\frac{1}{1+e^{-\\bm{\\theta}^T\\bm{x}_n}} )\\bm{x}_n} \\end{aligned}$ 在SGD中，我们每次只要均匀地、随机选取其中一个样本 $(\\bm{x_i},y_i)$,用它代表整体样本，即把它的值乘以N，就相当于获得了梯度的无偏估计值，即 $E(G(\\bm{\\theta})) = \\nabla F(\\bm{\\theta})$\nE代表期望值，通常表示为E(X)，其中X是一个随机变量，表示对这个随机变量的所有可能取值按概率加权的平均值。\n这样我们前面的求和就没有了，同时$\\eta N$都是常数,N的值刚好可以并入$\\eta$中，因此SGD的迭代更新公式为： $\\bm{\\theta}_{t+1} = \\bm{\\theta}_t + \\eta {(y_n- \\frac{1}{1+e^{-\\bm{\\theta}^T\\bm{x}_n}} )\\bm{x}_n}$ 其中$(\\bm{x_i},y_i)$是对所有样本随机抽样的一个结果。\n可解释性 辑回归最大的特点就是可解释性很强。 在模型训练完成之后，我们获得了一组n维的权重向量$\\theta*$和偏差$\\theta_0$ 对于权重向量$\\theta*$，它的每一个维度的值，代表了这个维度的特征对于最终分类结果的贡献大小。假如这个维度是正，说明这个特征对于结果是有正向的贡献，那么它的值越大，说明这个特征对于分类为正起到的作用越重要。\n对于偏差$\\theta_0$，一定程度代表了正负两个类别的判定的容易程度。假如$\\theta_0$是0，那么正负类别是均匀的。如果$\\theta_0$大于0，说明它更容易被分为正类，反之亦然。\n根据逻辑回归里的权重向量在每个特征上面的大小，就能够对于每个特征的重要程度有一个量化的清楚的认识，这就是为什么说逻辑回归模型有着很强的解释性的原因。\n正则项 对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）为了解决过拟合问题，具体参考:https://blog.csdn.net/liaomin416100569/article/details/130289602?spm=1001.2014.3001.5501。\n如何用逻辑回归处理多标签问题 逻辑斯蒂回归本身只能用于二分类问题，如果实际情况是多分类的，那么就需要对模型进行一些改动，以下是三种比较常用的将逻辑斯蒂回归用于多分类的方法：\nOne vs One OvO 的方法就是将多个类别中抽出来两个类别，然后将对应的样本输入到一个逻辑斯蒂回归的模型中，学到一个对这两个类别的分类器，然后重复以上的步骤，直到所有类别两两之间都存在一个分类器。 假设存在四个类别，那么分类器的数量为6个，表格如下： 分类器的数量直接使用 $C_2^k$就可以了，k 代表类别的数量。\n在预测时，需要运行每一个模型，然后记录每个分类器的预测结果，也就是每个分类器都进行一次投票，取获得票数最多的那个类别就是最终的多分类的结果。\n比如在以上的例子中，6个分类器有3个投票给了类别3，1个投票给了类别2，1个投票给类别1，最后一个投票给类别0，那么就取类别3为最终预测结果。\nOvO 的方法中，当需要预测的类别变得很多的时候，那么我们需要进行训练的分类器也变得很多了，这一方面提高了训练开销，但在另一方面，每一个训练器中，因为只需要输入两个类别对应的训练样本即可，这样就又减少了开销。\n从预测的角度考虑，这种方式需要运行的分类器非常多，而无法降低每个分类器的预测时间复杂度，因此预测的开销较大。\nOne vs All 针对问题：一个样本对应多个标签。 OvA 的方法就是从所有类别中依次选择一个类别作为1，其他所有类别作为0，来训练分类器，因此分类器的数量要比 OvO 的数量少得多。\n通过以上例子可以看到，分类器的数量实际上就是类别的数量，也就是k。\n虽然分类器的数量下降了，但是对于每一个分类器来说，训练时需要将所有的训练数据全部输入进去进行训练，因此每一个分类器的训练时间复杂度是高于 OvO 的。\n从预测的方面来说，因为分类器的数量较少，而每个分类器的预测时间复杂度不变，因此总体的预测时间复杂度小于 OvA。\n预测结果的确定，是根据每个分类器对其对应的类别1的概率进行排序，选择概率最高的那个类别作为最终的预测类别。\nsklearn实战 LogisticRegression训练乳腺癌肿瘤分类 klearn.linear_model模块提供了很多模型供我们使用，比如Logistic回归、Lasso回归、贝叶斯脊回归等，可见需要学习的东西还有很多很多。我们使用LogisticRegressioin。 让我们先看下LogisticRegression这个函数，一共有14个参数：\n参数说明如下：\npenalty：惩罚项，str类型，可选参数为l1和l2，默认为l2。用于指定惩罚项中使用的规范。newton-cg、sag和lbfgs求解算法只支持L2规范。L1G规范假设的是模型的参数满足拉普拉斯分布，L2假设的模型参数满足高斯分布，所谓的范式就是加上对参数的约束，使得模型更不会过拟合(overfit)，但是如果要说是不是加了约束就会好，这个没有人能回答，只能说，加约束的情况下，理论上应该可以获得泛化能力更强的结果。 dual：对偶或原始方法，bool类型，默认为False。对偶方法只用在求解线性多核(liblinear)的L2惩罚项上。当样本数量\u003e样本特征的时候，dual通常设置为False。 tol：停止求解的标准，float类型，默认为1e-4。就是求解到多少的时候，停止，认为已经求出最优解。 c：正则化系数λ的倒数，float类型，默认为1.0。必须是正浮点型数。像SVM一样，越小的数值表示越强的正则化。 fit_intercept：是否存在截距或偏差，bool类型，默认为True。 intercept_scaling：仅在正则化项为\"liblinear\"，且fit_intercept设置为True时有用。float类型，默认为1。 class_weight：用于标示分类模型中各种类型的权重，可以是一个字典或者’balanced’字符串，默认为不输入，也就是不考虑权重，即为None。如果选择输入的话，可以选择balanced让类库自己计算类型权重，或者自己输入各个类型的权重。举个例子，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9,1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。如果class_weight选择balanced，那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低，样本量越少，则权重越高。当class_weight为balanced时，类权重计算方法如下：n_samples / (n_classes * np.bincount(y))。n_samples为样本数，n_classes为类别数量，np.bincount(y)会输出每个类的样本数，例如y=[1,0,0,1,1],则np.bincount(y)=[2,3]。 那么class_weight有什么作用呢？ 在分类模型中，我们经常会遇到两类问题： 1.第一种是误分类的代价很高。比如对合法用户和非法用户进行分类，将非法用户分类为合法用户的代价很高，我们宁愿将合法用户分类为非法用户，这时可以人工再甄别，但是却不愿将非法用户分类为合法用户。这时，我们可以适当提高非法用户的权重。 第二种是样本是高度失衡的，比如我们有合法用户和非法用户的二元样本数据10000条，里面合法用户有9995条，非法用户只有5条，如果我们不考虑权重，则我们可以将所有的测试集都预测为合法用户，这样预测准确率理论上有99.95%，但是却没有任何意义。这时，我们可以选择balanced，让类库自动提高非法用户样本的权重。提高了某种分类的权重，相比不考虑权重，会有更多的样本分类划分到高权重的类别，从而可以解决上面两类问题。 random_state：随机数种子，int类型，可选参数，默认为无，仅在正则化优化算法为sag,liblinear时有用。 solver：优化算法选择参数，只有五个可选参数，即newton-cg,lbfgs,liblinear,sag,saga。默认为liblinear。solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是： liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。 lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。 sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。 saga：线性收敛的随机优化算法的的变重。 总结： liblinear适用于小数据集，而sag和saga适用于大数据集因为速度更快。 对于多分类问题，只有newton-cg,sag,saga和lbfgs能够处理多项损失，而liblinear受限于一对剩余(OvR)。啥意思，就是用liblinear的时候，如果是多分类问题，得先把一种类别作为一个类别，剩余的所有类别作为另外一个类别。一次类推，遍历所有类别，进行分类。 newton-cg,sag和lbfgs这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的L1正则化，只能用于L2正则化。而liblinear和saga通吃L1正则化和L2正则化。 同时，sag每次仅仅使用了部分样本进行梯度迭代，所以当样本量少的时候不要选择它，而如果样本量非常大，比如大于10万，sag是第一选择。但是sag不能用于L1正则化，所以当你有大量的样本，又需要L1正则化的话就要自己做取舍了。要么通过对样本采样来降低样本量，要么回到L2正则化。 从上面的描述，大家可能觉得，既然newton-cg, lbfgs和sag这么多限制，如果不是大样本，我们选择liblinear不就行了嘛！错，因为liblinear也有自己的弱点！我们知道，逻辑回归有二元逻辑回归和多元逻辑回归。对于多元逻辑回归常见的有one-vs-rest(OvR)和many-vs-many(MvM)两种。而MvM一般比OvR分类相对准确一些。郁闷的是liblinear只支持OvR，不支持MvM，这样如果我们需要相对精确的多元逻辑回归时，就不能选择liblinear了。也意味着如果我们需要相对精确的多元逻辑回归不能使用L1正则化了。 max_iter：算法收敛最大迭代次数，int类型，默认为10。仅在正则化优化算法为newton-cg, sag和lbfgs才有用，算法收敛的最大迭代次数。 multi_class：分类方式选择参数，str类型，可选参数为ovr和multinomial，默认为ovr。ovr即前面提到的one-vs-rest(OvR)，而multinomial即前面提到的many-vs-many(MvM)。如果是二元逻辑回归，ovr和multinomial并没有任何区别，区别主要在多元逻辑回归上。 OvR和MvM有什么不同？ OvR的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。其他类的分类模型获得以此类推。 而MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归，得到模型参数。我们一共需要T(T-1)/2次分类。 可以看出OvR相对简单，但分类效果相对略差（这里指大多数样本分布情况，某些样本分布下OvR可能更好）。而MvM分类相对精确，但是分类速度没有OvR快。如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg,lbfgs和sag都可以选择。但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。 verbose：日志冗长度，int类型。默认为0。就是不输出训练过程，1的时候偶尔输出结果，大于1，对于每个子模型都输出。 warm_start：热启动参数，bool类型。默认为False。如果为True，则下一次训练是以追加树的形式进行（重新使用上一次的调用作为初始化）。 n_jobs：并行数。int类型，默认为1。1的时候，用CPU的一个内核运行程序，2的时候，用CPU的2个内核运行程序。为-1的时候，用所有CPU的内核运行程序。 二分类使用乳腺癌肿瘤的临床测量指标来演示逻辑回归\nload_breast_cancer：乳腺癌数据集，共有569个样本，其中212个恶性、357个良性。 label中1表示恶性，0表示良性\n#%%\r\"\"\"\rsklearn中的load_breast_cancer数据集是一个二分类的数据集，包含了乳腺癌肿瘤的临床测量指标\r\"\"\"\rfrom sklearn.datasets import load_breast_cancer\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.metrics import accuracy_score\rdata = load_breast_cancer()\r# 加载数据集\rdata = load_breast_cancer()\rX = data.data # 特征\ry = data.target # 标签\r# 划分训练集和测试集\rX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r# 定义逻辑回归模型\r\"\"\"\r逻辑回归是一种广泛使用的二分类模型，其常用的优化算法是迭代算法，\r比如梯度下降算法。收敛是指在训练过程中，模型参数的更新已经收敛到某个稳定的值，\r此时继续迭代将不会产生更好的训练效果。max_iter是scikit-learn中LogisticRegression类的一个参数，\r表示最大迭代次数，一旦达到这个迭代次数，则认为模型已经收敛。\r\"\"\"\rlr_model = LogisticRegression(max_iter=2100)\r# 拟合模型\rlr_model.fit(X_train, y_train)\r# 预测训练集和测试集上的结果\rtrain_pred = lr_model.predict(X_train)\rtest_pred = lr_model.predict(X_test)\r# 输出准确率\rprint('Train accuracy score:', accuracy_score(y_train, train_pred))\rprint('Test accuracy score:', accuracy_score(y_test, test_pred)) # 输出数据集中标签的维度 输出 Train accuracy score: 0.9538461538461539 Test accuracy score: 0.956140350877193\n注意逻辑回归配置：max_iter=5000，代表收敛次数，默认100回抛出STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.，如果收敛次数不够可能导致梯度下降无法到达最小的位置。 如果使用默认的100，算出的准确率是0.94，设置为2100,准确率是0.95,设置为2100以上也是0.95时间就很慢，所以2100是个合适的值\n补充知识 accuracy_score和mean_squared_error都是用于评估模型性能的指标，但是适用于不同类型的问题。 accuracy_score通常用于分类问题中，它可以衡量分类器在数据集上的分类准确率，其计算公式如下： $$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$ 其中:\nTP表示真正例（True Positive），即真实为正类且被分类器预测为正类的样本数； TN表示真负例（True Negative），即真实为负类且被分类器预测为负类的样本数； FP表示假正例（False Positive），即真实为负类但被分类器预测为正类的样本数； FN表示假负例（False Negative），即真实为正类但被分类器预测为负类的样本数。 而mean_squared_error则通常用于回归问题中，它可以衡量预测值与真实值之间的差距，其计算公式如下： $$MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 $$\n总的来说，accuracy_score和mean_squared_error都是用于评估模型性能而且是分类和回归模型的两种标准指标，但是适用于不同类型的问题，accuracy_score适用于分类任务的评估，而mean_squared_error适用于回归任务的评估。\nOneVsRestClassifier葡萄酒数据集 OVR（O vs Rest[剩余部分]）分类葡萄酒数据,也就是前面的One vs All load_wine数据集是一个经典、易于理解的、多类别分类数据集，一共包含了178个葡萄酒样本，每个样本有13个特征，分为三个类别。这三个类别分别代表了三种不同的葡萄酒品种。具体而言，这三个类别分别为：\nclass_0: 代表第一种葡萄酒品种。 class_1: 代表第二种葡萄酒品种。 class_2: 代表第三种葡萄酒品种。 代码\n#%%\r\"\"\"\rsklearn中的load_breast_cancer数据集是一个二分类的数据集，包含了乳腺癌肿瘤的临床测量指标\r\"\"\"\rfrom sklearn.datasets import load_wine\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.metrics import accuracy_score\rdata = load_wine()\rX = data.data # 特征\ry = data.target # 标签\r# 划分训练集和测试集\rX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r# 定义逻辑回归模型\rlr_model = LogisticRegression(max_iter=2100)\rovr = OneVsRestClassifier(lr_model)\r# 拟合模型\rovr.fit(X_train, y_train)\r# 预测训练集和测试集上的结果\rtrain_pred = ovr.predict(X_train)\rtest_pred = ovr.predict(X_test)\r# 输出准确率\rprint('Train accuracy score:', accuracy_score(y_train, train_pred))\rprint('Test accuracy score:', accuracy_score(y_test, test_pred)) # 输出 输出：\nTrain accuracy score: 0.9788732394366197\rTest accuracy score: 1.0 测试数据集正确率100%\nOneVsOneClassifier葡萄酒数据集 同上数据集\novo = OneVsOneClassifier(lr_model)\rovo.fit(X_train, y_train)\r# 拟合模型\rovo.fit(X_train, y_train)\r# 预测训练集和测试集上的结果\rtrain_pred1 = ovo.predict(X_train)\rtest_pred1 = ovo.predict(X_test)\r# 输出准确率\rprint('Train accuracy score:', accuracy_score(y_train, train_pred1))\rprint('Test accuracy score:', accuracy_score(y_test, test_pred1)) 输出： Train accuracy score: 0.9929577464788732 Test accuracy score: 1.0\n测试数据集正确率100%",
    "description": "概述 逻辑回归（Logistic Regression）是一种用于解决二分类或多分类问题的统计学习方法。它以自变量线性组合的形式进行建模，并使用Sigmoid函数将结果映射到[0, 1]的值域内，表示样本属于某个类别的概率。 Logistic Regression是最广泛使用的一种算法，逻辑回归常年在机器学习算法排名前10。\n逻辑回归推导 线性回归 线性回归的表达式： $f(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+….+\\theta_nx_n$ 转换为矩阵乘法： $[[x_1,x_2….,x_n]]$点乘$[[\\theta_1,\\theta_2…..,\\theta_n]]^T$ 矩阵演示： 首先，假设我们有一个包含3个样本、每个样本有2个特征的训练集X：\nX = [[1, 2], [3, 4], [5, 6]] 其中，每个样本都有两个特征。接着，我们随机初始化参数向量θ：\nθ = [[0.5,0.5]]\rθ.T=[[0.5],[0.5]]\rX * θ = [[1, 2], [3, 4], [5, 6]] * [[0.5], [0.5]] = [[10.5+20.5], [30.5+40.5], [50.5+60.5]] = [[1.5], [3.5], [5.5]] 所以： $f(x)=\\theta_0+\\theta^Tx$ 如果在x数据集加上一列常量1，$\\theta_0$加入到$\\theta$矩阵中，也就能再次缩写 $f(x)=\\theta^Tx$\n$\\theta$是权值,它与输出y之间的关系强度。如果权值越大，则输入特征对输出的影响就越大；如果权值越小，则输入特征对输出的影响就越小。。\n逻辑回归 逻辑回归(Logistic Regression, LR)模型其实仅在线性回归的基础上，套用了一个逻辑函数，但也就由于这个逻辑函数，使得逻辑回归模型成为了机器学习领域一颗耀眼的明星，更是计算广告学的核心。 通常线性方程的值域为 $(-\\infty，+\\infty)$,而概率的值域为[0, 1]，因此我们在这个基础上做一个变形，完成从 $(-\\infty，+\\infty)$,到[0,1]的转换。 逻辑回归的假设函数可以表示为 $$h_\\theta(x)=g(\\theta^Tx)$$ 这个转换函数g就叫做Sigmoid函数，函数的表达式： $$g(z)={1\\over(1+e^{-z})}$$ 我们来看下Sigmoid函数的图形\nimport numpy as np\rimport matplotlib.pyplot as plt\rdef sigmoid(t):\rreturn 1 / (1 + np.exp(-t))\rx = np.linspace(-10, 10, 500)\ry = sigmoid(x)\rplt.plot(x, y)\rplt.show() 于是我们得到了这样的关系式：",
    "tags": [],
    "title": "机器学习实战教程（十）：逻辑回归",
    "uri": "/docs/programming/ai/machine_learning/algorithms/action_10_logic/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 核心算法",
    "content": "什么是SVM？ VM的英文全称是Support Vector Machines，我们叫它支持向量机。支持向量机是我们用于分类的一种算法。让我们以一个小故事的形式，开启我们的SVM之旅吧。\n在很久以前的情人节，一位大侠要去救他的爱人，但天空中的魔鬼和他玩了一个游戏。\n魔鬼在桌子上似乎有规律放了两种颜色的球，说：“你用一根棍分开它们？要求：尽量在放更多球之后，仍然适用。” 于是大侠这样放，干的不错？ 然后魔鬼，又在桌上放了更多的球，似乎有一个球站错了阵营。显然，大侠需要对棍做出调整。 SVM就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙。这个间隙就是球到棍的距离。 现在好了，即使魔鬼放了更多的球，棍仍然是一个好的分界线。 现在，大侠没有棍可以很好帮他分开两种球了，现在怎么办呢？当然像所有武侠片中一样大侠桌子一拍，球飞到空中。然后，凭借大侠的轻功，大侠抓起一张纸，插到了两种球的中间。 现在，从空中的魔鬼的角度看这些球，这些球看起来像是被一条曲线分开了。 再之后，无聊的大人们，把这些球叫做data，把棍子叫做classifier, 找到最大间隙的trick叫做optimization，拍桌子叫做kernelling, 那张纸叫做hyperplane。\n概述一下：\n当一个分类问题，数据是线性可分的，也就是用一根棍就可以将两种小球分开的时候，我们只要将棍的位置放在让小球距离棍的距离最大化的位置即可，寻找这个最大间隔的过程，就叫做最优化。但是，现实往往是很残酷的，一般的数据是线性不可分的，也就是找不到一个棍将两种小球很好的分类。这个时候，我们就需要像大侠一样，将小球拍起，用一张纸代替小棍将小球进行分类。想要让数据飞起，我们需要的东西就是核函数(kernel)，用于切分小球的纸，就是超平面。\n数学建模 支持向量机（Support Vector Machine, SVM）是一类按监督学习（supervised learning）方式对数据进行广义线性分类，其决策边界是对学习样本求解的最大边距超平面（maximum-margin hyperplane）。SVM可以通过核函数进行非线性分类，是常见的核学习（kernel learning）方法之一。\n支持向量机（support vector machines）是寻找一个超平面来对样本进行分割，分割的原则是间隔最大化，最终转化为一个凸二次规划问题来求解。它既能解决线性可分又能解决线性不可能，既能解决分类问题又能完成回归问题。\n间隔最大化 当训练样本线性可分时使用硬间隔最大化（Hard Margin SVM）或者近似线性可分时使用软件最大化（Soft Margin SVM）。当训练样本线性不可分时使用核函数和软间隔最大化。 在实际问题中往往都存在着决策边界不唯一的情况，这就是不适定问题。给定训练样本集 $D = {(x_{1}, y_{1}), (x_{2}, y_{2}), \\dots , (x_{m}, y_{m})}, y_{i} \\in {-1, 1}$ 分类算法的基本思想就是基于训练集在样本空间中找到一个划分超平面，但是能将训练样本分开的划分超平面可能有很多，所以，应该努力地去找哪一个？ 而svm找到的这条直线希望距离最近的红色的点和蓝色的点，距离决策边界尽可能的远，这样就能保证模型的泛化能力。svm尝试寻找一个最优的决策边界，距离两个类别最近的样本最远，图中3个点到决策边界距离相同。这三个点就叫做支持向量(support vector)。而平行于决策边界的两条直线之间的距离就是margin,svm就是要最大化 margin，这样就把这个问题转化称为最优化问题。\n最优化问题 分类间隔方程 svm最大化margin,margin=2d,，就对应于最大化d，也就是点到直线的距离最大。回忆解析几何，在二维空间中点$(x,y)$到直线$Ax+By+C=0$的距离公式 :$\\frac{\\mid Ax+By+C \\mid} {\\sqrt{A^2+B^2}}$ 将其拓展到n维$\\theta^T \\cdot x_{b}=0$ 其中$\\theta$包含截距和系数，$x_b$就是在 $x$ 样本中加入一行常数1，这就跟之前的线性回归中是一样的。\n如果将截距提出来就是$w^Tx + b = 0$ 其中的w就是对样本中的每一个数据赋予了一个权值。这就是直线方程的两种不同表示方式。那么，由此可以得到新的点到直线距离方程： $\\frac{\\mid w^Tx + b \\mid}{\\parallel w \\parallel}$，其中$\\parallel w \\parallel = \\sqrt {w_{1}^2 + w_{2}^2 + \\dots + w_{n}^2}$\n约束条件 看起来，我们已经顺利获得了目标函数的数学形式。但是为了求解w的最大值。我们不得不面对如下问题：\n我们如何判断超平面是否将样本点正确分类？ 我们知道要求距离d的最大值，我们首先需要找到支持向量上的点，怎么在众多的点中选出支持向量上的点呢？ 上述我们需要面对的问题就是约束条件，也就是说我们优化的变量d的取值范围受到了限制和约束。事实上约束条件一直是最优化问题里最让人头疼的东西。但既然我们已经知道了这些约束条件确实存在，就不得不用数学语言对他们在这里插入图片描述 进行描述。但SVM算法通过一些巧妙的小技巧，将这些约束条件融合到一个不等式里面。 这个二维平面上有两种点，我们分别对它们进行标记：\n红颜色的圆点标记为1，我们人为规定其为正样本； 蓝颜色的五角星标记为-1，我们人为规定其为负样本。 对每个样本点xi加上一个类别标签yi： 中间那根直线就是决策边界$w^Tx+b=0$ ,而上下两根直线意味着距离一定大于 d ，从而 $\\begin {cases} \\frac{w^Tx^{(i)} + b} {\\parallel w \\parallel} \\geq d \\qquad \\forall y^{(i)} = 1 \\ \\frac{w^Tx^{(i)} + b}{\\parallel w \\parallel} \\leq -d \\qquad \\forall y^{(i)} = -1 \\ \\end {cases}$ 此时，假设两类样本分别为1和-1。然后将上述式子的左右两侧同时除以d，得到 $\\begin {cases} \\frac{w^Tx^{(i)} + b} {\\parallel w \\parallel d} \\geq 1 \\qquad \\forall y^{(i)} = 1 \\ \\frac{w^Tx^{(i)} + b}{\\parallel w \\parallel d} \\leq -1 \\qquad \\forall y^{(i)} = -1 \\ \\end {cases}$\n其中 |w|和 d都是一个标量（不影响求极限值），此时消去分母，得到 $\\begin {cases} {w^Tx^{(i)} + b} \\geq 1 \\qquad \\forall y^{(i)} = 1 \\ {w^Tx^{(i)} + b} \\leq -1 \\qquad \\forall y^{(i)} = -1 \\ \\end {cases}$ 此时，就得到了两个式子，接下来通过一个小技巧将两个式子合并成一个。 $\\boxed {y_{i} (w^Tx^{(i)} + b) \\geq 1}$\n最大化目标函数 现在整合一下思路，我们已经得到我们的目标函数： $max \\frac{\\mid w^Tx +b \\mid}{\\parallel w \\parallel}$ 我们的优化目标是是d最大化。我们已经说过，我们是用支持向量上的样本点求解d的最大化的问题的。那么支持向量上的样本点有什么特点呢？ $|w^Tx+b|=1$ 所以转化为最大化$max \\frac{1}{\\parallel w \\parallel}$ 也即是最小化$min\\parallel w \\parallel$ 但是往往为了求导方便，通常都是最小化$\\boxed{min\\frac{1}{2} \\parallel w \\parallel^2}$ ，但是这是一个有约束条件的最优化问题，就是要满足$s.t. \\quad y_{i} (w^Tx^{(i)} + b) \\geq 1, \\quad i=1,2,\\dots,m$ 这里m是样本点的总个数，缩写s.t.表示\"Subject to\"，是\"服从某某条件\"的意思。上述公式描述的是一个典型的不等式约束条件下的二次型函数优化问题，同时也是支持向量机的基本数学模型。 解决有约束问题的最优化问题需要使用拉格朗日乘子法得到对偶问题。则该问题函数： $$\\boxed{L(w, b, \\alpha) = \\frac{1}{2} \\parallel w \\parallel^2 + \\sum_{i=1}^m \\alpha_{i} (1-y_{i}(w^Tx_{i}+b))}$$\n其中 $\\alpha_i$ 就是拉格朗日乘子。这就是Hard Margin SVM。\nSoft Margin和SVM正则化 如果在实际应用过程中有一个蓝色的点出现在红色的点附近，即两类相对较为接近，但整体跟蓝色的点差异明显，可以看做是一个特殊点或者错误的奇点，此时就会误导，导致最终的hard margin分类边界是直线1，此时模型的泛化能力就值得怀疑。正常来说应该像直线2一样忽略那个极度特殊的蓝点，保证大多数的数据到直线的距离最远，可能这才是最好的分类边界，也就是泛化能力更高。这也能间接地说明如果模型的准确率过高可能会导致模型的过拟合。\n还有一种更一般的例子，那就是如果有一个蓝色的点混进了红色的点当中，这就导致数据集根本就是线性不可分的情况，根本找不出一条直线能够将这两类分开，在这种情况下Hard margin就不再是泛化能力强不强的问题，而是根本找不出一条直线将其分开。 因此不管才以上两种情况的哪个角度出发，都应该考虑给予svm模型部分容错能力。由此引出Soft Margin SVM。 $min\\frac{1}{2} \\parallel w \\parallel^2， \\quad s.t. \\quad y_{i} (w^Tx^{(i)} + b) \\geq 1 - \\zeta_{i}, \\ \\quad \\zeta_{i} \u003e0, \\quad i=1,2,\\dots,m$ 相比于Hard Margin SVM，就相当于把条件放得更加宽松一些，直观从图上理解，就是相当于把上面这根直线放宽到虚线。此外，更加重要的一点是$\\zeta_{i}$并不是一个固定值，而是相对应于每个样本$x_i$都有一个$\\zeta_{i}$，但是对于这个$\\zeta_{i}$也需要一定限制，并不是说可以无限放宽这个条件。也就是这个容错空间不能太大，因此需要对其加以限制。 L1正则 $\\boxed { min(\\frac{1}{2} \\parallel w \\parallel^2 +C \\sum_{i=1}^m \\zeta_{i})， \\quad s.t. \\quad y_{i} (w^Tx^{(i)} + b) \\geq 1 - \\zeta_{i}, \\ \\qquad \\qquad \\qquad \\qquad \\zeta_{i} \\geq 0, \\quad i=1,2,\\dots,m }$ 引入一个新的超参数C去权衡容错能力和目标函数，其实这也可以理解为我们为其加入了L1正则项，避免模型向一个极端方向发展，使得对于极端的数据集不那么敏感，对于未知的数据有更好的泛化能力。这个所谓的L1正则跟之前的L1正则不同的是没有绝对值，只是因为已经限制了$\\zeta_{i} \\geq 0$，所以不加绝对值也就合理了。C越大越趋近于一个Hard Margin SVM，C越小，就意味着有更大的容错空间。svm的正则与线性回归的正则不同的是在于C的位置不同。具体原因后续理解更深刻之后再更新。 那么有L1正则，相对应就有L2正则,如下。 $\\boxed { min(\\frac{1}{2} \\parallel w \\parallel^2 +C \\sum_{i=1}^m \\zeta_{i}^2)， \\quad s.t. \\quad y_{i} (w^Tx^{(i)} + b) \\geq 1 - \\zeta_{i}, \\ \\qquad \\qquad \\qquad \\qquad \\zeta_{i} \u003e0, \\quad i=1,2,\\dots,m }$\nSklearn的svm 在实际使用SVM的时候和KNN一样需要对数据进行标准化处理，因为这两者都涉及距离。因为当数据尺度相差过大的话，比如下图横轴0-1，纵轴0-10000。所以先进行标准化是必要的。\n第一步，准备一个简单二分类数据集：\nimport numpy as np\rimport matplotlib.pyplot as plt\rfrom sklearn import datasets\r\"\"\"\rload_iris是一个经典的机器学习数据集，它包含了150个样本\r这个数据集中的四个特征分别是花萼长度（sepal length）、花萼宽度（sepal width）、花瓣长度（petal length）和花瓣宽度（petal width），\r它们都是以厘米（cm）为单位测量的。目标变量是鸢尾花的种类，\r有三种不同的种类：Setosa、Versicolour和Virginica。\r它们的中文名分别是山鸢尾、杂色鸢尾和维吉尼亚鸢尾。\r\"\"\"\riris = datasets.load_iris()\rx = iris.data\ry = iris.target\r# 只做一个简单的二分类,获取分类是山鸢尾、杂色鸢尾的数据，同时取2维的特征就行了\rx = x[y\u003c2, :2]\ry = y[y\u003c2]\r#分别绘制出分类是0和1的点，不同的scatter颜色不一样\rplt.scatter(x[y==0, 0], x[y==0, 1])\rplt.scatter(x[y==1, 0], x[y==1, 1])\rplt.show() 实现svm，先使用一个比较大的C。\n# 标准化数据\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.svm import LinearSVC\r#数据归一化\rstandardscaler = StandardScaler()\rstandardscaler.fit(x)\rx_standard = standardscaler.transform(x)\rsvc = LinearSVC(C=1e9)\rsvc.fit(x_standard, y)\rdef plot_decision_boundary(model, axis):\rx0, x1 = np.meshgrid(np.linspace(axis[0], axis[1], int((axis[1] - axis[0])*100)).reshape(1, -1),\rnp.linspace(axis[2], axis[3], int((axis[3] - axis[2])*100)).reshape(1, -1),)\rx_new = np.c_[x0.ravel(), x1.ravel()]\ry_predict = model.predict(x_new)\rzz = y_predict.reshape(x0.shape)\rfrom matplotlib.colors import ListedColormap\rcustom_cmap = ListedColormap(['#EF9A9A', '#FFF59D', '#90CAF9'])\rplt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)\rw = model.coef_[0]\rb = model.intercept_[0]\r# w0*x0 + w1*x1 + b = 0\r# x1 = -w0/w1 * x0 - b/w1\rplot_x = np.linspace(axis[0], axis[1], 200)\rup_y = -w[0]/w[1] * plot_x - b/w[1] + 1/w[1]\rdown_y = -w[0]/w[1] * plot_x - b/w[1] - 1/w[1]\rup_index = (up_y \u003e= axis[2]) \u0026 (up_y \u003c= axis[3])\rdown_index = (down_y \u003e= axis[2]) \u0026 (down_y \u003c= axis[3])\rplt.plot(plot_x[up_index], up_y[up_index], color='black')\rplt.plot(plot_x[down_index], down_y[down_index], color='black')\rplot_decision_boundary(svc, axis=[-3, 3, -3, 3])\rplt.scatter(x_standard[y==0, 0], x_standard[y==0, 1], color='red')\rplt.scatter(x_standard[y==1, 0], x_standard[y==1, 1], color='blue')\rplt.show() 使用一个比较小的C，对比C取不同值的效果。\nsvc2 = LinearSVC(C=0.01)\rsvc2.fit(x_standard, y)\rplot_decision_boundary(svc2, axis=[-3, 3, -3, 3])\rplt.scatter(x_standard[y==0, 0], x_standard[y==0, 1], color='red')\rplt.scatter(x_standard[y==1, 0], x_standard[y==1, 1], color='blue')\rplt.show() 对比两幅图可以发现，当C较小时，误将一个红色的点分到蓝色当中，这也再次验证了当C越小，就意味着有更大的容错空间。\nSVM中使用多项式特征 前面一直都在讲的是线性的svm，对于svm来说也可以解决非线性问题，类比线性回归到非线性回归的思想，首先使用多项式特征。\n首先生成数据集：\nimport numpy as np\rimport matplotlib.pyplot as plt\rfrom sklearn import datasets\rx, y = datasets.make_moons()\rx.shape\r# (100, 2)\ry.shape\r# (100,)\rplt.scatter(x[y==0, 0], x[y==0, 1])\rplt.scatter(x[y==1, 0], x[y==1, 1])\rplt.show() 接下来给数据添加一些随机噪声：\nx, y = datasets.make_moons(noise=0.15, random_state=666)\rplt.scatter(x[y==0, 0], x[y==0, 1])\rplt.scatter(x[y==1, 0], x[y==1, 1])\rplt.show() 使用多项式，归一，线性svm\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\rfrom sklearn.svm import LinearSVC\rfrom sklearn.pipeline import Pipeline\rdef PolynomiaSVC(degree, C=1.0):\rreturn Pipeline([\r('poly', PolynomialFeatures(degree=degree)),\r('std_scale', StandardScaler()),\r('linear_svc', LinearSVC(C=C))\r])\rpoly_svc = PolynomiaSVC(degree=3)\rpoly_svc.fit(x, y)\rdef plot_decision_boundary(model, axis):\rx0, x1 = np.meshgrid(np.linspace(axis[0], axis[1], int((axis[1] - axis[0])*100)).reshape(1, -1),\rnp.linspace(axis[2], axis[3], int((axis[3] - axis[2])*100)).reshape(1, -1),)\rx_new = np.c_[x0.ravel(), x1.ravel()]\ry_predict = model.predict(x_new)\rzz = y_predict.reshape(x0.shape)\rfrom matplotlib.colors import ListedColormap\rcustom_cmap = ListedColormap(['#EF9A9A', '#FFF59D', '#90CAF9'])\rplt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)\rplot_decision_boundary(poly_svc, axis=[-1.5, 2.5, -1.0, 1.5])\rplt.scatter(x1[y1==0, 0], x1[y1==0, 1])\rplt.scatter(x1[y1==1, 0], x1[y1==1, 1])\rplt.show() 除了使用这种增加多项式特征之后再给入线性svc中之外，还有一种方法可以实现类似的功能。\nfrom sklearn.svm import SVC\r# 这种方法训练的过程并不完全是先将数据进行标准化，再使用linearSVC这么一个过程\r# SVC中默认的C=0\rdef PolynomiaKernelSVC(degree, C=1.0):\rreturn Pipeline([\r('std_scale', StandardScaler()),\r('kernel_svc', SVC(kernel='poly', degree=degree, C=C))\r])\rpoly_kernel_svc = PolynomiaKernelSVC(degree=3)\rpoly_kernel_svc.fit(x1, y1)\r# Pipeline(memory=None,\r# steps=[('std_scale', StandardScaler(copy=True, with_mean=True, with_std=True)),\r# ('kernel_svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\r# decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\r# kernel='poly', max_iter=-1, probability=False, random_state=None,\r# shrinking=True, tol=0.001, verbose=False))])\rplot_decision_boundary(poly_kernel_svc, axis=[-1.5, 2.5, -1.0, 1.5])\rplt.scatter(x1[y1==0, 0], x1[y1==0, 1])\rplt.scatter(x1[y1==1, 0], x1[y1==1, 1])\rplt.show() 这种方法就是svm中kernel函数。接下来具体说明核函数。\n什么是核函数 在现实任务中，原始的样本空间也许并不存在一个能正确划分两类的超平面，对于这样一个问题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。因此核函数的作用就是使得原本线性不可分的数据变得线性可分。下面是一个使用多项式核映射的过程。 转换为以下函数后，数据变的线性可分。 常用 核函数： 1、线性核：$k(x_{i}, x_{j}) = x_{i}^T x_{j} + c$ 2、多项式核：$k(x_{i}, x_{j}) = (x_{i}^T x_{j} + c)^d$，当d=1时退化为线性核。 3、高斯核RBF： $k(x_{i}, x_{j}) = exp(-\\frac{\\parallel x_{i} - x_{j} \\parallel ^2}{2\\sigma ^2}), \\sigma \u003e0$为高斯核的带宽RBF核：Radial Basis Function Kernel 4、拉普拉斯核：$k(x_{i}, x_{j}) = exp(-\\frac{\\parallel x_{i} - x_{j} \\parallel}{\\sigma})， \\sigma \u003e 0$ 5、Sigmoid核：$k(x_{i}, x_{j}) = tanh(\\beta x_{i}^T x_{j} + \\theta)$\n高斯核函数 高斯核函数是一种常用的核函数，通常用于支持向量机（SVM）等机器学习算法中。它可以将数据从原始空间映射到更高维的空间，使得原本不可分的样本在新的空间中可以被分离开来。\n通俗地说，高斯核函数就像是一种“相似度度量方式”，它可以计算出两个样本之间的相似度。在使用高斯核函数时，我们会首先选择一个中心点，然后计算每个样本点与中心点之间的距离，并将距离作为相似度的度量值。这个距离通常用高斯分布函数进行加权，这也是高斯核函数名称的由来。 更具体地说，高斯核函数可以将两个样本点 $x_i$和$x_j$映射到更高维的空间中，计算它们在新空间中的内积，得到如下公式： $K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)$\n其中，$\\gamma$是一个控制高斯分布宽度的参数，$||x_i - x_j||^2$是样本点 $x_i$和$x_j$之间的欧几里得距离的平方。当 $\\gamma$取值较大时，高斯分布的峰值会变得较窄，相似度的度量会更加关注两个样本点之间的距离；当$\\gamma$取值较小时，高斯分布的峰值会变得较宽，相似度的度量会更加平滑，关注两个样本点之间的整体相似度。\n总的来说，高斯核函数是一种非常灵活、强大的相似度度量方式，可以用于许多机器学习算法中，特别是涉及到非线性分类问题时。\n接下来通过高斯核函数映射来更加直观地理解整个映射的过程。\nimport numpy as np\rimport matplotlib.pyplot as plt\rx = np.arange(-4, 5, 1)\r# array([-4, -3, -2, -1, 0, 1, 2, 3, 4])\ry = np.array((x \u003e= -2) \u0026 (x \u003c= 2), dtype='int')\r# array([0, 0, 1, 1, 1, 1, 1, 0, 0])\rplt.scatter(x[y==0], [0] * len(x[y==0]))\rplt.scatter(x[y==1], [0] * len(x[y==1]))\rplt.show() 高斯核后\ndef gaussian(x, l):\rgamma = 1.0\rreturn np.exp(-gamma *(x-l)**2)\rl1, l2 = -1, 1\rx_new = np.empty((len(x), 2))\rfor i,data in enumerate(x):\rx_new[i, 0] = gaussian(data, l1)\rx_new[i, 1] = gaussian(data, l2)\rplt.scatter(x_new[y==0, 0], x_new[y==0, 1])\rplt.scatter(x_new[y==1, 0], x_new[y==1, 1])\rplt.show() 其实，真正的高斯核函数实现的过程中并不是固定的$\\gamma$，而是对于每一个数据点都是$\\gamma$，这里写死 gamma = 1.0。 高斯函数： $g(x)= \\frac{1}{\\sigma \\sqrt{2 \\pi} } e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})}$ 其中 $\\mu$决定了函数的中心位置，$\\sigma$决定了整个图形 的靠拢程度， $\\sigma$越小，图像越高，图像相对比较集中。相反 $\\sigma$越大，图形就越分散。 $K(x_{i},x_{j}) = e^{-\\gamma \\parallel x_{i} - x_{j} \\parallel ^2}$ $\\gamma$越大，高斯分布越宽；\n$\\gamma$越小，高斯分布越窄。\n接下来，使用sklearn中封装的高斯核函数：\nimport numpy as np\rimport matplotlib.pyplot as plt\rfrom sklearn import datasets\rx, y = datasets.make_moons(noise=0.15, random_state=666)\rfrom sklearn.preprocessing import StandardScaler\rfrom sklearn.svm import SVC\rfrom sklearn.pipeline import Pipeline\rdef RBFKernelSVC(gamma=1.0):\rreturn Pipeline([\r('std_scale', StandardScaler()),\r('svc', SVC(kernel='rbf', gamma=gamma))\r])\rsvc = RBFKernelSVC(gamma=1.0)\rsvc.fit(x, y)\rdef plot_decision_boundary(model, axis):\rx0, x1 = np.meshgrid(np.linspace(axis[0], axis[1], int((axis[1] - axis[0])*100)).reshape(1, -1),\rnp.linspace(axis[2], axis[3], int((axis[3] - axis[2])*100)).reshape(1, -1),)\rx_new = np.c_[x0.ravel(), x1.ravel()]\ry_predict = model.predict(x_new)\rzz = y_predict.reshape(x0.shape)\rfrom matplotlib.colors import ListedColormap\rcustom_cmap = ListedColormap(['#EF9A9A', '#FFF59D', '#90CAF9'])\rplt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)\rplot_decision_boundary(svc, axis=[-1.5, 2.5, -1.0, 1.5])\rplt.scatter(x[y==0, 0], x[y==0, 1])\rplt.scatter(x[y==1, 0], x[y==1, 1])\rplt.show() svc_gamma100 = RBFKernelSVC(gamma=100)\rsvc_gamma100.fit(x, y)\rplot_decision_boundary(svc_gamma100, axis=[-1.5, 2.5, -1.0, 1.5])\rplt.scatter(x[y==0, 0], x[y==0, 1])\rplt.scatter(x[y==1, 0], x[y==1, 1])\rplt.show() svc_gamma10 = RBFKernelSVC(gamma=10)\rsvc_gamma10.fit(x, y)\rplot_decision_boundary(svc_gamma10, axis=[-1.5, 2.5, -1.0, 1.5])\rplt.scatter(x[y==0, 0], x[y==0, 1])\rplt.scatter(x[y==1, 0], x[y==1, 1])\rplt.show() svc_gamma01 = RBFKernelSVC(gamma=0.1)\rsvc_gamma01.fit(x, y)\rplot_decision_boundary(svc_gamma01, axis=[-1.5, 2.5, -1.0, 1.5])\rplt.scatter(x[y==0, 0], x[y==0, 1])\rplt.scatter(x[y==1, 0], x[y==1, 1])\rplt.show() gamma相当于是在调节模型的复杂度，gammma越小模型复杂度越低，gamma越高模型复杂度越高。因此需要调节超参数gamma平衡过拟合和欠拟合。\n本文文字和例题来源：\nhttps://cuijiahua.com/blog/2017/11/ml_8_svm_1.html https://zhuanlan.zhihu.com/p/79679104",
    "description": "什么是SVM？ VM的英文全称是Support Vector Machines，我们叫它支持向量机。支持向量机是我们用于分类的一种算法。让我们以一个小故事的形式，开启我们的SVM之旅吧。\n在很久以前的情人节，一位大侠要去救他的爱人，但天空中的魔鬼和他玩了一个游戏。\n魔鬼在桌子上似乎有规律放了两种颜色的球，说：“你用一根棍分开它们？要求：尽量在放更多球之后，仍然适用。” 于是大侠这样放，干的不错？ 然后魔鬼，又在桌上放了更多的球，似乎有一个球站错了阵营。显然，大侠需要对棍做出调整。 SVM就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙。这个间隙就是球到棍的距离。 现在好了，即使魔鬼放了更多的球，棍仍然是一个好的分界线。 现在，大侠没有棍可以很好帮他分开两种球了，现在怎么办呢？当然像所有武侠片中一样大侠桌子一拍，球飞到空中。然后，凭借大侠的轻功，大侠抓起一张纸，插到了两种球的中间。 现在，从空中的魔鬼的角度看这些球，这些球看起来像是被一条曲线分开了。 再之后，无聊的大人们，把这些球叫做data，把棍子叫做classifier, 找到最大间隙的trick叫做optimization，拍桌子叫做kernelling, 那张纸叫做hyperplane。",
    "tags": [],
    "title": "机器学习实战教程（十一）：支持向量机SVM",
    "uri": "/docs/programming/ai/machine_learning/algorithms/action_11_vectormachine/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 核心算法",
    "content": "聚类概念 聚类是一种无监督的机器学习方法，它主要是通过在数据集中找到相似的样本并将它们分组来发现数据中的模式和结构。聚类算法可以将数据分成具有相似特征的组，每个组被称为一个簇。\n常见的聚类算法有以下几种：\nK-means聚类算法：它是最常见的聚类算法之一，它的目标是将数据集分为K个簇，使得每个簇内的数据点相似度最高，不同簇之间的差异最大。\n层次聚类算法：该算法将数据集中的样本逐渐合并到一起，直到形成一个完整的聚类结构，从而形成一颗聚类树。\n密度聚类算法：它是一种基于数据点密度的聚类算法，它将数据点分为密集的区域和稀疏的区域，并将密集区域看作是一个簇。\n均值漂移聚类算法：该算法使用核密度估计来找到数据点的局部最大值，以确定簇的质心。\nDBSCAN聚类算法：它基于在数据集中的密度来确定簇的个数和形状，它可以识别任意形状的簇。\nK-means简介 K-means是一种基于距离度量的聚类算法，其主要思想是将数据集分成K个簇，每个簇包含距离最近的K个数据点。该算法通过迭代优化簇的中心点，来不断调整簇的划分，最终得到一组最优的簇划分结果。\n通俗来说，K-means算法就像是一位假设聪明的小学生在玩“猜数字”游戏。他会先猜一个数字，然后根据猜测与正确答案的距离（越接近答案距离越小），将答案所在的数字范围分成两个区域。接着，他会重复这个过程，直到将数字范围分成了K个区域为止，并记录下每个区域的中心点。最后，他会告诉你每个数字应该属于哪个区域（或者说簇），并告诉你每个区域的中心点。\n在K-means算法中，我们需要指定簇的个数K，然后随机选择K个数据点作为初始中心点。接着，我们计算每个数据点距离各个中心点的距离，并将其归入距离最近的簇中。然后，重新计算每个簇的中心点，并重复上述过程（在计算每个点到新中心的举例重新归类到簇），直到簇的中心点不再发生变化为止。最终，我们将得到K个簇，每个簇包含一组距离最近的数据点，并且每个数据点只属于一个簇。\n需要注意的是，由于K-means算法的初始中心点是随机选择的，因此可能会得到不同的簇划分结果。为了获得更好的结果，可以多次运行算法，并选择最优的簇划分结果。\nK-means和KNN区别 K-Means和K-NN是两种不同的机器学习算法，其区别如下：\nK-Means是一种聚类算法，它将数据集划分为K个簇，并将每个数据点分配到其最近的簇中心。K-NN是一种分类算法，它根据最近邻居的标签来预测新数据点的标签。\nK-Means需要指定簇的数量K，而K-NN不需要。\nK-Means是一种无监督学习算法，它不需要标记数据，而K-NN是一种监督学习算法，需要标记数据。\nK-Means使用欧几里得距离来计算数据点之间的相似度，而K-NN可以使用不同的距离度量，如曼哈顿距离、余弦相似度等。\nK-Means在处理大规模数据时可能会遇到性能问题，而K-NN可以轻松处理大规模数据。\n总的来说，K-Means和K-NN是两种不同的机器学习算法，适用于不同的问题和数据集。\nKmeans的计算过程 （1）适当选择c个类的初始中心； （2）在k次迭代中，对任意一个样本，求其到c各中心的距离（欧式距离），将该样本归到距离更短的中心所在的类； （3）利用均值等方法更新该类的中心值； （4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。\n假设 现在有4组数据，每组数据有2个维度，对其进行聚类分为2类，将其可视化一下。 $A=(1,1),B=(2,1),C=(4,3),D=(5,4)$ 假设选取两个星的位置为初始中心 $c_1=(1,1),c_2=(2,1)$，计算每个点到初始中心的距离，使用欧式距离得到4个点分别距离两个初始中心的距离，归于最近的类： $D^0第一行表示ABCD四个点到c1的举例，第二行表示ABCD四个点到c2的举例，举例使用欧氏距离公式计算出来，以C为例，到c1这一组的举例是3.61,到c2这一组的举例是2.83说明第一次迭代C是属于group-2$\n通过比较，将其进行归类。并使用平均法更新中心位置。 由于归于group1的只有一个点，一次更新后的中心位置$c_1=(1,1)$，而 $c_{2} = (\\frac{11}{3}, \\frac{8}{3})$\ngroup2的新中心点也就是$x={(x1+x2+x3)\\over 3} ={(2+4+5)\\over3}={11\\over3}$ $y={(y1+y2+y3)\\over 3} ={(1+3+4)\\over3}={8\\over3}$ 再次计算每个点与更新后的位置中心的距离\n继续迭代下去， 此时，与上一次的类别标记无变化，即可停止。\nKmeans的编程实现 #%%\rfrom sklearn.datasets import make_blobs\rfrom sklearn.cluster import KMeans\rimport matplotlib.pyplot as plt\r# 生成随机数据\rX, y = make_blobs(n_samples=300, centers=4, random_state=42)\r# 使用KMeans算法进行聚类\rkmeans = KMeans(n_clusters=4, random_state=42)\rkmeans.fit(X)\r# 绘制聚类结果\rplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\rplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='*', s=150, color='red')\rplt.show()",
    "description": "聚类概念 聚类是一种无监督的机器学习方法，它主要是通过在数据集中找到相似的样本并将它们分组来发现数据中的模式和结构。聚类算法可以将数据分成具有相似特征的组，每个组被称为一个簇。\n常见的聚类算法有以下几种：\nK-means聚类算法：它是最常见的聚类算法之一，它的目标是将数据集分为K个簇，使得每个簇内的数据点相似度最高，不同簇之间的差异最大。\n层次聚类算法：该算法将数据集中的样本逐渐合并到一起，直到形成一个完整的聚类结构，从而形成一颗聚类树。\n密度聚类算法：它是一种基于数据点密度的聚类算法，它将数据点分为密集的区域和稀疏的区域，并将密集区域看作是一个簇。\n均值漂移聚类算法：该算法使用核密度估计来找到数据点的局部最大值，以确定簇的质心。\nDBSCAN聚类算法：它基于在数据集中的密度来确定簇的个数和形状，它可以识别任意形状的簇。\nK-means简介 K-means是一种基于距离度量的聚类算法，其主要思想是将数据集分成K个簇，每个簇包含距离最近的K个数据点。该算法通过迭代优化簇的中心点，来不断调整簇的划分，最终得到一组最优的簇划分结果。\n通俗来说，K-means算法就像是一位假设聪明的小学生在玩“猜数字”游戏。他会先猜一个数字，然后根据猜测与正确答案的距离（越接近答案距离越小），将答案所在的数字范围分成两个区域。接着，他会重复这个过程，直到将数字范围分成了K个区域为止，并记录下每个区域的中心点。最后，他会告诉你每个数字应该属于哪个区域（或者说簇），并告诉你每个区域的中心点。\n在K-means算法中，我们需要指定簇的个数K，然后随机选择K个数据点作为初始中心点。接着，我们计算每个数据点距离各个中心点的距离，并将其归入距离最近的簇中。然后，重新计算每个簇的中心点，并重复上述过程（在计算每个点到新中心的举例重新归类到簇），直到簇的中心点不再发生变化为止。最终，我们将得到K个簇，每个簇包含一组距离最近的数据点，并且每个数据点只属于一个簇。\n需要注意的是，由于K-means算法的初始中心点是随机选择的，因此可能会得到不同的簇划分结果。为了获得更好的结果，可以多次运行算法，并选择最优的簇划分结果。\nK-means和KNN区别 K-Means和K-NN是两种不同的机器学习算法，其区别如下：\nK-Means是一种聚类算法，它将数据集划分为K个簇，并将每个数据点分配到其最近的簇中心。K-NN是一种分类算法，它根据最近邻居的标签来预测新数据点的标签。\nK-Means需要指定簇的数量K，而K-NN不需要。\nK-Means是一种无监督学习算法，它不需要标记数据，而K-NN是一种监督学习算法，需要标记数据。\nK-Means使用欧几里得距离来计算数据点之间的相似度，而K-NN可以使用不同的距离度量，如曼哈顿距离、余弦相似度等。\nK-Means在处理大规模数据时可能会遇到性能问题，而K-NN可以轻松处理大规模数据。\n总的来说，K-Means和K-NN是两种不同的机器学习算法，适用于不同的问题和数据集。\nKmeans的计算过程 （1）适当选择c个类的初始中心； （2）在k次迭代中，对任意一个样本，求其到c各中心的距离（欧式距离），将该样本归到距离更短的中心所在的类； （3）利用均值等方法更新该类的中心值； （4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。\n假设 现在有4组数据，每组数据有2个维度，对其进行聚类分为2类，将其可视化一下。 $A=(1,1),B=(2,1),C=(4,3),D=(5,4)$ 假设选取两个星的位置为初始中心 $c_1=(1,1),c_2=(2,1)$，计算每个点到初始中心的距离，使用欧式距离得到4个点分别距离两个初始中心的距离，归于最近的类： $D^0第一行表示ABCD四个点到c1的举例，第二行表示ABCD四个点到c2的举例，举例使用欧氏距离公式计算出来，以C为例，到c1这一组的举例是3.61,到c2这一组的举例是2.83说明第一次迭代C是属于group-2$\n通过比较，将其进行归类。并使用平均法更新中心位置。 由于归于group1的只有一个点，一次更新后的中心位置$c_1=(1,1)$，而 $c_{2} = (\\frac{11}{3}, \\frac{8}{3})$\ngroup2的新中心点也就是$x={(x1+x2+x3)\\over 3} ={(2+4+5)\\over3}={11\\over3}$ $y={(y1+y2+y3)\\over 3} ={(1+3+4)\\over3}={8\\over3}$ 再次计算每个点与更新后的位置中心的距离",
    "tags": [],
    "title": "机器学习实战教程（十二）：聚类算法Kmeans",
    "uri": "/docs/programming/ai/machine_learning/algorithms/action_12_cluster/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 机器学习 \u003e 核心算法",
    "content": "简介 集成学习是一种机器学习方法，它旨在通过将多个单独的学习器（称为基分类器或基学习器）的预测结果进行组合，来提高整体的预测准确率。\n集成学习可以看作是一种“多个人一起合作做事”的方法。每个基分类器都是独立的学习器，它们在训练数据上进行训练并产生一个预测结果。这些基分类器可以使用不同的算法、不同的参数设置或不同的训练数据。最后，将所有基分类器的预测结果汇总起来，通过一定的组合方式（例如投票、加权投票等）得到最终的预测结果。\n与单个分类器相比，集成学习可以显著提高分类器的准确率和泛化能力。这是因为集成学习可以有效地减少分类器的偏差和方差，从而避免过拟合和欠拟合问题。此外，集成学习还可以增加分类器的鲁棒性，使其对噪声和异常值具有更强的容忍性。\n目前，集成学习已经被广泛应用于各种领域，例如图像识别、自然语言处理、金融风险评估等。常见的集成学习方法包括Bagging、Boosting、Stacking等。\n在集成学习中，通常会涉及到几个概念，包括：\n基分类器（Base Classifier）：指的是单独的、独立的学习器，它们的预测结果会被组合起来生成最终的预测结果，集成学习中常用的基分类器有决策树、支持向量机、逻辑回归、朴素贝叶斯、神经网络等。不同的基分类器在不同的数据集和任务中可能会表现出不同的性能，因此在实际应用中需要根据具体情况选择合适的基分类器。。\n集成分类器（Ensemble Classifier）：指的是由多个基分类器组成的分类器。集成分类器可以看作是一个“元分类器”，它可以对多个基分类器的预测结果进行组合，从而得到更加准确的预测结果。\nBagging（Bootstrap Aggregating）：是一种基于自助采样法的集成学习方法。它通过对原始训练集进行多次有放回的采样，来产生多个训练集，并使用每个训练集产生一个基分类器。最后，将所有基分类器的预测结果进行投票或平均，得到最终的预测结果。\nBoosting：是一种迭代的、逐步提升基分类器性能的集成学习方法。它通过对训练集进行加权，使得基分类器更加关注那些被错误分类的样本，从而提高分类器的准确率。Boosting方法有很多种，比如AdaBoost、Gradient Boosting等。\nStacking：是一种将多个基分类器的预测结果作为输入，再训练一个“元分类器”的集成学习方法。Stacking方法可以看作是一种二级学习方法，它将基分类器的预测结果作为新的特征，再进行训练，从而得到更加准确的预测结果。\n这些概念是集成学习中非常基础且重要的内容，理解它们可以帮助我们更好地理解和应用集成学习算法。\n集成分类方法 常用的集成分类方法有以下几种：\nBagging：基于自助采样的方法，通过训练多个相互独立的基分类器，然后将它们的输出进行投票或平均。\nBoosting：通过逐步训练一系列弱分类器，每一轮训练都会根据前一轮分类器的错误情况来调整样本权重，使得被错误分类的样本得到更多的关注，从而提高分类器的性能。\n以下是两种集成方法的集成分类器实现\nRandom Forest（随机森林）：是一种基于决策树的Bagging集成方法，通过随机选择特征和样本来生成多个决策树，然后将它们的输出进行投票。\nAdaBoost：是一种基于Boosting的集成方法，通过逐步训练一系列弱分类器，每一轮训练都会根据前一轮分类器的错误情况来调整样本权重，并且在训练过程中给每个分类器分配一个权重，然后将它们的输出进行加权平均。\nGradient Boosting Decision Tree (GBDT)：是一种基于Boosting的集成方法，通过逐步训练一系列决策树来提高分类器的性能，每个决策树都是基于前一棵树的残差来进行训练的，然后将所有决策树的输出进行加权平均。\n这些集成分类器在不同的数据集和任务中可能会表现出不同的性能，因此在实际应用中需要根据具体情况选择合适的集成分类器。\nBagging 自举汇聚法（bootstrap aggregating），也称为bagging方法。Bagging对训练数据采用自举采样（boostrap sampling），即有放回地采样数据，主要思想：\n从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的） 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等） 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同） Boosting Boosting是一种与Bagging很类似的技术。Boosting的思路则是采用重赋权（re-weighting）法迭代地训练基分类器，主要思想：\n每一轮的训练数据样本赋予一个权重，并且每一轮样本的权值分布依赖上一轮的分类结果，也就是说当前样本的权重，受分类结果的权重影响，当前分类结果的错误率越高，当前样本的权重也就越高，利用指数函数放大。 基分类器之间采用序列式的线性加权方式进行组合。 Bagging、Boosting二者之间的区别 样本选择上：\nBagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。 Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。 样例权重：\nBagging：使用均匀取样，每个样例的权重相等。 Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大，也就是每个样本都有一个权重，错误率越高权重越大，需要被重新训练的概率越大。 预测函数： 在集成学习中，我们通常会为每个基分类器分配一个权重，这个权重取决于该分类器在训练集上的表现。对于表现好的分类器，我们会赋予更高的权重，以使它们在投票决策中占据更重要的地位。相反，表现差的分类器会被分配较低的权重，以减少它们对最终结果的影响。\nBagging：所有预测函数的权重相等。 Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。 并行计算：\nBagging：各个预测函数可以并行生成。 Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。 下面是将决策树与这些算法框架进行结合所得到的新的算法：\nBagging + 决策树 = 随机森林 AdaBoost + 决策树 = 提升树 Gradient Boosting + 决策树 = GBDT 随机森林 自助采样 随机森林是一种基于Bagging集成学习思想的算法，它使用多个决策树进行集成，同时通过引入随机特征选择和样本随机抽样的方法来增加模型的随机性，提高模型的泛化能力。下面举一个简单的例子来说明随机森林的计算过程。\n假设我们有一个包含1000个样本的数据集，每个样本包含5个特征。我们希望使用随机森林对这个数据集进行分类。\n首先，我们需要对数据集进行随机抽样，从原始数据集中有放回地随机抽取1000个样本（有放回地抽样意味着同一个样本可能会被抽取多次，比如抽取了一个样本1，记录到一个袋子中，放回到原数据集，然后又从原数据集1000个中抽一个，可能又抽到样本1，也可以能是样本100，每次都是从1000个中抽取，抽取1000次，运气好的情况下可能抽到1000个样本1，当然这几乎不可能，不要理解为从源数据集拿到一个数据就放到自助采样集，就不放回去了，自助采样集只是个样本的复制），这个新的数据集就是一个“袋子”，我们称之为一个“自助采样集”（bootstrap sample）。这个自助采样集中有一部分样本可能重复出现，另一些样本可能没有被抽取到。这个过程被称为自助采样（bootstrap）。\n接下来，我们需要使用基于决策树的分类器对这个自助采样集进行训练。在训练决策树时，我们需要对每个节点上的特征进行随机选择，具体地，每次从原始特征中随机选择一定数量的特征，然后从这些特征中选择最优的特征进行划分。这个过程被称为随机特征选择（random feature selection）。\n训练出第一个决策树之后，我们可以对剩余的样本进行预测，记录下每个样本的预测结果。接着，我们再次从原始数据集中有放回地随机抽取1000个样本，形成一个新的自助采样集，使用同样的方法训练第二个决策树，并记录下每个样本的预测结果。\n重复这个过程，直到训练出指定数量的决策树。最终，我们可以对每个样本的预测结果进行投票，得出随机森林的最终预测结果。\n需要注意的是，随机森林中的决策树通常是并行训练的，即每个决策树可以在独立的CPU核心上进行训练，从而加速模型的训练过程。另外，随机森林中的决策树可以使用一些剪枝策略来防止过拟合，比如最小样本数限制、最大深度限制等。\n预测数据 注意集成学习算法，不是通过数据训练从多个模型中选择出一个模型，而是多个模型输入结果预测，通过投票或者求平均值等方法获得最终的结果。\n随机森林是一种集成学习方法，它基于决策树算法进行构建。随机森林的预测数据的过程如下：\n从训练集中随机有放回地抽取n个样本作为一个子集，这个子集的大小和训练集的大小相同。\n针对这个子集，随机选择k个特征，其中k是一个固定的超参数，一般小于特征总数。\n基于这个子集和k个特征，训练一个决策树模型。\n重复1-3步骤m次，得到m个决策树模型。\n对于新的数据点，将它输入到每一个决策树模型中，得到m个预测结果。\n对于分类问题，使用投票的方式，将m个预测结果中得票最多的类别作为最终的预测结果。对于回归问题，使用平均值的方式，将m个预测结果进行平均，作为最终的预测结果。\n需要注意的是，在随机森林中，每个决策树模型的训练都是独立的，因此可以并行地进行训练和预测，从而提高模型的效率。\n鸢尾花预测 鸢尾花数据集是一组带有标签的多元数据，其中包含了三个不同品种的鸢尾花（山鸢尾、变色鸢尾、维吉尼亚鸢尾）的测量值。这些测量值（特征）包括花萼长度、花萼宽度、花瓣长度和花瓣宽度。每个样本都包含这四个测量值，共150个样本。\n该数据集的目的是通过这些测量值来区分不同品种的鸢尾花。这是一个非常常见的机器学习问题，被广泛用于分类和聚类算法的基准测试。\n鸢尾花数据集是机器学习领域最常用的数据集之一，它在数据可视化、模型评估、特征选择和算法比较等方面都有广泛的应用。\n我们首先使用sklearn库中的load_iris函数加载了鸢尾花数据集。然后，我们使用train_test_split函数将数据集分割为训练集和测试集，比例为0.3\nfrom sklearn.datasets import load_iris\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.ensemble import RandomForestClassifier\r# 加载鸢尾花数据集\riris = load_iris()\r# 打印特征数和数据集大小\rprint(\"Number of features: \", len(iris.feature_names))\rprint(\"Number of samples: \", len(iris.data))\rprint(iris.target) 输出：\nNumber of features: 4\rNumber of samples: 150\r[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\r1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\r2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\r2 2] 然后，我们使用train_test_split函数将数据集分割为训练集和测试集，比例为0.3\n# 分割数据集为训练集和测试集\rX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3,random_state=2) 直接使用决策树预测。\nfrom sklearn.tree import DecisionTreeClassifier\r# 定义决策树分类器\rclf = DecisionTreeClassifier(random_state=1)\r# 训练模型\rclf.fit(X_train, y_train)\r# 预测测试集\ry_pred = clf.predict(X_test)\raccuracy = clf.score(X_test, y_test)\rprint(\"Accuracy:\", accuracy) 输出：Accuracy: 0.9555555555555556\nRandomForestClassifier是一个基于随机森林算法实现的分类模型，它有一些重要的参数，下面我将对这些参数进行详细解释。\nn_estimators：表示构建随机森林时决策树的数量，默认值为100。n_estimators越大，模型的准确率和稳定性相对也会提高，但是训练时间会增加，因此需要在准确率和时间成本之间进行权衡。\ncriterion：表示衡量分裂质量的指标，可以选择\"gini\"或\"entropy\"。默认值为\"gini\"，意味着使用基尼系数来衡量分裂质量。而\"entropy\"表示使用信息增益来衡量分裂质量。一般来说，选择基尼系数比信息增益要快一些，但是在某些情况下，信息增益可能会表现得更好。\nmax_depth：表示决策树的最大深度，默认值为None，表示不限制深度。如果将max_depth设置为较小的值，可以避免过拟合的情况发生，但是可能会影响模型的准确率。\nmin_samples_split：表示在进行节点分裂之前，节点的最小样本数，默认值为2。如果将min_samples_split设置为较大的值，可以避免决策树在局部区域上过度拟合，但是可能会导致决策树欠拟合。\nmin_samples_leaf：表示叶节点的最小样本数，默认值为1。如果将min_samples_leaf设置为较小的值，可以使模型更加灵活，但是可能会导致决策树过拟合。\nmax_features：表示在确定节点分裂时，要考虑的特征数。可以输入整数、浮点数、字符串或None。如果输入整数，则表示考虑的特征数为该整数值；如果输入浮点数，则表示考虑的特征数为总特征数乘以该浮点数值；如果输入字符串\"auto\"，则表示考虑的特征数为总特征数；如果输入字符串\"sqrt\"，则表示考虑的特征数为总特征数的平方根；如果输入字符串\"log2\"，则表示考虑的特征数为总特征数的以2为底的对数；如果输入None，则表示考虑的特征数为总特征数。一般来说，随机森林的表现比较稳定，因此可以将max_features设置为默认值None。\nrandom_state：表示随机种子，用于控制随机模式的生成，可以使随机模式可重复。如果将random_state设置为整数，则表示使用该整数作为随机种子；如果设置为None，则表示使用默认的随机种子。\n使用随机森林\n# 创建随机森林分类器，指定决策树数量为100，其他参数采用默认值\rrfc = RandomForestClassifier(n_estimators=100)\r# 使用训练数据集进行训练\rrfc.fit(X_train, y_train)\r# 使用测试数据集进行预测\ry_pred = rfc.predict(X_test)\r# 计算模型的准确率\raccuracy = rfc.score(X_test, y_test)\rprint(\"Accuracy:\", accuracy) 输出：Accuracy: 0.9777777777777777\nAdaBoost AdaBoost（Adaptive Boosting）是一种集成学习方法，其目的是将多个弱分类器组合成一个强分类器。它的核心思想是每一次训练都加强那些前一次训练中被错误分类的样本的权重，减少那些被正确分类的样本的权重。这样，在每一次训练中，模型都会更加关注前面分类效果不好的样本，从而使得整个模型能够更好地适应数据集。\n算法的过程和公式 AdaBoost算法的过程和公式如下：\n初始化训练数据的权重：对于有N个样本的训练集D，每个样本的权重初始化为1/N。\n对于t=1,2,…T，进行以下操作：\na. 根据当前的训练数据权重分布，使用基分类器（如决策树）进行训练。\nb. 计算基分类器的错误率（误差率）：对于分类错误的样本，权重增加；对于分类正确的样本，权重减少。\nc. 计算基分类器的权重：基分类器的权重与其错误率相关，错误率越小的基分类器，其权重越大。\nd. 更新训练数据的权重：根据基分类器的权重，更新训练数据的权重分布，使得基分类器错误率大的样本权重增加，错误率小的样本权重减少。\n最终的分类器是基分类器的加权和，权重为每个基分类器的权重。\nAdaBoost算法的公式如下：\nStep 1：初始化权重\n$D_1(i)=\\frac{1}{N}, i=1,2,…,N$\nStep 2：对于t=1,2,…T，进行以下操作：\na. 训练基分类器\n$G_t(x):\\mathcal{X}\\rightarrow{-1,1}$\nb. 计算错误率\n$\\epsilon_t=P(G_t(x_i)\\ne y_i)=\\sum_{i=1}^N D_t(i)[G_t(x_i)\\ne y_i]$\nc. 计算基分类器的权重\n$\\alpha_t=\\frac{1}{2}\\ln\\frac{1-\\epsilon_t}{\\epsilon_t}$\nd. 更新权重\n$D_{t+1}(i)=\\frac{D_t(i)\\exp(-\\alpha_ty_iG_t(x_i))}{Z_t},i=1,2,…,N$ 其中，$Z_t$是规范化因子，使得$D_{t+1}$​成为概率分布。\nStep 3：最终分类器\n$f(x)=\\operatorname{sign}\\left(\\sum_{t=1}^T\\alpha_tG_t(x)\\right)$\n其中，$sign⁡(x)$是符号函数，如果x≥0，则$\\operatorname{sign}(x)=1$；否则，$\\operatorname{sign}(x)=-1$。\n鸢尾花预测 sklearn.ensemble模块提供了很多集成方法，AdaBoost、Bagging、随机森林等。本次使用的是AdaBoostClassifier。 让我们先看下AdaBoostClassifier这个函数，一共有5个参数： 参数说明如下：\nbase_estimator： 可选参数，默认为DecisionTreeClassifier。理论上可以选择任何一个分类或者回归学习器，不过需要支持样本权重。我们常用的一般是CART决策树或者神经网络MLP。默认是决策树，即AdaBoostClassifier默认使用CART分类树DecisionTreeClassifier，而AdaBoostRegressor默认使用CART回归树DecisionTreeRegressor。另外有一个要注意的点是，如果我们选择的AdaBoostClassifier算法是SAMME.R，则我们的弱分类学习器还需要支持概率预测，也就是在scikit-learn中弱分类学习器对应的预测方法除了predict还需要有predict_proba。 algorithm： 可选参数，默认为SAMME.R。scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R。两者的主要区别是弱学习器权重的度量，SAMME使用对样本集分类效果作为弱学习器权重，而SAMME.R使用了对样本集分类的预测概率大小来作为弱学习器权重。由于SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值也是SAMME.R。我们一般使用默认的SAMME.R就够了，但是要注意的是使用了SAMME.R， 则弱分类学习器参数base_estimator必须限制使用支持概率预测的分类器。SAMME算法则没有这个限制。 n_estimators： 整数型，可选参数，默认为50。弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是50。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。 learning_rate： 浮点型，可选参数，默认为1.0。每个弱学习器的权重缩减系数，取值范围为0到1，对于同样的训练集拟合效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的v开始调参，默认是1。 random_state： 整数型，可选参数，默认为None。如果RandomState的实例，random_state是随机数生成器; 如果None，则随机数生成器是由np.random使用的RandomState实例。 #%%\rfrom sklearn.datasets import load_iris\rfrom sklearn.ensemble import AdaBoostClassifier\rfrom sklearn.model_selection import train_test_split\riris = load_iris()\rX, y = iris.data, iris.target\rX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\rclf = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\rclf.fit(X_train, y_train)\raccuracy = clf.score(X_test, y_test)\rprint(\"Accuracy:\", accuracy) 输出：Accuracy: 1.0\n集成学习选择 AdaBoost和随机森林都是集成学习方法，通过组合多个弱分类器（或决策树）来构建一个强分类器。虽然它们都可以提高分类的准确率，但是在不同的数据集和场景下，它们的表现可能会有所不同。\n一般来说，随机森林适合处理高维数据和噪声较多的数据集，因为它可以随机选取特征和样本来构建多个决策树，从而减少过拟合的风险。而AdaBoost则适合处理低维数据和复杂的分类问题，因为它可以通过调整权重和重采样来训练多个弱分类器，并将它们组合起来得到一个更强的分类器。\n因此，在处理复杂的低维数据集时，AdaBoost通常能够表现出更好的分类准确率，而在处理高维数据集时，随机森林可能更适合。但是，这只是一般情况，在具体应用中，需要根据数据集的特点和需求来选择适合的算法。",
    "description": "简介 集成学习是一种机器学习方法，它旨在通过将多个单独的学习器（称为基分类器或基学习器）的预测结果进行组合，来提高整体的预测准确率。\n集成学习可以看作是一种“多个人一起合作做事”的方法。每个基分类器都是独立的学习器，它们在训练数据上进行训练并产生一个预测结果。这些基分类器可以使用不同的算法、不同的参数设置或不同的训练数据。最后，将所有基分类器的预测结果汇总起来，通过一定的组合方式（例如投票、加权投票等）得到最终的预测结果。\n与单个分类器相比，集成学习可以显著提高分类器的准确率和泛化能力。这是因为集成学习可以有效地减少分类器的偏差和方差，从而避免过拟合和欠拟合问题。此外，集成学习还可以增加分类器的鲁棒性，使其对噪声和异常值具有更强的容忍性。\n目前，集成学习已经被广泛应用于各种领域，例如图像识别、自然语言处理、金融风险评估等。常见的集成学习方法包括Bagging、Boosting、Stacking等。\n在集成学习中，通常会涉及到几个概念，包括：\n基分类器（Base Classifier）：指的是单独的、独立的学习器，它们的预测结果会被组合起来生成最终的预测结果，集成学习中常用的基分类器有决策树、支持向量机、逻辑回归、朴素贝叶斯、神经网络等。不同的基分类器在不同的数据集和任务中可能会表现出不同的性能，因此在实际应用中需要根据具体情况选择合适的基分类器。。\n集成分类器（Ensemble Classifier）：指的是由多个基分类器组成的分类器。集成分类器可以看作是一个“元分类器”，它可以对多个基分类器的预测结果进行组合，从而得到更加准确的预测结果。\nBagging（Bootstrap Aggregating）：是一种基于自助采样法的集成学习方法。它通过对原始训练集进行多次有放回的采样，来产生多个训练集，并使用每个训练集产生一个基分类器。最后，将所有基分类器的预测结果进行投票或平均，得到最终的预测结果。\nBoosting：是一种迭代的、逐步提升基分类器性能的集成学习方法。它通过对训练集进行加权，使得基分类器更加关注那些被错误分类的样本，从而提高分类器的准确率。Boosting方法有很多种，比如AdaBoost、Gradient Boosting等。\nStacking：是一种将多个基分类器的预测结果作为输入，再训练一个“元分类器”的集成学习方法。Stacking方法可以看作是一种二级学习方法，它将基分类器的预测结果作为新的特征，再进行训练，从而得到更加准确的预测结果。\n这些概念是集成学习中非常基础且重要的内容，理解它们可以帮助我们更好地理解和应用集成学习算法。\n集成分类方法 常用的集成分类方法有以下几种：\nBagging：基于自助采样的方法，通过训练多个相互独立的基分类器，然后将它们的输出进行投票或平均。\nBoosting：通过逐步训练一系列弱分类器，每一轮训练都会根据前一轮分类器的错误情况来调整样本权重，使得被错误分类的样本得到更多的关注，从而提高分类器的性能。\n以下是两种集成方法的集成分类器实现\nRandom Forest（随机森林）：是一种基于决策树的Bagging集成方法，通过随机选择特征和样本来生成多个决策树，然后将它们的输出进行投票。\nAdaBoost：是一种基于Boosting的集成方法，通过逐步训练一系列弱分类器，每一轮训练都会根据前一轮分类器的错误情况来调整样本权重，并且在训练过程中给每个分类器分配一个权重，然后将它们的输出进行加权平均。\nGradient Boosting Decision Tree (GBDT)：是一种基于Boosting的集成方法，通过逐步训练一系列决策树来提高分类器的性能，每个决策树都是基于前一棵树的残差来进行训练的，然后将所有决策树的输出进行加权平均。\n这些集成分类器在不同的数据集和任务中可能会表现出不同的性能，因此在实际应用中需要根据具体情况选择合适的集成分类器。\nBagging 自举汇聚法（bootstrap aggregating），也称为bagging方法。Bagging对训练数据采用自举采样（boostrap sampling），即有放回地采样数据，主要思想：\n从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的） 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等） 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同） Boosting Boosting是一种与Bagging很类似的技术。Boosting的思路则是采用重赋权（re-weighting）法迭代地训练基分类器，主要思想：\n每一轮的训练数据样本赋予一个权重，并且每一轮样本的权值分布依赖上一轮的分类结果，也就是说当前样本的权重，受分类结果的权重影响，当前分类结果的错误率越高，当前样本的权重也就越高，利用指数函数放大。 基分类器之间采用序列式的线性加权方式进行组合。 Bagging、Boosting二者之间的区别 样本选择上：\nBagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。 Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。 样例权重：\nBagging：使用均匀取样，每个样例的权重相等。 Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大，也就是每个样本都有一个权重，错误率越高权重越大，需要被重新训练的概率越大。 预测函数： 在集成学习中，我们通常会为每个基分类器分配一个权重，这个权重取决于该分类器在训练集上的表现。对于表现好的分类器，我们会赋予更高的权重，以使它们在投票决策中占据更重要的地位。相反，表现差的分类器会被分配较低的权重，以减少它们对最终结果的影响。\nBagging：所有预测函数的权重相等。 Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。 并行计算：",
    "tags": [],
    "title": "机器学习实战教程（十三）：集成学习",
    "uri": "/docs/programming/ai/machine_learning/algorithms/action_13_intelearn/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 编程语言 \u003e 汇编语言",
    "content": "简介 68000 汇编语言是为 Motorola 68000 微处理器设计的低级编程语言。68000 微处理器于 1979 年推出，因其强大的性能和灵活的架构而广泛应用于多种计算机系统和嵌入式设备中。以下是对 68000 汇编语言的背景、应用领域以及学习它的好处的详细介绍。\n产生背景 技术进步：\n68000 微处理器是 16 位架构，具有 32 位的地址总线，能够寻址高达 4GB 的内存。这使得它在当时的微处理器中具有较高的性能和灵活性。 其设计采用了复杂指令集计算（CISC）架构，支持多种寻址模式和丰富的指令集。 市场需求：\n1970 年代末和1980年代初，个人计算机和嵌入式系统的需求迅速增长。68000 微处理器因其性能和成本效益被广泛采用。 许多知名的计算机系统（如 Apple Macintosh、Atari ST 和 Sega Genesis）都使用了 68000 处理器。 应用领域 个人计算机：\n68000 微处理器被用于早期的个人计算机，如 Apple Macintosh 和 Atari ST。这些系统的操作系统和应用程序通常使用 68000 汇编语言进行开发。 嵌入式系统：\n68000 处理器也被广泛应用于嵌入式系统，如工业控制、汽车电子和消费电子产品。 游戏机：\nSega Genesis 和其他游戏机使用 68000 处理器，许多经典游戏都是用 68000 汇编语言编写的，学习后可以做一些hackrom的实战。 实时系统：\n由于其高效的性能，68000 处理器在实时系统中也得到了应用，如医疗设备和航空航天系统。 语言学习 EASy68K EASy68K 是一个 68000 结构化汇编语言集成开发环境（IDE）。EASy68K 允许您在 Windows PC 或 Wine 上编辑、汇编和运行 68000 程序。无需额外的硬件。EASy68K 是一个开源项目，根据 GNU 通用公共使用许可分发。 使用easy68k方便我们学习整套68000的编程和调试，学习这件基础知识，对我们hackrom或者逆向的基础。 下载地址：http://www.easy68k.com/files/SetupEASy68K.exe 安装完成后的目录结构 帮助文档 helm.chm提供了完整的编程和调试工具以及68k语言的学习入门资料，可以直接从该文档入手。 IDE使用 打开EDIT68K.exe，菜单file-\u003enew x68 source file 。 在source里面实现一个功能，打印helloworld，并从空值台输入一个字符串并打印。\n关于指令，标签，寄存器其他相关的内容请移步后续章节。\n源代码\n*-----------------------------------------------------------\r* Title :\r* Written by :\r* Date :\r* Description:\r*-----------------------------------------------------------\rORG $1000 ;告诉编译器代码从1000位置开始，不指定默认从0开始\rSTART: ; first instruction of program\r* 将text字符串地址写给A1\rlea text,A1\r* 将14号task print 给d0,并执行，14号任务自动获取A1地址的数据并打印\rmove #14,D0\rtrap #15\r* 执行2号任务，从输入流获取输入，自动写入到A1\rmove #2,D0\rtrap #15\r* 打印A1地址内容\rmove #14,D0\rtrap #15\r* Put program code here\r*-----------------------------------------------------------\r*HELLO：这是一个标签，标识字符串数据的起始位置。\r*DC.B：这是一个伪指令，表示“定义常量（Define Constant）”，后面的 .B 表示定义的是字节（Byte）数据。\r*'Hello World'：这是一个字符串常量，表示字符数组。每个字符占用一个字节。\r*$D：这是一个十六进制常量，表示一个字节的值。$D 的十进制值是 13，通常表示回车符（Carriage Return）。\r*$A：这是一个十六进制常量，表示一个字节的值。$A 的十进制值是 10，通常表示换行符（Line Feed）。\r*0：这是一个字节的值，表示字符串的结束符（null terminator），在 C 语言中常用来标识字符串的结束。\r*----------------------------------------------------------- text dc.b 'helloworld',0\rSIMHALT ; halt simulator\r* Put variables and constants here\rEND START ; last line of source 点击工具栏运行按钮（如果由错误会有提示，根据情况修正） 会弹出一个确认框 点击execute 绿色圈圈点击变成红色可下断点，F9运行，F8 stepover,F7 stepinto,点击运行可调试。 在view可打开内存窗口，栈窗口等 编程语言 汇编语言程序由以下部分组成：\nlabels 标签 - 用户创建的名称，用于标记程序中的位置。 opcode 操作码 - 微处理器可以执行的特定指令，比如ADD，MOVE等。 operands 操作数 - 某些指令所需的附加数据，比如#1表示10进制立即数1，$1表示16进制的1。 directives 指令 - 发给汇编器的命令，比如ORG $1000，告诉编译器，代码的开始位置，代码段不占用空间，类似于c语言的宏，编译阶段使用。 macros 宏 - 用户创建的源代码集合，可以在编写程序时轻松重用。 comments 注释 - 用户创建的文本字符串，用于记录程序。 寄存器：汇编语言编程需要与微处理器进行直接交互。68000 微处理器包含八个数据寄存器 D0 到 D7。数据寄存器是通用的，可以视为 8 位、16 位或 32 位的整数变量。还有八个地址寄存器 A0 到 A7，地址寄存器的长度为 32 位。它们通常用于引用变量。状态寄存器（SR）包含状态标志，用于指示比较的结果。 以下是一个例子 comments 在 Motorola 68000（68k）汇编语言中，注释用于帮助程序员理解代码的功能和逻辑。68k 汇编语言的注释格式如下：（*或者;开头的为注释）\n* Date TRAP #15 ;将3任务执行，自动打印D1的内容 operands 操作数 #,$,%区别 你可能也注意到了出现在32和0000001E前面的#和$符号，$符号是为了告诉汇编器这个数字是“十六进制”数字(是个地址)，而不是“十进制”数字，例如：\nmove.b #32,$0000001E 汇编器在进行汇编时会将32（十进制）转换为0010 0000（二进制）。0010 0000 是 20 十六进制，因此写 32 和写 $20 是一样的，0010 0000将写入地址0000001E。如果你想要写二进制数，可以使用 % 符号。\nmove.b #%00100000,$0000001E\rmove.b #$20,$0000001E\rmove.b #32,$0000001E 以上所有内容完全相同，顶部是二进制版本（%），中间是十六进制（$），底部是十进制。在本教程中，我们将更多地使用十六进制和二进制，而不是十进制，以帮助你更好地理解和掌握它们。\n另一方面，# 符号告诉汇编器，该数字是一个“立即”值，而不是一个偏移量。那么什么是“立即”值呢？稍安勿躁，让我们先看一个没有 # 符号的例子：\nmove.b $00000010,$0000002D 这将读取偏移量00000010处的字节，并将其复制到偏移量0000002D处，如果偏移量00000010处的字节是49，则0000002D处现在也将是49： 而现在回到“立即数”，在我看来，这只不过是“直接数字”的一个花哨名称，#符号告诉68k这个数字不是偏移量/地址。\n操作数移动 给个例子\nmove.w #$10,$0020 ;将立即数16进制10 写入内存地址$0020\rmove $0020,D0 ;将内存地址$0020的值10赋值给D0\rmove $0020,A0 ;将将内存地址$0020的值10赋值给地址寄存器A0\rmove #$0020,A1 ;将立即数$0020赋值给地址寄存器A1\rmove A1,D1 ;将A1地址#$0020赋予给D1\rmove (A1),D2 ;将A1地址#$0020内存的值10赋予给D2\rmove.w (a0),(a1) ;将a0地址的值赋给a1地址的内存\rmove.w d1,(a0)+ ;将d1的数据，写入a0+word(2个字节)，并且a0寄存器往后移动两位，比如a0=0000,执行完a0=0002\rmove.w d1,$10(a1) ;将d1数据写入a1+10个字节的位置，a1的指向不变，比如a1=0000，写入数据到0010，执行完a1=0000\rmove.b #$98,(a0)+ ;同上上，写入立即数\rmove.l $29(a0),$00120020 ;将a0+29位置的值写入$00120020位置\rmove.b $00120020,(a1)+ 注意：move.w $00000047,d0 这个会导致汇编程序崩溃，因为00000047是一个奇数（奇地址/偏移量），68k在处理时会有问题，并会因“地址错误”而崩溃，字w和双字l必须使用偶数地址，如果要使用奇数地址请使用字节b。\n你只能使用“字节”来访问奇地址上的数据：\nlabels 标签用于通过名称标识程序中的位置或内存位置。需要位置的指令或指令可以使用标签来指示该位置。标签通常在行的第一列开始，必须以空格、制表符或冒号结束。如果使用冒号，它不会成为标签的一部分。如果标签没有在第一列开始，则必须以冒号结束。标签的前 32 个字符是有效的。标签有两种类型：全局标签和局部标签。\n全局标签可以在程序的任何地方被引用。因此，全局标签必须是唯一的名称。全局标签应以字母开头，后面可以跟字母、数字或下划线。局部标签可以在程序中重复使用。局部标签必须以点 ‘.’ 开头，后面可以跟字母、数字或下划线。全局标签定义了局部标签的边界。当定义局部标签时，只有在遇到下一个全局标签之前，才能从局部标签上方或下方的代码中引用它。汇编器通过将局部标签名称附加到前面的全局标签并用冒号 ‘:’ 替换点来创建局部标签的唯一名称。结果名称的前 32 个字符是有效的。\n开始标签 标签可以用来指定程序的起始位置。如果标签 START 指定了程序的起始位置，那么 END 指令的写法如下：\nSTART: Start of program\rcode\rEND START 指令标签 标签常常放在某个指令前用来表示，定义变量，标签指向存储数据的首地址。 DC - DC 指令指示汇编器将后续的值放入当前内存位置。该指令有三种形式：DC.B 用于字节数据，DC.W 用于字（16 位）数据，DC.L 用于长（32 位）数据。定义常量指令不应与 C++ 中声明常量混淆。 例如\nORG $1000 start of the data region depart DC.B 'depart.wav',0 stores as a NULL terminated string in consecutive bytes DC.L $01234567 the value $01234567 is stored as a long word\rDC.W 1,2 two words are stored as $0001 and $0002\rDC.L 1,2 two long words are stored as $00000001 and $00000002 depart 就是一个label是这块内存区域的首地址。\n内存结果\n00001000 64 65 70 61 72 74 2E 77 61 76 00 0000100C 01234567 00001010 0001 0002 00001014 00000001 00000002 其他关于指令标签的用法参考，也可以到指令章节： 位置标签 可以定义一些位置标签，当进行特殊操作时，可以通过控制流opcode跳转到位置标签 实现一个从0，end_index的循环打印\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\r* 实现一个从0，end_index的循环打印\rmove #1,D1\rt:\rmove #3,D0\rTRAP #15 ;将3任务执行，自动打印D1的内容\radd.b #1,d1 ;让d1+1\rCMP #end_index,d1 ;比较d1和end_index的值\rBNE t ;如果不相等继续跳转到t label执行\rSIMHALT ; halt simulator\r* Put variables and constants here\rend_index equ 10\rEND START ; last line of source opcode 操作码 在 68K 汇编语言中，操作码（opcode）是指令的核心部分，定义了要执行的操作。以下是一些常用的 68K 操作码及其功能：\n常用操作码 注意大部分操作码都可以添加结尾.W表示字（2个字节16位）.L表示双字(4个字节32位)，.B（1个字节8位）\n数据传送 - `MOVE`：将数据从一个位置移动到另一个位置。\r- 例：`MOVE.W D0, D1`（将 D0 的值移动到 D1）\r- `MOVEA`：将地址从一个位置移动到另一个位置。\r- 例：`MOVEA.L A0, A1`（将 A0 的地址移动到 A1）\r算术运算 - `ADD`：将两个操作数相加。\r- 例：`ADD.W D0, D1`（将 D0 的值加到 D1）\r- `SUB`：从一个操作数中减去另一个操作数。\r- 例：`SUB.W D1, D0`（从 D0 中减去 D1）\r- `MULS`：有符号乘法。\r- 例：`MULS D0, D1`（将 D0 和 D1 相乘，结果存储在 D1）\r- `DIVS`：有符号除法。\r- 例：`DIVS D0, D1`（将 D1 除以 D0，结果存储在 D1）\r逻辑运算 - `AND`：按位与运算。\r- 例：`AND.W D0, D1`（D1 与 D0 按位与）\r- `OR`：按位或运算。\r- 例：`OR.W D0, D1`（D1 与 D0 按位或）\r- `EOR`：按位异或运算。\r- 例：`EOR.W D0, D1`（D1 与 D0 按位异或）\r- `NOT`：按位取反。\r- 例：`NOT.W D0`（D0 的值取反）\r控制流 常用如下：\r- `BRA`：无条件跳转。\r- 例：`BRA label`（跳转到指定标签）\r- `BEQ`：如果相等则跳转。\r- 例：`BEQ label`（如果零标志位被设置，则跳转）\r- `BNE`：如果不相等则跳转。\r- 例：`BNE label`（如果零标志位未设置，则跳转）\r- `JSR`：跳转到子程序。\r- 例：`JSR subroutine`（跳转到子程序并保存返回地址）\r- `RTS`：从子程序返回。\r- 例：`RTS`（返回到调用子程序的地址）\r分支跳转 该指令将在程序中引发分支，如果某些标志被设置。共有十五种检查标志的方法。每种方法都有一个由两个字母组成的符号，用于替换 “cc” 在 “Bcc” 中。\nBCC：分支如果进位标志清除 - 当 C 标志为 0 时分支。 BCS：分支如果进位标志设置 - 当 C 标志为 1 时分支。 BEQ：分支如果相等 - 当 Z 标志为 1 时分支。 BNE：分支如果不相等 - 当 Z 标志为 0 时分支。 BGE：分支如果大于或等于 - 当 N 和 V 相等时分支。 BGT：分支如果大于 - 当 N 和 V 相等且 Z=0 时分支。 BHI：分支如果高于 - 当 C 和 Z 都为 0 时分支。 BLE：分支如果小于或等于 - 当 Z=1 或 N 和 V 不同时分支。 BLS：分支如果小于或相同 - 当 C=1 或 Z=1 时分支。 BLT：分支如果小于 - 当 N 和 V 不同时分支。 BMI：分支如果负 - 当 N=1 时分支。 BPL：分支如果正 - 当 N=0 时分支。 BVC：分支如果溢出标志清除 - 当 V=0 时分支。 BVS：分支如果溢出标志设置 - 当 V=1 时分支。 BRA：无条件分支 - 始终分支。 上面这些opcode根据标志触发跳转，只能跳转到label，注意进入label后会往下执行，和函数调用不一样，函数调用会返回，继续执行之前代码的下一行，这个不会，是直接跳转过去不回来了。\n例子：\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rinput:\rmove.b #4,d0\rTRAP #15\rCMP #0,d1\rBNE input ;如果不等于0跳转到input标签，继续让输入数字\rBEQ exit ;如果等于0直接退出\rlabel\rexit:\rSIMHALT ; halt simulator\r* Put variables and constants here\rEND START ; last line of source 地址跳转 JMP（跳转）用于将程序控制转移到一个有效地址。它实际上相当于 MOVE.L xxx, PC，因为它将程序计数器更改为一个有效地址（计算得出）。\n注意JMP是无条件跳转，相对于B开头的跳转，他也支持 JMP label的语法，同时他也支持直接JMP 地址的跳转。\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rinput:\rmove.b #4,d0\rTRAP #15\rCMP #0,d1\rBNE input ;如果不等于0跳转到input标签，继续让输入数字\rBEQ exit ;如果等于0直接退出\rlabel\rexit:\rLEA quit,a0 ;=0跳转到这里后，将quit的地址给到a0，JMP直接跳转到地址,相当于：move.l a0,PC（这是伪代码）\rJMP (a0) ;如果想跳转到a0的下一个地址，可以1(a0) 或者n(a0),当然也可以直接JMP quit\rquit:\rSIMHALT ; halt simulator\r* Put variables and constants here\rEND START ; last line of source 子程序跳转 JSR/BSR（跳转到子例程）与 JMP（无条件跳转）类似，但在跳转之前，JSR 会将跳转指令后面的地址压入栈中，这样可以通过 RTS（返回子例程）指令返回，也就相当于调用函数，函数执行完了，执行代码的下一行。\nBSR适合同一代码段里的label直接调用，是相对掉哟个，JSR适合指定一个绝对地址调用(比如JSR $5000) ,但是实际上两个可以互相替换，没啥区别。\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rinput:\rJSR input_notion ;JSR执行完后会自动执行下一行代码，B开头的跳过去就不回来了\rmove.b #4,d0\rTRAP #15\rCMP #0,d1\rBNE input ;如果不等于0跳转到input标签，继续让输入数字\rBEQ exit ;如果等于0直接退出\rinput_notion: ;屏幕上输出提示语\rMOVE #14,D0\rLEA INPUT_STR,A1\rTRAP #15\rRTS ;注意返回了会运行调用这个函数的下一行\rconfirm_exit *屏幕上输出确认提示语\rMOVE #14,D0\rLEA CONFIRM_STR,A1\rTRAP #15\rRTS\rexit:\rJSR confirm_exit\rmove.b #4,d0\rTRAP #15\rCMP #0,d1\rBEQ quit\rBNE input\rquit:\rSIMHALT ; halt simulator\r* Put variables and constants here\rINPUT_STR: dc.b 'please input number(exit=0):',0\rCONFIRM_STR: dc.b 'confirm exit(:exit=0,not=1):',0\rEND START ; last line of source 效果 位操作 - `SHL`：左移。\r- 例：`SHL.W #1, D0`（D0 左移 1 位）\r- `SHR`：右移。\r- 例：`SHR.W #1, D0`（D0 右移 1 位）\r- `ROL`：循环左移。\r- 例：`ROL.W #1, D0`（D0 循环左移 1 位）\r- `ROR`：循环右移。\r- 例：`ROR.W #1, D0`（D0 循环右移 1 位）\r比较 - `CMP`：比较两个操作数。\r- 例：`CMP.W D0, D1`（比较 D0 和 D1 的值）\r堆栈操作 - `PUSH`：将数据压入堆栈。\r- 例：`PUSH.W D0`（将 D0 的值压入堆栈）\r- `POP`：从堆栈弹出数据。\r- 例：`POP.W D0`（从堆栈弹出值到 D0）\rIO操作码 TRAP #15 被用于触发 I/O. 不同的io流任务存储在： D0. 参考chm： 常用的输入输出任务：\n14: 将A1地址对应的字符串输出 以0结尾结束。 13：将A1地址对应的字符串输出 以0结尾结束，加上\\r\\n换行。 2: 从控制台获取一个字符串回车后存储在A1地址中 0结尾。 4：读取一个数字写入D1.L中。 例子\nSTART ORG $1000 Program load address.\rmove #14,D0 ;设置14号任务打印A1地址字符串\rlea text,A1 ;获text地址到A1\rtrap #15 ;激活任务\rSIMHALT text dc.b 'Hello World',0 ;0表示字符串结束\rEND START End of source with start address specified. 其他操作码 关于更加详情的指令参考chm directives 指令 指令是汇编器需要遵循的指令。它们占据源代码行中的第二个字段，与指令操作码占据的位置相同，但指令并不是 68000 操作码。 “DC” 和 “DCB” 是唯一会导致数据被添加到输出文件中的指令。指令还可以用于控制宏的汇编、条件汇编和结构化语法。\n在以下描述中，选项项用方括号 [] 表示。用斜体显示的项应替换为适当的语法。\nUsage:\r[label] directive[.size] [data,data,...]\r^ ^ ^\r\\_________________\\_________\\_____ varies by directive DC指令 全称：Define Constant（定义常量） 用途：用于定义并初始化数据常量。DC 指令可以用于定义一个或多个初始值，这些值会被存储在程序的输出文件中。 内存分配：DC 指令会在程序的内存中分配实际的存储空间，并将指定的值写入该空间。 示例： 使用语法： Usage:\r[label] DC.size data,data,... 例子：\nVALUE1 DC 10 ; 定义常量 VALUE1，值为 10\rVALUE2 DC 20, 30 ; 定义常量 VALUE2，值为 20 和 30 特性： 定义的值在程序运行时是不可更改的。 实际在内存中占用空间。 注意下面的代码修改地址的值是非法的，常量无法修改\nSTART: ; first instruction of program\rlea usercount,A0\rmove.b 20,(A0) ;修改A0地址的常量这是非法的。\r* Put program code here\rSIMHALT ; halt simulator\r* Put variables and constants here\rORG $1200\rusercount dc.b 10,20\rdc.w 23 EQU 指令 全称：Equate（等于）\n用途：用于定义一个符号并将其与一个值关联。EQU 定义的值在整个程序中是不可更改的，通常用于定义常量或符号地址，类似于c语言的#define在预编译将对应引用的地方替换为值。\n内存分配：EQU 不会在内存中分配实际的存储空间。它只是创建一个符号，所有使用该符号的地方都会被替换为其定义的值。\n示例：\nMAX_SIZE EQU 100 ; 定义常量 MAX_SIZE，值为 100\n特性：\n一旦定义，EQU 的值不能被修改。 不占用内存空间，编译时进行替换 ORG $1000 ; 程序起始地址\rSTART: ; 将立即数 10 移动到 D0 寄存器\r; 定义常量\rMAX_COUNT EQU 2 ; 定义 MAX_COUNT 为 100\rSTART_VALUE EQU 1 ; 定义 START_VALUE 为 10\rMOVE.B #10, D0\rADD.B #MAX_COUNT, D0 ; 将 MAX_COUNT (100) 加到 D0\rSUB.B #START_VALUE, D0 ; 将 START_VALUE (10) 从 D0 中减去\rSIMHALT ; 停止模拟器\rORG $1200 ; 数据段起始地址\rEND START SET 指令 用途：用于定义一个符号并赋予一个初始值，但与 DC 不同的是，SET 定义的值是可更改的。SET 通常用于在程序运行时动态地改变值。\n示例：\nCOUNT SET 0 ; 定义符号 COUNT，初始值为 0 COUNT SET COUNT + 1 ; 重新定义 COUNT，值为 COUNT + 1\n内存分配：SET 指令并不分配实际的存储空间来存储值，而是定义一个符号，允许在程序中动态地改变该符号的值。\nDS 指令 全称：Define Space（定义空间）\n用途：用于定义一块未初始化的内存空间。DS 指令只分配内存，但不初始化这些内存的值，随时可改。\n示例：\nBUFFER DS 256 ; 定义一个大小为 256 字节的缓冲区\n内存分配：DS 指令会在输出文件中分配指定大小的内存空间，但这些空间的初始值是未定义的（通常是随机值或零，具体取决于系统）。\n定义一个100字节的空间，可以理解为数组，将MULT_TABLE数字第一个位置设置为：12\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rmove.B #0,D0\rLEA MULT_TABLE, A0\rMOVE.B #12,(A0, D0)\rSIMHALT ; halt simulator\rORG $1200\r* Put variables and constants here\rMULT_TABLE: ; 乘法表的存储位置\rDS.B 10 * 10 ; 预留 10x10 的空间\rEND START ; last line of source 其他指令 参考chm 寄存器 程序计数器（PC） 程序计数器（有时在不同的体系结构中也称为指令指针或指令地址寄存器）保存下一条将要执行的指令的内存地址。每当 CPU 执行一条指令时，PC 的值会自动更新，以指向下一条指令。 更新机制：在大多数情况下，PC 在指令执行后自动加一（或加上指令的长度），以指向下一条指令的地址。 编写一个简单程序 运行，默认会从start:的写一条语句开始，PC寄存器指向初始代码的地址（注意有效的代码时左侧绿色点点的，其他都是指令或者注释） 按下F8执行到下一条 我这里将usercount的地址指向A0 ,同时加了ORG $1200从1200这个地址写入。点击A0的地址可以查看内存： 状态寄存器（SR） 在 68k（Motorola 68000）架构中，状态寄存器（SR，Status Register）是一个重要的寄存器，用于存储处理器的状态信息和控制标志。状态寄存器的内容影响程序的执行流程，特别是在条件跳转和中断处理时。以下是对 68k 状态寄存器的详细介绍：\n状态寄存器的结构 68k 的状态寄存器是一个 16 位的寄存器，包含多个标志位。主要的标志位包括：\nN（Negative）:\n表示最近一次运算的结果是否为负数。 如果结果的最高位（符号位）为 1，则 N 标志被设置。 Z（Zero）:\n表示最近一次运算的结果是否为零。 如果结果为 0，则 Z 标志被设置。 V（Overflow）:\n表示最近一次运算是否发生了溢出。 溢出通常发生在有符号数运算中，当结果超出可表示的范围时，V 标志被设置。 C（Carry）:\n表示最近一次运算是否产生了进位或借位。 在加法运算中，如果产生了进位，C 标志被设置；在减法运算中，如果发生了借位，C 标志也会被设置。 I（Interrupt Mask）:\n这是一个 3 位的中断屏蔽位，控制中断的响应。 I0、I1 和 I2 位用于设置中断优先级，值越大，响应的中断优先级越低。 T（Trace）:\n这是一个单个位，用于启用或禁用跟踪模式。 当 T 位被设置时，处理器将在每个指令执行后产生一个中断，适用于调试。 S（Supervisor）:\n这是一个单个位，指示当前处理器是否处于特权模式（超级用户模式）。 当 S 位被设置时，处理器处于超级用户模式，允许执行特权指令。 状态寄存器的作用 条件跳转: 状态寄存器中的标志位用于条件跳转指令（如 BEQ、BNE 等），根据运算结果的状态决定程序的执行路径。 中断处理: 中断标志位控制中断的响应，允许或禁止特定级别的中断。 运算结果的状态: 通过检查 N、Z、V 和 C 标志，程序可以根据运算结果的状态做出相应的处理。 示例 以下是一个简单的示例，展示如何使用状态寄存器的标志位：\nMOVE.L #5, D0 ; 将 5 加载到 D0\rMOVE.L #3, D1 ; 将 3 加载到 D1\rSUB.L D1, D0 ; D0 = D0 - D1，结果为 2\r; 检查 Z 标志\rBEQ zero_result ; 如果 Z 标志为 1，跳转到 zero_result\r; 检查 N 标志\rBPL positive_result ; 如果 N 标志为 0，跳转到 positive_result\rzero_result:\r; 处理结果为零的情况\r; ...\rpositive_result:\r; 处理结果为正的情况\r; ... 数据寄存器（D) 在 68000（68k）架构中，D 寄存器（数据寄存器）是用于存储数据和操作数的寄存器。68k 处理器有 8 个数据寄存器，分别为 D0 到 D7。\nD 寄存器的特点 数量:\n68k 处理器有 8 个数据寄存器，编号为 D0 到 D7。 大小:\n每个 D 寄存器的大小为 32 位（4 字节），可以存储 32 位的整数或指针。 用途:\nD 寄存器主要用于存储运算的操作数、结果以及临时数据。它们在算术运算、逻辑运算、数据传输等操作中被广泛使用。 寻址模式:\nD 寄存器可以与多种寻址模式结合使用，支持直接寻址、间接寻址等方式，方便数据的访问和操作。 操作:\nD 寄存器可以参与各种指令的操作，如加法、减法、位运算等。指令可以直接对 D 寄存器进行操作，也可以将 D 寄存器的值存储到内存中或从内存中加载数据。 D 寄存器的使用场景 算术运算: D 寄存器用于存储参与运算的数值。 数据传输: 在数据传输指令中，D 寄存器可以作为源或目标。 函数参数: 在调用子程序时，D 寄存器常用于传递参数。 示例 以下是一个简单的汇编代码示例，展示如何使用 D 寄存器进行基本的算术运算：\nMOVE.L #10, D0 ; 将 10 加载到 D0 寄存器\rMOVE.L #5, D1 ; 将 5 加载到 D1 寄存器\rADD.L D1, D0 ; D0 = D0 + D1，D0 现在为 15 地址寄存器（A) 68000（68k）架构中，A 寄存器（地址寄存器）是用于存储内存地址的寄存器。68k 处理器有 8 个地址寄存器，分别为 A0 到 A7。以下是对 A 寄存器的详细描述：\nA 寄存器的特点 数量:\n68k 处理器有 8 个地址寄存器，编号为 A0 到 A7。 大小:\n每个 A 寄存器的大小为 32 位（4 字节），可以存储 32 位的内存地址。 用途:\nA 寄存器主要用于存储内存地址，支持数据的加载和存储操作。它们在指令中用于指向数据或指令的内存位置。 寻址模式:\nA 寄存器可以与多种寻址模式结合使用，包括直接寻址、间接寻址、基址寻址和相对寻址等。这使得程序能够灵活地访问内存中的数据。 堆栈指针:\nA7 寄存器通常用作堆栈指针（SP），指向当前堆栈的顶部。堆栈用于存储函数调用的返回地址、局部变量等。 A 寄存器的使用场景 内存访问: A 寄存器用于指向数据在内存中的位置，支持数据的读取和写入。 函数调用: 在函数调用中，A 寄存器可以用于传递参数和返回地址。 堆栈管理: A7 寄存器作为堆栈指针，管理函数调用的堆栈帧。 示例 以下是一个简单的汇编代码示例，展示如何使用 A 寄存器进行内存操作：\nLEA array, A0 ; 将数组的地址加载到 A0 寄存器\rMOVE.L (A0), D0 ; 从 A0 指向的地址加载数据到 D0 寄存器\rADD.L #1, D0 ; D0 = D0 + 1\rMOVE.L D0, (A0) ; 将 D0 的值存储回 A0 指向的地址 堆栈寄存器（SS) 在68k架构中，堆栈寄存器是用于管理程序运行时的堆栈的关键组件。68k系列处理器使用一个专用的寄存器来指向当前堆栈的顶部，这个寄存器被称为堆栈指针（Stack Pointer）。\n在68k架构中，堆栈指针寄存器通常是 A7（地址寄存器7），它指向当前堆栈的顶部。 堆栈是一个后进先出（LIFO）的数据结构，用于存储临时数据，如函数调用的返回地址、局部变量和中断处理程序的上下文。\n堆栈操作 我们来看下堆栈指针的移动和数据写入逻辑。 在68k汇编语言中，-(A7) 和 (A7)+ 分别用于表示压栈和出栈操作。 执行代码\nmove.l #10,-(a7) 未执行前原始堆栈地址A7指向：01000000，没有任何数据 执行：move.l #10,-(a7) 执行：move.l #20,-(a7) 执行出栈：move.l (a7)+,d0 std函数模拟 我们知道c语言的std约定是：调用函数先压入执行代码的后一个位置，然后参数从右往左压入，在函数内部出栈从左（后入先出）往右获取参数，执行完成获取代码执行的位置，跳转。 我们来模拟这个过程： 假设函数: public int add(int a,int b) 用98k模拟堆栈实现：\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rmove.l #10,-(a7) #第二个参数压栈。\rmove.l #20,-(a7) #第一个参数压栈。\rLEA *+12, A0 *计算下LEA占用4个字节，一直到move.l d0,d2是12个字节，*+12就是从PC当前位置+12个就是下一个执行代码的位置\rmove.l a0,4(a7) *将下一个执行的地址压栈\rJMP add\rmove.l d0,d2\rSIMHALT ; halt simulator\radd:\rmove.l (a7)+,a0 ;地址出栈\rmove.l (a7)+,d0 ;第一个参数出栈\rmove.l (a7)+,d1 ;第二个参数出栈\radd.l d1,d0\rJMP (a0)\r* Put variables and constants here\rEND START ; last line of source 案例(9*9乘法表) *-----------------------------------------------------------\r* Title :\r* Written by :\r* Date :\r* Description:\r*-----------------------------------------------------------\rORG $1000\rSTART: ; first instruction of program\r* Put program code here\rmove.b #start_index,d2 ;行索引\rmove.b #start_index,d3 ;列索引\rrow:\rjsr print_str_line ;到row的部分就添加一个换行，jsr调用子程序，子程序需要RTS返回\radd.b #1,d2 ;每运行一次+1\rmove.b #start_index,d3\rcmp #end_index+1,d2 ;到达最后一行+1直接退出\rBEQ exit\rcol:\radd.b #1,d3\rmove.b d2,d1\rjsr print_num ;打印行的数字\rlea tmp_str,a1\rmove.b #'*',(a1) ;打印一个*\rjsr print_str\rmove.b d3,d1\rjsr print_num ;打印一个列的数字\rmove.b #'=',(a1) jsr print_str ;打印一个=\rmove.b #1,d4\rmuls d2,d4\rmuls d3,d4\rmove.b d4,d1\rjsr print_num ;打印一个列的数字\rmove.b #' ',(a1) jsr print_str ;打印一个空格\rcmp d3,d2\rBEQ row\rBNE col\rprint_num:\rmove.b #3,d0\rTRAP #15 RTS print_str:\rmove.b #0,1(a1) ;打印字符的结尾\rmove.b #14,d0\rTRAP #15 RTS print_str_line:\rmove.b #0,(a1) ;打印字符的结尾\rmove.b #13,d0\rTRAP #15 RTS exit:\rSIMHALT ; halt simulator\r* Put variables and constants here\rtmp_str ds.b 2\rend_index equ 9\rstart_index equ 0\rEND START ; last line of source 效果 其他68k速查的在线文档：\n指令列表：https://github.com/prb28/m68k-instructions-documentation?tab=readme-ov-file 0基础入门：https://mrjester.hapisan.com/04_MC68/ romhack相关所有资源（文档工具）：https://github.com/zengfr/romhack 常用指令备忘录：https://github.com/zengfr/romhack/blob/adf6412c2a969486918bb00c18a2c989abdeaad5/M68000/M68000%E6%8C%87%E4%BB%A4-%E5%A4%87%E5%BF%98%E8%A1%A5%E5%85%85(%E6%95%B4%E7%90%86zengfr).txt",
    "description": "简介 68000 汇编语言是为 Motorola 68000 微处理器设计的低级编程语言。68000 微处理器于 1979 年推出，因其强大的性能和灵活的架构而广泛应用于多种计算机系统和嵌入式设备中。以下是对 68000 汇编语言的背景、应用领域以及学习它的好处的详细介绍。\n产生背景 技术进步：\n68000 微处理器是 16 位架构，具有 32 位的地址总线，能够寻址高达 4GB 的内存。这使得它在当时的微处理器中具有较高的性能和灵活性。 其设计采用了复杂指令集计算（CISC）架构，支持多种寻址模式和丰富的指令集。 市场需求：\n1970 年代末和1980年代初，个人计算机和嵌入式系统的需求迅速增长。68000 微处理器因其性能和成本效益被广泛采用。 许多知名的计算机系统（如 Apple Macintosh、Atari ST 和 Sega Genesis）都使用了 68000 处理器。 应用领域 个人计算机：\n68000 微处理器被用于早期的个人计算机，如 Apple Macintosh 和 Atari ST。这些系统的操作系统和应用程序通常使用 68000 汇编语言进行开发。 嵌入式系统：\n68000 处理器也被广泛应用于嵌入式系统，如工业控制、汽车电子和消费电子产品。 游戏机：\nSega Genesis 和其他游戏机使用 68000 处理器，许多经典游戏都是用 68000 汇编语言编写的，学习后可以做一些hackrom的实战。 实时系统：\n由于其高效的性能，68000 处理器在实时系统中也得到了应用，如医疗设备和航空航天系统。 语言学习 EASy68K EASy68K 是一个 68000 结构化汇编语言集成开发环境（IDE）。EASy68K 允许您在 Windows PC 或 Wine 上编辑、汇编和运行 68000 程序。无需额外的硬件。EASy68K 是一个开源项目，根据 GNU 通用公共使用许可分发。 使用easy68k方便我们学习整套68000的编程和调试，学习这件基础知识，对我们hackrom或者逆向的基础。 下载地址：http://www.easy68k.com/files/SetupEASy68K.exe 安装完成后的目录结构",
    "tags": [],
    "title": "68000汇编实战01-编程基础",
    "uri": "/docs/programming/languages/assembly/68000_01_base/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "liaomin416100569博客",
    "uri": "/docs/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "Hugging Face Hugging Face是一个人工智能（AI）公司，它致力于开发和推广自然语言处理（NLP）相关的技术和工具。该公司以其开源项目和社区而闻名，其最知名的项目之一是Transformers库，它提供了一系列预训练的语言模型，包括BERT、GPT和RoBERTa等。这些模型已经在各种NLP任务中取得了显著的成功，并成为了许多研究和工业应用的基础。\n除了提供预训练的模型之外，Hugging Face还开发了一系列工具和平台，使得使用和部署这些模型变得更加简单。其中包括：\nTransformers库：提供了各种预训练的语言模型的接口和工具，使得开发者可以轻松地使用这些模型进行文本分类、命名实体识别、语言生成等任务。\nDatasets库：包含了各种NLP数据集的接口和工具，使得开发者可以方便地使用这些数据集进行模型训练和评估。\nTrainer库：提供了一个训练和微调模型的框架，使得开发者可以方便地使用自己的数据集对预训练模型进行微调，以适应特定的任务和应用场景。\nModel Hub：一个模型分享和发布平台，开发者可以在这里分享自己训练的模型，也可以找到其他人分享的模型，并且可以直接在自己的项目中使用这些模型。\ndatasets数据集处理，transformers预训练微调等相关教程请参考官网hugging face官方文档。\nTransformers Transformers 是由 Hugging Face 开发的一个 NLP 包，支持加载目前绝大部分的预训练模型。随着 BERT、GPT 等大规模语言模型的兴起，越来越多的公司和研究者采用 Transformers 库来构建 NLP 应用，官网地址。 它提供了各种预训练的 Transformer 模型，包括 BERT、GPT、RoBERTa、DistilBERT 等。这些模型在多个 NLP 任务上取得了 state-of-the-art 的性能，并且 Transformers 库提供了简单易用的接口，使得使用这些预训练模型变得非常便捷。\n安装 官网安装教程参考：https://huggingface.co/docs/transformers/installation\n您可以通过 pip 安装 Transformers 库。在终端或命令行界面中执行以下命令（我这里使用pytorch，如果需要tensorflow的版本参考官网）：\npip install 'transformers[torch]'\n这将会自动从 PyPI（Python Package Index）下载并安装最新版本的 Transformers 库及其依赖项。\n如果您使用的是 Anaconda 环境，您也可以通过 conda 安装：\nconda install -c huggingface transformers\n这将会从 Anaconda 仓库中下载并安装 Transformers 库及其依赖项。\n安装完成后，您就可以在 Python 环境中使用 Transformers 库了。您可以编写代码来加载预训练模型、执行各种 NLP 任务，或者使用 Transformers 提供的高级 API，如 pipelines 来快速完成任务。\n模型 自然语言处理（NLP）模型是指用于处理和理解自然语言文本的计算机模型。这些模型的设计和训练旨在使计算机能够自动处理和分析语言数据，以执行各种语言相关的任务。以下是几种常见的NLP模型类型及其功能：\n基于规则的模型：这些模型使用手工制定的规则和规则集来处理文本，例如语法分析或关键词提取。这种方法的局限性在于需要大量的人工工作和难以处理的复杂性。\n基于统计的模型：这些模型利用统计学习技术从大量文本数据中学习语言模式和规律。例如，n-gram语言模型可以预测给定单词序列的下一个单词，而隐马尔可夫模型则用于词性标注和语音识别。\n神经网络模型：随着深度学习的发展，神经网络在NLP中的应用越来越广泛。这些模型使用多层神经网络结构来学习复杂的语言特征和模式。例如，递归神经网络（RNN）、长短时记忆网络（LSTM）和变压器（Transformer）等模型已经在机器翻译、文本生成、情感分析等任务中取得了显著的成就。\n预训练语言模型：这些模型通过在大规模文本数据上进行自监督学习来预先训练，例如BERT（Bidirectional Encoder Representations from Transformers）、GPT（Generative Pre-trained Transformer）等。预训练模型在各种NLP任务中表现出色，并通过微调适应特定的下游任务。\nNLP模型的选择取决于任务的性质和复杂度，以及可用的训练数据和计算资源。随着技术的进步和研究的深入，NLP模型不断演进和改进，为语言处理领域带来了许多创新和新的应用可能性。\n文件结构 训练模型通常具有以下常见的目录文件结构和文件：\nvocab.json: 这是一个包含词汇表的文件，它将模型训练时使用的词汇映射到整数索引。这对于将文本转换为模型可以理解的输入格式（如整数索引或者词嵌入）非常重要。\ntokenizer_config.json: 这个文件包含有关模型使用的分词器（tokenizer）的配置信息，例如分词器的类型、参数设置等。分词器用于将文本划分为词语或子词的序列，并将其转换为模型可以处理的输入格式。\ntokenizer.json: 如果模型使用了特定的分词器，此文件可能包含分词器的具体实现和配置信息。这对于加载和使用模型的时候确保分词器能正确地工作非常重要。\nconfig.json: 这个文件包含了模型本身的配置信息，例如模型的类型（如BERT、GPT）、层数、隐藏单元数等超参数设置。这些信息对于构建和初始化模型极为关键。\npytorch_model.bin 或 tensorflow_model.h5：这是包含预训练模型权重的二进制文件，其格式取决于所使用的深度学习框架。这些权重是模型学习到的参数，用于实际的预测和推理任务。\nspecial_tokens_map.json: 如果模型包含了特殊标记（如填充标记、起始标记等），此文件将包含这些特殊标记的定义及其在模型中的使用方式。\nmerges.txt（对于BERT等子词级别的模型）：这个文件包含将词汇划分为子词或者字符的规则或者合并操作，这对于分词器的工作非常关键。\nREADME.md 或者 model_card.md：这些文件通常包含了关于模型的详细信息，如作者、许可证、训练数据集、性能评估等，对于了解和使用模型非常有帮助。\n每个模型的具体结构和文件可能会有所不同，但上述文件是构成大多数预训练模型的基本要素。通过理解和操作这些文件，可以更好地理解和使用预训练模型进行自然语言处理任务。\npipelines 在 Hugging Face Transformers 中，pipelines 是一种方便的高级 API，用于执行各种自然语言处理（NLP）任务，如文本分类、命名实体识别、问答等。使用 pipelines，您无需编写大量的代码来加载模型、预处理输入数据、执行推理等操作，而是可以通过简单的函数调用来完成这些任务。\nransformers 库将目前的 NLP 任务归纳为几下几类：\n文本分类：例如情感分析、句子对关系判断等； 对文本中的词语进行分类：例如词性标注 (POS)、命名实体识别 (NER) 等； 文本生成：例如填充预设的模板 (prompt)、预测文本中被遮掩掉 (masked) 的词语； 从文本中抽取答案：例如根据给定的问题从一段文本中抽取出对应的答案； 根据输入文本生成新的句子：例如文本翻译、自动摘要等。 Transformers 库最基础的对象就是 pipeline() 函数，它封装了预训练模型和对应的前处理和后处理环节。我们只需输入文本，就能得到预期的答案。目前常用的 pipelines 有：\naudio-classification（音频分类）：用于对音频进行分类，识别音频中的类别或属性。 automatic-speech-recognition（自动语音识别）：用于将音频转换为文本，实现语音识别的功能。 conversational（会话式处理）：用于构建和处理对话系统，实现对话式交互的功能。 depth-estimation（深度估计）：用于从单张图片或视频中估计场景的深度信息。 document-question-answering（文档问答）：用于从文档中回答问题，帮助用户获取文档内容中的相关信息。 feature-extraction（特征提取）：用于从文本、图片等数据中提取特征，用于后续的任务或分析。 fill-mask（填空）：用于给定带有空白的句子，预测并填补空白处的单词或短语。 image-classification（图片分类）：用于对图片进行分类，识别图片中的类别或属性。 image-feature-extraction（图片特征提取）：用于从图片中提取特征，用于后续的任务或分析。 image-segmentation（图片分割）：用于将图片分割成不同的区域或对象，进行图像分割任务。 image-to-image（图片到图片）：用于执行图片到图片的转换，如图像风格转换、图像去噪等。 image-to-text（图片到文本）：用于从图片中提取文本信息，实现图片中的文字识别功能。 mask-generation（遮罩生成）：用于生成图片中的遮罩或掩码，用于图像处理或分割任务。 object-detection（目标检测）：用于从图片或视频中检测和识别出图像中的目标对象。 question-answering（问答）：用于回答给定问题的模型，从文本中找出包含答案的部分。 summarization（摘要生成）：用于生成文本的摘要或总结，将文本内容压缩为简短的形式。 table-question-answering（表格问答）：用于从表格数据中回答问题，帮助用户从表格中获取信息。 text2text-generation（文本到文本生成）：用于生成文本的模型，可以执行文本到文本的转换或生成任务。 text-classification（文本分类）：(别名\"sentiment-analysis\" 可用，情感分析)用于将文本进行分类，识别文本中的类别或属性。 text-generation（文本生成）：用于生成文本的模型，可以生成连续的文本序列。 text-to-audio（文本到音频）：用于将文本转换为语音，实现文本到语音的功能。 token-classification（标记分类）：别名\"ner\" 可用，命名实体识别，用于将文本中的每个标记或单词进行分类，识别每个标记的类别或属性。 translation（翻译）：用于执行文本的翻译任务，将文本从一种语言翻译成另一种语言。 video-classification（视频分类）：用于对视频进行分类，识别视频中的类别或属性。 visual-question-answering（视觉问答）：用于从图片或视频中回答问题，结合视觉和文本信息进行问答。 zero-shot-classification（零样本分类）：用于执行零样本分类任务，即在没有见过该类别的情况下对新样本进行分类。 zero-shot-image-classification（零样本图片分类）：用于执行零样本分类任务，即在没有见过该类别的情况下对新图片进行分类。 zero-shot-audio-classification（零样本音频分类）：用于执行零样本分类任务，即在没有见过该类别的情况下对新音频进行分类。 zero-shot-object-detection（零样本目标检测）：用于执行零样本目标检测任务，即在没有见过该类别的情况下对新图片中的目标对象进行检测。 如果需要了解更多的task类型更新，参考官网pipeline： 下面我们以常见的几个 NLP 任务为例，展示如何调用这些 pipeline 模型。\n图片转文本 教程参考自官网：https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/pipelines#transformers.ImageToTextPipeline from transformers import pipeline\ritt=pipeline(\"image-to-text\",model=\"ydshieh/vit-gpt2-coco-en\") #model不指定会使用默认模型。\rrtn=itt(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\rprint(rtn) 可以看到输出是需要先下载模型（下载一次，自动缓存），下载在C:\\Users\\admin.cache\\huggingface\\hub目录下。\n:\\python\\evn311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--ydshieh--vit-gpt2-coco-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\rTo support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\rwarnings.warn(message) 最后输出：[{‘generated_text’: ’two birds are standing next to each other ‘}]\n如果希望使用其他的image-to-text模型可以在官网搜索 https://huggingface.co/models?pipeline_tag=image-to-text\u0026sort=trending 比如选择image-to-text右侧文本框输入chinese，看下是不是有中文描述的 使用这个模型来测试下\nimage_path=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\rfrom transformers import pipeline\ritt=pipeline(\"image-to-text\",model=\"IDEA-CCNL/Taiyi-BLIP-750M-Chinese\")\rrtn=itt(image_path)\rprint(rtn) 输出（效果没有英文的模型好，明明是两只鹦鹉啊，不过识别出了鹦鹉，英文的只是两只鸟） [{‘generated_text’: ‘一 只 鹦 鹉 的 黑 白 照 片 。’}]\n文本生成 我们首先根据任务需要构建一个模板 (prompt)，然后将其送入到模型中来生成后续文本。注意，由于文本生成具有随机性，因此每次运行都会得到不同的结果。\n#%%\rfrom transformers import pipeline\rgenerator = pipeline(\"text-generation\",model=\"openai-community/gpt2\")\rprint(generator(\"I can't believe you did such a \")) 输出：\n[{'generated_text': 'I can\\'t believe you did such a _____t!\"\\n\\n\"You know I\\'m kind of an asshole to you, I mean?\"\\n\\n\"Just because I had one thing to do doesn\\'t mean I hate you. I know you'}] 在huggerface上搜索一个古诗词生成的模型， 左侧选择tag Text Generation ，搜索poem，选择最多人喜欢。 from transformers import pipeline\rgenerator = pipeline(\"text-generation\",model=\"uer/gpt2-chinese-poem\")\rprint(generator(\"[CLS] 离 离 原 上 草 ，\")) 输出\n[{'generated_text': '[CLS] 离 离 原 上 草 ， 濯 濯 原 上 桑 。 春 风 吹 罗 衣 ， 行 人 泪 成 行 。 离 人 不 可 留 ， 况 乃 隔 河 梁 。 当 和 露 餐 ， 勿 复 怨 秋 凉 。 愿 言 崇 令 德 ， 以 配 君 子 光 。 毋 怀 远 心 ， 皓 月 鉴 我 伤 。 莫 怨 东 风 ， 飘 然 入 西 楼 。 举 手 倚 阑 干 ， 举 酒 相 劝 酬 。 良 时 焉 可 再 ， 逝 水 何 悠 悠 。 我 金 石 交 ， 沉 邈 焉 能 求'}] 情感分析 借助情感分析 pipeline，我们只需要输入文本，就可以得到其情感标签（积极/消极）以及对应的概率：\nfrom transformers import pipeline\rclassifier = pipeline(\"sentiment-analysis\")\rresult = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\rprint(result)\rresults = classifier(\r[\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\r)\rprint(results) No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\r[{'label': 'POSITIVE', 'score': 0.9598048329353333}]\r[{'label': 'POSITIVE', 'score': 0.9598048329353333}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}] pipeline 模型会自动完成以下三个步骤：\n将文本预处理为模型可以理解的格式； 将预处理好的文本送入模型； 对模型的预测值进行后处理，输出人类可以理解的格式。 pipeline 会自动选择合适的预训练模型来完成任务。例如对于情感分析，默认就会选择微调好的英文情感模型 distilbert-base-uncased-finetuned-sst-2-english。\nTransformers 库会在创建对象时下载并且缓存模型，只有在首次加载模型时才会下载，后续会直接调用缓存好的模型。\n零训练样本分类 零训练样本分类 pipeline 允许我们在不提供任何标注数据的情况下自定义分类标签。\nfrom transformers import pipeline\rclassifier = pipeline(\"zero-shot-classification\")\rresult = classifier(\r\"This is a course about the Transformers library\",\rcandidate_labels=[\"education\", \"politics\", \"business\"],\r)\rprint(result) No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\r{'sequence': 'This is a course about the Transformers library', 'labels': ['education', 'business', 'politics'], 'scores': [0.8445973992347717, 0.11197526752948761, 0.043427325785160065]} 可以看到，pipeline 自动选择了预训练好的 facebook/bart-large-mnli 模型来完成任务。\n遮盖词填充 给定一段部分词语被遮盖掉 (masked) 的文本，使用预训练模型来预测能够填充这些位置的词语。\nfrom transformers import pipeline\runmasker = pipeline(\"fill-mask\")\rresults = unmasker(\"This course will teach you all about \u003cmask\u003e models.\", top_k=2)\rprint(results) No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\r[{'sequence': 'This course will teach you all about mathematical models.', 'score': 0.19619858264923096, 'token': 30412, 'token_str': ' mathematical'}, {'sequence': 'This course will teach you all about computational models.', 'score': 0.04052719101309776, 'token': 38163, 'token_str': ' computational'}] 可以看到，pipeline 自动选择了预训练好的 distilroberta-base 模型来完成任务。\n命名实体识别 命名实体识别 (NER) pipeline 负责从文本中抽取出指定类型的实体，例如人物、地点、组织等等。\nfrom transformers import pipeline\rner = pipeline(\"ner\", grouped_entities=True)\rresults = ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\rprint(results) No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\r[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.97960186, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}] 可以看到，模型正确地识别出了 Sylvain 是一个人物，Hugging Face 是一个组织，Brooklyn 是一个地名。\n这里通过设置参数 grouped_entities=True，使得 pipeline 自动合并属于同一个实体的多个子词 (token)，例如这里将“Hugging”和“Face”合并为一个组织实体，实际上 Sylvain 也进行了子词合并，因为分词器会将 Sylvain 切分为 S、##yl 、##va 和 ##in 四个 token。\n自动问答 自动问答 pipeline 可以根据给定的上下文回答问题，例如：\nfrom transformers import pipeline\rquestion_answerer = pipeline(\"question-answering\")\ranswer = question_answerer(\rquestion=\"Where do I work?\",\rcontext=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\r)\rprint(answer) No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\r{'score': 0.6949771046638489, 'start': 33, 'end': 45, 'answer': 'Hugging Face'} 可以看到，pipeline 自动选择了在 SQuAD 数据集上训练好的 distilbert-base 模型来完成任务。这里的自动问答 pipeline 实际上是一个抽取式问答模型，即从给定的上下文中抽取答案，而不是生成答案。\n根据形式的不同，自动问答 (QA) 系统可以分为三种：\n**抽取式 QA (extractive QA)：**假设答案就包含在文档中，因此直接从文档中抽取答案； **多选 QA (multiple-choice QA)：**从多个给定的选项中选择答案，相当于做阅读理解题； **无约束 QA (free-form QA)：**直接生成答案文本，并且对答案文本格式没有任何限制。 自动摘要 自动摘要 pipeline 旨在将长文本压缩成短文本，并且还要尽可能保留原文的主要信息，例如：\nfrom transformers import pipeline\rsummarizer = pipeline(\"summarization\")\rresults = summarizer(\r\"\"\"\rAmerica has changed dramatically during recent years. Not only has the number of graduates in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering declined, but in most of the premier American universities engineering curricula now concentrate on and encourage largely the study of engineering science. As a result, there are declining offerings in engineering subjects dealing with infrastructure, the environment, and related issues, and greater concentration on high technology subjects, largely supporting increasingly complex scientific developments. While the latter is important, it should not be at the expense of more traditional engineering.\rRapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance the teaching of engineering. Both China and India, respectively, graduate six and eight times as many traditional engineers as does the United States. Other industrial countries at minimum maintain their output, while America suffers an increasingly serious decline in the number of engineering graduates and a lack of well-educated engineers.\r\"\"\"\r)\rprint(results) No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\r[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance engineering .'}] 可以看到，pipeline 自动选择了预训练好的 distilbart-cnn-12-6 模型来完成任务。与文本生成类似，我们也可以通过 max_length 或 min_length 参数来控制返回摘要的长度。\npipeline 背后做了什么？ 这些简单易用的 pipeline 模型实际上封装了许多操作，下面我们就来了解一下它们背后究竟做了啥。以第一个情感分析 pipeline 为例，我们运行下面的代码\nfrom transformers import pipeline\rclassifier = pipeline(\"sentiment-analysis\")\rresult = classifier(\"This course is amazing!\")\rprint(result) 就会得到结果：\n[{'label': 'POSITIVE', 'score': 0.9998824596405029}] 实际上它的背后经过了三个步骤：\n预处理 (preprocessing)，将原始文本转换为模型可以接受的输入格式； 将处理好的输入送入模型； 对模型的输出进行后处理 (postprocessing)，将其转换为人类方便阅读的格式。 使用分词器进行预处理 因为神经网络模型无法直接处理文本，因此首先需要通过预处理环节将文本转换为模型可以理解的数字。具体地，我们会使用每个模型对应的分词器 (tokenizer) 来进行：\n将输入切分为词语、子词或者符号（例如标点符号），统称为 tokens； 根据模型的词表将每个 token 映射到对应的 token 编号（就是一个数字）； 根据模型的需要，添加一些额外的输入。 我们对输入文本的预处理需要与模型自身预训练时的操作完全一致，只有这样模型才可以正常地工作。注意，每个模型都有特定的预处理操作，如果对要使用的模型不熟悉，可以通过 Model Hub 查询。这里我们使用 AutoTokenizer 类和它的 from_pretrained() 函数，它可以自动根据模型 checkpoint 名称来获取对应的分词器。\n情感分析 pipeline 的默认 checkpoint 是 distilbert-base-uncased-finetuned-sst-2-english，下面我们手工下载并调用其分词器：\nfrom transformers import AutoTokenizer\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\rprint(inputs) {\r'input_ids': tensor([\r[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102],\r[ 101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0,\r0, 0, 0, 0, 0, 0]\r]), 'attention_mask': tensor([\r[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\r])\r} 可以看到，输出中包含两个键 input_ids 和 attention_mask，其中 input_ids 对应分词之后的 tokens 映射到的数字编号列表，而 attention_mask 则是用来标记哪些 tokens 是被填充的（这里“1”表示是原文，“0”表示是填充字符）。\n先不要关注 padding、truncation 这些参数，以及 attention_mask 项，后面我们会详细介绍:)。\n将预处理好的输入送入模型 预训练模型的下载方式和分词器 (tokenizer) 类似，Transformers 包提供了一个 AutoModel 类和对应的 from_pretrained() 函数。下面我们手工下载这个 distilbert-base 模型：\nfrom transformers import AutoModel\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rmodel = AutoModel.from_pretrained(checkpoint) 预训练模型的本体只包含基础的 Transformer 模块，对于给定的输入，它会输出一些神经元的值，称为 hidden states 或者特征 (features)。对于 NLP 模型来说，可以理解为是文本的高维语义表示。这些 hidden states 通常会被输入到其他的模型部分（称为 head），以完成特定的任务，例如送入到分类头中完成文本分类任务。\n其实前面我们举例的所有 pipelines 都具有类似的模型结构，只是模型的最后一部分会使用不同的 head 以完成对应的任务。 Transformers库结构 Transformers 库封装了很多不同的结构，常见的有：\n*Model （返回 hidden states） *ForCausalLM （用于条件语言模型），是一种用于因果语言模型的Transformer接口类型。在Transformer架构中,ForCausalLM是一个特殊的模型头,它被设计用于生成文本,即从前一个词预测下一个词,这类模型主要用于以下任务：1.根据已有文本生成后续文本，例如自动写作、对话生成等,2.语言建模：预测给定文本序列中下一个词的概率。 *ForSeq2SeqLM代表用于序列到序列（Sequence-to-Sequence）任务的语言模型。这类模型通常用于以下任务：1.机器翻译：将一种语言的句子翻译成另一种语言,2.文本摘要：将长文本生成简短的摘要,3.问答系统：从上下文中生成回答。。 *ForMaskedLM （用于遮盖语言模型），在MLM任务中,输入序列中的某些词会被随机mask掉,模型的目标是预测这些被mask的词。 *ForMultipleChoice （用于多选任务） *ForQuestionAnswering （用于自动问答任务） *ForSequenceClassification （用于文本分类任务），类似api如：图像分类：*ForImageClassification，输出维度对应于整个序列的类别预测,即整个输入序列只有一个类别输出。 *ForTokenClassification （用于 token 分类任务，例如 NER），输出维度对应于每个token的类别预测,即每个输入token都有一个类别输出。 模块输出 Transformer 模块的输出是一个维度为 (Batch size, Sequence length, Hidden size) 的三维张量，其中 Batch size 表示每次输入的样本（文本序列）数量，即每次输入多少个句子，上例中为 2；Sequence length 表示文本序列的长度，即每个句子被分为多少个 token，上例中为 16；Hidden size 表示每一个 token 经过模型编码后的输出向量（语义表示）的维度。\n预训练模型编码后的输出向量的维度通常都很大，例如 Bert 模型 base 版本的输出为 768 维，一些大模型的输出维度为 3072 甚至更高。\n我们可以打印出这里使用的 distilbert-base 模型的输出维度：\nfrom transformers import AutoTokenizer, AutoModel\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rmodel = AutoModel.from_pretrained(checkpoint)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\rprint(outputs.last_hidden_state.shape) torch.Size([2, 16, 768]) Transformers 模型的输出格式类似 namedtuple 或字典，可以像上面那样通过属性访问，也可以通过键（outputs[\"last_hidden_state\"]），甚至索引访问（outputs[0]）。\n对于情感分析任务，很明显我们最后需要使用的是一个文本分类 head。因此，实际上我们不会使用 AutoModel 类，而是使用 AutoModelForSequenceClassification：\nfrom transformers import AutoTokenizer\rfrom transformers import AutoModelForSequenceClassification\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\rprint(outputs.logits.shape) torch.Size([2, 2]) 可以看到，对于 batch 中的每一个样本，模型都会输出一个两维的向量（每一维对应一个标签，positive 或 negative）。\n对模型输出进行后处理 由于模型的输出只是一些数值，因此并不适合人类阅读。例如我们打印出上面例子的输出：\nfrom transformers import AutoTokenizer\rfrom transformers import AutoModelForSequenceClassification\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\rprint(outputs.logits) tensor([[-1.5607, 1.6123],\r[ 4.1692, -3.3464]], grad_fn=\u003cAddmmBackward0\u003e) 模型对第一个句子输出 [−1.5607,1.6123]，对第二个句子输出 [4.1692,−3.3464]，它们并不是概率值，而是模型最后一层输出的 logits 值。要将他们转换为概率值，还需要让它们经过一个 SoftMax 层，例如：\nimport torch\rpredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\rprint(predictions) tensor([[4.0195e-02, 9.5980e-01],\r[9.9946e-01, 5.4418e-04]], grad_fn=\u003cSoftmaxBackward0\u003e) 所有 Transformers 模型都会输出 logits 值，因为训练时的损失函数通常会自动结合激活函数（例如 SoftMax）与实际的损失函数（例如交叉熵 cross entropy）。\n这样模型的预测结果就是容易理解的概率值：第一个句子 [0.0402,0.9598]，第二个句子 [0.9995,0.0005]。最后，为了得到对应的标签，可以读取模型 config 中提供的 id2label 属性：\nprint(model.config.id2label) {0: 'NEGATIVE', 1: 'POSITIVE'} 于是我们可以得到最终的预测结果：\n第一个句子: NEGATIVE: 0.0402, POSITIVE: 0.9598 第二个句子: NEGATIVE: 0.9995, POSITIVE: 0.0005 本文部分文本引用自：https://transformers.run/",
    "description": "Hugging Face Hugging Face是一个人工智能（AI）公司，它致力于开发和推广自然语言处理（NLP）相关的技术和工具。该公司以其开源项目和社区而闻名，其最知名的项目之一是Transformers库，它提供了一系列预训练的语言模型，包括BERT、GPT和RoBERTa等。这些模型已经在各种NLP任务中取得了显著的成功，并成为了许多研究和工业应用的基础。\n除了提供预训练的模型之外，Hugging Face还开发了一系列工具和平台，使得使用和部署这些模型变得更加简单。其中包括：\nTransformers库：提供了各种预训练的语言模型的接口和工具，使得开发者可以轻松地使用这些模型进行文本分类、命名实体识别、语言生成等任务。\nDatasets库：包含了各种NLP数据集的接口和工具，使得开发者可以方便地使用这些数据集进行模型训练和评估。\nTrainer库：提供了一个训练和微调模型的框架，使得开发者可以方便地使用自己的数据集对预训练模型进行微调，以适应特定的任务和应用场景。\nModel Hub：一个模型分享和发布平台，开发者可以在这里分享自己训练的模型，也可以找到其他人分享的模型，并且可以直接在自己的项目中使用这些模型。\ndatasets数据集处理，transformers预训练微调等相关教程请参考官网hugging face官方文档。\nTransformers Transformers 是由 Hugging Face 开发的一个 NLP 包，支持加载目前绝大部分的预训练模型。随着 BERT、GPT 等大规模语言模型的兴起，越来越多的公司和研究者采用 Transformers 库来构建 NLP 应用，官网地址。 它提供了各种预训练的 Transformer 模型，包括 BERT、GPT、RoBERTa、DistilBERT 等。这些模型在多个 NLP 任务上取得了 state-of-the-art 的性能，并且 Transformers 库提供了简单易用的接口，使得使用这些预训练模型变得非常便捷。\n安装 官网安装教程参考：https://huggingface.co/docs/transformers/installation\n您可以通过 pip 安装 Transformers 库。在终端或命令行界面中执行以下命令（我这里使用pytorch，如果需要tensorflow的版本参考官网）：\npip install 'transformers[torch]'\n这将会自动从 PyPI（Python Package Index）下载并安装最新版本的 Transformers 库及其依赖项。\n如果您使用的是 Anaconda 环境，您也可以通过 conda 安装：\nconda install -c huggingface transformers\n这将会从 Anaconda 仓库中下载并安装 Transformers 库及其依赖项。\n安装完成后，您就可以在 Python 环境中使用 Transformers 库了。您可以编写代码来加载预训练模型、执行各种 NLP 任务，或者使用 Transformers 提供的高级 API，如 pipelines 来快速完成任务。",
    "tags": [],
    "title": "Transformers实战01-开箱即用的 pipelines",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_01/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "BERT（Bidirectional Encoder Representations from Transformers）是一种基于 Transformer 模型的预训练语言表示方法，由Google研究团队于2018年提出。BERT 通过在大规模文本语料上进行无监督的预训练，学习了通用的语言表示，并且在各种自然语言处理任务中取得了显著的性能提升。\nBERT仅使用了Transformer架构的Encoder部分。BERT自2018年由谷歌发布后，在多种NLP任务中（例如QA、文本生成、情感分析等等）都实现了更好的结果。\n“Word2vec与GloVe都有一个特点，就是它们是上下文无关（context-free）的词嵌入。所以它们没有解决：一个单词在不同上下文中代表不同的含义的问题。例如，对于单词bank，它在不同的上下文中，有银行、河畔这种差别非常大的含义。BERT的出现，解决了这个问题。\nBERT 的主要特点包括：\n双向性：BERT 使用双向 Transformer 模型来处理输入序列，从而能够同时考虑上下文的信息，而不仅仅是单向的上下文信息。这种双向性使得 BERT 能够更好地理解句子中的语义和语境。\n预训练-微调框架：BERT 使用了预训练-微调的方法。首先，在大规模文本语料上进行无监督的预训练，通过 Masked Language Model（MLM）和 Next Sentence Prediction（NSP）任务学习语言表示；然后，在特定的下游任务上微调模型参数，使其适应于特定的任务，如文本分类、命名实体识别等。\nTransformer 模型：BERT 基于 Transformer 模型结构，其中包括多层的编码器，每个编码器由自注意力机制和前馈神经网络组成。这种结构能够有效地捕获输入序列中的长距离依赖关系，有助于提高模型在各种自然语言处理任务中的性能。\n多层表示：BERT 提供了多层的语言表示，使得用户可以根据具体任务选择不同层的表示进行应用。较底层的表示通常更加接近原始输入，而较高层的表示则更加抽象，包含了更多的语义信息。\n开放源代码：BERT 的源代码和预训练模型已经在 GitHub 上开放，使得研究人员和开发者可以基于 BERT 进行进一步的研究和应用开发。\nBERT 通过预训练大规模文本语料上的通用语言表示，以及在各种下游任务上的微调，有效地提高了自然语言处理任务的性能，并且成为了当前领域内最具影响力的预训练模型之一。\ntransformer提供了不同领域中常见的机器学习模型类型：\nTEXT MODELS（文本模型）：用于处理和分析文本数据的模型，如自然语言处理（NLP）中的BERT、GPT等。\nVISION MODELS（视觉模型）：用于处理和分析图像数据的模型，如卷积神经网络（CNN）中的ResNet、VGG,Vision Transformer (ViT)等。\nAUDIO MODELS（音频模型）：用于处理和分析音频数据的模型，如声学模型、语音识别模型等。\nVIDEO MODELS（视频模型）：用于处理和分析视频数据的模型，如视频分类、目标检测、行为识别等。\nMULTIMODAL MODELS（多模态模型）：结合多种数据类型（如文本、图像、音频等）进行分析和预测的模型，如OpenAI的CLIP。\nREINFORCEMENT LEARNING MODELS（强化学习模型）：用于解决强化学习问题的模型，如Deep Q-Networks（DQN）、Actor-Critic等。\nTIME SERIES MODELS（时间序列模型）：用于分析和预测时间序列数据的模型，如循环神经网络（RNN）、长短期记忆网络（LSTM）等。\nGRAPH MODELS（图模型）：用于处理和分析图数据的模型，如图神经网络（GNN）、图卷积网络（GCN）等。\nBERT的基本原理 BERT基于的是Transformer模型，并且仅使用Transformer模型的Encoder部分。在Transformer模型中，Encoder的输入是一串序列，输出的是对序列中每个字符的表示。同样，在BERT中，输入的是一串序列，输出的是也是对应序列中每个单词的编码。 以“He got bit by Python”为例，BERT的输入输出如下图所示： 其中输入为序列“He got bit by Python”，输出的是对每个单词的编码$R_{word}$。这样在经过了BERT处理后，即得到了对每个单词包含的上下文表示$R_{word}$。\n分词 from transformers import AutoModel, BertTokenizer\rmodel_name=\"bert-base-chinese\" #bert-base-uncased\rmodel=AutoModel.from_pretrained(model_name)\rtokenizer=BertTokenizer.from_pretrained(model_name)\rprint(type(model),type(tokenizer))\rsequence = [\"我出生在湖南A阳,我得家在深圳.\",\"我得儿子是廖X谦\"]\r#输出中包含两个键 input_ids 和 attention_mask，其中 input_ids 对应分词之后的 tokens 映射到的数字编号列表，而 attention_mask 则是用来标记哪些 tokens #是被填充的（这里“1”表示是原文，“0”表示是填充字符）。\rprint(tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\",pair=True))\r#将输入切分为词语、子词或者符号（例如标点符号），统称为 tokens；\rprint(tokenizer.tokenize(sequence[0]),len(tokenizer.tokenize(sequence[0])))\r#我们通过 convert_tokens_to_ids() 将切分出的 tokens 转换为对应的 token IDs：\rprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence[0])))\r#可以通过 encode() 函数将这两个步骤合并，并且 encode() 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加[CLS] 和 [SEP]\rprint(tokenizer.encode(sequence[0]))\r#解码还原文字，可以看到encode前后加了[CLS] 和 [SEP]\rprint(tokenizer.decode(tokenizer.encode(sequence[1]))) 输出\n\u003cclass 'transformers.models.bert.modeling_bert.BertModel'\u003e \u003cclass 'transformers.models.bert.tokenization_bert.BertTokenizer'\u003e\r{'input_ids': tensor([[ 101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 2533,\r2157, 1762, 3918, 1766, 119, 102],\r[ 101, 2769, 2533, 1036, 2094, 3221, 2445, 3813, 6472, 102, 0, 0,\r0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\r['我', '出', '生', '在', '湖', '南', 'A', '阳', ',', '我', '得', '家', '在', '深', '圳', '.'] 16\r[2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 2533, 2157, 1762, 3918, 1766, 119]\r[101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 2533, 2157, 1762, 3918, 1766, 119, 102]\r[CLS] 我 得 儿 子 是 廖 X 谦 [SEP] 模型输出 #这里演示最终输出隐藏状态得输出\rfrom transformers import AutoModel,AutoTokenizer\rmodel_name=\"bert-base-chinese\" #bert-base-uncased\rmodel=AutoModel.from_pretrained(model_name)\rtokenizer=BertTokenizer.from_pretrained(model_name)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\rprint(\"词个数\",len(tokenizer.encode(raw_inputs[0])))\r\"\"\"\r在BERT模型中，last_hidden_state 的形状是 [batch_size, sequence_length, hidden_size]，其中：\rbatch_size 表示批量大小，即输入的样本数量。在你的例子中，batch_size 是 2，表示你有两个句子。\rsequence_length 表示序列长度，即输入文本中词元的数量。在你的例子中，sequence_length 是 19，表示每个句子包含 19 个词元,我爱中国，我就是一个词元，爱也是一个词元。\rhidden_size 表示隐藏状态的维度，通常是模型的隐藏层的大小。在BERT-base模型中，hidden_size 是 768，表示每个词元的隐藏状态是一个包含 768 个值的向量。\r\"\"\"\rprint(outputs.last_hidden_state.shape) 输出：torch.Size([2, 19, 768])\nBERT预训练的方法 BERT的预训练语料库使用的是Toronto BookCorpus和Wikipedia数据集。在准备训练数据时，首先从语料库中采样2条句子，例如Sentence-A与Sentence-B。这里需要注意的是：2条句子的单词之和不能超过512个。对于采集的这些句子，50%为两个句子是相邻句子，另50%为两个句子毫无关系。\n假设采集了以下2条句子：\nBeijing is a beautiful city\rI love Beijing 对这2条句子先做分词：\nTokens = [ [CLS], Beijing, is, a, beautiful, city, [SEP], I, love, Beijing, [SEP] ] 然后，以15%的概率遮挡单词，并遵循80%-10%-10%的规则。假设遮挡的单词为city，则：\nTokens = [ [CLS], Beijing, is, a, beautiful, [MASK], [SEP], I, love, Beijing, [SEP] ]\n接下来将Tokens送入到BERT中，并训练BERT预测被遮挡的单词，同时也要预测这2条句子是否为相邻（句子2是句子1的下一条句子）。也就是说，BERT是同时训练Masked Language Modeling和NSP任务。\nBERT的训练参数是：1000000个step，每个batch包含256条序列（256 * 512个单词 = 128000单词/batch）。使用的是Adam，learning rate为1e-4、β1 = 0.9、β2 = 0.999。L2正则权重的衰减参数为0.01。对于learning rete，前10000个steps使用了rate warmup，之后开始线性衰减learning rate（简单地说，就是前期训练使用一个较大的learning rate，后期开始线性减少）。对所有layer使用0.1概率的dropout。使用的激活函数为gelu，而非relu。 验证使用两条句子。\ncheckpoint = \"bert-base-chinese\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rraw_inputs = [\r\"拼多多得货物真是差劲.\",\r\"我喜欢天猫，天猫货物都很好\",\r]\rraw_inputs1 = [\r\"拼多多买了一件掉色衣服.\",\r\"我在天猫买的衣服颜色还行\",\r]\r#允许传入两个数组，相同索引会自动通过[SEP]拼接。\rinputs = tokenizer(raw_inputs,raw_inputs1, padding=True, truncation=True, return_tensors=\"pt\")\rprint(tokenizer.decode(inputs.input_ids[0]))\rprint(tokenizer.decode(inputs.input_ids[1])) 输出\n[CLS] 拼 多 多 得 货 物 真 是 差 劲. [SEP] 拼 多 多 买 了 一 件 掉 色 衣 服. [SEP] [PAD] [PAD]\r[CLS] 我 喜 欢 天 猫 ， 天 猫 货 物 都 很 好 [SEP] 我 在 天 猫 买 的 衣 服 颜 色 还 行 [SEP] 预测的整个过程\n#演示预测的整个过程。\rimport torch\rfrom transformers import AutoModelForSequenceClassification\r#情感分析任务\rcheckpoint = \"bert-base-chinese\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\rprint(type(model))\rraw_inputs = [\r\"拼多多得货物真是差劲.\",\r\"我喜欢天猫，天猫货物都很好\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\r#将分词的词反编码出来\rprint(tokenizer.decode(inputs.input_ids[0]),tokenizer.decode(inputs.input_ids[1]))\r#\"Logits\" 是指模型在分类问题中输出的未经过 softmax 或 sigmoid 函数处理的原始预测值。\rprint(\"分类输出形状:\",outputs.logits.shape)\rprint(\"分类输出:\",outputs.logits)\r#经过softmax就是预测的结果了\rpredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\r#预测的每一行是一个句子，第一列表示积极的概率，第二列表示不积极的概率\rprint(\"预测结果:\",predictions)\r#有两种分类0表示积极，1表示不积极\rprint(\"label和索引:\",print(model.config.id2label)) 输出\n[CLS] 拼 多 多 得 货 物 真 是 差 劲. [SEP] [PAD] [PAD] [CLS] 我 喜 欢 天 猫 ， 天 猫 货 物 都 很 好 [SEP]\r分类输出形状: torch.Size([2, 2])\r分类输出: tensor([[0.4789, 1.0043],\r[0.2907, 0.7432]], grad_fn=\u003cAddmmBackward0\u003e)\r预测结果: tensor([[0.3716, 0.6284],\r[0.3888, 0.6112]], grad_fn=\u003cSoftmaxBackward0\u003e)\r{0: 'LABEL_0', 1: 'LABEL_1'} BERT模型微调 加载数据集 我们以同义句判断任务为例（每次输入两个句子，判断它们是否为同义句），带大家构建我们的第一个 Transformers 模型。我们选择蚂蚁金融语义相似度数据集 AFQMC 作为语料，它提供了官方的数据划分，训练集（train.json） / 验证集（dev.json） / 测试集(test.json)分别包含 34334 / 4316 / 3861 个句子对，标签 0 表示非同义句，1 表示同义句：\n{\"sentence1\": \"还款还清了，为什么花呗账单显示还要还款\", \"sentence2\": \"花呗全额还清怎么显示没有还款\", \"label\": \"1\"} 训练集用于训练模型，验证集用于每次epoch后训练集的正确率，测试集用于验证最后生成模型的准确率。\nDataset Pytorch 通过 Dataset 类和 DataLoader 类处理数据集和加载样本。同样地，这里我们首先继承 Dataset 类构造自定义数据集，以组织样本和标签。AFQMC 样本以 json 格式存储，因此我们使用 json 库按行读取样本，并且以行号作为索引构建数据集。\nclass MyDataSet(Dataset):\rdef __init__(self,filePath):\rself.data={}\rcurrent_directory = os.getcwd()\rwith open(current_directory+\"/dataset/\"+filePath,\"rt\", encoding=\"utf-8\") as f:\rfor idx,line in enumerate(f):\rself.data[idx]=json.loads(line.strip())\rdef __getitem__(self, item):\rreturn self.data[item]\rdef __len__(self):\rreturn len(self.data)\rtrain_data=MyDataSet(\"train.json\")\rdev_data=MyDataSet(\"dev.json\")\rprint(dev_data[1]) 输出:\n{'id': 1, 'sentence1': '网商贷怎么转变成借呗', 'sentence2': '如何将网商贷切换为借呗'} 可以看到，我们编写的 AFQMC 类成功读取了数据集，每一个样本都以字典形式保存，分别以 sentence1、sentence2 和 label 为键存储句子对和标签。\n如果数据集非常巨大，难以一次性加载到内存中，我们也可以继承 IterableDataset 类构建迭代型数据集：\nclass MyDataSetIter(IterableDataset):\rdef __init__(self,filePath):\rself.filePath=filePath\rdef __iter__(self):\rcurrent_directory = os.getcwd()\rwith open(current_directory+\"/dataset/\"+self.filePath,\"rt\", encoding=\"utf-8\") as f:\rfor _,line in enumerate(f):\rdata=json.loads(line.strip())\ryield data\rprint(next(iter(MyDataSetIter(\"dev.json\")))) 输出：\n{'sentence1': '双十一花呗提额在哪', 'sentence2': '里可以提花呗额度', 'label': '0'} DataLoader 接下来就需要通过 DataLoader 库按批 (batch) 加载数据，并且将样本转换成模型可以接受的输入格式。对于 NLP 任务，这个环节就是将每个 batch 中的文本按照预训练模型的格式进行编码（包括 Padding、截断等操作）。\n我们通过手工编写 DataLoader 的批处理函数 collate_fn 来实现。首先加载分词器，然后对每个 batch 中的所有句子对进行编码，同时把标签转换为张量格式：\n#DataLoader处理数据为seq1 [SEP] seq2\rfrom transformers import AutoTokenizer\rimport torch\rfrom torch.utils.data import DataLoader\rcheckpoint = \"bert-base-chinese\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rdef collote_fn(batch_samples):\rbatch_sentence_1, batch_sentence_2 = [], []\rbatch_label = []\rfor sample in batch_samples:\rbatch_sentence_1.append(sample['sentence1'])\rbatch_sentence_2.append(sample['sentence2'])\rbatch_label.append(int(sample['label']))\rX = tokenizer(\rbatch_sentence_1,\rbatch_sentence_2,\rpadding=True,\rtruncation=True,\rreturn_tensors=\"pt\"\r)\ry = torch.tensor(batch_label)\rreturn X, y\rtrain_loader=DataLoader(train_data,batch_size=4,shuffle=False,collate_fn=collote_fn)\rX,y=next(iter(train_loader))\rprint(\"label的维度\",y.shape)\rprint(\"s1,s2合并的维度\",X.input_ids.shape)\rfor idx,d in enumerate(X.input_ids):\rprint(\"第一批次4个元素中的第{}个：{},label={}\".format(idx,tokenizer.decode(X.input_ids[idx]),y[idx])) 输出\nlabel的维度 torch.Size([4])\rs1,s2合并的维度 torch.Size([4, 30])\r第一批次4个元素中的第0个：[CLS] 蚂 蚁 借 呗 等 额 还 款 可 以 换 成 先 息 后 本 吗 [SEP] 借 呗 有 先 息 到 期 还 本 吗 [SEP],label=0\r第一批次4个元素中的第1个：[CLS] 蚂 蚁 花 呗 说 我 违 约 一 次 [SEP] 蚂 蚁 花 呗 违 约 行 为 是 什 么 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD],label=0\r第一批次4个元素中的第2个：[CLS] 帮 我 看 一 下 本 月 花 呗 账 单 有 没 有 结 清 [SEP] 下 月 花 呗 账 单 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD],label=0\r第一批次4个元素中的第3个：[CLS] 蚂 蚁 借 呗 多 长 时 间 综 合 评 估 一 次 [SEP] 借 呗 得 评 估 多 久 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD],label=0 可以看到，DataLoader 按照我们设置的 batch size 每次对 4 个样本进行编码，并且通过设置 padding=True 和 truncation=True 来自动对每个 batch 中的样本进行补全和截断。这里我们选择 BERT 模型作为 checkpoint，所以每个样本都被处理成了“了“[CLS] sen1 [SEP] sen2 [SEP]”的形式。\n这种只在一个 batch 内进行补全的操作被称为动态补全 (Dynamic padding)，Hugging Face 也提供了 DataCollatorWithPadding 类来进行，如果感兴趣可以自行了解。\n训练模型 构建模型 常见的写法是继承 Transformers 库中的预训练模型来创建自己的模型。例如这里我们可以继承 BERT 模型（BertPreTrainedModel 类）来创建一个与上面模型结构完全相同的分类器：\n#构建模型\rfrom transformers import BertPreTrainedModel,BertModel,AutoConfig\rfrom torch import nn\rclass BertForPartwiseCLs(BertPreTrainedModel):\r\"\"\"\r定义模型继承自BertPreTrainedModel\r\"\"\"\rdef __init__(self,config):\r\"\"\"\r传入config，原始镜像的config\r\"\"\"\rsuper().__init__(config)\r#定义BertModel\rself.model=BertModel(config, add_pooling_layer=False)\r#丢弃10%\rself.dropout=nn.Dropout(config.hidden_dropout_prob)\r#全连接为2分类\rself.classifier=nn.Linear(768,2)\r#初始化权重参数\rself.post_init()\rdef forward(self,input):\r#执行模型产生一个(批次，词元,隐藏神经元)的输出\rbert_output=self.model(**input)\r#输出的数据有多个词元，取第一个[CLS]词元，因为每个词元通过注意力机制都包含了和其他词的语义信息，所以只需要一个即可\r#这里句子的维度编程了[批次,1,768]\rvector_data=bert_output.last_hidden_state[:,0,:]\rvector_data=self.dropout(vector_data)\rlogits=self.classifier(vector_data)\rreturn logits\rcheckpoint = \"bert-base-chinese\"\rconfig=AutoConfig.from_pretrained(checkpoint)\rmodel=BertForPartwiseCLs.from_pretrained(checkpoint,config=config)\rprint(model)\rX,y=next(iter(train_loader))\rprint(model(X).shape) 输出\nD:\\python\\evn311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\rwarnings.warn(\rSome weights of BertForPartwiseCLs were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['bert.classifier.bias', 'bert.classifier.weight', 'bert.model.embeddings.LayerNorm.bias', 'bert.model.embeddings.LayerNorm.weight', 'bert.model.embeddings.position_embeddings.weight', 'bert.model.embeddings.token_type_embeddings.weight', 'bert.model.embeddings.word_embeddings.weight', 'bert.model.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.0.attention.output.dense.bias', 'bert.model.encoder.layer.0.attention.output.dense.weight', 'bert.model.encoder.layer.0.attention.self.key.bias', 'bert.model.encoder.layer.0.attention.self.key.weight', 'bert.model.encoder.layer.0.attention.self.query.bias', 'bert.model.encoder.layer.0.attention.self.query.weight', 'bert.model.encoder.layer.0.attention.self.value.bias', 'bert.model.encoder.layer.0.attention.self.value.weight', 'bert.model.encoder.layer.0.intermediate.dense.bias', 'bert.model.encoder.layer.0.intermediate.dense.weight', 'bert.model.encoder.layer.0.output.LayerNorm.bias', 'bert.model.encoder.layer.0.output.LayerNorm.weight', 'bert.model.encoder.layer.0.output.dense.bias', 'bert.model.encoder.layer.0.output.dense.weight', 'bert.model.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.1.attention.output.dense.bias', 'bert.model.encoder.layer.1.attention.output.dense.weight', 'bert.model.encoder.layer.1.attention.self.key.bias', 'bert.model.encoder.layer.1.attention.self.key.weight', 'bert.model.encoder.layer.1.attention.self.query.bias', 'bert.model.encoder.layer.1.attention.self.query.weight', 'bert.model.encoder.layer.1.attention.self.value.bias', 'bert.model.encoder.layer.1.attention.self.value.weight', 'bert.model.encoder.layer.1.intermediate.dense.bias', 'bert.model.encoder.layer.1.intermediate.dense.weight', 'bert.model.encoder.layer.1.output.LayerNorm.bias', 'bert.model.encoder.layer.1.output.LayerNorm.weight', 'bert.model.encoder.layer.1.output.dense.bias', 'bert.model.encoder.layer.1.output.dense.weight', 'bert.model.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.10.attention.output.dense.bias', 'bert.model.encoder.layer.10.attention.output.dense.weight', 'bert.model.encoder.layer.10.attention.self.key.bias', 'bert.model.encoder.layer.10.attention.self.key.weight', 'bert.model.encoder.layer.10.attention.self.query.bias', 'bert.model.encoder.layer.10.attention.self.query.weight', 'bert.model.encoder.layer.10.attention.self.value.bias', 'bert.model.encoder.layer.10.attention.self.value.weight', 'bert.model.encoder.layer.10.intermediate.dense.bias', 'bert.model.encoder.layer.10.intermediate.dense.weight', 'bert.model.encoder.layer.10.output.LayerNorm.bias', 'bert.model.encoder.layer.10.output.LayerNorm.weight', 'bert.model.encoder.layer.10.output.dense.bias', 'bert.model.encoder.layer.10.output.dense.weight', 'bert.model.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.11.attention.output.dense.bias', 'bert.model.encoder.layer.11.attention.output.dense.weight', 'bert.model.encoder.layer.11.attention.self.key.bias', 'bert.model.encoder.layer.11.attention.self.key.weight', 'bert.model.encoder.layer.11.attention.self.query.bias', 'bert.model.encoder.layer.11.attention.self.query.weight', 'bert.model.encoder.layer.11.attention.self.value.bias', 'bert.model.encoder.layer.11.attention.self.value.weight', 'bert.model.encoder.layer.11.intermediate.dense.bias', 'bert.model.encoder.layer.11.intermediate.dense.weight', 'bert.model.encoder.layer.11.output.LayerNorm.bias', 'bert.model.encoder.layer.11.output.LayerNorm.weight', 'bert.model.encoder.layer.11.output.dense.bias', 'bert.model.encoder.layer.11.output.dense.weight', 'bert.model.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.2.attention.output.dense.bias', 'bert.model.encoder.layer.2.attention.output.dense.weight', 'bert.model.encoder.layer.2.attention.self.key.bias', 'bert.model.encoder.layer.2.attention.self.key.weight', 'bert.model.encoder.layer.2.attention.self.query.bias', 'bert.model.encoder.layer.2.attention.self.query.weight', 'bert.model.encoder.layer.2.attention.self.value.bias', 'bert.model.encoder.layer.2.attention.self.value.weight', 'bert.model.encoder.layer.2.intermediate.dense.bias', 'bert.model.encoder.layer.2.intermediate.dense.weight', 'bert.model.encoder.layer.2.output.LayerNorm.bias', 'bert.model.encoder.layer.2.output.LayerNorm.weight', 'bert.model.encoder.layer.2.output.dense.bias', 'bert.model.encoder.layer.2.output.dense.weight', 'bert.model.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.3.attention.output.dense.bias', 'bert.model.encoder.layer.3.attention.output.dense.weight', 'bert.model.encoder.layer.3.attention.self.key.bias', 'bert.model.encoder.layer.3.attention.self.key.weight', 'bert.model.encoder.layer.3.attention.self.query.bias', 'bert.model.encoder.layer.3.attention.self.query.weight', 'bert.model.encoder.layer.3.attention.self.value.bias', 'bert.model.encoder.layer.3.attention.self.value.weight', 'bert.model.encoder.layer.3.intermediate.dense.bias', 'bert.model.encoder.layer.3.intermediate.dense.weight', 'bert.model.encoder.layer.3.output.LayerNorm.bias', 'bert.model.encoder.layer.3.output.LayerNorm.weight', 'bert.model.encoder.layer.3.output.dense.bias', 'bert.model.encoder.layer.3.output.dense.weight', 'bert.model.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.4.attention.output.dense.bias', 'bert.model.encoder.layer.4.attention.output.dense.weight', 'bert.model.encoder.layer.4.attention.self.key.bias', 'bert.model.encoder.layer.4.attention.self.key.weight', 'bert.model.encoder.layer.4.attention.self.query.bias', 'bert.model.encoder.layer.4.attention.self.query.weight', 'bert.model.encoder.layer.4.attention.self.value.bias', 'bert.model.encoder.layer.4.attention.self.value.weight', 'bert.model.encoder.layer.4.intermediate.dense.bias', 'bert.model.encoder.layer.4.intermediate.dense.weight', 'bert.model.encoder.layer.4.output.LayerNorm.bias', 'bert.model.encoder.layer.4.output.LayerNorm.weight', 'bert.model.encoder.layer.4.output.dense.bias', 'bert.model.encoder.layer.4.output.dense.weight', 'bert.model.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.5.attention.output.dense.bias', 'bert.model.encoder.layer.5.attention.output.dense.weight', 'bert.model.encoder.layer.5.attention.self.key.bias', 'bert.model.encoder.layer.5.attention.self.key.weight', 'bert.model.encoder.layer.5.attention.self.query.bias', 'bert.model.encoder.layer.5.attention.self.query.weight', 'bert.model.encoder.layer.5.attention.self.value.bias', 'bert.model.encoder.layer.5.attention.self.value.weight', 'bert.model.encoder.layer.5.intermediate.dense.bias', 'bert.model.encoder.layer.5.intermediate.dense.weight', 'bert.model.encoder.layer.5.output.LayerNorm.bias', 'bert.model.encoder.layer.5.output.LayerNorm.weight', 'bert.model.encoder.layer.5.output.dense.bias', 'bert.model.encoder.layer.5.output.dense.weight', 'bert.model.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.6.attention.output.dense.bias', 'bert.model.encoder.layer.6.attention.output.dense.weight', 'bert.model.encoder.layer.6.attention.self.key.bias', 'bert.model.encoder.layer.6.attention.self.key.weight', 'bert.model.encoder.layer.6.attention.self.query.bias', 'bert.model.encoder.layer.6.attention.self.query.weight', 'bert.model.encoder.layer.6.attention.self.value.bias', 'bert.model.encoder.layer.6.attention.self.value.weight', 'bert.model.encoder.layer.6.intermediate.dense.bias', 'bert.model.encoder.layer.6.intermediate.dense.weight', 'bert.model.encoder.layer.6.output.LayerNorm.bias', 'bert.model.encoder.layer.6.output.LayerNorm.weight', 'bert.model.encoder.layer.6.output.dense.bias', 'bert.model.encoder.layer.6.output.dense.weight', 'bert.model.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.7.attention.output.dense.bias', 'bert.model.encoder.layer.7.attention.output.dense.weight', 'bert.model.encoder.layer.7.attention.self.key.bias', 'bert.model.encoder.layer.7.attention.self.key.weight', 'bert.model.encoder.layer.7.attention.self.query.bias', 'bert.model.encoder.layer.7.attention.self.query.weight', 'bert.model.encoder.layer.7.attention.self.value.bias', 'bert.model.encoder.layer.7.attention.self.value.weight', 'bert.model.encoder.layer.7.intermediate.dense.bias', 'bert.model.encoder.layer.7.intermediate.dense.weight', 'bert.model.encoder.layer.7.output.LayerNorm.bias', 'bert.model.encoder.layer.7.output.LayerNorm.weight', 'bert.model.encoder.layer.7.output.dense.bias', 'bert.model.encoder.layer.7.output.dense.weight', 'bert.model.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.8.attention.output.dense.bias', 'bert.model.encoder.layer.8.attention.output.dense.weight', 'bert.model.encoder.layer.8.attention.self.key.bias', 'bert.model.encoder.layer.8.attention.self.key.weight', 'bert.model.encoder.layer.8.attention.self.query.bias', 'bert.model.encoder.layer.8.attention.self.query.weight', 'bert.model.encoder.layer.8.attention.self.value.bias', 'bert.model.encoder.layer.8.attention.self.value.weight', 'bert.model.encoder.layer.8.intermediate.dense.bias', 'bert.model.encoder.layer.8.intermediate.dense.weight', 'bert.model.encoder.layer.8.output.LayerNorm.bias', 'bert.model.encoder.layer.8.output.LayerNorm.weight', 'bert.model.encoder.layer.8.output.dense.bias', 'bert.model.encoder.layer.8.output.dense.weight', 'bert.model.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.9.attention.output.dense.bias', 'bert.model.encoder.layer.9.attention.output.dense.weight', 'bert.model.encoder.layer.9.attention.self.key.bias', 'bert.model.encoder.layer.9.attention.self.key.weight', 'bert.model.encoder.layer.9.attention.self.query.bias', 'bert.model.encoder.layer.9.attention.self.query.weight', 'bert.model.encoder.layer.9.attention.self.value.bias', 'bert.model.encoder.layer.9.attention.self.value.weight', 'bert.model.encoder.layer.9.intermediate.dense.bias', 'bert.model.encoder.layer.9.intermediate.dense.weight', 'bert.model.encoder.layer.9.output.LayerNorm.bias', 'bert.model.encoder.layer.9.output.LayerNorm.weight', 'bert.model.encoder.layer.9.output.dense.bias', 'bert.model.encoder.layer.9.output.dense.weight']\rYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\rBertForPartwiseCLs(\r(model): BertModel(\r(embeddings): BertEmbeddings(\r(word_embeddings): Embedding(21128, 768, padding_idx=0)\r(position_embeddings): Embedding(512, 768)\r(token_type_embeddings): Embedding(2, 768)\r(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(dropout): Dropout(p=0.1, inplace=False)\r)\r(encoder): BertEncoder(\r(layer): ModuleList(\r(0-11): 12 x BertLayer(\r(attention): BertAttention(\r(self): BertSelfAttention(\r(query): Linear(in_features=768, out_features=768, bias=True)\r(key): Linear(in_features=768, out_features=768, bias=True)\r(value): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.1, inplace=False)\r)\r(output): BertSelfOutput(\r(dense): Linear(in_features=768, out_features=768, bias=True)\r(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(dropout): Dropout(p=0.1, inplace=False)\r)\r)\r(intermediate): BertIntermediate(\r(dense): Linear(in_features=768, out_features=3072, bias=True)\r(intermediate_act_fn): GELUActivation()\r)\r(output): BertOutput(\r(dense): Linear(in_features=3072, out_features=768, bias=True)\r(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(dropout): Dropout(p=0.1, inplace=False)\r)\r)\r)\r)\r)\r(dropout): Dropout(p=0.1, inplace=False)\r(classifier): Linear(in_features=768, out_features=2, bias=True)\r)\rtorch.Size([4, 2]) 可以看到模型输出了一个 4×2 的张量，符合我们的预期（每个样本输出 2 维的 logits 值分别表示两个类别的预测分数，batch 内共 4 个样本）。\ntqdm使用 tqdm是一个Python库,用于在终端中显示进度条。它广泛应用于各种数据处理任务中,如循环、迭代器、pandas数据帧等。以下是对tqdm的简要介绍:\n简单易用: tqdm提供了简单直观的API,可以快速集成到代码中,只需要几行代码即可实现进度条显示。 丰富的功能: tqdm不仅可以显示进度条,还可以显示预估的剩余时间、完成百分比、已处理的数据量等信息。 自动检测环境: tqdm可以自动检测运行环境,在支持ANSI转义码的终端中使用动态进度条,在不支持的环境中使用静态进度条。 支持各种迭代器: tqdm支持各种Python内置迭代器,如list、range、enumerate等,也支持自定义迭代器。 可定制性强: tqdm提供了丰富的参数供用户自定义进度条的样式和行为,如颜色、宽度、刷新间隔等。 代码\n#tqdm进度条使用\rfrom tqdm.auto import tqdm\rimport time\r# 创建一个迭代对象，比如一个列表\ritems = range(10)\r# 使用tqdm来迭代这个对象，并显示进度条\rfor item in tqdm(items, desc='Processing'):\r# 在这里执行你的任务\rtime.sleep(0.1) # 模拟一些长时间运行的任务\r# range(10) 其实就是0-9\rprint([i for i in range(10)])\r#创建一个tqdm对象，传入得必须是range对象，range(10) 其实就是0-9\rprint(range(10),len(range(10)))\rtdm=tqdm(range(10), desc='Processing')\rfor item in range(10):\rtime.sleep(1) # 模拟一些长时间运行的任务\r#更新一次,其实就是进度条加上： 1/len(range(10))\rtdm.update(1) 效果 训练模型 在训练模型时，我们将每一轮 Epoch 分为训练循环和验证/测试循环。在训练循环中计算损失、优化模型的参数，在验证/测试循环中评估模型的性能，与 Pytorch 类似，Transformers 库同样实现了很多的优化器，并且相比 Pytorch 固定学习率，Transformers 库的优化器会随着训练过程逐步减小学习率（通常会产生更好的效果）。例如我们前面使用过的 AdamW 优化器 完整的训练过程，可与使用colab来进行训练。\n#训练模型和验证测试\r#定义损失函数\rfrom torch.nn import CrossEntropyLoss\rfrom transformers import get_scheduler\r#定义优化函数,from torch.optim import AdamW\rfrom transformers import AdamW\rfrom tqdm.auto import tqdm\r#定义epoch训练次数\repochs = 3\r#默认学习率\rlearning_rate = 1e-5\r# batchsize\rbatch_size=4\r#AdamW是Adam优化器的一种变体，它在Adam的基础上进行了一些改进，旨在解决Adam优化器可能引入的权重衰减问题。\roptimizer=AdamW(model.parameters(),lr=1e-5)\r#定义交叉熵损失函数\rloss_fn=CrossEntropyLoss()\r#重新初始化数据集\rtrain_loader=DataLoader(train_data,batch_size=batch_size,shuffle=False,collate_fn=collote_fn)\rdev_loader=DataLoader(MyDataSet(\"dev.json\"),batch_size=batch_size,shuffle=False,collate_fn=collote_fn)\r#总步数=epoch*批次数(总记录数train_data/一批次多少条数据batch_size)\rnum_training_steps = epochs * len(train_loader)\r#默认情况下，优化器会线性衰减学习率，对于上面的例子，学习率会线性地从le-5 降到0\r#。为了正确地定义学习率调度器，我们需要知道总的训练步数 (step)，它等于训练轮数 (Epoch number) 乘以每一轮中的步数（也就是训练 dataloader 的大小）\rlr_scheduler = get_scheduler(\r\"linear\",\roptimizer=optimizer,\rnum_warmup_steps=0,\rnum_training_steps=num_training_steps,\r)\r#初始化模型\rdevice = 'cuda' if torch.cuda.is_available() else 'cpu'\rcheckpoint = \"bert-base-chinese\"\rconfig=AutoConfig.from_pretrained(checkpoint)\rmodel=BertForPartwiseCLs.from_pretrained(checkpoint,config=config).to(device)\r#定义总损失\rtotal_loss=0\r#完成总batch\rcomplete_batch_count=0\r#最好的正确率\rbest_acc = 0.\rcurrent_directory = os.getcwd()\rfor step in range(epochs):\r#进入训练模式\rmodel.train()\rprint(f\"Epoch {step+1}/{epochs}\\n-------------------------------\")\rprogress_bar=tqdm(range(len(train_loader)))\rfor batch,(X,y) in enumerate(train_loader):\rX,y=X.to(device),y.to(device)\r#获取预测结果\rpred=model(X)\r#计算损失函数\rloss=loss_fn(pred,y)\r#清空梯度\roptimizer.zero_grad()\r#前向传播\rloss.backward();\r#更新模型参数\roptimizer.step();\r#学习率线性下降,必须是更新模型参数之后，函数根据设定的规则来调整学习率。这个调整需要基于当前的模型状态,包括参数、损失函数值等,所以要放在optimizer.step()之后。\rlr_scheduler.step()\rtotal_loss+=loss.item()\rcomplete_batch_count+=1\ravg_loss=total_loss/complete_batch_count\rprogress_bar.set_description(\"loss:{}\".format(avg_loss))\rprogress_bar.update(1)\r#使用验证集验证模型正确性。\r#进入预测模式,当前这一次epoch训练数据的正确率\rmodel.eval()\rcorrect=0\r#加载验证集的数据\rfor batch,(X,y) in enumerate(dev_loader):\r#获取预测结果\rpred=model(X)\r#因为是[[0.9,0.1],[0.3,0.4]]所以取dim=1维度上最大值的索引，概率大的索引就是预测的类别，如果和label值y相等就加起来，算个数\rcorrect += (pred.argmax(dim=1) == y).type(torch.float).sum().item()\r#正确/总数就是争取率\rvalid_acc=correct/len(dev_loader.dataset)\rprint(f\"{step+1} Accuracy: {(100*valid_acc):\u003e0.1f}%\\n\")\rif valid_acc \u003e best_acc:\rbest_acc = valid_acc\rprint('saving new weights...\\n')\rtorch.save(model.state_dict(), current_directory+f'/epoch_{step+1}_valid_acc_{(100*valid_acc):0.1f}_model_weights.bin')\rprint(\"Done!\") 模型预测 最后，我们加载验证集上最优的模型权重，汇报其在测试集上的性能。由于 AFQMC 公布的测试集上并没有标签，无法评估性能，这里我们暂且用验证集代替进行演示：\ncurrent_directory = os.getcwd()\rmodel.load_state_dict(torch.load(current_directory+'/model_weights.bin'))\rmodel.eval()\rtest_loader=DataLoader(test_data,batch_size=4,shuffle=False,collate_fn=collote_fn)\rX,y=next(iter(test_loader))\rX, y = X.to(device), y.to(device)\rpred = model(X)\rprint(pred.argmax(1) == y) 文章部分文字引用：https://transformers.run/c2/2021-12-17-transformers-note-4/",
    "description": "BERT（Bidirectional Encoder Representations from Transformers）是一种基于 Transformer 模型的预训练语言表示方法，由Google研究团队于2018年提出。BERT 通过在大规模文本语料上进行无监督的预训练，学习了通用的语言表示，并且在各种自然语言处理任务中取得了显著的性能提升。\nBERT仅使用了Transformer架构的Encoder部分。BERT自2018年由谷歌发布后，在多种NLP任务中（例如QA、文本生成、情感分析等等）都实现了更好的结果。\n“Word2vec与GloVe都有一个特点，就是它们是上下文无关（context-free）的词嵌入。所以它们没有解决：一个单词在不同上下文中代表不同的含义的问题。例如，对于单词bank，它在不同的上下文中，有银行、河畔这种差别非常大的含义。BERT的出现，解决了这个问题。\nBERT 的主要特点包括：\n双向性：BERT 使用双向 Transformer 模型来处理输入序列，从而能够同时考虑上下文的信息，而不仅仅是单向的上下文信息。这种双向性使得 BERT 能够更好地理解句子中的语义和语境。\n预训练-微调框架：BERT 使用了预训练-微调的方法。首先，在大规模文本语料上进行无监督的预训练，通过 Masked Language Model（MLM）和 Next Sentence Prediction（NSP）任务学习语言表示；然后，在特定的下游任务上微调模型参数，使其适应于特定的任务，如文本分类、命名实体识别等。\nTransformer 模型：BERT 基于 Transformer 模型结构，其中包括多层的编码器，每个编码器由自注意力机制和前馈神经网络组成。这种结构能够有效地捕获输入序列中的长距离依赖关系，有助于提高模型在各种自然语言处理任务中的性能。\n多层表示：BERT 提供了多层的语言表示，使得用户可以根据具体任务选择不同层的表示进行应用。较底层的表示通常更加接近原始输入，而较高层的表示则更加抽象，包含了更多的语义信息。\n开放源代码：BERT 的源代码和预训练模型已经在 GitHub 上开放，使得研究人员和开发者可以基于 BERT 进行进一步的研究和应用开发。\nBERT 通过预训练大规模文本语料上的通用语言表示，以及在各种下游任务上的微调，有效地提高了自然语言处理任务的性能，并且成为了当前领域内最具影响力的预训练模型之一。\ntransformer提供了不同领域中常见的机器学习模型类型：\nTEXT MODELS（文本模型）：用于处理和分析文本数据的模型，如自然语言处理（NLP）中的BERT、GPT等。\nVISION MODELS（视觉模型）：用于处理和分析图像数据的模型，如卷积神经网络（CNN）中的ResNet、VGG,Vision Transformer (ViT)等。\nAUDIO MODELS（音频模型）：用于处理和分析音频数据的模型，如声学模型、语音识别模型等。\nVIDEO MODELS（视频模型）：用于处理和分析视频数据的模型，如视频分类、目标检测、行为识别等。\nMULTIMODAL MODELS（多模态模型）：结合多种数据类型（如文本、图像、音频等）进行分析和预测的模型，如OpenAI的CLIP。\nREINFORCEMENT LEARNING MODELS（强化学习模型）：用于解决强化学习问题的模型，如Deep Q-Networks（DQN）、Actor-Critic等。\nTIME SERIES MODELS（时间序列模型）：用于分析和预测时间序列数据的模型，如循环神经网络（RNN）、长短期记忆网络（LSTM）等。\nGRAPH MODELS（图模型）：用于处理和分析图数据的模型，如图神经网络（GNN）、图卷积网络（GCN）等。\nBERT的基本原理 BERT基于的是Transformer模型，并且仅使用Transformer模型的Encoder部分。在Transformer模型中，Encoder的输入是一串序列，输出的是对序列中每个字符的表示。同样，在BERT中，输入的是一串序列，输出的是也是对应序列中每个单词的编码。 以“He got bit by Python”为例，BERT的输入输出如下图所示： 其中输入为序列“He got bit by Python”，输出的是对每个单词的编码$R_{word}$。这样在经过了BERT处理后，即得到了对每个单词包含的上下文表示$R_{word}$。",
    "tags": [],
    "title": "Transformers实战02-BERT预训练模型微调",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_02/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "简介 PEFT PEFT（Parameter-Efficient Fine-Tuning）是一个用于高效地将大型预训练模型适配到各种下游应用的库，而无需对模型的所有参数进行微调，因为这在计算上是非常昂贵的。PEFT 方法只微调少量的（额外的）模型参数——显著降低了计算和存储成本——同时其性能与完全微调的模型相当。这使得在消费者硬件上训练和存储大型语言模型（LLMs）变得更加可行。\nPEFT 集成了 Transformers、Diffusers 和 Accelerate 库，以提供更快、更简单的方法来加载、训练和使用大型模型进行推理。\nLORA方法 一种高效训练大型模型的流行方法是在注意力块中插入较小的可训练矩阵，这些矩阵是微调期间要学习的增量权重矩阵的低秩分解。预训练模型的原始权重矩阵被冻结，仅更新较小的矩阵。这减少了可训练参数的数量，降低了内存使用和训练时间，而这些在大型模型中可能非常昂贵。\n有几种不同的方法可以将权重矩阵表示为低秩分解，但最常见的方法是低秩适应（LoRA原理）。PEFT 库支持几种其他 LoRA 变体，例如低秩Hadamard积（LoHa）、低秩Kronecker积（LoKr）和自适应低秩适应（AdaLoRA）。你可以在适配器指南中了解这些方法的概念工作原理。如果你有兴趣将这些方法应用于其他任务和用例，比如语义分割、标记分类，可以看看我们的笔记本集合！\nVision Transformer (ViT) Vision Transformer（ViT）模型是由Alexey Dosovitskiy，Lucas Beyer，Alexander Kolesnikov，Dirk Weissenborn，Xiaohua Zhai，Thomas Unterthiner，Mostafa Dehghani，Matthias Minderer，Georg Heigold，Sylvain Gelly，Jakob Uszkoreit，Neil Houlsby在《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》中提出的。这是第一篇成功在ImageNet上训练Transformer编码器并获得非常好结果的论文。\n这篇论文的摘要是：\n虽然Transformer架构已经成为自然语言处理任务的事实标准，但它在计算机视觉中的应用仍然有限。在视觉领域，注意力要么与卷积网络一起应用，要么用来替换卷积网络的某些组件，同时保持其总体结构不变。我们展示了在这种对CNN的依赖不是必要的，纯Transformer直接应用于图像块序列可以在图像分类任务上表现得非常好。当在大量数据上进行预训练并转移到多个中等规模或小型图像识别基准数据集（ImageNet，CIFAR-100，VTAB等）时，Vision Transformer（ViT）与最先进的卷积网络相比取得了出色的结果，同时训练所需的计算资源大大减少。\n具体关于该模型得结构参考：https://huggingface.co/docs/transformers/model_doc/vit\nlora方法实战 图像分类微调 本指南将向你展示如何快速训练一个图像分类模型——使用低秩分解方法——来识别图像中显示的食物类别。 案例来自官网：https://huggingface.co/docs/peft/task_guides/lora_based_methods\n模型选择 google/vit-base-patch16-224-in21k 是一个基于Transformer编码器的模型（类似于BERT），在监督方式下，即ImageNet-21k上以224x224像素的分辨率预训练了大量图像。\n图像被呈现给模型作为固定大小的补丁序列（分辨率为16x16），这些补丁被线性嵌入。在序列的开头还添加了一个[CLS]标记，用于分类任务。在将序列馈送到Transformer编码器的层之前，还会添加绝对位置嵌入。\n需要注意的是，这个模型不提供任何经过微调的头部，因为这些头部被Google研究人员清零了。但是，模型包括预训练的汇聚层，可以用于下游任务（如图像分类）。\n通过预训练模型，它学习了图像的内部表示，然后可以用于提取对下游任务有用的特征：例如，如果您有一个带标签的图像数据集，可以通过在预训练编码器顶部放置一个线性层来训练标准分类器。通常将线性层放置在[CLS]标记的顶部，因为该标记的最后隐藏状态可以被视为整个图像的表示。\nfrom transformers import ViTImageProcessor, FlaxViTModel\rfrom PIL import Image\rimport requests\rurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\rimage = Image.open(requests.get(url, stream=True).raw)\rprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\rmodel = FlaxViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\rinputs = processor(images=image, return_tensors=\"np\")\routputs = model(**inputs)\rlast_hidden_states = outputs.last_hidden_state\rprint(last_hidden_states.shape) 不包含分类信息，不包含label信息\n模块 一个模型通常有很多个模块和层，模块是nn.Module是一个更高级别的组织单元，可以包含多个层、子模块或其他操作，层（Layer） 是模块的基本组成部分，用于执行特定的数学运算或神经网络中的某一步骤。\n# 自定义一个简单的模块，包含两个线性层\rclass MyModule(nn.Module):\rdef __init__(self):\rsuper(MyModule, self).__init__()\rself.layer1 = nn.Linear(10, 20)\rself.layer2 = nn.Linear(20, 5)\rdef forward(self, x):\rx = self.layer1(x)\rx = self.layer2(x)\rreturn x 其中MyModule是模块，self.layer1就是层，都可以直接运行，反向传播。 我们可以看看’google/vit-base-patch16-224-in21k’有哪些模块和层\nfrom transformers import AutoModelForImageClassification, TrainingArguments, Trainer\rmodel = AutoModelForImageClassification.from_pretrained(\r\"google/vit-base-patch16-224-in21k\",\rignore_mismatched_sizes=True,\r)\r# 打印模型的结构，查看有哪些模块\rfor name, module in model.named_modules():\rprint(name,\"=\", module) 输出，其中ViTForImageClassification就是用于图形分类的模块，如果通过AutoModelForImageClassification，加载的必然是这个模块，后面我们通过lora优化的是其中的模块的和层，输出的是最后的classifier层。\n= ViTForImageClassification(\r(vit): ViTModel(\r(embeddings): ViTEmbeddings(\r(patch_embeddings): ViTPatchEmbeddings(\r(projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(encoder): ViTEncoder(\r(layer): ModuleList(\r(0-11): 12 x ViTLayer(\r(attention): ViTSdpaAttention(\r(attention): ViTSdpaSelfAttention(\r(query): Linear(in_features=768, out_features=768, bias=True)\r(key): Linear(in_features=768, out_features=768, bias=True)\r(value): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(output): ViTSelfOutput(\r(dense): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r)\r(intermediate): ViTIntermediate(\r(dense): Linear(in_features=768, out_features=3072, bias=True)\r(intermediate_act_fn): GELUActivation()\r)\r(output): ViTOutput(\r(dense): Linear(in_features=3072, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r)\r)\r)\r(layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r)\r(classifier): Linear(in_features=768, out_features=2, bias=True)\r)\rvit = ViTModel(\r(embeddings): ViTEmbeddings(\r(patch_embeddings): ViTPatchEmbeddings(\r(projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(encoder): ViTEncoder(\r(layer): ModuleList(\r(0-11): 12 x ViTLayer(\r(attention): ViTSdpaAttention(\r(attention): ViTSdpaSelfAttention(\r(query): Linear(in_features=768, out_features=768, bias=True)\r(key): Linear(in_features=768, out_features=768, bias=True)\r(value): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(output): ViTSelfOutput(\r(dense): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r)\r(intermediate): ViTIntermediate(\r(dense): Linear(in_features=768, out_features=3072, bias=True)\r(intermediate_act_fn): GELUActivation()\r)\r(output): ViTOutput(\r(dense): Linear(in_features=3072, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r)\r)\r)\r(layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r)\rvit.embeddings = ViTEmbeddings(\r(patch_embeddings): ViTPatchEmbeddings(\r(projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r)\r(dropout): Dropout(p=0.0, inplace=False)\r)\rvit.embeddings.patch_embeddings = ViTPatchEmbeddings(\r(projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r)\r。。。\rclassifier = Linear(in_features=768, out_features=2, bias=True) google/vit-base-patch16-224 是一个在大规模图像数据集上进行监督预训练的转换器编码器模型（类似于BERT），即ImageNet-21k，分辨率为224x224像素。接下来，该模型在ImageNet上进行微调（也称为ILSVRC2012），这是一个包含100万张图像和1000个类别的数据集，分辨率也为224x224。\n图像被呈现给模型作为固定大小补丁（分辨率为16x16）的序列，这些补丁被线性嵌入。还在序列开始处添加了一个[CLS]标记，以用于分类任务。在将序列馈送到Transformer编码器的层之前，还会添加绝对位置嵌入。\n通过对模型进行预训练，它学习了图像的内部表示，然后可以用于提取对下游任务有用的特征：例如，如果您有一个带标签的图像数据集，您可以在预训练编码器之上放置一个标准分类器的线性层。通常将线性层放置在[CLS]标记的顶部，因为该标记的最后隐藏状态可以被视为整个图像的表示。\nfrom transformers import AutoImageProcessor, ViTForImageClassification\rfrom PIL import Image\rimport requests,torch\rurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\rimage = Image.open(requests.get(url, stream=True).raw)\rprocessor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')\rmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\rinputs = processor(images=image, return_tensors=\"pt\")\rprint(inputs)\rprint(inputs[\"pixel_values\"].shape)\routputs = model(**inputs)\rwith torch.no_grad():\rlogits = model(**inputs).logits\rpredicted_label = logits.argmax(-1).item()\rprint(model.config.id2label[predicted_label]) 输出：\n{'pixel_values': tensor([[[[ 0.1137, 0.1686, 0.1843, ..., -0.1922, -0.1843, -0.1843],\r[ 0.1373, 0.1686, 0.1843, ..., -0.1922, -0.1922, -0.2078],\r[ 0.1137, 0.1529, 0.1608, ..., -0.2314, -0.2235, -0.2157],\r...,\r[ 0.8353, 0.7882, 0.7333, ..., 0.7020, 0.6471, 0.6157],\r[ 0.8275, 0.7961, 0.7725, ..., 0.5843, 0.4667, 0.3961],\r[ 0.8196, 0.7569, 0.7569, ..., 0.0745, -0.0510, -0.1922]],\r[[-0.8039, -0.8118, -0.8118, ..., -0.8902, -0.8902, -0.8980],\r[-0.7882, -0.7882, -0.7882, ..., -0.8745, -0.8745, -0.8824],\r[-0.8118, -0.8039, -0.7882, ..., -0.8902, -0.8902, -0.8902],\r...,\r[-0.2706, -0.3176, -0.3647, ..., -0.4275, -0.4588, -0.4824],\r[-0.2706, -0.2941, -0.3412, ..., -0.4824, -0.5451, -0.5765],\r[-0.2784, -0.3412, -0.3490, ..., -0.7333, -0.7804, -0.8353]],\r[[-0.5451, -0.4667, -0.4824, ..., -0.7412, -0.6941, -0.7176],\r[-0.5529, -0.5137, -0.4902, ..., -0.7412, -0.7098, -0.7412],\r[-0.5216, -0.4824, -0.4667, ..., -0.7490, -0.7490, -0.7647],\r...,\r[ 0.5686, 0.5529, 0.4510, ..., 0.4431, 0.3882, 0.3255],\r[ 0.5451, 0.4902, 0.5137, ..., 0.3020, 0.2078, 0.1294],\r[ 0.5686, 0.5608, 0.5137, ..., -0.2000, -0.4275, -0.5294]]]])}\rtorch.Size([1, 3, 224, 224])\r{0: 'tench, Tinca tinca', 1: 'goldfish, Carassius auratus', 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias', 3: 'tiger shark, Galeocerdo cuvieri', 4: 'hammerhead, hammerhead shark', 5: 'electric ray, crampfish, numbfish, torpedo', 6: 'stingray', 7: 'cock', 8: 'hen', 9: 'ostrich, Struthio camelus', 10: 'brambling, Fringilla montifringilla', 11: 'goldfinch, Carduelis carduelis', 12: 'house finch, linnet, Carpodacus mexicanus', 13: 'junco, snowbird', 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea', 15: 'robin, American robin, Turdus migratorius', 16: 'bulbul', 17: 'jay', 18: 'magpie', 19: 'chickadee', 20: 'water ouzel, dipper', 21: 'kite', 22: 'bald eagle, American eagle, Haliaeetus leucocephalus', 23: 'vulture', 24: 'great grey owl, great gray owl, Strix nebulosa', 25: 'European fire salamander, Salamandra salamandra', 26: 'common newt, Triturus vulgaris', 27: 'eft', 28: 'spotted salamander, Ambystoma maculatum', 29: 'axolotl, mud puppy, Ambystoma mexicanum', 30: 'bullfrog, Rana catesbeiana', 31: 'tree frog, tree-frog', 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui', 33: 'loggerhead, loggerhead turtle, Caretta caretta', 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea', 35: 'mud turtle', 36: 'terrapin', 37: 'box turtle, box tortoise', 38: 'banded gecko', 39: 'common iguana, iguana, Iguana iguana', 40: 'American chameleon, anole, Anolis carolinensis', 41: 'whiptail, whiptail lizard', 42: 'agama', 43: 'frilled lizard, Chlamydosaurus kingi', 44: 'alligator lizard', 45: 'Gila monster, Heloderma suspectum', 46: 'green lizard, Lacerta viridis', 47: 'African chameleon, Chamaeleo chamaeleon', 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis', 49: 'African crocodile, Nile crocodile, Crocodylus niloticus', 50: 'American alligator, Alligator mississipiensis', 51: 'triceratops', 52: 'thunder snake, worm snake, Carphophis amoenus', 53: 'ringneck snake, ring-necked snake, ring snake', 54: 'hognose snake, puff adder, sand viper', 55: 'green snake, grass snake', 56: 'king snake, kingsnake', 57: 'garter snake, grass snake', 58: 'water snake', 59: 'vine snake', 60: 'night snake, Hypsiglena torquata', 61: 'boa constrictor, Constrictor constrictor', 62: 'rock python, rock snake, Python sebae', 63: 'Indian cobra, Naja naja', 64: 'green mamba', 65: 'sea snake', 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus', 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus', 68: 'sidewinder, horned rattlesnake, Crotalus cerastes', 69: 'trilobite', 70: 'harvestman, daddy longlegs, Phalangium opilio', 71: 'scorpion', 72: 'black and gold garden spider, Argiope aurantia', 73: 'barn spider, Araneus cavaticus', 74: 'garden spider, Aranea diademata', 75: 'black widow, Latrodectus mactans', 76: 'tarantula', 77: 'wolf spider, hunting spider', 78: 'tick', 79: 'centipede', 80: 'black grouse', 81: 'ptarmigan', 82: 'ruffed grouse, partridge, Bonasa umbellus', 83: 'prairie chicken, prairie grouse, prairie fowl', 84: 'peacock', 85: 'quail', 86: 'partridge', 87: 'African grey, African gray, Psittacus erithacus', 88: 'macaw', 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita', 90: 'lorikeet', 91: 'coucal', 92: 'bee eater', 93: 'hornbill', 94: 'hummingbird', 95: 'jacamar', 96: 'toucan', 97: 'drake', 98: 'red-breasted merganser, Mergus serrator', 99: 'goose', 100: 'black swan, Cygnus atratus', 101: 'tusker', 102: 'echidna, spiny anteater, anteater', 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus', 104: 'wallaby, brush kangaroo', 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus', 106: 'wombat', 107: 'jellyfish', 108: 'sea anemone, anemone', 109: 'brain coral', 110: 'flatworm, platyhelminth', 111: 'nematode, nematode worm, roundworm', 112: 'conch', 113: 'snail', 114: 'slug', 115: 'sea slug, nudibranch', 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore', 117: 'chambered nautilus, pearly nautilus, nautilus', 118: 'Dungeness crab, Cancer magister', 119: 'rock crab, Cancer irroratus', 120: 'fiddler crab', 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica', 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus', 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish', 124: 'crayfish, crawfish, crawdad, crawdaddy', 125: 'hermit crab', 126: 'isopod', 127: 'white stork, Ciconia ciconia', 128: 'black stork, Ciconia nigra', 129: 'spoonbill', 130: 'flamingo', 131: 'little blue heron, Egretta caerulea', 132: 'American egret, great white heron, Egretta albus', 133: 'bittern', 134: 'crane', 135: 'limpkin, Aramus pictus', 136: 'European gallinule, Porphyrio porphyrio', 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana', 138: 'bustard', 139: 'ruddy turnstone, Arenaria interpres', 140: 'red-backed sandpiper, dunlin, Erolia alpina', 141: 'redshank, Tringa totanus', 142: 'dowitcher', 143: 'oystercatcher, oyster catcher', 144: 'pelican', 145: 'king penguin, Aptenodytes patagonica', 146: 'albatross, mollymawk', 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus', 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca', 149: 'dugong, Dugong dugon', 150: 'sea lion', 151: 'Chihuahua', 152: 'Japanese spaniel', 153: 'Maltese dog, Maltese terrier, Maltese', 154: 'Pekinese, Pekingese, Peke', 155: 'Shih-Tzu', 156: 'Blenheim spaniel', 157: 'papillon', 158: 'toy terrier', 159: 'Rhodesian ridgeback', 160: 'Afghan hound, Afghan', 161: 'basset, basset hound', 162: 'beagle', 163: 'bloodhound, sleuthhound', 164: 'bluetick', 165: 'black-and-tan coonhound', 166: 'Walker hound, Walker foxhound', 167: 'English foxhound', 168: 'redbone', 169: 'borzoi, Russian wolfhound', 170: 'Irish wolfhound', 171: 'Italian greyhound', 172: 'whippet', 173: 'Ibizan hound, Ibizan Podenco', 174: 'Norwegian elkhound, elkhound', 175: 'otterhound, otter hound', 176: 'Saluki, gazelle hound', 177: 'Scottish deerhound, deerhound', 178: 'Weimaraner', 179: 'Staffordshire bullterrier, Staffordshire bull terrier', 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier', 181: 'Bedlington terrier', 182: 'Border terrier', 183: 'Kerry blue terrier', 184: 'Irish terrier', 185: 'Norfolk terrier', 186: 'Norwich terrier', 187: 'Yorkshire terrier', 188: 'wire-haired fox terrier', 189: 'Lakeland terrier', 190: 'Sealyham terrier, Sealyham', 191: 'Airedale, Airedale terrier', 192: 'cairn, cairn terrier', 193: 'Australian terrier', 194: 'Dandie Dinmont, Dandie Dinmont terrier', 195: 'Boston bull, Boston terrier', 196: 'miniature schnauzer', 197: 'giant schnauzer', 198: 'standard schnauzer', 199: 'Scotch terrier, Scottish terrier, Scottie', 200: 'Tibetan terrier, chrysanthemum dog', 201: 'silky terrier, Sydney silky', 202: 'soft-coated wheaten terrier', 203: 'West Highland white terrier', 204: 'Lhasa, Lhasa apso', 205: 'flat-coated retriever', 206: 'curly-coated retriever', 207: 'golden retriever', 208: 'Labrador retriever', 209: 'Chesapeake Bay retriever', 210: 'German short-haired pointer', 211: 'vizsla, Hungarian pointer', 212: 'English setter', 213: 'Irish setter, red setter', 214: 'Gordon setter', 215: 'Brittany spaniel', 216: 'clumber, clumber spaniel', 217: 'English springer, English springer spaniel', 218: 'Welsh springer spaniel', 219: 'cocker spaniel, English cocker spaniel, cocker', 220: 'Sussex spaniel', 221: 'Irish water spaniel', 222: 'kuvasz', 223: 'schipperke', 224: 'groenendael', 225: 'malinois', 226: 'briard', 227: 'kelpie', 228: 'komondor', 229: 'Old English sheepdog, bobtail', 230: 'Shetland sheepdog, Shetland sheep dog, Shetland', 231: 'collie', 232: 'Border collie', 233: 'Bouvier des Flandres, Bouviers des Flandres', 234: 'Rottweiler', 235: 'German shepherd, German shepherd dog, German police dog, alsatian', 236: 'Doberman, Doberman pinscher', 237: 'miniature pinscher', 238: 'Greater Swiss Mountain dog', 239: 'Bernese mountain dog', 240: 'Appenzeller', 241: 'EntleBucher', 242: 'boxer', 243: 'bull mastiff', 244: 'Tibetan mastiff', 245: 'French bulldog', 246: 'Great Dane', 247: 'Saint Bernard, St Bernard', 248: 'Eskimo dog, husky', 249: 'malamute, malemute, Alaskan malamute', 250: 'Siberian husky', 251: 'dalmatian, coach dog, carriage dog', 252: 'affenpinscher, monkey pinscher, monkey dog', 253: 'basenji', 254: 'pug, pug-dog', 255: 'Leonberg', 256: 'Newfoundland, Newfoundland dog', 257: 'Great Pyrenees', 258: 'Samoyed, Samoyede', 259: 'Pomeranian', 260: 'chow, chow chow', 261: 'keeshond', 262: 'Brabancon griffon', 263: 'Pembroke, Pembroke Welsh corgi', 264: 'Cardigan, Cardigan Welsh corgi', 265: 'toy poodle', 266: 'miniature poodle', 267: 'standard poodle', 268: 'Mexican hairless', 269: 'timber wolf, grey wolf, gray wolf, Canis lupus', 270: 'white wolf, Arctic wolf, Canis lupus tundrarum', 271: 'red wolf, maned wolf, Canis rufus, Canis niger', 272: 'coyote, prairie wolf, brush wolf, Canis latrans', 273: 'dingo, warrigal, warragal, Canis dingo', 274: 'dhole, Cuon alpinus', 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus', 276: 'hyena, hyaena', 277: 'red fox, Vulpes vulpes', 278: 'kit fox, Vulpes macrotis', 279: 'Arctic fox, white fox, Alopex lagopus', 280: 'grey fox, gray fox, Urocyon cinereoargenteus', 281: 'tabby, tabby cat', 282: 'tiger cat', 283: 'Persian cat', 284: 'Siamese cat, Siamese', 285: 'Egyptian cat', 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 287: 'lynx, catamount', 288: 'leopard, Panthera pardus', 289: 'snow leopard, ounce, Panthera uncia', 290: 'jaguar, panther, Panthera onca, Felis onca', 291: 'lion, king of beasts, Panthera leo', 292: 'tiger, Panthera tigris', 293: 'cheetah, chetah, Acinonyx jubatus', 294: 'brown bear, bruin, Ursus arctos', 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus', 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus', 297: 'sloth bear, Melursus ursinus, Ursus ursinus', 298: 'mongoose', 299: 'meerkat, mierkat', 300: 'tiger beetle', 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle', 302: 'ground beetle, carabid beetle', 303: 'long-horned beetle, longicorn, longicorn beetle', 304: 'leaf beetle, chrysomelid', 305: 'dung beetle', 306: 'rhinoceros beetle', 307: 'weevil', 308: 'fly', 309: 'bee', 310: 'ant, emmet, pismire', 311: 'grasshopper, hopper', 312: 'cricket', 313: 'walking stick, walkingstick, stick insect', 314: 'cockroach, roach', 315: 'mantis, mantid', 316: 'cicada, cicala', 317: 'leafhopper', 318: 'lacewing, lacewing fly', 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\", 320: 'damselfly', 321: 'admiral', 322: 'ringlet, ringlet butterfly', 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus', 324: 'cabbage butterfly', 325: 'sulphur butterfly, sulfur butterfly', 326: 'lycaenid, lycaenid butterfly', 327: 'starfish, sea star', 328: 'sea urchin', 329: 'sea cucumber, holothurian', 330: 'wood rabbit, cottontail, cottontail rabbit', 331: 'hare', 332: 'Angora, Angora rabbit', 333: 'hamster', 334: 'porcupine, hedgehog', 335: 'fox squirrel, eastern fox squirrel, Sciurus niger', 336: 'marmot', 337: 'beaver', 338: 'guinea pig, Cavia cobaya', 339: 'sorrel', 340: 'zebra', 341: 'hog, pig, grunter, squealer, Sus scrofa', 342: 'wild boar, boar, Sus scrofa', 343: 'warthog', 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius', 345: 'ox', 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis', 347: 'bison', 348: 'ram, tup', 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis', 350: 'ibex, Capra ibex', 351: 'hartebeest', 352: 'impala, Aepyceros melampus', 353: 'gazelle', 354: 'Arabian camel, dromedary, Camelus dromedarius', 355: 'llama', 356: 'weasel', 357: 'mink', 358: 'polecat, fitch, foulmart, foumart, Mustela putorius', 359: 'black-footed ferret, ferret, Mustela nigripes', 360: 'otter', 361: 'skunk, polecat, wood pussy', 362: 'badger', 363: 'armadillo', 364: 'three-toed sloth, ai, Bradypus tridactylus', 365: 'orangutan, orang, orangutang, Pongo pygmaeus', 366: 'gorilla, Gorilla gorilla', 367: 'chimpanzee, chimp, Pan troglodytes', 368: 'gibbon, Hylobates lar', 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus', 370: 'guenon, guenon monkey', 371: 'patas, hussar monkey, Erythrocebus patas', 372: 'baboon', 373: 'macaque', 374: 'langur', 375: 'colobus, colobus monkey', 376: 'proboscis monkey, Nasalis larvatus', 377: 'marmoset', 378: 'capuchin, ringtail, Cebus capucinus', 379: 'howler monkey, howler', 380: 'titi, titi monkey', 381: 'spider monkey, Ateles geoffroyi', 382: 'squirrel monkey, Saimiri sciureus', 383: 'Madagascar cat, ring-tailed lemur, Lemur catta', 384: 'indri, indris, Indri indri, Indri brevicaudatus', 385: 'Indian elephant, Elephas maximus', 386: 'African elephant, Loxodonta africana', 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens', 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca', 389: 'barracouta, snoek', 390: 'eel', 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch', 392: 'rock beauty, Holocanthus tricolor', 393: 'anemone fish', 394: 'sturgeon', 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus', 396: 'lionfish', 397: 'puffer, pufferfish, blowfish, globefish', 398: 'abacus', 399: 'abaya', 400: \"academic gown, academic robe, judge's robe\", 401: 'accordion, piano accordion, squeeze box', 402: 'acoustic guitar', 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier', 404: 'airliner', 405: 'airship, dirigible', 406: 'altar', 407: 'ambulance', 408: 'amphibian, amphibious vehicle', 409: 'analog clock', 410: 'apiary, bee house', 411: 'apron', 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin', 413: 'assault rifle, assault gun', 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack', 415: 'bakery, bakeshop, bakehouse', 416: 'balance beam, beam', 417: 'balloon', 418: 'ballpoint, ballpoint pen, ballpen, Biro', 419: 'Band Aid', 420: 'banjo', 421: 'bannister, banister, balustrade, balusters, handrail', 422: 'barbell', 423: 'barber chair', 424: 'barbershop', 425: 'barn', 426: 'barometer', 427: 'barrel, cask', 428: 'barrow, garden cart, lawn cart, wheelbarrow', 429: 'baseball', 430: 'basketball', 431: 'bassinet', 432: 'bassoon', 433: 'bathing cap, swimming cap', 434: 'bath towel', 435: 'bathtub, bathing tub, bath, tub', 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon', 437: 'beacon, lighthouse, beacon light, pharos', 438: 'beaker', 439: 'bearskin, busby, shako', 440: 'beer bottle', 441: 'beer glass', 442: 'bell cote, bell cot', 443: 'bib', 444: 'bicycle-built-for-two, tandem bicycle, tandem', 445: 'bikini, two-piece', 446: 'binder, ring-binder', 447: 'binoculars, field glasses, opera glasses', 448: 'birdhouse', 449: 'boathouse', 450: 'bobsled, bobsleigh, bob', 451: 'bolo tie, bolo, bola tie, bola', 452: 'bonnet, poke bonnet', 453: 'bookcase', 454: 'bookshop, bookstore, bookstall', 455: 'bottlecap', 456: 'bow', 457: 'bow tie, bow-tie, bowtie', 458: 'brass, memorial tablet, plaque', 459: 'brassiere, bra, bandeau', 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty', 461: 'breastplate, aegis, egis', 462: 'broom', 463: 'bucket, pail', 464: 'buckle', 465: 'bulletproof vest', 466: 'bullet train, bullet', 467: 'butcher shop, meat market', 468: 'cab, hack, taxi, taxicab', 469: 'caldron, cauldron', 470: 'candle, taper, wax light', 471: 'cannon', 472: 'canoe', 473: 'can opener, tin opener', 474: 'cardigan', 475: 'car mirror', 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig', 477: \"carpenter's kit, tool kit\", 478: 'carton', 479: 'car wheel', 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM', 481: 'cassette', 482: 'cassette player', 483: 'castle', 484: 'catamaran', 485: 'CD player', 486: 'cello, violoncello', 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone', 488: 'chain', 489: 'chainlink fence', 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour', 491: 'chain saw, chainsaw', 492: 'chest', 493: 'chiffonier, commode', 494: 'chime, bell, gong', 495: 'china cabinet, china closet', 496: 'Christmas stocking', 497: 'church, church building', 498: 'cinema, movie theater, movie theatre, movie house, picture palace', 499: 'cleaver, meat cleaver, chopper', 500: 'cliff dwelling', 501: 'cloak', 502: 'clog, geta, patten, sabot', 503: 'cocktail shaker', 504: 'coffee mug', 505: 'coffeepot', 506: 'coil, spiral, volute, whorl, helix', 507: 'combination lock', 508: 'computer keyboard, keypad', 509: 'confectionery, confectionary, candy store', 510: 'container ship, containership, container vessel', 511: 'convertible', 512: 'corkscrew, bottle screw', 513: 'cornet, horn, trumpet, trump', 514: 'cowboy boot', 515: 'cowboy hat, ten-gallon hat', 516: 'cradle', 517: 'crane', 518: 'crash helmet', 519: 'crate', 520: 'crib, cot', 521: 'Crock Pot', 522: 'croquet ball', 523: 'crutch', 524: 'cuirass', 525: 'dam, dike, dyke', 526: 'desk', 527: 'desktop computer', 528: 'dial telephone, dial phone', 529: 'diaper, nappy, napkin', 530: 'digital clock', 531: 'digital watch', 532: 'dining table, board', 533: 'dishrag, dishcloth', 534: 'dishwasher, dish washer, dishwashing machine', 535: 'disk brake, disc brake', 536: 'dock, dockage, docking facility', 537: 'dogsled, dog sled, dog sleigh', 538: 'dome', 539: 'doormat, welcome mat', 540: 'drilling platform, offshore rig', 541: 'drum, membranophone, tympan', 542: 'drumstick', 543: 'dumbbell', 544: 'Dutch oven', 545: 'electric fan, blower', 546: 'electric guitar', 547: 'electric locomotive', 548: 'entertainment center', 549: 'envelope', 550: 'espresso maker', 551: 'face powder', 552: 'feather boa, boa', 553: 'file, file cabinet, filing cabinet', 554: 'fireboat', 555: 'fire engine, fire truck', 556: 'fire screen, fireguard', 557: 'flagpole, flagstaff', 558: 'flute, transverse flute', 559: 'folding chair', 560: 'football helmet', 561: 'forklift', 562: 'fountain', 563: 'fountain pen', 564: 'four-poster', 565: 'freight car', 566: 'French horn, horn', 567: 'frying pan, frypan, skillet', 568: 'fur coat', 569: 'garbage truck, dustcart', 570: 'gasmask, respirator, gas helmet', 571: 'gas pump, gasoline pump, petrol pump, island dispenser', 572: 'goblet', 573: 'go-kart', 574: 'golf ball', 575: 'golfcart, golf cart', 576: 'gondola', 577: 'gong, tam-tam', 578: 'gown', 579: 'grand piano, grand', 580: 'greenhouse, nursery, glasshouse', 581: 'grille, radiator grille', 582: 'grocery store, grocery, food market, market', 583: 'guillotine', 584: 'hair slide', 585: 'hair spray', 586: 'half track', 587: 'hammer', 588: 'hamper', 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier', 590: 'hand-held computer, hand-held microcomputer', 591: 'handkerchief, hankie, hanky, hankey', 592: 'hard disc, hard disk, fixed disk', 593: 'harmonica, mouth organ, harp, mouth harp', 594: 'harp', 595: 'harvester, reaper', 596: 'hatchet', 597: 'holster', 598: 'home theater, home theatre', 599: 'honeycomb', 600: 'hook, claw', 601: 'hoopskirt, crinoline', 602: 'horizontal bar, high bar', 603: 'horse cart, horse-cart', 604: 'hourglass', 605: 'iPod', 606: 'iron, smoothing iron', 607: \"jack-o'-lantern\", 608: 'jean, blue jean, denim', 609: 'jeep, landrover', 610: 'jersey, T-shirt, tee shirt', 611: 'jigsaw puzzle', 612: 'jinrikisha, ricksha, rickshaw', 613: 'joystick', 614: 'kimono', 615: 'knee pad', 616: 'knot', 617: 'lab coat, laboratory coat', 618: 'ladle', 619: 'lampshade, lamp shade', 620: 'laptop, laptop computer', 621: 'lawn mower, mower', 622: 'lens cap, lens cover', 623: 'letter opener, paper knife, paperknife', 624: 'library', 625: 'lifeboat', 626: 'lighter, light, igniter, ignitor', 627: 'limousine, limo', 628: 'liner, ocean liner', 629: 'lipstick, lip rouge', 630: 'Loafer', 631: 'lotion', 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system', 633: \"loupe, jeweler's loupe\", 634: 'lumbermill, sawmill', 635: 'magnetic compass', 636: 'mailbag, postbag', 637: 'mailbox, letter box', 638: 'maillot', 639: 'maillot, tank suit', 640: 'manhole cover', 641: 'maraca', 642: 'marimba, xylophone', 643: 'mask', 644: 'matchstick', 645: 'maypole', 646: 'maze, labyrinth', 647: 'measuring cup', 648: 'medicine chest, medicine cabinet', 649: 'megalith, megalithic structure', 650: 'microphone, mike', 651: 'microwave, microwave oven', 652: 'military uniform', 653: 'milk can', 654: 'minibus', 655: 'miniskirt, mini', 656: 'minivan', 657: 'missile', 658: 'mitten', 659: 'mixing bowl', 660: 'mobile home, manufactured home', 661: 'Model T', 662: 'modem', 663: 'monastery', 664: 'monitor', 665: 'moped', 666: 'mortar', 667: 'mortarboard', 668: 'mosque', 669: 'mosquito net', 670: 'motor scooter, scooter', 671: 'mountain bike, all-terrain bike, off-roader', 672: 'mountain tent', 673: 'mouse, computer mouse', 674: 'mousetrap', 675: 'moving van', 676: 'muzzle', 677: 'nail', 678: 'neck brace', 679: 'necklace', 680: 'nipple', 681: 'notebook, notebook computer', 682: 'obelisk', 683: 'oboe, hautboy, hautbois', 684: 'ocarina, sweet potato', 685: 'odometer, hodometer, mileometer, milometer', 686: 'oil filter', 687: 'organ, pipe organ', 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO', 689: 'overskirt', 690: 'oxcart', 691: 'oxygen mask', 692: 'packet', 693: 'paddle, boat paddle', 694: 'paddlewheel, paddle wheel', 695: 'padlock', 696: 'paintbrush', 697: \"pajama, pyjama, pj's, jammies\", 698: 'palace', 699: 'panpipe, pandean pipe, syrinx', 700: 'paper towel', 701: 'parachute, chute', 702: 'parallel bars, bars', 703: 'park bench', 704: 'parking meter', 705: 'passenger car, coach, carriage', 706: 'patio, terrace', 707: 'pay-phone, pay-station', 708: 'pedestal, plinth, footstall', 709: 'pencil box, pencil case', 710: 'pencil sharpener', 711: 'perfume, essence', 712: 'Petri dish', 713: 'photocopier', 714: 'pick, plectrum, plectron', 715: 'pickelhaube', 716: 'picket fence, paling', 717: 'pickup, pickup truck', 718: 'pier', 719: 'piggy bank, penny bank', 720: 'pill bottle', 721: 'pillow', 722: 'ping-pong ball', 723: 'pinwheel', 724: 'pirate, pirate ship', 725: 'pitcher, ewer', 726: \"plane, carpenter's plane, woodworking plane\", 727: 'planetarium', 728: 'plastic bag', 729: 'plate rack', 730: 'plow, plough', 731: \"plunger, plumber's helper\", 732: 'Polaroid camera, Polaroid Land camera', 733: 'pole', 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria', 735: 'poncho', 736: 'pool table, billiard table, snooker table', 737: 'pop bottle, soda bottle', 738: 'pot, flowerpot', 739: \"potter's wheel\", 740: 'power drill', 741: 'prayer rug, prayer mat', 742: 'printer', 743: 'prison, prison house', 744: 'projectile, missile', 745: 'projector', 746: 'puck, hockey puck', 747: 'punching bag, punch bag, punching ball, punchball', 748: 'purse', 749: 'quill, quill pen', 750: 'quilt, comforter, comfort, puff', 751: 'racer, race car, racing car', 752: 'racket, racquet', 753: 'radiator', 754: 'radio, wireless', 755: 'radio telescope, radio reflector', 756: 'rain barrel', 757: 'recreational vehicle, RV, R.V.', 758: 'reel', 759: 'reflex camera', 760: 'refrigerator, icebox', 761: 'remote control, remote', 762: 'restaurant, eating house, eating place, eatery', 763: 'revolver, six-gun, six-shooter', 764: 'rifle', 765: 'rocking chair, rocker', 766: 'rotisserie', 767: 'rubber eraser, rubber, pencil eraser', 768: 'rugby ball', 769: 'rule, ruler', 770: 'running shoe', 771: 'safe', 772: 'safety pin', 773: 'saltshaker, salt shaker', 774: 'sandal', 775: 'sarong', 776: 'sax, saxophone', 777: 'scabbard', 778: 'scale, weighing machine', 779: 'school bus', 780: 'schooner', 781: 'scoreboard', 782: 'screen, CRT screen', 783: 'screw', 784: 'screwdriver', 785: 'seat belt, seatbelt', 786: 'sewing machine', 787: 'shield, buckler', 788: 'shoe shop, shoe-shop, shoe store', 789: 'shoji', 790: 'shopping basket', 791: 'shopping cart', 792: 'shovel', 793: 'shower cap', 794: 'shower curtain', 795: 'ski', 796: 'ski mask', 797: 'sleeping bag', 798: 'slide rule, slipstick', 799: 'sliding door', 800: 'slot, one-armed bandit', 801: 'snorkel', 802: 'snowmobile', 803: 'snowplow, snowplough', 804: 'soap dispenser', 805: 'soccer ball', 806: 'sock', 807: 'solar dish, solar collector, solar furnace', 808: 'sombrero', 809: 'soup bowl', 810: 'space bar', 811: 'space heater', 812: 'space shuttle', 813: 'spatula', 814: 'speedboat', 815: \"spider web, spider's web\", 816: 'spindle', 817: 'sports car, sport car', 818: 'spotlight, spot', 819: 'stage', 820: 'steam locomotive', 821: 'steel arch bridge', 822: 'steel drum', 823: 'stethoscope', 824: 'stole', 825: 'stone wall', 826: 'stopwatch, stop watch', 827: 'stove', 828: 'strainer', 829: 'streetcar, tram, tramcar, trolley, trolley car', 830: 'stretcher', 831: 'studio couch, day bed', 832: 'stupa, tope', 833: 'submarine, pigboat, sub, U-boat', 834: 'suit, suit of clothes', 835: 'sundial', 836: 'sunglass', 837: 'sunglasses, dark glasses, shades', 838: 'sunscreen, sunblock, sun blocker', 839: 'suspension bridge', 840: 'swab, swob, mop', 841: 'sweatshirt', 842: 'swimming trunks, bathing trunks', 843: 'swing', 844: 'switch, electric switch, electrical switch', 845: 'syringe', 846: 'table lamp', 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle', 848: 'tape player', 849: 'teapot', 850: 'teddy, teddy bear', 851: 'television, television system', 852: 'tennis ball', 853: 'thatch, thatched roof', 854: 'theater curtain, theatre curtain', 855: 'thimble', 856: 'thresher, thrasher, threshing machine', 857: 'throne', 858: 'tile roof', 859: 'toaster', 860: 'tobacco shop, tobacconist shop, tobacconist', 861: 'toilet seat', 862: 'torch', 863: 'totem pole', 864: 'tow truck, tow car, wrecker', 865: 'toyshop', 866: 'tractor', 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi', 868: 'tray', 869: 'trench coat', 870: 'tricycle, trike, velocipede', 871: 'trimaran', 872: 'tripod', 873: 'triumphal arch', 874: 'trolleybus, trolley coach, trackless trolley', 875: 'trombone', 876: 'tub, vat', 877: 'turnstile', 878: 'typewriter keyboard', 879: 'umbrella', 880: 'unicycle, monocycle', 881: 'upright, upright piano', 882: 'vacuum, vacuum cleaner', 883: 'vase', 884: 'vault', 885: 'velvet', 886: 'vending machine', 887: 'vestment', 888: 'viaduct', 889: 'violin, fiddle', 890: 'volleyball', 891: 'waffle iron', 892: 'wall clock', 893: 'wallet, billfold, notecase, pocketbook', 894: 'wardrobe, closet, press', 895: 'warplane, military plane', 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin', 897: 'washer, automatic washer, washing machine', 898: 'water bottle', 899: 'water jug', 900: 'water tower', 901: 'whiskey jug', 902: 'whistle', 903: 'wig', 904: 'window screen', 905: 'window shade', 906: 'Windsor tie', 907: 'wine bottle', 908: 'wing', 909: 'wok', 910: 'wooden spoon', 911: 'wool, woolen, woollen', 912: 'worm fence, snake fence, snake-rail fence, Virginia fence', 913: 'wreck', 914: 'yawl', 915: 'yurt', 916: 'web site, website, internet site, site', 917: 'comic book', 918: 'crossword puzzle, crossword', 919: 'street sign', 920: 'traffic light, traffic signal, stoplight', 921: 'book jacket, dust cover, dust jacket, dust wrapper', 922: 'menu', 923: 'plate', 924: 'guacamole', 925: 'consomme', 926: 'hot pot, hotpot', 927: 'trifle', 928: 'ice cream, icecream', 929: 'ice lolly, lolly, lollipop, popsicle', 930: 'French loaf', 931: 'bagel, beigel', 932: 'pretzel', 933: 'cheeseburger', 934: 'hotdog, hot dog, red hot', 935: 'mashed potato', 936: 'head cabbage', 937: 'broccoli', 938: 'cauliflower', 939: 'zucchini, courgette', 940: 'spaghetti squash', 941: 'acorn squash', 942: 'butternut squash', 943: 'cucumber, cuke', 944: 'artichoke, globe artichoke', 945: 'bell pepper', 946: 'cardoon', 947: 'mushroom', 948: 'Granny Smith', 949: 'strawberry', 950: 'orange', 951: 'lemon', 952: 'fig', 953: 'pineapple, ananas', 954: 'banana', 955: 'jackfruit, jak, jack', 956: 'custard apple', 957: 'pomegranate', 958: 'hay', 959: 'carbonara', 960: 'chocolate sauce, chocolate syrup', 961: 'dough', 962: 'meat loaf, meatloaf', 963: 'pizza, pizza pie', 964: 'potpie', 965: 'burrito', 966: 'red wine', 967: 'espresso', 968: 'cup', 969: 'eggnog', 970: 'alp', 971: 'bubble', 972: 'cliff, drop, drop-off', 973: 'coral reef', 974: 'geyser', 975: 'lakeside, lakeshore', 976: 'promontory, headland, head, foreland', 977: 'sandbar, sand bar', 978: 'seashore, coast, seacoast, sea-coast', 979: 'valley, vale', 980: 'volcano', 981: 'ballplayer, baseball player', 982: 'groom, bridegroom', 983: 'scuba diver', 984: 'rapeseed', 985: 'daisy', 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\", 987: 'corn', 988: 'acorn', 989: 'hip, rose hip, rosehip', 990: 'buckeye, horse chestnut, conker', 991: 'coral fungus', 992: 'agaric', 993: 'gyromitra', 994: 'stinkhorn, carrion fungus', 995: 'earthstar', 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa', 997: 'bolete', 998: 'ear, spike, capitulum', 999: 'toilet tissue, toilet paper, bathroom tissue'}\rEgyptian cat 经过转码后得json结构是一个key=pixel_values的像素数组，维度是：[批次，通道数，宽度，高度]。 通过model.config.id2label可以看到总共1000个label。\n数据集 food101包含多种食物类别，数据集地址：https://huggingface.co/datasets/ethz/food101。\nfrom datasets import load_dataset\rds = load_dataset(\"food101\")\rprint(\"数据集\",ds)\r#获取训练集数据\rds = load_dataset(\"food101\",split=\"train\")\rprint(\"训练集\",ds)\rprint(\"第一个数据集\",ds[0])\r#获取所有label\rlabels = ds.features[\"label\"].names\rprint(labels)\rprint(len(labels)) 输出\n数据集 DatasetDict({\rtrain: Dataset({\rfeatures: ['image', 'label'],\rnum_rows: 75750\r})\rvalidation: Dataset({\rfeatures: ['image', 'label'],\rnum_rows: 25250\r})\r})\r训练集 Dataset({\rfeatures: ['image', 'label'],\rnum_rows: 75750\r})\r第一个数据集 {'image': \u003cPIL.Image.Image image mode=RGB size=384x512 at 0x7A1FCE415750\u003e, 'label': 6}\r['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\r101 总共101个food总类。 显示第二张图片和label\nimport matplotlib.pyplot as plt\rplt.imshow(ds[1][\"image\"])\rplt.axis('off') # 关闭坐标轴\rplt.show()\rprint(labels[ds[1][\"label\"]]) 显示 这里我们看到第二个数据集的label=6,也就是beignets。\n我们需要生成label和id关系的字典。\nlabels = ds[\"train\"].features[\"label\"].names\rlabel2id, id2label = dict(), dict()\rfor i, label in enumerate(labels):\rlabel2id[label] = i\rid2label[i] = label\rprint(id2label)\rprint(label2id) 输出：\n{0: 'apple_pie', 1: 'baby_back_ribs', 2: 'baklava', 3: 'beef_carpaccio', 4: 'beef_tartare', 5: 'beet_salad', 6: 'beignets', 7: 'bibimbap', 8: 'bread_pudding', 9: 'breakfast_burrito', 10: 'bruschetta', 11: 'caesar_salad', 12: 'cannoli', 13: 'caprese_salad', 14: 'carrot_cake', 15: 'ceviche', 16: 'cheesecake', 17: 'cheese_plate', 18: 'chicken_curry', 19: 'chicken_quesadilla', 20: 'chicken_wings', 21: 'chocolate_cake', 22: 'chocolate_mousse', 23: 'churros', 24: 'clam_chowder', 25: 'club_sandwich', 26: 'crab_cakes', 27: 'creme_brulee', 28: 'croque_madame', 29: 'cup_cakes', 30: 'deviled_eggs', 31: 'donuts', 32: 'dumplings', 33: 'edamame', 34: 'eggs_benedict', 35: 'escargots', 36: 'falafel', 37: 'filet_mignon', 38: 'fish_and_chips', 39: 'foie_gras', 40: 'french_fries', 41: 'french_onion_soup', 42: 'french_toast', 43: 'fried_calamari', 44: 'fried_rice', 45: 'frozen_yogurt', 46: 'garlic_bread', 47: 'gnocchi', 48: 'greek_salad', 49: 'grilled_cheese_sandwich', 50: 'grilled_salmon', 51: 'guacamole', 52: 'gyoza', 53: 'hamburger', 54: 'hot_and_sour_soup', 55: 'hot_dog', 56: 'huevos_rancheros', 57: 'hummus', 58: 'ice_cream', 59: 'lasagna', 60: 'lobster_bisque', 61: 'lobster_roll_sandwich', 62: 'macaroni_and_cheese', 63: 'macarons', 64: 'miso_soup', 65: 'mussels', 66: 'nachos', 67: 'omelette', 68: 'onion_rings', 69: 'oysters', 70: 'pad_thai', 71: 'paella', 72: 'pancakes', 73: 'panna_cotta', 74: 'peking_duck', 75: 'pho', 76: 'pizza', 77: 'pork_chop', 78: 'poutine', 79: 'prime_rib', 80: 'pulled_pork_sandwich', 81: 'ramen', 82: 'ravioli', 83: 'red_velvet_cake', 84: 'risotto', 85: 'samosa', 86: 'sashimi', 87: 'scallops', 88: 'seaweed_salad', 89: 'shrimp_and_grits', 90: 'spaghetti_bolognese', 91: 'spaghetti_carbonara', 92: 'spring_rolls', 93: 'steak', 94: 'strawberry_shortcake', 95: 'sushi', 96: 'tacos', 97: 'takoyaki', 98: 'tiramisu', 99: 'tuna_tartare', 100: 'waffles'}\r{'apple_pie': 0, 'baby_back_ribs': 1, 'baklava': 2, 'beef_carpaccio': 3, 'beef_tartare': 4, 'beet_salad': 5, 'beignets': 6, 'bibimbap': 7, 'bread_pudding': 8, 'breakfast_burrito': 9, 'bruschetta': 10, 'caesar_salad': 11, 'cannoli': 12, 'caprese_salad': 13, 'carrot_cake': 14, 'ceviche': 15, 'cheesecake': 16, 'cheese_plate': 17, 'chicken_curry': 18, 'chicken_quesadilla': 19, 'chicken_wings': 20, 'chocolate_cake': 21, 'chocolate_mousse': 22, 'churros': 23, 'clam_chowder': 24, 'club_sandwich': 25, 'crab_cakes': 26, 'creme_brulee': 27, 'croque_madame': 28, 'cup_cakes': 29, 'deviled_eggs': 30, 'donuts': 31, 'dumplings': 32, 'edamame': 33, 'eggs_benedict': 34, 'escargots': 35, 'falafel': 36, 'filet_mignon': 37, 'fish_and_chips': 38, 'foie_gras': 39, 'french_fries': 40, 'french_onion_soup': 41, 'french_toast': 42, 'fried_calamari': 43, 'fried_rice': 44, 'frozen_yogurt': 45, 'garlic_bread': 46, 'gnocchi': 47, 'greek_salad': 48, 'grilled_cheese_sandwich': 49, 'grilled_salmon': 50, 'guacamole': 51, 'gyoza': 52, 'hamburger': 53, 'hot_and_sour_soup': 54, 'hot_dog': 55, 'huevos_rancheros': 56, 'hummus': 57, 'ice_cream': 58, 'lasagna': 59, 'lobster_bisque': 60, 'lobster_roll_sandwich': 61, 'macaroni_and_cheese': 62, 'macarons': 63, 'miso_soup': 64, 'mussels': 65, 'nachos': 66, 'omelette': 67, 'onion_rings': 68, 'oysters': 69, 'pad_thai': 70, 'paella': 71, 'pancakes': 72, 'panna_cotta': 73, 'peking_duck': 74, 'pho': 75, 'pizza': 76, 'pork_chop': 77, 'poutine': 78, 'prime_rib': 79, 'pulled_pork_sandwich': 80, 'ramen': 81, 'ravioli': 82, 'red_velvet_cake': 83, 'risotto': 84, 'samosa': 85, 'sashimi': 86, 'scallops': 87, 'seaweed_salad': 88, 'shrimp_and_grits': 89, 'spaghetti_bolognese': 90, 'spaghetti_carbonara': 91, 'spring_rolls': 92, 'steak': 93, 'strawberry_shortcake': 94, 'sushi': 95, 'tacos': 96, 'takoyaki': 97, 'tiramisu': 98, 'tuna_tartare': 99, 'waffles': 100} 加载一个图像处理器，以正确调整大小并对训练和评估图像的像素值进行归一化。\nfrom transformers import AutoImageProcessor\rimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\") 您还可以使用图像处理器来准备一些转换函数，用于数据增强和像素缩放。\nfrom torchvision.transforms import (\rCenterCrop,\rCompose,\rNormalize,\rRandomHorizontalFlip,\rRandomResizedCrop,\rResize,\rToTensor,\r)\rnormalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\rtrain_transforms = Compose(\r[\rRandomResizedCrop(image_processor.size[\"height\"]),\rRandomHorizontalFlip(),\rToTensor(),\rnormalize,\r]\r)\rval_transforms = Compose(\r[\rResize(image_processor.size[\"height\"]),\rCenterCrop(image_processor.size[\"height\"]),\rToTensor(),\rnormalize,\r]\r)\rdef preprocess_train(example_batch):\rexample_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\rreturn example_batch\rdef preprocess_val(example_batch):\rexample_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\rreturn example_batch 定义训练和验证数据集，并使用set_transform函数在运行时应用转换。\ntrain_ds = ds[\"train\"]\rval_ds = ds[\"validation\"]\rtrain_ds.set_transform(preprocess_train)\rval_ds.set_transform(preprocess_val) 最后，您需要一个数据整理器来创建训练和评估数据的批次，并将标签转换为torch.tensor对象。\nimport torch\rdef collate_fn(examples):\rpixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\rlabels = torch.tensor([example[\"label\"] for example in examples])\rreturn {\"pixel_values\": pixel_values, \"labels\": labels} 模型 现在让我们加载一个预训练模型作为基础模型。本指南使用了google/vit-base-patch16-224-in21k模型，但您可以使用任何您想要的图像分类模型。将label2id和id2label字典传递给模型，以便它知道如何将整数标签映射到它们的类标签，并且如果您正在微调已经微调过的检查点，可以选择传递ignore_mismatched_sizes=True参数。\nfrom transformers import AutoModelForImageClassification, TrainingArguments, Trainer\rmodel = AutoModelForImageClassification.from_pretrained(\r\"google/vit-base-patch16-224-in21k\",\rlabel2id=label2id,\rid2label=id2label,\rignore_mismatched_sizes=True,\r) PEFT configuration and model 每个 PEFT 方法都需要一个配置，其中包含了指定 PEFT 方法应该如何应用的所有参数。一旦配置设置好了，就将其传递给 get_peft_model() 函数，同时还要传递基础模型，以创建一个可训练的 PeftModel。\n调用 print_trainable_parameters() 方法来比较 PeftModel 的参数数量与基础模型的参数数量！\nLoRA将权重更新矩阵分解为两个较小的矩阵。这些低秩矩阵的大小由其秩或r确定。更高的秩意味着模型有更多的参数需要训练，但也意味着模型有更大的学习能力。您还需要指定 target_modules，确定较小矩阵插入的位置。对于本指南，您将针对注意力块的查询和值矩阵进行目标指定。设置的其他重要参数包括 lora_alpha（缩放因子）、bias（是否应该训练none、all或只有 LoRA 偏置参数）、modules_to_save（除了 LoRA 层之外需要训练和保存的模块）。所有这些参数 - 以及更多 - 都可以在 LoraConfig 中找到。\nfrom peft import LoraConfig, get_peft_model\rconfig = LoraConfig(\rr=16,\rlora_alpha=16,\rtarget_modules=[\"query\", \"value\"],\rlora_dropout=0.1,\rbias=\"none\",\rmodules_to_save=[\"classifier\"],\r)\rmodel = get_peft_model(model, config)\rmodel.print_trainable_parameters() 输出：“trainable params: 667,493 || all params: 86,543,818 || trainable%: 0.7712775047664294”\n在LoRA中，为了简化和精简，可能只针对查询和值矩阵进行权重分解，而不对键矩阵进行处理。这样可以在一定程度上减少计算量和参数数量，同时仍然提高模型的学习能力和灵活性。\n参数说明：\ntask_type：指定任务类型。如：条件生成任务（SEQ_2_SEQ_LM），因果语言建模（CAUSAL_LM）等。 inference_mode：是否在推理模式下使用Peft模型。 r： LoRA低秩矩阵的维数。关于秩的选择，通常，使用4，8，16即可。 lora_alpha： LoRA低秩矩阵的缩放系数，为一个常数超参，调整alpha与调整学习率类似。 lora_dropout：LoRA 层的丢弃（dropout）率，取值范围为[0, 1)。 target_modules：要替换为 LoRA 的模块名称列表或模块名称的正则表达式。针对不同类型的模型，模块名称不一样，因此，我们需要根据具体的模型进行设置，比如，LLaMa的默认模块名为[q_proj, v_proj]，我们也可以自行指定为：[q_proj,k_proj,v_proj,o_proj]。 在 PEFT 中支持的模型默认的模块名如下所示： TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\r\"t5\": [\"q\", \"v\"],\r\"mt5\": [\"q\", \"v\"],\r\"bart\": [\"q_proj\", \"v_proj\"],\r\"gpt2\": [\"c_attn\"],\r\"bloom\": [\"query_key_value\"],\r\"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"],\r\"opt\": [\"q_proj\", \"v_proj\"],\r\"gptj\": [\"q_proj\", \"v_proj\"],\r\"gpt_neox\": [\"query_key_value\"],\r\"gpt_neo\": [\"q_proj\", \"v_proj\"],\r\"bert\": [\"query\", \"value\"],\r\"roberta\": [\"query\", \"value\"],\r\"xlm-roberta\": [\"query\", \"value\"],\r\"electra\": [\"query\", \"value\"],\r\"deberta-v2\": [\"query_proj\", \"value_proj\"],\r\"deberta\": [\"in_proj\"],\r\"layoutlm\": [\"query\", \"value\"],\r\"llama\": [\"q_proj\", \"v_proj\"],\r\"chatglm\": [\"query_key_value\"],\r\"gpt_bigcode\": [\"c_attn\"],\r\"mpt\": [\"Wqkv\"],\r} 训练 对于训练，让我们使用Transformers中的Trainer类。Trainer类包含一个PyTorch训练循环，当您准备好时，调用train开始训练。要自定义训练运行，请在TrainingArguments类中配置训练超参数。对于类似LoRA的方法，您可以承受更高的批量大小和学习率。\nbatch_size = 128\rargs = TrainingArguments(\r#peft_model_id,\routput_dir=\"/kaggle/working\",\rremove_unused_columns=False,\revaluation_strategy=\"epoch\",\rsave_strategy=\"epoch\",\rlearning_rate=5e-3,\rreport_to=\"none\",\rper_device_train_batch_size=batch_size,\rgradient_accumulation_steps=4,\rper_device_eval_batch_size=batch_size,\rfp16=True,\rnum_train_epochs=5,\rlogging_steps=10,\rload_best_model_at_end=True,\rlabel_names=[\"labels\"],\r) 这里是对TrainingArguments中参数的解释：\noutput_dir: 指定训练过程中输出模型和日志的目录。 remove_unused_columns: 控制是否在训练过程中删除未使用的列。 evaluation_strategy: 指定评估策略，这里设置为“epoch”表示在每个epoch结束时进行评估。 save_strategy: 指定模型保存策略，这里设置为“epoch”表示在每个epoch结束时保存模型。 learning_rate: 学习率设置为5e-3，即0.005。 report_to: 控制训练过程中的报告输出，这里设置为“none”表示不输出任何报告。 per_device_train_batch_size: 每个设备上的训练批量大小。 gradient_accumulation_steps: 梯度累积步数。 per_device_eval_batch_size: 每个设备上的评估批量大小。 fp16: 控制是否使用混合精度训练。 num_train_epochs: 训练的总epoch数。 logging_steps: 控制日志输出的步数。 load_best_model_at_end: 在训练结束时是否加载最佳模型。 label_names: 标签的名称列表。 这些参数是用来配置训练过程的，例如指定训练和评估的批量大小、学习率、训练时长等等。 开始训练\ntrainer = Trainer(\rmodel,\rargs,\rtrain_dataset=train_ds,\reval_dataset=val_ds,\rtokenizer=image_processor,\rdata_collator=collate_fn,\r)\rtrainer.train() 使用kaggle的免费gpu T4*2(双倍时间消耗累计)，gpu基本100%，gpu是一周30hhrs免费时间的,我为了节省时间，用2epoch，batch_size=128,因为kaggle的session有效期在12hours内，越快越好，否则session断开就白训练了，简单看下效果，大概1个小时左右，也可以save session让他在后台跑。 看下速度 第一次epoch完成 查看输出 点击后面的三个点下载所有的文件，然后将模型下载下来，点击输入的上传-new model 输入model名称，选择私有，点击create model 输入平台：transformer，点击addnewvariation 定义附件名称，选择协议，点击右下侧的create 关闭后点击return to notebook，就可以看到输入的模型了，点击右侧的复制路径即可。 这里input的是持久的不会丢失，output数据再页面关闭session关闭后就丢失，所以尽快保存下来，或者上传到huggingface。\n预测 切换到p100(按分钟算，省钱)验证 代码\nmodel_name=\"/kaggle/input/image-classifity/transformers/version1/1/checkpoint-74\" #复制输入的路径\rfrom peft import PeftConfig, PeftModel\rfrom transformers import AutoImageProcessor, AutoModelForImageClassification\rfrom PIL import Image\rimport requests,torch\rfrom datasets import load_dataset\rds = load_dataset(\"food101\")\rlabels = ds[\"train\"].features[\"label\"].names\rlabel2id, id2label = dict(), dict()\rfor i, label in enumerate(labels):\rlabel2id[label] = i\rid2label[i] = label\rconfig = PeftConfig.from_pretrained(model_name)\rmodel = AutoModelForImageClassification.from_pretrained(\rconfig.base_model_name_or_path,#google/vit-base-patch16-224-in21k\rlabel2id=label2id,\rid2label=id2label,\rignore_mismatched_sizes=True,\r)\rmodel = PeftModel.from_pretrained(model, model_name)\rurl = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg\"\rimage = Image.open(requests.get(url, stream=True).raw)\rprint(image)\rimage_processor = AutoImageProcessor.from_pretrained(config.base_model_name_or_path)\rencoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")\rwith torch.no_grad():\routputs = model(**encoding)\rlogits = outputs.logits\rpredicted_class_idx = logits.argmax(-1).item()\rprint(\"Predicted class:\", model.config.id2label[predicted_class_idx]) 输出：beignets",
    "description": "简介 PEFT PEFT（Parameter-Efficient Fine-Tuning）是一个用于高效地将大型预训练模型适配到各种下游应用的库，而无需对模型的所有参数进行微调，因为这在计算上是非常昂贵的。PEFT 方法只微调少量的（额外的）模型参数——显著降低了计算和存储成本——同时其性能与完全微调的模型相当。这使得在消费者硬件上训练和存储大型语言模型（LLMs）变得更加可行。\nPEFT 集成了 Transformers、Diffusers 和 Accelerate 库，以提供更快、更简单的方法来加载、训练和使用大型模型进行推理。\nLORA方法 一种高效训练大型模型的流行方法是在注意力块中插入较小的可训练矩阵，这些矩阵是微调期间要学习的增量权重矩阵的低秩分解。预训练模型的原始权重矩阵被冻结，仅更新较小的矩阵。这减少了可训练参数的数量，降低了内存使用和训练时间，而这些在大型模型中可能非常昂贵。\n有几种不同的方法可以将权重矩阵表示为低秩分解，但最常见的方法是低秩适应（LoRA原理）。PEFT 库支持几种其他 LoRA 变体，例如低秩Hadamard积（LoHa）、低秩Kronecker积（LoKr）和自适应低秩适应（AdaLoRA）。你可以在适配器指南中了解这些方法的概念工作原理。如果你有兴趣将这些方法应用于其他任务和用例，比如语义分割、标记分类，可以看看我们的笔记本集合！\nVision Transformer (ViT) Vision Transformer（ViT）模型是由Alexey Dosovitskiy，Lucas Beyer，Alexander Kolesnikov，Dirk Weissenborn，Xiaohua Zhai，Thomas Unterthiner，Mostafa Dehghani，Matthias Minderer，Georg Heigold，Sylvain Gelly，Jakob Uszkoreit，Neil Houlsby在《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》中提出的。这是第一篇成功在ImageNet上训练Transformer编码器并获得非常好结果的论文。\n这篇论文的摘要是：\n虽然Transformer架构已经成为自然语言处理任务的事实标准，但它在计算机视觉中的应用仍然有限。在视觉领域，注意力要么与卷积网络一起应用，要么用来替换卷积网络的某些组件，同时保持其总体结构不变。我们展示了在这种对CNN的依赖不是必要的，纯Transformer直接应用于图像块序列可以在图像分类任务上表现得非常好。当在大量数据上进行预训练并转移到多个中等规模或小型图像识别基准数据集（ImageNet，CIFAR-100，VTAB等）时，Vision Transformer（ViT）与最先进的卷积网络相比取得了出色的结果，同时训练所需的计算资源大大减少。\n具体关于该模型得结构参考：https://huggingface.co/docs/transformers/model_doc/vit\nlora方法实战 图像分类微调 本指南将向你展示如何快速训练一个图像分类模型——使用低秩分解方法——来识别图像中显示的食物类别。 案例来自官网：https://huggingface.co/docs/peft/task_guides/lora_based_methods\n模型选择 google/vit-base-patch16-224-in21k 是一个基于Transformer编码器的模型（类似于BERT），在监督方式下，即ImageNet-21k上以224x224像素的分辨率预训练了大量图像。\n图像被呈现给模型作为固定大小的补丁序列（分辨率为16x16），这些补丁被线性嵌入。在序列的开头还添加了一个[CLS]标记，用于分类任务。在将序列馈送到Transformer编码器的层之前，还会添加绝对位置嵌入。\n需要注意的是，这个模型不提供任何经过微调的头部，因为这些头部被Google研究人员清零了。但是，模型包括预训练的汇聚层，可以用于下游任务（如图像分类）。\n通过预训练模型，它学习了图像的内部表示，然后可以用于提取对下游任务有用的特征：例如，如果您有一个带标签的图像数据集，可以通过在预训练编码器顶部放置一个线性层来训练标准分类器。通常将线性层放置在[CLS]标记的顶部，因为该标记的最后隐藏状态可以被视为整个图像的表示。\nfrom transformers import ViTImageProcessor, FlaxViTModel\rfrom PIL import Image\rimport requests\rurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\rimage = Image.open(requests.get(url, stream=True).raw)\rprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\rmodel = FlaxViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\rinputs = processor(images=image, return_tensors=\"np\")\routputs = model(**inputs)\rlast_hidden_states = outputs.last_hidden_state\rprint(last_hidden_states.shape) 不包含分类信息，不包含label信息",
    "tags": [],
    "title": "Transformers实战03-PEFT库使用LORA方法微调VIT图像分类。",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_03/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "简介 GPT-2（Generative Pre-trained Transformer 2）是由OpenAI开发的一种基于Transformer架构的自然语言处理模型。以下是关于GPT-2的一些关键特点和信息：\nTransformer架构：GPT-2基于Transformer模型架构，这是一种使用自注意力机制来捕捉输入序列中词语之间依赖关系的深度学习模型。\n预训练：GPT-2是一个预训练的语言模型，意味着它在大规模的文本数据上进行了预训练，以学习文本数据的统计特性和语言模式。\n无监督学习：在预训练过程中，GPT-2采用了无监督学习的方式，即模型仅仅通过文本数据本身来学习，而没有使用人工标注的标签或监督信号。\n生成式任务：GPT-2被设计用于生成式任务，如文本生成、对话生成和摘要生成等。它可以根据给定的上下文生成连贯的文本，并且在语言理解和生成方面表现出色。\n多层次架构：GPT-2具有多层的Transformer编码器，其中包含数百万个参数，使得模型能够捕获复杂的语言结构和语义关系。\n大小变种：GPT-2有多个大小的变种，从117M到1.5B个参数不等，每个变种都具有不同的性能和资源要求。更大的模型往往在生成更加准确和流畅的文本方面表现更好，但同时也需要更多的计算资源。\n开放许可：GPT-2是在OpenAI的研究下开发的，其模型和相关资源以开放许可的形式发布，使得研究人员和开发者可以自由地使用和构建基于GPT-2的应用。\n总的来说，GPT-2是一种强大的语言模型，具有广泛的应用潜力，可用于自然语言生成、理解、翻译等各种NLP任务。\n案例 该案例来源huggingface学习中心nlp-course，Training a causal language model from scratch 文章\n描述 我们将构建一个缩减版的代码生成模型：我们将专注于一行补全，而不是完整的函数或类，使用Python代码的一个子集。在Python中处理数据时，您会频繁接触到Python数据科学栈，包括matplotlib、seaborn、pandas和scikit-learn库。在使用这些框架时，经常需要查找特定的命令，因此如果我们可以使用一个模型来为我们完成这些调用，那将是很好的。\n收集数据 我们使用huggingface收集得content包含：“pandas”, “sklearn”, “matplotlib”, “seaborn” 这些关键字python代码 这个数据集是从github公共仓库爬取，比如 from datasets import load_dataset, DatasetDict\rds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\rds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\rraw_datasets = DatasetDict(\r{\r\"train\": ds_train, # .shuffle().select(range(50000)),\r\"valid\": ds_valid, # .shuffle().select(range(500))\r}\r) 让我们看一个数据集中的例子。我们只需显示每个字段的前200个字符：\nfor key in raw_datasets[\"train\"][0]:\rprint(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\") 输出\n'REPO_NAME: kmike/scikit-learn'\r'PATH: sklearn/utils/__init__.py'\r'COPIES: 3'\r'SIZE: 10094'\r'''CONTENT: \"\"\"\rThe :mod:`sklearn.utils` module includes various utilites.\r\"\"\"\rfrom collections import Sequence\rimport numpy as np\rfrom scipy.sparse import issparse\rimport warnings\rfrom .murmurhash import murm\rLICENSE: bsd-3-clause''' 数据集处理 首先要对数据进行标记化，这样我们才能用它进行训练。由于我们的目标主要是自动补全短函数调用，所以我们可以保持上下文大小相对较小。这样做的好处是我们可以更快地训练模型，并且需要的内存量明显较少。如果你的应用程序需要更多的上下文（例如，如果你希望模型能够基于包含函数定义的文件编写单元测试），请确保增加该数字，但也要记住这会增加GPU的内存占用。目前，让我们将上下文大小固定为128个标记，而不是 GPT-2 或 GPT-3 中分别使用的 1,024 或 2,048。\n回顾预处理 input_ids和attention_mask： input_ids是tokenizer处理后得到的输入特征，它将文本转换为模型能够处理的数字序列。每个单词或者标记（token）都会被映射成对应的唯一整数。这些整数序列就是模型的实际输入。 示例：假设原始文本经过tokenizer处理后，生成的input_ids可能是一个整数序列，如[101, 2023, 2003, 1037, 2814, 2242, 102]，每个整数对应一个token。\nattention_mask用于告诉模型哪些部分是真实的输入，哪些部分是填充（padding）的，以便模型在计算时能够正确处理。 对于输入中的真实token，对应位置的attention_mask值为1；对于填充的位置，attention_mask值为0。 示例：如果input_ids是[101, 2023, 2003, 1037, 2814, 2242, 102]，那么对应的attention_mask可能是[1, 1, 1, 1, 1, 1, 1]，表示所有位置都是真实的输入，如果某个句子词元比他小，可能就需要填充。\n#这里演示分词器\rfrom transformers import AutoModel, BertTokenizer\rmodel_name=\"bert-base-chinese\" #bert-base-uncased\rmodel=AutoModel.from_pretrained(model_name)\rtokenizer=BertTokenizer.from_pretrained(model_name)\rprint(type(model),type(tokenizer))\rsequence = [\"我出生在湖南岳阳,我的家在深圳.\",\"我得儿子是小谦谦\"]\r#输出中包含两个键 input_ids 和 attention_mask，其中 input_ids 对应分词之后的 tokens 映射到的数字编号列表，而 attention_mask 则是用来标记哪些 tokens #是被填充的（这里“1”表示是原文，“0”表示是填充字符）。\rprint(tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\",pair=True))\r# 获取填充token的id\rpad_token_id = tokenizer.pad_token_id\r# 获取填充token的字符串表示\rpad_token = tokenizer.convert_ids_to_tokens(pad_token_id)\rprint(f\"实际填充是id,padid={pad_token_id},padtoken={pad_token}\")\r#获取词汇表大小\rvocab = tokenizer.get_vocab()\rvocab_size = len(vocab)\rprint(\"词汇表大小:\", vocab_size,len(tokenizer))\r# 打印词汇表内容（可选）\rprint(\"词汇表内容:\", vocab)\r#将输入切分为词语、子词或者符号（例如标点符号），统称为 tokens；\rprint(tokenizer.tokenize(sequence[0]),len(tokenizer.tokenize(sequence[0])))\r#我们通过 convert_tokens_to_ids() 将切分出的 tokens 转换为对应的 token IDs：\rprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence[0])))\r#可以通过 encode() 函数将这两个步骤合并，并且 encode() 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加[CLS] 和 [SEP]\rprint(tokenizer.encode(sequence[0]))\r#解码还原文字，可以看到encode前后加了[CLS] 和 [SEP]\rprint(tokenizer.decode(tokenizer.encode(sequence[1]))) 输出\n\u003cclass 'transformers.models.bert.modeling_bert.BertModel'\u003e \u003cclass 'transformers.models.bert.tokenization_bert.BertTokenizer'\u003e\r{'input_ids': tensor([[ 101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638,\r2157, 1762, 3918, 1766, 119, 102],\r[ 101, 2769, 2533, 1036, 2094, 3221, 2207, 6472, 6472, 102, 0, 0,\r0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\r实际填充是id,padid=0,padtoken=[PAD]\r词汇表大小: 21128 21128\r词汇表内容: {'[PAD]': 0, '[unused1]': 1, '[unused2]': 2, '[unused3]': 3, '[unused4]': 4, '[unused5]': 5, '[unused6]': 6, '[unused7]': 7, '[unused8]': 8, '[unused9]': 9, '[unused10]': 10, '[unused11]': 11,。。。。。。。。。。。\r['我', '出', '生', '在', '湖', '南', '岳', '阳', ',', '我', '的', '家', '在', '深', '圳', '.'] 16\r[2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638, 2157, 1762, 3918, 1766, 119]\r[101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638, 2157, 1762, 3918, 1766, 119, 102]\r[CLS] 我 得 儿 子 是 小 谦 谦 [SEP] special token Tokenizer 的特殊标记（special tokens）是在处理文本数据时经常用到的一些特殊符号或者字符串，它们在自然语言处理中起着重要的作用。这些特殊标记通常包括以下几类：\nPadding token ([PAD]) pad_token：\n在进行批量处理时，序列长度不一致是很常见的情况。为了保证输入数据的统一性，我们通常会使用 [PAD] 标记来填充较短的序列，使其与其他序列的长度相同。\nStart of sequence token ([CLS]) bos_token：\n在许多自然语言处理任务（如文本分类）中，需要在输入序列的开头添加一个特殊标记，例如 [CLS]，用于模型理解这是一个序列的起始点，gpt2的开始token是：\u003c|endoftext|\u003e。\nEnd of sequence token ([SEP]) eos_token：\n类似地，[SEP] 标记通常用于表示序列的结束，特别是在处理多个句子或文本对时，可以用 [SEP] 分隔它们。\nMask token ([MASK]) mask_token：\n在预训练语言模型中，为了进行语言模型的掩码语言建模（Masked Language Modeling），我们需要将一些单词或子词随机地用 [MASK] 标记替换掉，让模型预测被掩码的部分。\nunk_token 是 tokenizer 中的一个特殊标记，通常用来表示未登录词（Unknown Token）。在自然语言处理中，未登录词指的是在训练数据中没有出现过的词汇或者子词。当模型在处理输入文本时遇到未登录词，它会用 unk_token 来替代这些词，以便继续进行处理或预测。\nsep_token 是 tokenizer 中的另一个特殊标记，通常用来表示序列的分隔符。在自然语言处理（NLP）任务中，sep_token 主要用于以下几个方面： 某些预训练语言模型（如 BERT）要求输入数据按照特定格式组织，包括使用 sep_token 来分隔输入的各个部分。例如，在文本对分类任务中，可以用 [SEP] 标记分隔两个句子： [CLS] Sentence A [SEP] Sentence B [SEP]\ncls_token 是 tokenizer 中的另一个特殊标记，通常用来表示序列的开头或者分类任务中的特殊标记。\n这些特殊标记在不同的任务和模型中具有不同的用途，但它们的共同作用是帮助模型更好地处理文本数据，处理输入序列的长度变化，以及在特定任务中引导模型学习和预测。通过适当使用特殊标记，可以有效地增强模型对语言数据的理解和处理能力。\n#特殊token\rfrom transformers import GPT2Tokenizer,AutoTokenizer\r# 初始化 GPT-2 分词器\rtokenizer = GPT2Tokenizer.from_pretrained('gpt2')\rtokenizer1 = AutoTokenizer.from_pretrained('bert-base-chinese')\r# 打印所有特殊标记\rprint(\"gpt2特殊标记:\")\rfor token_name, token_value in tokenizer.special_tokens_map.items():\rprint(f\"{token_name}: {token_value}\")\rprint(\"bert-base-chinese特殊标记:\")\rfor token_name, token_value in tokenizer1.special_tokens_map.items():\rprint(f\"{token_name}: {token_value}\") 输出\ngpt2特殊标记:\rbos_token: \u003c|endoftext|\u003e\reos_token: \u003c|endoftext|\u003e\runk_token: \u003c|endoftext|\u003e\r--------------------------\rbert-base-chinese特殊标记:\runk_token: [UNK]\rsep_token: [SEP]\rpad_token: [PAD]\rcls_token: [CLS]\rmask_token: [MASK] chunk 当你有多个句子或文本段落需要处理时，你可以将它们划分成固定长度的小块（chunks），以便输入到模型中进行处理。这个过程通常用于处理较长的文本，以确保模型可以有效地处理输入数据，特别是在使用Transformer等模型时，其输入长度通常是有限制的。\nchunk的逻辑是，输入数据的每一行句子，超过max_length 都会被截断，当前句子被拆分成的chuck的个数为：len(句子)%max_length +1，当前有些模型会添加一些开始和分割字符 比如[CLS][SEQ]等也要算入长度。\n注意tokenizer拆分小块的开启由 truncation=True,决定，如果是False max_length等就无效了。\n#truck的问题。\rcontent = [\"This is the first sentence. This is the second sentence.\",\"i am a stupid man\"]\rfrom transformers import AutoTokenizer\r# 选择一个预训练模型和对应的tokenizer\rmodel_name = \"bert-base-uncased\"\rtokenizer = AutoTokenizer.from_pretrained(model_name)\r# 最大的字符长度，因为字符的最前面会加一个[CLS],最后会补一个[SEP]，每一个句子都会被拆分一次，也就是一个truck行只能10个字符，content【0】因为超过10个字符，所以被切割成2个truck。\r# 输出的trucklength是[10,6]，第二个句子不满10个只有7个，最后length=[10, 6, 7]\rmax_length = 10\r# 进行tokenization，并返回结果\routputs = tokenizer(\rcontent,\rtruncation=True,\rmax_length=max_length,\rreturn_overflowing_tokens=True,\rreturn_length=True,\r)\r# 输出结果\rprint(outputs)\rprint(tokenizer.decode(outputs['input_ids'][0]))\rprint(tokenizer.decode(outputs['input_ids'][1])) 输出\n{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 2023, 2003, 102], [101, 1996, 2117, 6251, 1012, 102], [101, 1045, 2572, 1037, 5236, 2158, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]], 'length': [10, 6, 7], 'overflow_to_sample_mapping': [0, 0, 1]}\r[CLS] this is the first sentence. this is [SEP]\r[CLS] the second sentence. [SEP] 注意overflow_to_sample_mapping中是标识每个小chuck属于之前哪个句子索引，第1-2个chuck是属于第0个索引也就是第一个句子，3个第二个句子。\n如果加了 padding=True,所有的子句都会自动补上padding_id，最终length都会是10，结果就变成\n{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 2023, 2003, 102], [101, 1996, 2117, 6251, 1012, 102, 0, 0, 0, 0], [101, 1045, 2572, 1037, 5236, 2158, 102, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], 'length': [10, 10, 10], 'overflow_to_sample_mapping': [0, 0, 1]}\r[CLS] this is the first sentence. this is [SEP]\r[CLS] the second sentence. [SEP] [PAD] [PAD] [PAD] [PAD] 其他更详细的预处理参考：https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb\ndatacollator DataCollatorForLanguageModeling 的主要功能是为掩码语言模型（Masked Language Modeling，MLM）任务准备数据。它的主要作用是随机地掩盖输入中的一些标记，并生成相应的标签，以便模型在训练时能够预测这些被掩盖的标记。 DataCollatorWithPadding：对输入进行填充，使得输入张量具有相同的长度。 更多相关类的实现，请参考官方api\n以下是一个例子\nimport torch\rfrom transformers import BertTokenizer, DataCollatorForLanguageModeling\r# 初始化BERT分词器\rtokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r# 定义示例文本\rtexts = [\"Hello, how are you?\", \"I am fine, thank you.\"]\r# 对文本进行编码\rinputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\r# 打印编码后的输入\rprint(\"Encoded inputs:\", inputs)\r# 将输入转换为列表，以适应DataCollatorForLanguageModeling的输入格式,他的格式要求有多少个句子就多少行[{'input_ids':,'attention_mask':},{'input_ids':,'attention_mask':}]\r# tokenizer encode的格式是字典 {'input_ids': [[],[]]是在二维数组体现，所以强制转一下\rbatch = [{key: val[i] for key, val in inputs.items()} for i in range(len(texts))]\rprint(\"collator需要格式\",batch)\r# 初始化数据整理器，指定进行掩码语言模型任务\rdata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\r# 对输入数据进行整理\rcollated_inputs = data_collator(batch)\r# 打印整理后的输入,这里因为mlm=True是自动掩盖，有15%的数据被掩盖，被掩盖的数据在input_ids被替换成103，然后在生成的labels上，没有被掩盖的数据都变成-100，被掩盖的数据替换为之前的数据\r# labels是最后的标签，通过训练反向就能很好的优化模型，这就是masked模型数据处理\rprint(\"Collated inputs:\", collated_inputs)\rdata_collator1 = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\rcollated_inputs = data_collator1(batch)\r#mlm=False，不会产生遮盖，所有的输入生成的是输出相同的labels，如果是padding字符，labels是-100\rprint(\"Collated inputs:\", collated_inputs) 输出\nEncoded inputs: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0],\r[ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\r[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\rcollator需要格式 [{'input_ids': tensor([ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0])}, {'input_ids': tensor([ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012, 102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])}]\rCollated inputs: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0],\r[ 101, 1045, 2572, 2986, 103, 4067, 103, 1012, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\r[1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100],\r[-100, -100, -100, -100, 1010, -100, 2017, -100, -100]])}\rCollated inputs: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0],\r[ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\r[1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, -100],\r[ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012, 102]])} map 在使用 transformers 库时，datasets 中的 map 方法是一个非常有用的工具，用于对数据集进行预处理、特征提取、数据增强等操作。下面是一个示例，展示如何使用 map 方法对数据集进行预处理，以便于将其用于训练一个文本分类模型。 详细处理参考：https://huggingface.co/docs/datasets/use_dataset map 函数是 datasets 库中一个非常强大的工具，它允许你对数据集的每个样本或批次进行操作和变换。以下是 map 函数的几个关键参数及其解释：\nfunction 这是一个用户定义的函数，它将应用于数据集的每个样本或批次。函数可以接受一个样本或一组样本作为输入，并返回一个或多个新的字段。\ndef preprocess_function(examples): # 你的预处理逻辑 return examples\nbatched 类型：bool 默认值：False 解释：如果设置为 True，function 将会批量应用到数据集中。这意味着 function 将接收一个包含多个样本的字典作为输入。 dataset.map(preprocess_function, batched=True)\nbatch_size 类型：int 默认值：1000 解释：指定批量处理时的批次大小。仅当 batched=True 时有效。 dataset.map(preprocess_function, batched=True, batch_size=32)\nremove_columns 类型：list or str 默认值：None 解释：指定要从数据集中移除的列。这对于清理不需要的字段非常有用。 dataset.map(preprocess_function, remove_columns=[\"column_name\"])\n# 导入必要的库\rfrom datasets import Dataset\r# 创建一个简单的数据集\rdata = {\r'text': [\r\"This is the first sentence.\",\r\"Here's the second sentence.\",\r\"And this is the third one.\"\r],\r'label': [1, 0, 1]\r}\r# 转换为 Dataset 对象\rdataset = Dataset.from_dict(data)\r# 打印原始数据集\rprint(\"原始数据集：\")\rprint(dataset)\r# 导入必要的库\rfrom transformers import AutoTokenizer\r# 加载预训练的分词器\rtokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\r# 定义预处理函数\rdef preprocess_function(examples):\rprint(\"传入数据集\",examples)\r# 使用分词器对文本进行编码\rencoded_tokenizer = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=8)\rprint(\"分词数据集\",encoded_tokenizer)\r#返回的字典数据会被累加到原始数据集上。\rreturn encoded_tokenizer\r# 使用 map 方法应用预处理函数\rencoded_dataset = dataset.map(preprocess_function, batched=True,batch_size=2)\r# 打印预处理后的数据集\rprint(\"\\n预处理后的数据集结构：\",encoded_dataset)\rprint(\"\\n预处理后的数据集：\",encoded_dataset[0:3])\r# 使用 map 方法应用预处理函数,remove_columns表示删除某些列是个数组。\rencoded_dataset = dataset.map(preprocess_function, batched=True,batch_size=2,remove_columns=dataset.features)\r# 打印预处理后的数据集\rprint(\"\\n预处理后的数据集：\",encoded_dataset[0:3]) 输出：\n原始数据集：\rDataset({\rfeatures: ['text', 'label'],\rnum_rows: 3\r})\rMap: 100%\r3/3 [00:00\u003c00:00, 138.43 examples/s]\r传入数据集 {'text': ['This is the first sentence.', \"Here's the second sentence.\", 'And this is the third one.'], 'label': [1, 0, 1]}\r分词数据集 {'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\r预处理后的数据集结构： Dataset({\rfeatures: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\rnum_rows: 3\r})\r预处理后的数据集： {'text': ['This is the first sentence.', \"Here's the second sentence.\", 'And this is the third one.'], 'label': [1, 0, 1], 'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\r预处理后的数据集： {'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]} 预处理 大多数文档的标记数远超过 128 个，因此简单地将输入截断到最大长度会消除我们数据集的很大一部分。相反，我们将使用 return_overflowing_tokens 选项来对整个输入进行标记，并将其拆分为几个块。我们还将使用 return_length 选项自动返回每个创建块的长度。通常，最后一个块会小于上下文大小，我们将去掉这些部分以避免填充问题；实际上我们不需要它们，因为我们有很多数据。 让我们通过查看前两个例子来看看这到底是如何工作的：\nfrom transformers import AutoTokenizer\rcontext_length = 128\r#这个分词器专门为代码搜索和理解任务设计。它主要用于处理编程语言（如 Python、JavaScript、Java 等）的源代码。\rtokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\routputs = tokenizer(\r#获取0，1这两个数据集的脚本内容\rraw_datasets[\"train\"][:2][\"content\"],\rtruncation=True,\rmax_length=context_length,\rreturn_overflowing_tokens=True,\rreturn_length=True,\r)\rprint(f\"Input IDs length: {len(outputs['input_ids'])}\")\rprint(f\"Input chunk lengths: {(outputs['length'])}\")\rprint(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\") huggingface-course/code-search-net-tokenizer\n设计目标：这个分词器专门为代码搜索和理解任务设计。它主要用于处理编程语言（如 \u003ePython、JavaScript、Java 等）的源代码。 训练数据：该分词器使用 CodeSearchNet 数据集进行训练，数据集中包含了大量的代码示例\u003e和注释。 应用领域：适用于代码搜索、代码补全、代码生成和其他与代码相关的任务。 词汇表：词汇表中包含了大量的编程语言特定的标记（如关键字、操作符、变量名等），以及\u003e常见的编程语言语法和结构。 注意：分词器模型的作用是将单词转换为一个个的数字，训练时使用的数字计算数字之间的上下文关系，最后推算对应的数字后，反向通过词典解析成文字，所以如果需要训练中文，你只需要有一个中文分词模型即可，训练只和数字相关。 输出：\nInput IDs length: 34\rInput chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\rChunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 我们可以看到从这两个例子中总共得到了 34 个片段。查看片段长度，我们可以看到两个文档末尾的片段都少于 128 个标记（分别为 117 和 41）。这些仅占我们拥有的总片段的一小部分，因此我们可以安全地丢弃它们。使用 overflow_to_sample_mapping 字段，我们还可以重建哪些片段属于哪些输入样本。\n通过这个操作，我们利用了 🤗 Datasets 中 Dataset.map() 函数的一个便利功能，即它不需要一一对应的映射，我们可以创建比输入批次多或少的元素批次。当进行数据增强或数据过滤等会改变元素数量的操作时，这非常有用。在我们的例子中，当将每个元素标记为指定上下文大小的块时，我们从每个文档中创建了许多样本。我们只需要确保删除现有列，因为它们的大小不一致。如果我们想保留它们，可以适当重复并在 Dataset.map() 调用中返回它们：\ndef tokenize(element):\routputs = tokenizer(\relement[\"content\"],\rtruncation=True,\rmax_length=context_length,\rreturn_overflowing_tokens=True,\rreturn_length=True,\r)\rinput_batch = []\r#获取当前input_ids和长度，末尾chuck不等于context_length，就不需要加入了\rfor length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\rif length == context_length:\rinput_batch.append(input_ids)\rreturn {\"input_ids\": input_batch}\rtokenized_datasets = raw_datasets.map(\rtokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\r)\rtokenized_datasets 输出：\nDatasetDict({\rtrain: Dataset({\rfeatures: ['input_ids'],\rnum_rows: 16702061\r})\rvalid: Dataset({\rfeatures: ['input_ids'],\rnum_rows: 93164\r})\r}) 我们现在有 1670 万个例子，每个例子有 128 个标记，总共对应大约 21 亿个标记。供参考，OpenAI 的 GPT-3 和 Codex 模型分别在 300 和 1000 亿个标记上训练，其中 Codex 模型是从 GPT-3 检查点初始化的。我们在这一部分的目标不是与这些模型竞争，这些模型可以生成长而连贯的文本，而是创建一个缩减版本，为数据科学家提供快速自动补全功能。 现在我们已经准备好数据集，接下来让我们设置模型！\n初始化模型 回顾模型 参数计算 在 PyTorch 中，t.numel() 是一个张量方法，用于返回张量中所有元素的数量。它等价于计算张量的大小（shape）的所有维度的乘积。例如，一个形状为 (3, 4, 5) 的张量有 3 * 4 * 5 = 60 个元素。\n在你提供的代码中：\nmodel_size = sum(t.numel() for t in model.parameters())\n这里 model.parameters() 返回模型中所有参数的一个生成器。通过 t.numel() 计算每个参数张量中的元素数量，然后使用 sum() 函数将所有这些数量加起来，得到整个模型中所有参数的总元素数量，即模型的总大小。 示例 假设有一个简单的神经网络模型：\nimport torch\rimport torch.nn as nn\rclass SimpleModel(nn.Module):\rdef __init__(self):\rsuper(SimpleModel, self).__init__()\rself.fc1 = nn.Linear(10, 20)\rself.fc2 = nn.Linear(20, 30)\rdef forward(self, x):\rx = self.fc1(x)\rx = self.fc2(x)\rreturn x\rmodel = SimpleModel() 计算模型大小的代码如下：\nmodel_size = sum(t.numel() for t in model.parameters())\rprint(model_size) 在这个例子中，model.parameters() 会返回 fc1 和 fc2 的参数张量。\nfc1 的权重张量形状为 (20, 10)，有 20 * 10 = 200 个元素。 fc1 的偏置张量形状为 (20,)，有 20 个元素。 fc2 的权重张量形状为 (30, 20)，有 30 * 20 = 600 个元素。 fc2 的偏置张量形状为 (30,)，有 30 个元素。 总计模型中有 200 + 20 + 600 + 30 = 850 个参数元素。因此，model_size 的值将是 850。\n初始化 我们的第一步是初始化一个GPT-2模型。我们将为我们的模型使用与小型GPT-2模型相同的配置，因此我们加载预训练的配置，确保标记器大小与模型词汇大小匹配，并传递bos和eos（序列开始和结束）令牌ID：\nfrom transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\rconfig = AutoConfig.from_pretrained(\r\"gpt2\",\rvocab_size=len(tokenizer), #获取词汇表大小\rn_ctx=context_length,\rbos_token_id=tokenizer.bos_token_id,\reos_token_id=tokenizer.eos_token_id,\r) 因为使用了不同的分词器，所以重新加载配置\n通过该配置，我们可以加载一个新模型。请注意，这是我们第一次不使用 from_pretrained() 函数，因为我们实际上是在自己初始化一个模型：\nmodel = GPT2LMHeadModel(config)\rmodel_size = sum(t.numel() for t in model.parameters())\rprint(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\") 输出\nGPT-2 size: 124.2M parameters 我们的模型有 124M 个参数需要调优。在开始训练之前，我们需要设置一个数据整理器，来处理创建批次的工作。我们可以使用 DataCollatorForLanguageModeling 整理器，它是专门为语言建模设计的（正如其名称微妙地暗示的那样）。除了堆叠和填充批次外，它还负责创建语言模型标签——在因果语言建模中，输入也作为标签（仅偏移一个元素），这个数据整理器在训练过程中实时创建它们，因此我们不需要重复 input_ids。 请注意，DataCollatorForLanguageModeling 支持掩码语言建模 (MLM) 和因果语言建模 (CLM)。默认情况下，它为 MLM 准备数据，但我们可以通过设置参数 mlm=False 切换到 CLM：\nfrom transformers import DataCollatorForLanguageModeling\rtokenizer.pad_token = tokenizer.eos_token\rdata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False) 让我们来看一个例子：\nout = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\rfor key in out:\rprint(f\"{key} shape: {out[key].shape}\") 输出\ninput_ids shape: torch.Size([5, 128])\rattention_mask shape: torch.Size([5, 128])\rlabels shape: torch.Size([5, 128]) 我们可以看到示例已经被堆叠，所有张量形状相同。\n剩下的就是配置训练参数并启动训练器。我们将使用余弦学习率调度，并进行一些预热，实际批量大小为256（per_device_train_batch_size * gradient_accumulation_steps）。当单个批次无法适应内存时，会使用梯度累积，它通过多次前向/反向传递逐步累积梯度。当我们使用🤗 Accelerate 创建训练循环时，我们将看到这一点的实际应用。\nfrom transformers import Trainer, TrainingArguments\rargs = TrainingArguments(\routput_dir=\"codeparrot-ds\",\rper_device_train_batch_size=32,\rper_device_eval_batch_size=32,\revaluation_strategy=\"steps\",\reval_steps=5_000,\rlogging_steps=5_000,\rgradient_accumulation_steps=8,\rnum_train_epochs=1,\rweight_decay=0.1,\rwarmup_steps=1_000,\rlr_scheduler_type=\"cosine\",\rlearning_rate=5e-4,\rsave_steps=5_000,\rfp16=True,\rpush_to_hub=True,\r)\rtrainer = Trainer(\rmodel=model,\rtokenizer=tokenizer,\rargs=args,\rdata_collator=data_collator,\rtrain_dataset=tokenized_datasets[\"train\"],\reval_dataset=tokenized_datasets[\"valid\"],\r) 现在我们可以启动训练器并等待训练完成。根据您是在完整的训练集上运行还是在子集上运行，这将分别需要 20 小时或 2 小时，所以准备几杯咖啡和一本好书来阅读吧！\ntrainer.train() 完整代码 from datasets import load_dataset, DatasetDict\rds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\rds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\rraw_datasets = DatasetDict(\r{\r\"train\": ds_train, # .shuffle().select(range(50000)),\r\"valid\": ds_valid, # .shuffle().select(range(500))\r}\r)\rfrom transformers import AutoTokenizer\rcontext_length = 128\r#这个分词器模型专门为代码搜索和理解任务设计。它主要用于处理编程语言（如 Python、JavaScript、Java 等）的源代码，分词器的目的是将对应词元转换为数字，让模型通过计算来理解数字和数字之间的关系，选择模型的分词器非常重要。\rtokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\rdef tokenize(element):\routputs = tokenizer(\relement[\"content\"],\rtruncation=True,\rmax_length=context_length,\rreturn_overflowing_tokens=True,\rreturn_length=True,\r)\rinput_batch = []\r#获取当前input_ids和长度，末尾chuck不等于context_length，就不需要加入了\rfor length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\rif length == context_length:\rinput_batch.append(input_ids)\rreturn {\"input_ids\": input_batch}\rtokenized_datasets = raw_datasets.map(\rtokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\r)\rfrom transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\rconfig = AutoConfig.from_pretrained(\r\"gpt2\",\rvocab_size=len(tokenizer), #获取词汇表大小\rn_ctx=context_length,\rbos_token_id=tokenizer.bos_token_id,\reos_token_id=tokenizer.eos_token_id,\r)\rmodel = GPT2LMHeadModel(config)\rfrom transformers import DataCollatorForLanguageModeling\rtokenizer.pad_token = tokenizer.eos_token\rdata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\rfrom transformers import Trainer, TrainingArguments\rargs = TrainingArguments(\routput_dir=\"/kaggle/working\",\rper_device_train_batch_size=32,\rper_device_eval_batch_size=32,\revaluation_strategy=\"steps\",\reval_steps=5_000,\rlogging_steps=5_000,\rgradient_accumulation_steps=8,\rnum_train_epochs=1,\rweight_decay=0.1,\rwarmup_steps=1_000,\rlr_scheduler_type=\"cosine\",\rlearning_rate=5e-4,\rsave_steps=5_000,\rfp16=True,\rreport_to=\"none\",\rpush_to_hub=False,\r)\rtrainer = Trainer(\rmodel=model,\rtokenizer=tokenizer,\rargs=args,\rdata_collator=data_collator,\rtrain_dataset=tokenized_datasets[\"train\"],\reval_dataset=tokenized_datasets[\"valid\"],\r)\rtrainer.train() 测试 由于使用kaggle的gpu无法在12小时训练完成，所以这里只能用官方已经训练好的镜像测试了。\n现在是见证结果的时刻：让我们看看训练好的模型实际表现如何！我们可以在日志中看到损失值一直在稳定下降，但为了真正测试模型的效果，我们来看看它在一些提示信息上的表现。为此，我们将模型封装到一个文本生成管道中，并如果条件允许的话，将其部署到 GPU 上以实现快速生成：\nimport torch\rfrom transformers import pipeline\rdevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\rpipe = pipeline(\r\"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\r) 让我们从创建散点图的简单任务开始：\ntxt = \"\"\"\\\r# create some data\rx = np.random.randn(100)\ry = np.random.randn(100)\r# create scatter plot with x, y\r\"\"\"\rprint(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) 输出：\n# create some data\rx = np.random.randn(100)\ry = np.random.randn(100)\r# create scatter plot with x, y\rplt.scatter(x, y)\r# create scatter 结果看起来是正确的。对于 pandas 的操作是否也适用呢？我们来看看能否从两个数组创建一个 DataFrame：\ntxt = \"\"\"\\\r# create some data\rx = np.random.randn(100)\ry = np.random.randn(100)\r# create dataframe from x and y\r\"\"\"\rprint(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) 输出\n# create some data\rx = np.random.randn(100)\ry = np.random.randn(100)\r# create dataframe from x and y\rdf = pd.DataFrame({'x': x, 'y': y})\rdf.insert(0,'x', x)\rfor 好的，这是正确的答案——尽管随后又插入了列 x。由于生成的令牌数量有限，下面的 for 循环被截断了。我们来看看能否做一些更复杂的事情，并让模型帮助我们使用 groupby 操作：\ntxt = \"\"\"\\\r# dataframe with profession, income and name\rdf = pd.DataFrame({'profession': x, 'income':y, 'name': z})\r# calculate the mean income per profession\r\"\"\"\rprint(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) 输出\n# dataframe with profession, income and name\rdf = pd.DataFrame({'profession': x, 'income':y, 'name': z})\r# calculate the mean income per profession\rprofession = df.groupby(['profession']).mean()\r# compute the 还不错；这样做是对的。最后，让我们看看是否也能用它来为 scikit-learn 设置一个随机森林模型：\ntxt = \"\"\"\r# import random forest regressor from scikit-learn\rfrom sklearn.ensemble import RandomForestRegressor\r# fit random forest model with 300 estimators on X, y:\r\"\"\"\rprint(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) 输出\n# import random forest regressor from scikit-learn\rfrom sklearn.ensemble import RandomForestRegressor\r# fit random forest model with 300 estimators on X, y:\rrf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)\rrf.fit(X, y)\rrf 查看这几个例子，模型似乎学到了一些 Python 数据科学套件的语法。",
    "description": "简介 GPT-2（Generative Pre-trained Transformer 2）是由OpenAI开发的一种基于Transformer架构的自然语言处理模型。以下是关于GPT-2的一些关键特点和信息：\nTransformer架构：GPT-2基于Transformer模型架构，这是一种使用自注意力机制来捕捉输入序列中词语之间依赖关系的深度学习模型。\n预训练：GPT-2是一个预训练的语言模型，意味着它在大规模的文本数据上进行了预训练，以学习文本数据的统计特性和语言模式。\n无监督学习：在预训练过程中，GPT-2采用了无监督学习的方式，即模型仅仅通过文本数据本身来学习，而没有使用人工标注的标签或监督信号。\n生成式任务：GPT-2被设计用于生成式任务，如文本生成、对话生成和摘要生成等。它可以根据给定的上下文生成连贯的文本，并且在语言理解和生成方面表现出色。\n多层次架构：GPT-2具有多层的Transformer编码器，其中包含数百万个参数，使得模型能够捕获复杂的语言结构和语义关系。\n大小变种：GPT-2有多个大小的变种，从117M到1.5B个参数不等，每个变种都具有不同的性能和资源要求。更大的模型往往在生成更加准确和流畅的文本方面表现更好，但同时也需要更多的计算资源。\n开放许可：GPT-2是在OpenAI的研究下开发的，其模型和相关资源以开放许可的形式发布，使得研究人员和开发者可以自由地使用和构建基于GPT-2的应用。\n总的来说，GPT-2是一种强大的语言模型，具有广泛的应用潜力，可用于自然语言生成、理解、翻译等各种NLP任务。\n案例 该案例来源huggingface学习中心nlp-course，Training a causal language model from scratch 文章\n描述 我们将构建一个缩减版的代码生成模型：我们将专注于一行补全，而不是完整的函数或类，使用Python代码的一个子集。在Python中处理数据时，您会频繁接触到Python数据科学栈，包括matplotlib、seaborn、pandas和scikit-learn库。在使用这些框架时，经常需要查找特定的命令，因此如果我们可以使用一个模型来为我们完成这些调用，那将是很好的。\n收集数据 我们使用huggingface收集得content包含：“pandas”, “sklearn”, “matplotlib”, “seaborn” 这些关键字python代码 这个数据集是从github公共仓库爬取，比如 from datasets import load_dataset, DatasetDict\rds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\rds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\rraw_datasets = DatasetDict(\r{\r\"train\": ds_train, # .shuffle().select(range(50000)),\r\"valid\": ds_valid, # .shuffle().select(range(500))\r}\r) 让我们看一个数据集中的例子。我们只需显示每个字段的前200个字符：",
    "tags": [],
    "title": "Transformers实战04-微调gpt-2生成python代码。",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_04/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "简介 模型量化（Model Quantization）是一种优化技术，旨在减少机器学习模型的计算资源需求和存储空间，同时在精度损失最小化的前提下提高推理效率。量化通过将模型权重和激活函数的数值从高精度（如 32 位浮点数）转换为低精度（如 8 位整数），显著减少了模型大小和计算复杂度。\n主要类型 静态量化（Post-Training Quantization, PTQ）\n在模型训练完成后进行量化。 通过分析训练数据的分布，将权重和激活函数映射到低精度表示。 不需要重新训练模型。 适用于对性能影响较小的场景。 动态量化（Dynamic Quantization）\n在推理时动态地将浮点数转换为低精度整数。 在运行过程中对激活函数进行量化。 比静态量化更简单，因为不需要分析训练数据。 对推理速度有显著提升，尤其是对模型输入依赖较少的层（如全连接层）。 量化感知训练（Quantization-Aware Training, QAT）\n在训练过程中模拟量化影响。 模型在训练过程中考虑量化误差，以便在量化后保持更高的精度。 比静态量化和动态量化需要更多的计算资源，但精度损失最小。 适用于对精度要求较高的应用。 这里例子就演示下动态量化，bitsandbytes本身以上三种都支持。\n量化的优点 减小模型大小：通过将权重和激活函数表示从 32 位浮点数转换为 8 位整数，模型大小可以显著减少。 加快推理速度：低精度运算速度更快，可以显著提高推理效率。 降低内存带宽需求：低精度表示占用更少的内存，减少了内存带宽的需求。 量化的缺点 精度损失：由于数值表示的精度降低，模型可能会经历一定程度的精度损失，具体程度取决于模型结构和数据分布。 复杂性增加：在某些情况下，量化过程可能会增加模型部署的复杂性，尤其是需要进行量化感知训练时。 量化过程 以下过程只是一种最简单的思路，方便理解，实际要比这更复杂。\n量化过程 确定值域： 首先，确定要量化的数据的值域范围。例如，假设我们有一组数据的值域为 $[min,max]$。\n确定量化级别： 确定量化的级别或分辨率，这决定了将值域划分成多少个区间。在4位整数的情况下，共有 $2^4=16$ 个可能的值。\n线性映射： 将原始数据映射到4位整数的范围内。通常使用线性映射来实现，计算公式如下： $$\\text{quantized_value} = \\frac{\\text{original_value} - \\text{min}}{\\text{max} - \\text{min}} \\times (\\text{number of levels} - 1)$$\n这里的 number of levels 是16（对应4位整数的值域范围）。\n反量化过程 解码反量化： 在使用量化数据进行计算之前，需要将其解码回原始的数据表示形式（如32位浮点数或其他高精度表示）。解码公式通常为： $$\\text{original_value} = \\text{quantized_value} \\times \\frac{\\text{max} - \\text{min}}{\\text{number of levels} - 1} + \\text{min}$$ 这里的 quantized_value是是量化后的4位整数值,min和max是原始数据的最小值和最大值。\n两个不同的原始值在量化后可能相同，被还原为同一个值。这种情况表明精度损失是不可避免的。为了减少这种精度损失带来的影响，通常采取以下策略：\n增加量化级别： 增加量化级别（如使用8位、16位量化）以减少不同原始值被量化为同一个值的概率。\n量化感知训练（Quantization-aware training）： 在训练过程中模拟量化误差，以提高模型在量化后的精度表现。\n非线性量化： 使用对数量化或其他非线性量化方法，使得量化更适应数据的分布特性，从而减少精度损失。\n精细调节量化参数： 通过精细调整量化的最小值、最大值和比例因子，尽量减少量化误差对关键值的影响。\n精度和参数 模型中每个参数常见的存储类型包括：\nFP32（32-bit Floating Point）: 每个参数占用 4 字节（32 位），单精度浮点数（32位浮点数），范围大约：$[-3.4 \\times 10^{38}, 3.4 \\times 10^{38}]$。 FP16（16-bit Floating Point）: 每个参数占用 2 字节（16 位），半精度浮点数使用16位（1位符号、5位指数、10位尾数），FP16的数值范围大约是 [−65504,65504]，大约 3 位有效数字。 INT8（8-bit Integer）: 每个参数占用 1 字节（8 位），将模型的权重和激活值量化为8位整数（范围通常是0到255），相对于32位浮点数，精度的损失较小。8-bit量化比4-bit提供更好的精度，并且通常可以更接近原始模型的性能。 INT4（4-bit Integer）: 每个参数占用4位，将模型的权重和激活值量化为4位整数（范围通常是-8到7或者0到15），因此相对于32位浮点数，它的精度显著降低。这种量化可以显著减小模型的大小和计算需求，但可能会损失一定的模型精度。 如何获取某个模型的精度了\nimport torch\rfrom transformers import AutoModel, BertTokenizer\rmodel_name=\"bert-base-chinese\" #bert-base-uncased\rmodel=AutoModel.from_pretrained(model_name)\r#获取模型参数的精度\r\"\"\"\rFP32（32-bit Floating Point）: 每个参数占用 4 字节（32 位）。\rFP16（16-bit Floating Point）: 每个参数占用 2 字节（16 位）。\rINT8（8-bit Integer）: 每个参数占用 1 字节（8 位）。\r\"\"\"\rdtype=list(model.parameters())[0].dtype\rprint(\"精度:\",dtype)\rtotal_params = sum(p.numel() for p in model.parameters())\rdtype_to_bytes = {\rtorch.float32: 4, # FP32: 4字节\rtorch.float16: 2, # FP16: 2字节\rtorch.int8: 1, # INT8: 1字节\rtorch.int32: 4, # INT32: 4字节\rtorch.int64: 8, # INT64: 8字节\rtorch.float64: 8, # FP64 (double): 8字节\r}\rmodel_size = total_params * dtype_to_bytes[dtype]\rprint(f'Model size: {model_size / (1024**2):.2f} MB') 输出\n精度: torch.float32\rModel size: 390.12 MB 量化实例 bitsandbytes bitsandbytes 通过 PyTorch 的 k 位量化技术使大型语言模型的访问变得可行。bitsandbytes 提供了三个主要功能以显著降低推理和训练时的内存消耗：\n8 位优化器采用区块式量化技术，在极小的内存成本下维持 32 位的表现。 LLM.Int() 或 8 位量化使大型语言模型推理只需一半的内存需求，并且不会有任何性能下降。该方法基于向量式的量化技术将大部分特性量化到 8 位，并且用 16 位矩阵乘法单独处理异常值。 QLoRA 或 4 位量化使大型语言模型训练成为可能，它结合了几种节省内存的技术，同时又不牺牲性能。该方法将模型量化至 4 位，并插入一组可训练的低秩适应（LoRA）权重来允许训练。 安装bitsandbytes bitsandbytes 仅支持 CUDA 版本 11.0 - 12.5 的 CUDA GPU。\n!pip install -U bitsandbytes\r!pip install transformers\r!pip install accelerate 4bit量化(加载) 加载并量化一个模型到4位，并使用bfloat16数据类型进行计算：\n您使用 bnb_4bit_compute_dtype=torch.bfloat16，这意味着计算过程中会反量化使用 bfloat16 数据类型，而存储时则可能使用4位表示。这解释了为什么您看到的 dtype 仍然是 fp16 或者 bfloat16。\nBigScience 是一个全球性的开源AI研究合作项目，旨在推动大型语言模型（LLM）的发展。bloom-1b7 是 BigScience 项目下的一部分，具体来说，是一个包含约17亿参数的语言模型。\nimport torch\rfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\rmodel_name=\"bigscience/bloom-1b7\" quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\rmodel = AutoModelForCausalLM.from_pretrained(\rmodel_name,\rdevice_map=\"auto\",\r)\rmodel_4bit = AutoModelForCausalLM.from_pretrained(\rmodel_name,\rdevice_map=\"auto\",\rquantization_config=quantization_config,\r)\rdtype=list(model.parameters())[0].dtype\rprint(\"原始精度:\",dtype)\rdest_dtype=list(model_4bit.parameters())[0].dtype\rprint(\"量化精度:\",dest_dtype)\r# 检查模型的量化配置\rprint(\"量化配置:\", model_4bit.config.quantization_config)\rdef print_model_info(model):\rtotal_params = 0\rfor name, param in model.named_parameters():\rtotal_params += param.numel()\r#print(f\"Total parameters: {total_params / 1e6}M\")\rreturn total_params\rtotal_model_size=print_model_info(model)\rtotal_model_4bit_size=print_model_info(model_4bit)\rprint(\"模型参数个数：\",total_model_size)\rprint(\"量化后的模型参数个数：\",total_model_4bit_size)\rdtype_to_bytes = {\rtorch.float32: 4, # FP32: 4字节\rtorch.float16: 2, # FP16: 2字节\rtorch.int8: 1, # INT8: 1字节\rtorch.int32: 4, # INT32: 4字节\rtorch.int64: 8, # INT64: 8字节\rtorch.float64: 8, # FP64 (double): 8字节\r}\rmodel_size = total_model_size * dtype_to_bytes[dtype]\rmodel_size = total_model_size * dtype_to_bytes[dtype]\rprint(f'origin Model size: {model_size / (1024**2):.2f} MB')\rmodel_size = total_model_4bit_size * dtype_to_bytes[dest_dtype]\rprint(f'quan Model size: {model_size / (1024**2):.2f} MB')\rmodel_4bit.save_pretrained(\"/tmp/p\")\rmodel.save_pretrained(\"/tmp/o\") 输出：\n原始精度: torch.float32\r量化精度: torch.float16\r量化配置: BitsAndBytesConfig {\r\"_load_in_4bit\": true,\r\"_load_in_8bit\": false,\r\"bnb_4bit_compute_dtype\": \"bfloat16\",\r\"bnb_4bit_quant_storage\": \"uint8\",\r\"bnb_4bit_quant_type\": \"fp4\",\r\"bnb_4bit_use_double_quant\": false,\r\"llm_int8_enable_fp32_cpu_offload\": false,\r\"llm_int8_has_fp16_weight\": false,\r\"llm_int8_skip_modules\": null,\r\"llm_int8_threshold\": 6.0,\r\"load_in_4bit\": true,\r\"load_in_8bit\": false,\r\"quant_method\": \"bitsandbytes\"\r}\r模型参数信息： 1722408960\r量化后的模型参数信息： 1118429184\rorigin Model size: 6570.47 MB\rquan Model size: 2133.23 MB 总的参数个数减少。这通常是由于量化过程中进行了优化或者参数压缩的操作。 量化在深度学习中通常是指将模型中的浮点数参数转换为更低精度的整数或定点数表示，以节省内存和提高计算效率。\n为啥量化模型的dtype是fp16了而不是int4，以下是对量化模型加载过程中 dtype 问题的一些解释：\n参数存储与计算类型的区别：\n存储时，模型参数可能被压缩或量化为较低位宽的整数类型（如4位整数）。 加载时，为了方便后续计算，这些参数可能会被解码为较高精度的浮点类型（如 fp16 或 bfloat16）。 量化过程的具体实现：\n许多量化库在加载模型时，会将低位宽的量化参数解码为浮点类型，以便在计算时可以直接使用这些参数。 这就是为什么即使您使用了 load_in_4bit=True，在加载后检查参数的 dtype 时仍然看到的是 fp16。 通过查看模型保存的就可以确定了 查看量化的模型：\n!ls /tmp/p -al --block-size=M | grep model 输出:\n-rw-r--r-- 1 root root 1630M Aug 6 08:04 model.safetensors 可以看到我们之前在内存中打印的是2133.23（内存中计算还是会被反量化到bnb_4bit_compute_dtype指定类型，但是参数都是压缩后去掉了一些参数） ，存储后变成了1630M，比之前计算的少一些，说明存储使用了4bit。 在看下没有量化的模型：\n!ls /tmp/o -al --block-size=M | grep model 输出了：\n-rw-r--r-- 1 root root 4714M Aug 6 08:05 model-00001-of-00002.safetensors\r-rw-r--r-- 1 root root 1857M Aug 6 08:05 model-00002-of-00002.safetensors\r-rw-r--r-- 1 root root 1M Aug 6 08:05 model.safetensors.index.json 可以看到我们之前在内存中打印的是6570.47 MB ，存储后没变，分文件存储了4714M+1857M 。\n8bit量化(加载) 代码和4bit相似，调整下配置即可\nquantization_config = BitsAndBytesConfig(load_in_8bit=True) 同4bit代码，输出\n原始精度: torch.float32\r量化精度: torch.float16\r量化配置: BitsAndBytesConfig {\r\"_load_in_4bit\": false,\r\"_load_in_8bit\": true,\r\"bnb_4bit_compute_dtype\": \"float32\",\r\"bnb_4bit_quant_storage\": \"uint8\",\r\"bnb_4bit_quant_type\": \"fp4\",\r\"bnb_4bit_use_double_quant\": false,\r\"llm_int8_enable_fp32_cpu_offload\": false,\r\"llm_int8_has_fp16_weight\": false,\r\"llm_int8_skip_modules\": null,\r\"llm_int8_threshold\": 6.0,\r\"load_in_4bit\": false,\r\"load_in_8bit\": true,\r\"quant_method\": \"bitsandbytes\"\r}\r模型参数信息： 1722408960\r量化后的模型参数信息： 1722408960\rorigin Model size: 6570.47 MB\rquan Model size: 3285.23 MB 可以看到8bit不需要指定内存计算的类型，量化内存计算精度默认就是fp16。 查看模型保存大小\n!ls /tmp/p -al --block-size=M | grep model\r#----------------------------------------------------------------------------------------------------\r!ls /tmp/o -al --block-size=M | grep model 输出\n-rw-r--r-- 1 root root 2135M Aug 6 08:30 model.safetensors\r#----------------------------------------------------------------------------------------------------\r-rw-r--r-- 1 root root 4714M Aug 6 08:30 model-00001-of-00002.safetensors\r-rw-r--r-- 1 root root 1857M Aug 6 08:31 model-00002-of-00002.safetensors\r-rw-r--r-- 1 root root 1M Aug 6 08:31 model.safetensors.index.json 验证效果 这里用之前的4bit模型来和原始模型比较\nimport time\rdef benchmark_model(model, input_text, tokenizer):\rinputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\rstart_time = time.time()\rwith torch.no_grad():\routputs = model.generate(**inputs)\r# 解码并打印生成的文本\rgenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\rprint(\"Generated text:\", generated_text)\rend_time = time.time()\rinference_time = end_time - start_time\rprint(f\"Inference time: {inference_time:.2f} seconds\")\rfrom transformers import AutoTokenizer\rtokenizer = AutoTokenizer.from_pretrained(model_name)\rinput_text = \"Hello, how are you?\"\rprint(\"未量化模型性能测试：\")\rbenchmark_model(model, input_text, tokenizer)\rprint(\"量化模型性能测试：\")\rbenchmark_model(model_4bit, input_text, tokenizer) 输出\n未量化模型性能测试：\rGenerated text: Hello, how are you? I hope you are doing well. I am a newbie in this\rInference time: 0.31 seconds\r量化模型性能测试：\rGenerated text: Hello, how are you?\"\r\"I'm fine,\" I said.\r\"I'm just a\rInference time: 0.62 seconds 这里看到量化的模型反而推理需要更多的时间，量化模型在理论上应该提高推理速度和减少内存占用,这里使用float16gpu显存占用肯定少了一半以上，但是推理速度比较慢，在实际应用中，可能会因为多个因素导致性能下降。",
    "description": "简介 模型量化（Model Quantization）是一种优化技术，旨在减少机器学习模型的计算资源需求和存储空间，同时在精度损失最小化的前提下提高推理效率。量化通过将模型权重和激活函数的数值从高精度（如 32 位浮点数）转换为低精度（如 8 位整数），显著减少了模型大小和计算复杂度。\n主要类型 静态量化（Post-Training Quantization, PTQ）\n在模型训练完成后进行量化。 通过分析训练数据的分布，将权重和激活函数映射到低精度表示。 不需要重新训练模型。 适用于对性能影响较小的场景。 动态量化（Dynamic Quantization）\n在推理时动态地将浮点数转换为低精度整数。 在运行过程中对激活函数进行量化。 比静态量化更简单，因为不需要分析训练数据。 对推理速度有显著提升，尤其是对模型输入依赖较少的层（如全连接层）。 量化感知训练（Quantization-Aware Training, QAT）\n在训练过程中模拟量化影响。 模型在训练过程中考虑量化误差，以便在量化后保持更高的精度。 比静态量化和动态量化需要更多的计算资源，但精度损失最小。 适用于对精度要求较高的应用。 这里例子就演示下动态量化，bitsandbytes本身以上三种都支持。\n量化的优点 减小模型大小：通过将权重和激活函数表示从 32 位浮点数转换为 8 位整数，模型大小可以显著减少。 加快推理速度：低精度运算速度更快，可以显著提高推理效率。 降低内存带宽需求：低精度表示占用更少的内存，减少了内存带宽的需求。 量化的缺点 精度损失：由于数值表示的精度降低，模型可能会经历一定程度的精度损失，具体程度取决于模型结构和数据分布。 复杂性增加：在某些情况下，量化过程可能会增加模型部署的复杂性，尤其是需要进行量化感知训练时。 量化过程 以下过程只是一种最简单的思路，方便理解，实际要比这更复杂。\n量化过程 确定值域： 首先，确定要量化的数据的值域范围。例如，假设我们有一组数据的值域为 $[min,max]$。\n确定量化级别： 确定量化的级别或分辨率，这决定了将值域划分成多少个区间。在4位整数的情况下，共有 $2^4=16$ 个可能的值。\n线性映射： 将原始数据映射到4位整数的范围内。通常使用线性映射来实现，计算公式如下： $$\\text{quantized_value} = \\frac{\\text{original_value} - \\text{min}}{\\text{max} - \\text{min}} \\times (\\text{number of levels} - 1)$$\n这里的 number of levels 是16（对应4位整数的值域范围）。",
    "tags": [],
    "title": "Transformers实战05-模型量化",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_05/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "前言 Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。\n在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。\nAttention is All You Need：Attention Is All You Need\nTransformer 整体结构 首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构： 可以看到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：\n第一步：获取输入句子的每一个单词的表示向量 X，X由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。 第二步：将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 x) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 C，如下图。单词向量矩阵用 $X_{n\\times d}$ 表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。 第三步：将 Encoder 输出的编码信息矩阵 C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。 上图 Decoder 接收了 Encoder 的编码矩阵 C，然后首先输入一个翻译开始符 “\"，预测第一个单词 “I”；然后输入翻译开始符 “” 和单词 “I”，预测单词 “have”，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。\nTransformer 的输入 Transformer 中单词的输入表示 x由单词 Embedding 和位置 Embedding （Positional Encoding）相加得到。 单词 Embedding 单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。\n原理 什么是Word Embedding（词嵌入）？\n词嵌入是自然语言处理中语言模型与表征技术技术的统称。讲人话就是： 就是把词语（字符串类型）这一文本数据转换成 计算机能认识 的数字表征的数据（一般为浮点型数据）。因为我们的机器学习模型或者深度学习模型，需要的数据都是数字类型的，无法处理文本类型的数据，所以我们需要把单词转换成数字类型。 词嵌入为 文本AI系统的上游任务，只有通过词嵌入模型才能得到文本AI系统才能得到数字类型的输入数据。 现有的词嵌入模型有：word2vec，GloVe，ELMo，BERT等 以下使用word2vec的原理来解释下词embedding实现逻辑\nword2vec是词向量化技术的一种，通过神经网络来实现。其在表面上看起来是一种无监督学习技术，但本质上仍然是有监督学习。 利用文本的上下文信息构造有监督数据集，通过这一数据集来训练神经网络，最后取得训练好的神经网络两个网络层之间的权重 矩阵作为的词向量表（每个单词对应其中一行数据）。\nword2vec 有两个模型：\nSkip-gram模型：其特点为，根据当前单词预测上下文单词，使用中心词来预测上下文词。 CBOW模型：全称为 Continuous Bag-of-Word，连续词袋模型，该模型的特点是，输入已知的上下文，输出对当前单词的预测，其实就是利用中心两侧的词来预测中心的词。 以下两幅图展现了CBOW模型和Skip-gram模型。 CBOW 模型 如果对以下神经网络连接不太清楚的，可以先去看看：https://blog.csdn.net/liaomin416100569/article/details/130572559?spm=1001.2014.3001.5501\none-hot 参考：rnn中关于one-hot和nn.embedding章节\nOne-hot编码又称一位有效编码，是将文字数字化的过程。假如我们有一个语料库：”I drink coffee everyday“。我们对其以” “（空格）进行分词，则我们会得到4个单词，假设这4个单词是我们所有的单词种类（也就是说，我们的字典中只有这四个单词），这样我们对其进行one-hot编码后，可以得到如下编码结果： 表1\n单词 One-hot编码 I [1, 0, 0, 0] drink [0, 1, 0, 0] coffee [0, 0, 1, 0] everyday [0, 0, 0, 1] 这里使用one-hot的原因是列的个数就是单词的格式，最后使用隐藏层的w作为嵌入结果，刚好是(列数，隐藏层神经元个数)\n构建 CBOW 训练数据集 cbow是使用两侧的词语，预测中心的词语，预测窗口大小为 2，输入就是左侧和右侧的两个单词，预测的单词就是中心的单词。 我们语料库仍然为：”I drink coffee everyday“，假设我们的预测窗口大小为 2，通过语料库我们可以构建以下训练集，表2\n输入词 预测词 [drink, coffee] I [I, coffee, everyday] drink [I, drink, everyday] coffee [drink, coffee] everyday 构建 CBOW 神经网络 从上可知，我们的输入层有4个输入单元（one-hot的4列，因为one-hot所以就是原始单词个数），输出层神经元的个数应该跟输入层保持一致，输出层也是4个神经元，加入我们想要每个单词为一个五维的向量表示，那么我们的隐藏层则为五个神经元。由此，我们可以构建一个输入层为4，隐藏层为5，输出层为4的全连接神经网络，如下图所示，训练好的模型的权重矩阵w1可以作为我们的词向量化表。 训练 CBOW 神经网络 这时我们可以根据构建的CBOW数据集对模型进行训练了，假设我们要预测的词是coffee，那么由表2可知，我们输入词为[I, drink, everyday]，我们可以得到如下训练过程。 首先，我们将输入词[I, drink, everyday]转换为对应的one-hot编码向量。假设我们的词汇表中有四个词（I, drink, coffee, everyday），则输入词的one-hot编码分别为：\nI: [1, 0, 0, 0]\rdrink: [0, 1, 0, 0]\reveryday: [0, 0, 0, 1] 接下来，我们将每个one-hot编码向量乘以词嵌入矩阵，以获取词嵌入向量。假设我们已经有了每个词的词嵌入矩阵（这些矩阵在实际应用中是通过训练得到的）,这也是我们经过多次训练之后，最终得到的嵌入矩阵，因为初始化肯定是一个初始值，经过训练反向传播得到一个最佳值，这里假设它们分别为： $$ W = \\begin{bmatrix} 0.1 \u0026 0.2 \u0026 0.3 \u0026 0.4 \u0026 0.5 \\ 0.2 \u0026 0.3 \u0026 0.4 \u0026 0.5 \u0026 0.6 \\ 0.3 \u0026 0.4 \u0026 0.5 \u0026 0.6 \u0026 0.7 \\ 0.4 \u0026 0.5 \u0026 0.6 \u0026 0.7 \u0026 0.8 \\ \\end{bmatrix} $$ 接下来，我们将每个one-hot编码向量乘以词嵌入矩阵，以获取词嵌入向量。例如：\n输入词I的词嵌入向量：$$[1, 0, 0, 0] \\times W = [0.1, 0.2, 0.3, 0.4, 0.5] $$ 输入词drink的词嵌入向量：$$[0, 1, 0, 0] \\times W = [0.2, 0.3, 0.4, 0.5, 0.6] $$ 输入词everyday的词嵌入向量：$$ [0, 0, 0, 1] \\times W = [0.4, 0.5, 0.6, 0.7, 0.8] $$ 接下来，我们将上下文单词的词嵌入向量加起来或求平均以获取一个特征向量。在这个例子中，我们将对它们求平均。\n平均特征向量 = $$\\text{平均特征向量} = \\frac{( \\text{词嵌入向量(I)} + \\text{词嵌入向量(drink)} + \\text{词嵌入向量(everyday)} )}{3}$$ $$= \\frac{( [0.1, 0.2, 0.3, 0.4, 0.5] + [0.2, 0.3, 0.4, 0.5, 0.6] + [0.4, 0.5, 0.6, 0.7, 0.8] )}{3}$$ $$= \\left[ \\frac{(0.1 + 0.2 + 0.4)}{3}, \\frac{(0.2 + 0.3 + 0.5)}{3}, \\frac{(0.3 + 0.4 + 0.6)}{3}, \\frac{(0.4 + 0.5 + 0.7)}{3}, \\frac{(0.5 + 0.6 + 0.8)}{3} \\right]$$ $$= [0.233, 0.333, 0.433, 0.533, 0.633]$$ 现在，我们得到了一个特征向量$$ [0.233, 0.333, 0.433, 0.533, 0.633]$$它表示了上下文单词[I, drink, everyday]的语义信息。\n理解CBOW模型中将上下文单词的词嵌入向量加起来或求平均的原因需要考虑两个方面： 1.上下文信息的整合：CBOW模型的目标是通过上下文单词来预测目标词。因此，对于一个给定的目标词，在预测时需要综合考虑其周围的上下文信息。将上下文单词的词嵌入向量加起来或求平均，可以将这些单词的语义信息整合到一个特征向量中，使得该特征向量更全面地表示了整个句子的语境信息，而不仅仅是单个词的信息。这样可以帮助模型更准确地捕捉句子的语义信息，从而提高模型在目标词预测任务上的性能。 2.语义信息的提取：虽然CBOW模型是用来预测目标词的，但实际上，在训练过程中，模型会学习到每个词的词嵌入向量，这些词嵌入向量包含了每个单词的语义信息。当将上下文单词的词嵌入向量加起来或求平均时，实际上是在利用这些已经学习到的词嵌入向量来提取整个句子的语义信息。由于词嵌入向量是通过大规模语料库训练得到的，其中包含了丰富的语义信息，因此将它们加起来或求平均可以帮助提取句子的语义特征，而不仅仅是单个词的语义特征。\n接下来，我们将特征向量输入到一个全连接层（也称为投影层），并应用softmax函数以获取预测概率。假设全连接层的权重矩阵为： $$W_{proj} = \\begin{bmatrix} 0.1 \u0026 0.2 \u0026 0.3 \u0026 0.4 \u0026 0.5 \\ 0.2 \u0026 0.3 \u0026 0.4 \u0026 0.5 \u0026 0.6 \\ 0.3 \u0026 0.4 \u0026 0.5 \u0026 0.6 \u0026 0.7 \\ 0.4 \u0026 0.5 \u0026 0.6 \u0026 0.7 \u0026 0.8 \\ 0.5 \u0026 0.6 \u0026 0.7 \u0026 0.8 \u0026 0.9 \\ \\end{bmatrix}$$ 我们将特征向量乘以权重矩阵，并应用softmax函数，以获取每个词作为预测目标的概率。 $$z = [0.233, 0.333, 0.433, 0.533, 0.633] \\times W_{proj}$$\n经过训练之后，我们拿 W1( 4*5 权重矩阵) 作为我们的词向量化表，我们可以得到**如下词向量化表（假设）。\n单词索引 向量 I [0.11, 0.22, 0.23, 0.25, 0.31] drink [0.32, 0.22, 0.33, 0.11, 0.32] coffee [0.23, 0.03, 0.62, 0.12, 0.17] everyday [0.05, 0.25, 0.55, 0.17, 0.47 ] 假如我们要词向量化”I drink coffee“这句话，我们便可以直接查询上表，拿到我们的词向量矩阵，即为$$[ [0.11, 0.22, 0.23, 0.25, 0.31],\\ [0.32, 0.22, 0.33, 0.11, 0.32], \\ [0.23, 0.03, 0.62, 0.12, 0.17] ]$$\nSkip-gram 模型 one-hot 参考：rnn中关于one-hot和nn.embedding章节\nOne-hot编码又称一位有效编码，是将文字数字化的过程。假如我们有一个语料库：”I drink coffee everyday“。我们对其以” “（空格）进行分词，则我们会得到4个单词，假设这4个单词是我们所有的单词种类（也就是说，我们的字典中只有这四个单词），这样我们对其进行one-hot编码后，可以得到如下编码结果： 表1\n单词 One-hot编码 I [1, 0, 0, 0] drink [0, 1, 0, 0] coffee [0, 0, 1, 0] everyday [0, 0, 0, 1] 这里使用one-hot的原因是列的个数就是单词的格式，最后使用隐藏层的w作为嵌入结果，刚好是(列数，隐藏层神经元个数)\n构建 Skip-gram训练数据集 cbow是使用两侧的词语，预测中心的词语，预测窗口大小为 2，输入就是左侧和右侧的两个单词，预测的单词就是中心的单词。 skip-gram是使用中心的词语，预测两侧的词语，预测窗口大小为 2，输入就是中心词语，预测的单词就是左侧和右侧的两个单词。 我们语料库仍然为：”I drink coffee everyday“，假设我们的预测窗口大小为 2，通过语料库我们可以构建以下训练集，表2\n预测词 输入词 I drink I coffee drink I drink coffee drink everyday coffee I coffee drink coffee everyday everyday drink everyday coffee 注意输入是一个词，输出是一个词\n训练 Skip-gram神经网络 这时我们可以根据构建的Skip-gram数据集对模型进行训练了，假设我们要预测的词是coffee，那么由表2可知，我们输入词为[I, drink, everyday]中的任何一个，由表2可知，对其进行one-hot编码后的结果为 [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1]], **我们选择其中一个就可以得到一个 1*4 的输入向量，那么我们可以得到如下训练过程。 经过训练之后，我们拿 W1( 4*5 权重矩阵) 作为我们的词向量化表。 训练过程不表,类似于CBOW 。\nWord2Vec实例 数据训练 导入必要的库： #安装 pip install gensim jieba from gensim.models import Word2Vec\rimport logging # 用来设置日志输出\rimport jieba 准备文本数据： context = [\"word2vec是监督学习算法，其会通过句子中词的前后顺序构建有标签数据集，通过数据集 训练神经网络模型 得到这一数据集的 词向量 表（可以理解成我们的新华字典）。\"\r,\"word2vec是用来进行 对词语进行向量化 的模型，也就是对文本类型的数据进行 特征提取\"\r,\"word2vec一般为一个3层（输入层、隐藏层、输出层） 的 全连接神经网络。\"\r,\"本文主要从原理、代码实现 理论结合实战两个角度来剖析word2vec算法\"\r,\"理论部分主要是关于 什么是 word2vec，其两种常见的模型\"\r,\"实战部分主要是通过Gensim库中的word2vec模型，实现文本特征提取\"] 中文分词：\n使用jieba库对文本进行中文分词，并将分词结果保存在context列表中。 for i in range(len(context)):\rsplit_s = context[i]\rcontext[i] = \" \".join(jieba.cut(split_s, HMM=True))\rcontext = [e.split(\" \") for e in context] 配置日志：\n配置日志输出格式和级别。\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) 训练Word2Vec模型：\n使用Word2Vec类来训练模型，传入分词后的文本数据以及一些参数：\nsentences: 分词后的文本数据。 workers: 训练时使用的线程数。 window: 上下文窗口大小，表示一个词周围的上下文词数量。 vector_size: 词向量的维度大小。 epochs: 训练轮数。 min_count: 忽略词频低于此值的词语。 model = Word2Vec(sentences=context, workers=8, window=4, vector_size=10, epochs=30, min_count=3) 查看词汇表和词向量： print(model.wv.key_to_index) # 打印词汇表\rprint(model.wv[\"word2vec\"]) model.wv.key_to_index用于查看词汇表，而model.wv[\"word2vec\"]则用于查看特定词的词向量，这里是查询单词word2vec的词向量。 输出结果\n{'': 0, '的': 1, 'word2vec': 2, '，': 3, '是': 4, '层': 5, '模型': 6, '数据': 7, '主要': 8, '、': 9, '进行': 10, '集': 11, '通过': 12}\r[ 0.07315318 0.05167933 0.06995787 0.00852275 0.0644208 -0.03653978\r-0.00503093 0.06105096 -0.081814 -0.04047652] 可以使用Gensim提供的save()方法将训练好的Word2Vec模型保存到文件。这样可以在之后加载模型并重用它。以下是保存模型的示例代码：\n注意：词汇表里单词都是词频次数超过min_count的词。\n保存和加载 保存模型\nmodel.save(\"word2vec_model.bin\") 这将把训练好的模型保存到名为\"word2vec_model.bin\"的文件中。然后，您可以使用以下代码加载保存的模型：\nfrom gensim.models import Word2Vec\r# 加载模型\rloaded_model = Word2Vec.load(\"word2vec_model.bin\")",
    "description": "前言 Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。\n在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。\nAttention is All You Need：Attention Is All You Need\nTransformer 整体结构 首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构： 可以看到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：\n第一步：获取输入句子的每一个单词的表示向量 X，X由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。 第二步：将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 x) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 C，如下图。单词向量矩阵用 $X_{n\\times d}$ 表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。 第三步：将 Encoder 输出的编码信息矩阵 C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。 上图 Decoder 接收了 Encoder 的编码矩阵 C，然后首先输入一个翻译开始符 “\"，预测第一个单词 “I”；然后输入翻译开始符 “” 和单词 “I”，预测单词 “have”，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。",
    "tags": [],
    "title": "Transformer模型详解01-Word Embedding",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_01/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "什么是位置编码 在transformer的encoder和decoder的输入层中，使用了Positional Encoding，使得最终的输入满足： $$input = input_embedding + positional_encoding $$\nword embedding：理解参考\n这里，input_embedding是通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model，由于是相加关系，自然而然地，这里的positional_encoding也是一个d_model维度的向量。（在原论文里，d_model = 512） 注意：在Transformer模型中，“token”（标记）是指输入序列中的每个元素，它通常是一个单词、一个子词或一个字符，假设我们有一个句子：“The cat sat on the mat.\"，单词级别的标记： [“The”, “cat”, “sat”, “on”, “the”, “mat”, “.\"]。然后被转换成词嵌入（word embeddings）和位置嵌入（position embeddings），然后这两种嵌入会被相加起来形成输入嵌入（input embeddings）。这个输入嵌入会作为模型的输入，并传递到Transformer的神经网络中进行处理,token本身不会再作为数据传递到模型中。\nInput Embedding为什么解决的是语义问题，没有解决位置问题？？，语义不是有顺序才有吗？？\nInput Embedding (输入嵌入):\ninput_embedding 主要解决的是词汇语义的表示问题。通过将单词映射为连续的低维向量空间，词嵌入技术（如Word2Vec、GloVe等）可以捕获单词之间的语义关系，比如单词的近义词、反义词等。这使得神经网络在处理文本时能够更好地理解单词的含义，从而提高了对语义的建模能力。 但是，词嵌入并没有直接解决词序的问题。即使单词被嵌入到向量空间中，神经网络在处理这些向量时仍然不知道它们在句子中的位置。这就是为什么我们需要进一步引入位置编码的原因。 Positional Encoding (位置编码):\npositional_encoding 解决的是序列数据的位置信息丢失问题。在自然语言处理中，文本是由单词或字符组成的序列，这些单词的排列顺序对句子的含义至关重要。通过引入位置编码，我们可以向神经网络提供关于单词在序列中位置的信息，从而使网络能够区分不同位置的单词并更好地处理序列数据。 位置编码通常是与词嵌入相加的方式来融合位置信息和语义信息。这样，神经网络在处理输入数据时既能考虑单词的语义关系，又能考虑单词在句子中的位置关系，从而更全面地理解文本数据。 因此，input_embedding 和 positional_encoding 两者都是为了帮助神经网络更好地理解文本数据，但它们解决的是不同层面的问题：input_embedding 解决的是语义表示问题，而 positional_encoding 解决的是位置信息丢失问题。这两者结合起来能够提高神经网络对文本数据的建模能力。\n在transformer的self-attention模块中，序列的输入输出如下（不了解self-attention没关系，这里只要关注它的输入输出就行）： 在self-attention模型中，输入是一整排的tokens，对于人来说，我们很容易知道tokens的位置信息，比如：\n（1）绝对位置信息。a1是第一个token，a2是第二个token……\n（2）相对位置信息。a2在a1的后面一位，a4在a2的后面两位……\n（3）不同位置间的距离。a1和a3差两个位置，a1和a4差三个位置….\n但是这些对于self-attention来说，是无法分辩的信息，因为self-attention的运算是无向的。因为，我们要想办法，把tokens的位置信息，喂给模型。\n连续有界 有界又连续的概念是数学中对函数或者集合的性质进行描述的。一个函数或者集合被称为有界的意思是它在某个范围内有限，即它的值不能无限增长或减小；而连续则表示函数或者集合中的元素在某个区间内没有断裂或跳跃。\n举个例子，考虑函数 (f(x) = \\sin(x))。这个函数是有界的，因为正弦函数的值范围在 ([-1, 1]) 之间，不会超出这个范围。而且，正弦函数在定义域内是连续的，没有断点或跳跃。因此，正弦函数 (f(x) = \\sin(x)) 是一个有界又连续的函数。\n另一个例子是闭区间 ([0, 1]) 上的实数集合。这个集合是有界的，因为它的元素都在区间 ([0, 1]) 内；同时，这个集合是连续的，因为在闭区间内没有任何间隔或断裂。\n总的来说，有界又连续的概念在数学中非常常见，许多函数、集合以及数学对象都可以被描述为有界又连续的。\n为什么要有界 在Transformer等模型中，位置编码用于为序列中的不同位置提供唯一的标识，以便模型能够区分不同位置的词语。通常情况下，位置编码是与词嵌入向量相加的，因此需要确保位置编码与词嵌入向量的范围相匹配，以避免结果的数值过大或过小。\n此外，由于模型的输入通常是通过词嵌入向量表示的，而词嵌入向量通常是有限范围的，因此位置编码的范围也被限制在一个合理的范围内，以保持整个输入的稳定性和可训练性。\n因此，尽管位置编码并不一定必须是有界的，但在实践中，为了保持模型的稳定性和可训练性，通常会设计位置编码为有界的。\n为什么要连续 位置编码必须是连续的，因为它们用于表示序列中的位置信息，而序列中的位置是连续的。在自然语言处理任务中，如语言模型或机器翻译，序列中的每个词或标记都对应着一个连续的位置。\n如果位置编码不是连续的，那么模型将无法正确地理解序列中各个位置之间的关系。例如，如果某个位置的位置编码与其相邻位置的位置编码之间存在不连续性，模型可能会误解序列中的顺序关系，从而影响其性能。\n另外，连续的位置编码有助于模型更好地捕捉序列中的局部和全局关系，因为它们可以在连续的空间中表示位置信息，使模型能够更准确地理解序列中不同位置之间的距离和关联。\n综上所述，位置编码必须是连续的，以确保模型能够有效地理解序列中的位置信息，并正确地捕捉序列中的关系和结构。\n位置编码的演变 用整型值标记位置 一种自然而然的想法是，给第一个token标记0，给第二个token标记1…，以此类推。 这种方法产生了以下几个主要问题：\n模型可能遇见比训练时所用的序列更长的序列。不利于模型的泛化。 模型的位置表示是无界的。随着序列长度的增加，位置值会越来越大。 用[0,1]范围标记位置 为了解决整型值带来的问题，可以考虑将位置值的范围限制在[0, 1]之内，其中，0表示第一个token，1表示最后一个token。比如有3个token，那么位置信息就表示成[0, 0.5, 1]；若有四个token，位置信息就表示成[0, 0.33, 0.69, 1]。 但这样产生的问题是，当序列长度不同时，token间的相对距离是不一样的。例如在序列长度为3时，token间的相对距离为0.5；在序列长度为4时，token间的相对距离就变为0.33。 因此，我们需要这样一种位置表示方式，满足于：\n它能用来表示一个token在序列中的绝对位置 在序列长度不同的情况下，不同序列中token的相对位置/距离也要保持一致 可以用来表示模型在训练过程中从来没有看到过的句子长度。 用二进制向量标记位置 考虑到位置信息作用在input embedding上，因此比起用单一的值，更好的方案是用一个和input embedding维度一样的向量来表示位置。这时我们就很容易想到二进制编码。如下图，假设d_model = 3，那么我们的位置向量可以表示成： 这下所有的值都是有界的（位于0，1之间），且transformer中的d_model本来就足够大，基本可以把我们要的每一个位置都编码出来了。 但是这种编码方式也存在问题：这样编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。假设d_model = 2，我们有4个位置需要编码，这四个位置向量可以表示成[0,0],[0,1],[1,0],[1,1]。我们把它的位置向量空间做出来： 用周期函数（sin）来表示位置 sin函数 先回顾下sin函数的几个概念，因为下面要用到sin函数：\n周期 周期是从一个最高点到下一个最高点（或任何一点到下一个相对点）： 振幅，相移，垂直位移 振幅是从中（平）线到最高点的高度（或到最低点），也是从最高点到最低点的距离除以2。 相移是函数比通常的位置水平向右移了多远。 垂直位移是函数比通常的位置垂直向上移了多远。 我们可以全部放进一个方程里：\ny = A sin(Bx + C) + D\n振幅是：A 周期是：2π/B 相移是：−C/B 垂直移位是：D 例子：sin(x) 这是正弦的基本公式。A = 1, B = 1, C = 0 and D = 0\n所以振幅是1，周期是2π，没有相移或垂直移位： 振幅 1，周期 2pi，没有相移或垂直移位 频率 频率是在一个时间单位里发生多少次（每 “1”）。 例子：这个正弦函数在0到1之间重复了4次： 所以频率是 4 周期是 $\\frac{1}{4}$ 其实周期和频率是相连的,周期越大，频率越小： 频率 = $\\frac{1}{周期}$ 周期 = $\\frac{1}{频率}$\n波长 波长λ=vT，其中v是波速，T是周长。波长是一个周期内波前进的距离，而这段周期内波都是匀速直线前进的，所以直接使用匀速直线运动的位移公式即可。\nsin表示位置 回想一下，现在我们需要一个有界又连续的函数，最简单的，正弦函数sin就可以满足这一点。我们可以考虑把位置向量当中的每一个元素都用一个sin函数来表示，则第t个token的位置向量($d_{model}$表示嵌入向量维度)可以表示为： $$ PE_t = [sin(\\frac{1}{2^0}t),sin(\\frac{1}{2^1}t)…,sin(\\frac{1}{2^{i-1}}t), …,sin(\\frac{1}{2^{d_{model}-1}}t)]\\ $$\nPE:位置编码：Positional Encoding，t表示第t个token，i表示位置编码是第几个列。 列sin函数，越往右波长（入*(2π/B)）越长，频率越低。\n说个题外话，说说音量调节，后面会有用： 假设你在调节音量。如果你向右旋转音量旋钮，音量（精度）可能会从低到高逐渐增加。一开始，当音量较低时，每次向右旋转可能只会增加一点音量，这时候你可能希望更细微地调整音量。但是，当音量已经相对较高时，每次向右旋转可能会增加更多的音量，这时候你可能不希望调整得太大，因此需要更小的步进来精确地调整音量。 因此，可以概括，向右旋转旋钮会增加调整参数的精度，也就是每次移动的步幅会变小，以便更精细地调整参数的值。\n言归正传： 结合下图，来理解一下这样设计的含义。图中每一行表示一个$PE_t$，每一列表示$PE_t$中的第i个元素。旋钮用于调整精度，越往右边的旋钮，需要调整的精度越大，因此指针移动的步伐越小。每一排的旋钮都在上一排的基础上进行调整（函数中t的作用）。通过频率sin(12i−1t)sin(\\frac{1}{2^{i-1}}t)sin(2i−11​t)来控制sin函数的波长，频率不断减小，则波长不断变大，此时sin函数对t的变动越不敏感，以此来达到越向右的旋钮，指针移动步伐越小的目的。 由于sin是周期函数，因此从纵向来看，如果当函数的频率增大并导致波长缩短时，意味着波形在相同时间内完成了更多的周期，则不同t下的位置向量可能出现重合的情况。比如在下图中(d_model = 3），图中的点表示每个token的位置向量，颜色越深，token的位置越往后，在频率偏大的情况下，位置相连点连成了一个闭环，靠前位置（黄色）和靠后位置（棕黑色）竟然靠得非常近： 为了避免这种情况，我们尽量将函数的波长拉长。一种简单的解决办法是同一把所有的频率都设成一个非常小的值。因此在transformer的论文中，采用了$\\frac{1}{10000^{i/(d_{model}-1)}}$这个频率（这里i其实不是表示第i个位置，但是大致意思差不多，下面会细说） 总结一下，到这里我们把位置向量表示为： $$ PE_t = [sin(w_0t),sin(w_1t)…,sin(w_{i-1}t), …,sin(w_{d_{model}-1}t)]\\ $$ 其中，$w_{i} = \\frac{1}{10000^{i/(d_{model}-1)}}$\n用sin和cos交替来表示位置 先来回顾下线性变化旋转的相关概念，后续用到。\n线形变换——旋转 在二维坐标系中，一个位置向量的旋转公式可以由三角函数的几何意义推出。 如上图假设：\n已知：假设向量$R_{A}$=(x0，y0) 角度为：A，向右旋转了角度B，新向量$R_{A+B}$角度为：A+B，模：$|\\mathbf{R}|=|\\mathbf{R_{A}}|= |\\mathbf{R_{A+B}}| = \\sqrt{x_0^2 + y_0^2}$。 未知：旋转后向量为:(x1,y1) 上面的命题就是向量$R_{A}$旋转了角度B，求新向量$R_{A+B}$（模大小相同）,$R_{A}$和$R_{A+B}$之间有绝对关系也有相对关系，相对一个角度B，我们需要通过公式来获得一个$R_{A+B}$和$R_{A}$的关系 $R_{A+B}=T_B*R_A$,$T_B$表示一个线性变换矩阵,我们可以通过公式推算出来。 在左图中，我们有关系： $x0 = |R| * cosA =\u003e cosA = x0 / |R|$ $y0 = |R| * sinA =\u003e sinA = y0 / |R|$ 在右图中，我们有关系： $x1 = |R| * cos（A+B）$ $y1 = |R| * sin（A+B）$ 其中（x1， y1）就是（x0， y0）旋转角B后得到的点。我们展开cos（A+B）和sin（A+B），得到： $x1 = |R| * （cosAcosB - sinAsinB）$ $y1 = |R| * （sinAcosB + cosAsinB）$ 现在把 $cosA = x0 / |R|$ 和 $sinA = y0 / |R|$ 代入上面的式子，得到： $x1 = |R| * （x0 * cosB / |R| - y0 * sinB / |R|） =\u003e x1 = x0 * cosB - y0 * sinB$ $y1 = |R| * （y0 * cosB / |R| + x0 * sinB / |R|） =\u003e y1 = x0 * sinB + y0 * cosB$ 这样我们就得到了二维坐标下向量围绕圆点的逆时针旋转公式。顺时针旋转就把角度变为负： $x1 = x0 * cos（-B） - y0 * sin（-B） =\u003e x1 = x0 * cosB + y0 * sinB$ $y1 = x0 * sin（-B） + y0 * cos（-B）=\u003e y1 = -x0 * sinB + y0 * cosB$ 现在我要把这个旋转公式写成矩阵的形式，有一个概念我简单提一下，平面或空间里的每个线性变换（这里就是旋转变换）都对应一个矩阵，叫做变换矩阵。对一个点实施线性变换就是通过乘上该线性变换的矩阵完成的。好了，打住，不然就跑题了。\n现在来将下面公式转换成矩阵 逆时针： $x1 = x0 * cosB - y0 * sinB$ $y1 = x0 * sinB + y0 * cosB$ 所以二维旋转变换矩阵就是： $$ [x, y] * \\left[\\begin{matrix}cosB \u0026 sinB \\ -sinB \u0026 cosB\\end{matrix}\\right] = [xcosB-ysinB ,xsinB+ycosB] $$ 变换矩阵为$\\left[\\begin{matrix}cosB \u0026 sinB \\ -sinB \u0026 cosB\\end{matrix}\\right]$ 顺时针： $x1 = x0 * cosB + y0 * sinB$ $y1 = -x0 * sinB + y0 * cosB$ 同理变换矩阵为：$\\left[\\begin{matrix}cosB \u0026 -sinB \\ sinB \u0026 cosB\\end{matrix}\\right]$\nsin和cos交替表示位置 目前为止，我们的位置向量实现了如下功能：\n每个token的向量唯一（每个sin函数的频率足够小） 位置向量的值是有界的，且位于连续空间中。模型在处理位置向量时更容易泛化，即更好处理长度和训练数据分布不一致的序列（sin函数本身的性质） 那现在我们对位置向量再提出一个要求，不同的位置向量是可以通过线性转换得到的。这样，我们不仅能表示一个token的绝对位置，还可以表示一个token的相对位置(也就是两个token之间得线性关系)，即我们想要： $$ PE_{t+\\bigtriangleup t} = T_{\\bigtriangleup t} * PE_{t} $$ 这里，T表示一个线性变换矩阵。观察下面这个目标式子，联想到在向量空间中一种常用的线形变换——旋转。在这里，我们将t想象为一个角度，那么 $\\bigtriangleup t$就是其旋转的角度，则上面的式子可以进一步写成： $$\\begin{pmatrix} \\sin(t + \\bigtriangleup t)\\ \\cos((t + \\bigtriangleup t) \\end{pmatrix}=\\begin{pmatrix} \\cos\\bigtriangleup t\u0026\\sin\\bigtriangleup t \\ -\\sin\\bigtriangleup t\u0026\\cos\\bigtriangleup t \\end{pmatrix}\\begin{pmatrix} \\sin t\\ \\cos t \\end{pmatrix} $$ 有了这个构想，我们就可以把原来元素全都是sin函数的 $PE_{t}$ 做一个替换，我们让位置两两一组，分别用sin和cos的函数对来表示它们，则现在我们有： $$ PE_t = [sin(w_0t),cos(w_0t), sin(w_1t),cos(w_1t),…,sin(w_{\\frac{d_{model}}{2}-1}t), cos(w_{\\frac{d_{model}}{2}-1}t)]\\ $$ 在这样的表示下，我们可以很容易用一个线性变换，把 $PE_{t}$ 转变为 $PE_{t+\\bigtriangleup t}$ $$PE_{t+\\bigtriangleup t} = T_{\\bigtriangleup t} * PE_{t} =\\begin{pmatrix} \\begin{bmatrix} cos(w_0\\bigtriangleup t)\u0026 sin(w_0\\bigtriangleup t)\\ -sin(w_0\\bigtriangleup t)\u0026 cos(w_0\\bigtriangleup t) \\end{bmatrix}\u0026…\u00260 \\ …\u0026 …\u0026 …\\ 0\u0026 …\u0026 \\begin{bmatrix} cos(w_{\\frac{d_{model}}{2}-1 }\\bigtriangleup t)\u0026 sin(w_{\\frac{d_{model}}{2}-1}\\bigtriangleup t)\\ -sin(w_{\\frac{d_{model}}{2}-1}\\bigtriangleup t)\u0026 cos(w_{\\frac{d_{model}}{2}-1}\\bigtriangleup t) \\end{bmatrix} \\end{pmatrix}\\begin{pmatrix} sin(w_0t)\\ cos(w_0t)\\ …\\ sin(w_{\\frac{d_{model}}{2}-1}t)\\ cos(w_{\\frac{d_{model}}{2}-1}t) \\end{pmatrix} = \\begin{pmatrix} sin(w_0(t+\\bigtriangleup t))\\ cos(w_0(t+\\bigtriangleup t))\\ …\\ sin(w_{\\frac{d_{model}}{2}-1}(t+\\bigtriangleup t))\\ cos(w_{\\frac{d_{model}}{2}-1}(t+\\bigtriangleup t)) \\end{pmatrix}$$ 变换矩阵，也是两个一组和$PE_{t}$进行点乘，变换数组一行就有多组，最后也是个由转换角度+参数(常量)的线性变换。\nTransformer中位置编码方法 Transformer 位置编码定义 有了上面的演变过程后，现在我们就可以正式来看transformer中的位置编码方法了。\n定义：\nt是这个token在序列中的实际位置（例如第一个token为1，第二个token为2…）\n-$PE_t\\in\\mathbb{R}^d$是这个token的位置向量， $PE_{t}^{(i)}$表示这个位置向量里的第i个元素 $d_{model}$是这个token的维度（在论文中，是512) 则 $PE_{t}^{(i)}$ 可以表示为： $$PE_{t}^{(i)} = \\left{\\begin{matrix} \\sin(w_kt),\u0026if\\ i=2k ( 偶数行 ) \\ \\cos(w_kt),\u0026if\\ i = 2k+1(奇数行) \\end{matrix}\\right.$$ 这里： $w_k = \\frac{1}{10000^{2k/d_{model}}}$ $i = 0,1,2,3,…,\\frac{d_{model}}{2} -1$\n注意：当使用 $w_k = \\frac{1}{10000^{2k/d_{\\text{model}}}}$作为位置编码的调节因子时，当 k 增大时，分母中的指数项会变得非常大，可能导致数值溢出或者数值精度问题。为了避免这种情况，可以使用其对数形式$-\\frac{\\log(10000.0)}{d_{\\text{model}}}$这样做有以下几个优点：\n数值稳定性： 对数形式避免了指数项过大导致的数值溢出或者数值精度问题。 计算效率： 对数形式的计算更加高效，避免了重复计算指数项。 一致性： 使用对数形式可以保持代码中的一致性，因为在其他部分可能也会涉及到对数形式的处理。 把512维的向量两两一组，每组都是一个sin和一个cos，这两个函数共享同一个频率$w_i$ ，一共有256组，由于我们从0开始编号，所以最后一组编号是255。sin/cos函数的波长（由 $w_i$ 决定）则从 $2\\pi$增长到 $2\\pi*10000$,下面是代码实现\nclass PositionalEncoding(nn.Module): def __init__(self, d_model, dropout, max_len=5000): \"\"\" 位置编码器类的初始化函数 共有三个参数，分别是 d_model：词嵌入维度 dropout: dropout触发比率 max_len：每个句子的最大长度 \"\"\" super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings # 注意下面代码的计算方式与公式中给出的是不同的，但是是等价的，你可以尝试简单推导证明一下。 # 这样计算是为了避免中间的数值计算结果超出float的范围， pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer('pe', pe) def forward(self, x): x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) return self.dropout(x) 确认是否维度越往后，是否波长越长\nplt.figure(figsize=(15, 5))\rpe = PositionalEncoding(20, 0)\ry = pe.forward(torch.zeros(1, 100, 20))\rplt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\rplt.legend([\"dim %d\"%p for p in [4,5,6,7]]) Transformer位置编码可视化 下图是一串序列长度为100，位置编码维度为512的位置编码可视化结果： 途中y轴表示单词的位置，从0开始到100，横坐标表示每个单词的512维度，颜色表示值，sin，cos函数的值在【-1，1】之间\n可以发现，由于sin/cos函数的性质，位置向量的每一个值都位于[-1, 1]之间。同时，纵向来看，图的右半边几乎都是黄色的，这是因为越往后的位置，频率越小，波长越长，所以不同的t对最终的结果影响不大。而越往左边走，颜色交替的频率越频繁。 代码：\nimport matplotlib.pyplot as plt\rimport numpy as np\r# 设置序列长度和模型维度\rsequence_length = 100 # 序列长度\rd_model = 512 # 模型维度\r# 初始化位置编码矩阵\rpositional_encoding = np.zeros((sequence_length, d_model))\r# 计算位置编码\rfor pos in range(sequence_length):\rfor i in range(d_model):\rif i % 2 == 0:\r# 偶数索引使用正弦函数\rpositional_encoding[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\relse:\r# 奇数索引使用余弦函数\rpositional_encoding[pos, i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\r# 绘制位置编码的图像\rplt.figure(figsize=(10, 8))\rplt.imshow(positional_encoding, cmap='hot', interpolation='nearest')\rplt.title('Positional Encoding')\rplt.xlabel('Depth')\rplt.ylabel('Position')\rplt.colorbar()\rplt.show()",
    "description": "什么是位置编码 在transformer的encoder和decoder的输入层中，使用了Positional Encoding，使得最终的输入满足： $$input = input_embedding + positional_encoding $$\nword embedding：理解参考\n这里，input_embedding是通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model，由于是相加关系，自然而然地，这里的positional_encoding也是一个d_model维度的向量。（在原论文里，d_model = 512） 注意：在Transformer模型中，“token”（标记）是指输入序列中的每个元素，它通常是一个单词、一个子词或一个字符，假设我们有一个句子：“The cat sat on the mat.\"，单词级别的标记： [“The”, “cat”, “sat”, “on”, “the”, “mat”, “.\"]。然后被转换成词嵌入（word embeddings）和位置嵌入（position embeddings），然后这两种嵌入会被相加起来形成输入嵌入（input embeddings）。这个输入嵌入会作为模型的输入，并传递到Transformer的神经网络中进行处理,token本身不会再作为数据传递到模型中。\nInput Embedding为什么解决的是语义问题，没有解决位置问题？？，语义不是有顺序才有吗？？\nInput Embedding (输入嵌入):\ninput_embedding 主要解决的是词汇语义的表示问题。通过将单词映射为连续的低维向量空间，词嵌入技术（如Word2Vec、GloVe等）可以捕获单词之间的语义关系，比如单词的近义词、反义词等。这使得神经网络在处理文本时能够更好地理解单词的含义，从而提高了对语义的建模能力。 但是，词嵌入并没有直接解决词序的问题。即使单词被嵌入到向量空间中，神经网络在处理这些向量时仍然不知道它们在句子中的位置。这就是为什么我们需要进一步引入位置编码的原因。 Positional Encoding (位置编码):\npositional_encoding 解决的是序列数据的位置信息丢失问题。在自然语言处理中，文本是由单词或字符组成的序列，这些单词的排列顺序对句子的含义至关重要。通过引入位置编码，我们可以向神经网络提供关于单词在序列中位置的信息，从而使网络能够区分不同位置的单词并更好地处理序列数据。 位置编码通常是与词嵌入相加的方式来融合位置信息和语义信息。这样，神经网络在处理输入数据时既能考虑单词的语义关系，又能考虑单词在句子中的位置关系，从而更全面地理解文本数据。 因此，input_embedding 和 positional_encoding 两者都是为了帮助神经网络更好地理解文本数据，但它们解决的是不同层面的问题：input_embedding 解决的是语义表示问题，而 positional_encoding 解决的是位置信息丢失问题。这两者结合起来能够提高神经网络对文本数据的建模能力。",
    "tags": [],
    "title": "Transformer模型详解02-Positional Encoding（位置编码）",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_02/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "简介 下图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 Multi-Head Attention，是由多个 Self-Attention组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add \u0026 Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。\n因为 Self-Attention是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。 基础知识 向量的内积 向量的内积是什么，如何计算，最重要的，其几何意义是什么？\n内积的计算方法是将两个向量对应分量相乘，然后将结果相加。 内积的几何意义是非常重要的。在二维空间中，两个向量的内积等于两个向量的模（长度）之积乘以它们之间的夹角的余弦值。具体来说，如果 θ 是两个向量之间的夹角，则它们的内积为： $$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| |\\mathbf{b}| \\cos(\\theta)$$ 这个公式表明，内积可以用来衡量两个向量的相似程度。当两个向量的夹角为 0时(cos0=1)，它们的内积取得最大值，表示它们的方向相同；当夹角为 90时(cos90=0)，内积为 0，表示它们的方向垂直；当夹角为180(cos180=-1) 时，内积取得最小值，表示它们的方向相反。\n矩阵与转置相乘 一个矩阵 与其自身的转置相乘，得到的结果有什么意义？ 矩阵的对称性指的是矩阵在某种变换下保持不变的性质。对称矩阵是一种特殊的矩阵，它满足以下性质：矩阵的转置等于它自身。 具体来说，对称矩阵 A 满足以下条件： $$\\mathbf{A} = \\mathbf{A}^\\intercal$$ 这意味着矩阵的主对角线上的元素保持不变，而其他元素关于主对角线对称。\n例如，如果一个矩阵 A 的元素为： $$\\mathbf{A} = \\begin{pmatrix} a \u0026 b \u0026 c \\ b \u0026 d \u0026 e \\ c \u0026 e \u0026 f \\end{pmatrix}$$ 矩阵中的元素对称于主对角线。 对称矩阵在数学和工程领域中非常重要，因为它们具有许多有用的性质，比如特征值都是实数、可以通过正交变换对角化等。在应用中，对称矩阵广泛用于描述对称系统、表示物理现象等。\n当一个矩阵与其自身的转置相乘时，得到的结果矩阵具有重要的性质，其中最显著的是结果矩阵是一个对称矩阵。这个性质在许多领域中都有重要的应用，比如在统计学中用于协方差矩阵的计算，以及在机器学习中用于特征提取和数据降维。\n让我们用一个具体的矩阵示例来演示这个性质。考虑一个 $3 \\times 2$的矩阵 的矩阵$\\mathbf{A}$： $$\\mathbf{A} = \\begin{pmatrix} 1 \u0026 2 \\ 3 \u0026 4 \\ 5 \u0026 6 \\end{pmatrix}$$ 首先，我们计算 A 的转置 $\\mathbf{A}^\\intercal$ $$\\mathbf{A}^\\intercal = \\begin{pmatrix} 1 \u0026 3 \u0026 5 \\ 2 \u0026 4 \u0026 6 \\end{pmatrix}$$ 然后，我们将 𝐴 相乘$\\mathbf{A}^\\intercal$，得到结果矩阵 $\\mathbf{A} \\mathbf{A}^\\intercal$ $$\\mathbf{A} \\mathbf{A}^\\intercal = \\begin{pmatrix} 1 \u0026 2 \\ 3 \u0026 4 \\ 5 \u0026 6 \\end{pmatrix} \\begin{pmatrix} 1 \u0026 3 \u0026 5 \\ 2 \u0026 4 \u0026 6 \\end{pmatrix} = \\begin{pmatrix} 5 \u0026 11 \u0026 17 \\ 11 \u0026 25 \u0026 39 \\ 17 \u0026 39 \u0026 61 \\end{pmatrix}$$\n可以观察到，结果矩阵 $\\mathbf{A} \\mathbf{A}^\\intercal$是一个对称矩阵。\nQ,K,V 在Transformer模型中，Q（Query）、K（Key）和V（Value）在注意力机制中起着关键作用。让我们通过一个简单的例子来理解它们的物理意义：\n假设我们要翻译一段文本，比如将英文句子 “The cat sat on the mat” 翻译成法文。在这个例子中，Q、K 和 V 可以被解释为：\nQuery（查询）：在翻译时，Query表示当前正在翻译的单词或者短语。例如，当我们尝试翻译 “sat” 这个词时，“sat” 就是当前的 Query。\nKey（键）：Key表示源语言（英文）中其他位置的信息，用于与当前 Query 进行比较。在翻译任务中，Key可以是源语言句子中的其他单词或者短语。比如，在翻译 “sat” 时，Key 可能是源语言句子中的 “The”、“cat”、“on” 等单词。\nValue（值）：Value包含了与 Key 相关的实际数值信息。在翻译任务中，Value 可以是源语言句子中与 Key 对应的词语的嵌入向量或者表示。比如，与 Key “The” 相关的 Value 可能是 “Le”，与 Key “cat” 相关的 Value 可能是 “chat”，等等。\n在注意力机制中，系统会计算当前 Query（如 “sat”）与所有 Key（如 “The”、“cat”、“on”）之间的相关性得分，然后使用这些得分对 Value（如 “Le”、“chat”）进行加权求和，以产生最终的翻译输出。这样，模型可以根据输入的 Query（即要翻译的单词或短语）选择性地关注源语言句子中与之相关的信息，并生成相应的翻译结果。\n通过上面的例子稍微理解Q,K,V概念后，对后续理解公式有帮助。\n什么是Attention 所谓Attention，顾名思义：注意力，意思是处理一个问题的时候把\"注意力\"放到重要的地方上。Attention思想其实是从人类的习惯中提取出来的。人们在第一次看一张照片的时候，第一眼一定落到这张照片的某个位置上，可能是个显著的建筑物，或者是一个有特点的人等等，总之，人们通常并没有看清图片的全部内容，而是将注意力集中在了图片的焦点上。\n2017年的某一天,Google 机器翻译团队发表了《Attention is All You Need》这篇论文，犹如一道惊雷，Attention横空出世了！（有一说一，这标题也太他喵嚣张了，不过人家有这个资本(oﾟ▽ﾟ)o ）\nAttention 机制最早是在计算机视觉里应用的，随后在NLP领域也开始应用了，真正发扬光大是在NLP领域，由于2018年GPT模型的效果显著，Transformer和Attention这些核心才开始被大家重点关注。\n下面举个例子上说明一下注意力和自注意力，可能不够严谨，但足以说明注意力和自注意力是什么了。 首先我们不去考虑得到注意力分数的细节，而是把这个操作认为是一个封装好的函数。比如定义为attention_score(a,b)，表示词a和b的注意力分数。现在有两个句子A=“you are beautiful”和B=“你很漂亮”，我们想让B句子中的词“你”更加关注A句子中的词“you”，该怎么做呢？答案是对于每一个A句子中的词，计算一下它与“you”的注意力分数。也就是把\nattention_score(“you”,“你”)\rattention_score(“are”,“你”)\rattention_score(“beautiful”,“你”) 都计算一遍，在实现attention_score这个函数的时候，底层的运算会让相似度比较大的两个词分数更高，因此attention_score(“you”,“你”)的分数最高，也相当于告诉了计算机，在对B句子中“你”进行某些操作的时候，你应该更加关注A句子中的“you”，而不是“are”或者“beautiful”。 以上这种方式就是注意力机制，两个不同的句子去进行注意力的计算。而当句子只有一个的时候，只能去计算自己与自己的注意力，这种方式就是自注意力机制。比如只看A句子，去计算\nattention_score(“you”,“you”)\rattention_score(“are”,“you”)\rattention_score(“beautiful”,“you”) 这种方式可以把注意力放在句子内部各个单词之间的联系，非常适合寻找一个句子内部的语义关系。\n再举个例子比如这句话“这只蝴蝶真漂亮，停在花朵上，我很喜欢它”，我们怎么知道这个“它”指的是“蝴蝶”还是“花朵”呢？答案是用自注意力机制计算出这个“它”和其他所有输入词的“分数”，这个“分数”一定程度上决定了其他单词与这个联系。可以理解成越相似的，分就越高（通过权重来控制）。通过计算，发现对于“它”这个字，“蝴蝶”比“花朵”打的分高。所以对于“它”来说，“蝴蝶”更重要，我们可以认为这个“它”指的就是蝴蝶。\nSelf Attention 原理 通俗易懂理解 在人类的理解中，对待问题是有明显的侧重。具体举个例子来说：“我喜欢踢足球，更喜欢打篮球。”，对于人类来说，显然知道这个人更喜欢打篮球。但对于深度学习来说，在不知道”更“这个字的含义前，是没办法知道这个结果的。所以在训练模型的时候，我们会加大“更”字的权重，让它在句子中的重要性获得更大的占比。比如： $$C(seq) = F(0.1d(我)，0.1d(喜)，…，0.8d(更)，0.2d(喜)，…) $$ 在知道了attention在机器学习中的含义之后（下文都称之为注意力机制）。人为设计的注意力机制，是非常主观的，而且没有一个准则来评定，这个权重设置为多少才好。所以，如何让模型自己对变量的权重进行自赋值成了一个问题，这个权重自赋值的过程也就是self-attention。\n定义：假设有四个输入变量$a^1$，$a^2$，$a^3$，$a^4$，希望它们经过一个self-attention layer之后变为$b^1$，$b^2$，$b^3$，$b^4$ 拿$a^1$和$b^1$做例子,$b^1$这个结果是综合了$a^1$，$a^2$，$a^3$，$a^4$而得出来的一个结果。既然得到一个b 是要综合所有的a才行，那么最直接的做法就是$a^1$与$a^2$，$a^3$，$a^4$ 都做一次运算，得到的结果就代表了这个变量的注意力系数。直接做乘法太暴力了，所以选择一个更柔和的方法：引入三个变量$W^q$,$W^k$,$W^v$这三个变量与$a^1$相乘得到$q^1$,$k^1$,$v^1$ 同样的方法对$a^2$，$a^3$，$a^4$都做一次,至于这里的q , k , v具体代表什么，下面就慢慢展开讲解。 然后拿自己的q与别人的k相乘就可以得到一个系数$\\alpha$。这里$q^1$在和其他的k做内积时，可近似的看成是在做相似度计算(前面基础向量的内积)。比如： $$ \\alpha_{1,1} =q^1\\cdot k^1\\ \\alpha_{1,2} =q^1\\cdot k^2\\ \\alpha_{1,3} =q^1\\cdot k^3\\ \\alpha_{1,4} =q^1\\cdot k^4\\ $$\n在实际的神经网络计算过程中，还得除于一个缩放系数$\\sqrt{d}$这个d是指q和k的维度,因为q和k会做内积，所以维度是一样的。之所以要除$\\sqrt{d}$​，是因为做完内积之后，，$\\alpha$会随着它们的维度增大而增大，除$\\sqrt{d}$相当于标准化。 得到了四个$\\alpha$之后，我们分别对其进行softmax，得到四个 $\\hat{\\alpha}_1$，增加模型的非线性。 四个$\\alpha$分别是 $\\hat{\\alpha}_1$,$\\hat{\\alpha}2$,$\\hat{\\alpha}3$,$\\hat{\\alpha}4$,别忘了还有我们一开始计算出来的 ${v}^1$,${v}^2$,${v}^3$,${v}^4$，直接把各个$\\hat{\\alpha}1$直接与各个a 相乘不就得出了最后的结果了吗？虽然这么说也没错，但为了增加网络深度，将a变成v也可以减少原始的a对最终注意力计算的影响。 那么距离最后计算出$b^1$只剩最后一步，我们将所有的$\\hat{\\alpha}1$ 与所有的v分别相乘，然后求和，就得出$b^1$啦！具体计算如下： $$b^1=\\hat{\\alpha}{1,1}*v^1+\\hat{\\alpha}{1,2}*v^2+\\hat{\\alpha}{1,3}*v^3+\\hat{\\alpha}{1,4}*v^4 $$ 公式简化为： $$b^1=\\sum_i\\hat{\\alpha}{1,i}*v^i $$ 同样的计算过程，我们对剩下的a都进行一次，就可以得到$b^2$,$b^3$,$b^4$,每个b都是综合了每个a之间的相关性计算出来的，这个相关性就是我们所说的注意力机制,。那么我们将这样的计算层称为self-attention layer。 我们把一个句子中的每个字代入上图的 $x^1$，$x^2$ ,$x^3$， $x^4$\n矩阵计算 通过注意力计算出来的结果是每个位置单词的上下文表示，每个位置的上下文表示是指在自注意力机制中，通过将每个位置的词嵌入向量与注意力权重进行加权求和，得到的每个位置的语义表示。这个语义表示包含了输入序列中每个位置的语义信息，经过加权后更加全局和丰富。\n举个例子来说明：\n假设我们有一个输入序列：“The cat sat on the mat.\"，并且使用 Transformer 模型进行编码，其中每个单词对应一个位置。在自注意力机制中，模型会计算每个位置对其他位置的注意力权重，然后将这些权重与对应位置的词嵌入向量进行加权求和，得到每个位置的上下文表示。\n考虑位置 3，对应单词 “sat”。在计算注意力权重时，模型会考虑 “sat” 与其他单词之间的关联程度。假设在这个例子中，“sat” 与 “cat”、“mat” 之间有较高的注意力权重，而与 “on” 的关联较低。因此，经过加权求和后，位置 3 的上下文表示将会强调 “sat” 与 “cat”、“mat” 之间的语义关系。\n通过这种方式，每个位置的上下文表示会受到整个输入序列中所有位置的影响，从而更好地捕捉输入序列的语义结构和信息。\n上图是 Self-Attention 的结构，在计算的时候需要用到矩阵Q(查询),K(键值),V(值)。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而Q,K,V正是通过 Self-Attention 的输入进行线性变换得到的。\nQ，K，V计算 输入矩阵X=position encoding+word embedding,其中维度d_model，行为句子中单词的个数。 需要知道x的格式参考：Word2Vec实例\nSelf-Attention 的输入用矩阵X进行表示，则可以使用线性变阵矩阵WQ,WK,WV计算得到Q,K,V。计算 如下图所示，注意 X, Q, K, V 的每一行都表示一个单词，WQ，WK，WV是一个d_model(输入矩阵的列)行的线性变阵参数，X的每一行都会都会有自己的QKV，比如$X_1$对应$Q_1,K_1,V_1$,即$X_n$对应$Q_n,K_n,V_n$，所以 X, Q, K, V 的每一行都表示一个单词。 这里通过线性变换的Q,K,V的物理意义是包含了原始数据的信息，可能关注的特征点不一样。\nSelf-Attention 的输出 得到矩阵 Q, K, V之后就可以计算出 Self-Attention 的输出了，计算的公式如下： 公式中计算矩阵Q和K每一行向量的内积，为了防止内积过大，因此除以$d_k$的平方根，缩放注意力，以使得注意力分布的方差在不同维度上保持一致，从而更好地控制梯度的稳定性。。Q乘以K的转置后，得到的矩阵行列数都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为Q乘以$K^T$, 1234 表示的是句子中的单词。 $QK^T$其实终算出来的是，每一个单词与其他所有的单词的注意力系数，因为Q代表单词本身假如 我有一只猫，分词后1=我，2=有，3=一只，4=猫。 因为K代表其他单词，转至相乘最终$QK^T$矩阵的(1,1)这个各自就是标识我和我的注意力权重，(1,2)就是我和有的注意力权重，(2,4)就是有和猫的注意力权重\n得到$QK^T$之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1. 得到 Softmax 矩阵之后可以和V相乘，得到最终的输出Z。 上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出$Z_1$等于所有单词 i 的值$V_i$根据 attention 系数的比例加在一起得到，如下图所示： 这里算出来的$Z_1$第一行就是第一个单词和其他单词的注意力系数权重，\n优势 从self-attention的原理中可以看出，这一层需要学习的参数只有$W^q$ ,$W^k$, $W^v$，大部分变量来自于内部计算得出来的，所以它的参数量少但每个参数所涵盖的信息多，这是它的第一个优点。 每个b的计算都是独立的，这一点相比之前的RNN来说很不一样，RNN是需要等前面的$a^1$算完了才能算$a^2$，是串行的。所以RNN无论是训练还是推理，都会因为不能计算并行而变慢，这是它的的第二个优点。 RNN的一个最大的问题是：前面的变量在经过多次RNN计算后，已经失去了原有的特征。越到后面，最前面的变量占比就越小，这是一个很反人类的设计。而self-attention在每次计算中都能保证每个输入变量 a的初始占比是一样的，这样才能保证经过self-attention layer计算后他的注意力系数是可信的。 所以总结下来，它的三个优点分别是：\n需要学习的参数量少 可以并行计算 能够保证每个变量初始占比是一样的 代码实现 import torch\rimport torch.nn as nn\rimport torch.nn.functional as F\rclass SelfAttention(nn.Module):\rdef __init__(self, embed_size, num_heads):\r\"\"\"\r初始化 SelfAttention 层\r参数:\rembed_size (int): 输入特征的维度\rnum_heads (int): 注意力头的数量\r\"\"\"\rsuper(SelfAttention, self).__init__()\rself.embed_size = embed_size\rself.num_heads = num_heads\rself.head_dim = embed_size // num_heads\r# 确保 embed_size 能被 num_heads 整除\rassert (\rself.head_dim * num_heads == embed_size\r), \"Embedding size needs to be divisible by heads\"\r# 初始化线性层\rself.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\rself.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\rself.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\rself.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\rdef forward(self, values, keys, query):\r\"\"\"\r前向传播函数\r参数:\rvalues (Tensor): 值的张量，形状为 (batch_size, value_len, embed_size)\rkeys (Tensor): 键的张量，形状为 (batch_size, key_len, embed_size)\rquery (Tensor): 查询的张量，形状为 (batch_size, query_len, embed_size)\r返回:\rout (Tensor): 输出张量，形状为 (batch_size, query_len, embed_size)\r\"\"\"\r# 获取张量的大小\rN = query.shape[0]\rvalue_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\r# 将输入张量按头数和头维度进行切分\rvalues = values.reshape(N, value_len, self.num_heads, self.head_dim)\rkeys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\rqueries = query.reshape(N, query_len, self.num_heads, self.head_dim)\r# 通过线性层进行变换\rvalues = self.values(values)\rkeys = self.keys(keys)\rqueries = self.queries(queries)\r# 计算点积注意力\renergy = torch.einsum(\"nqhd,nkhd-\u003enhqk\", [queries, keys]) # batch_size, num_heads, query_len, key_len\r# 计算注意力权重\rattention = torch.nn.functional.softmax(energy / (self.embed_size ** (1/2)), dim=3)\r# 将注意力权重应用到值上\rout = torch.einsum(\"nhql,nlhd-\u003enqhd\", [attention, values]).reshape(\rN, query_len, self.num_heads * self.head_dim\r)\r# 合并多个头并通过线性层进行变换\rout = self.fc_out(out)\rreturn out Multi-head self-attention 为什么要多头 在多头注意力机制中，每个注意力头学习不同的特征表示，这是为了提高模型的表征能力和泛化能力。这种设计允许模型在不同抽象级别上关注输入的不同部分，从而更好地捕获输入之间的关系。\n具体来说，每个注意力头都有自己的权重矩阵（通常是通过学习得到的），这些权重矩阵决定了每个头对输入的不同部分的关注程度。通过允许多个头并且每个头学习不同的特征表示，模型可以同时关注输入的不同方面，从而更好地捕获输入之间的复杂关系。\n举例来说，考虑一个用于自然语言处理的 Transformer 模型。在这种情况下，每个注意力头可以学习关注句子中的不同单词或短语，其中一些头可能更关注主语-谓语关系，另一些头可能更关注宾语-谓语关系，而其他头可能关注句子中的修饰词或者语法结构等。通过允许每个头学习不同的特征表示，模型可以更好地捕获句子中不同部分之间的语义关系，从而提高了模型的性能。\n总的来说，多头注意力机制允许模型以多个不同的视角来观察输入数据，从而提高了模型对输入数据的表征能力和泛化能力。\n原理 通俗易懂理解 multi-head self-attention，所谓head也就是指一个a 衍生出几个q , k , v 。上述所讲解的self-attention是基于single-head的。以2 head为例： 首先，$a^i$先生成$q^1$,$k^1$,$v^1$,然后，接下来就和single-head不一样了，$q^i$生成$q^{i,1},q^{i,2}$生成的方式有两种：\n$q^i$乘上一个$W^{q,1}$得到$q^{i,2}$，这个和single-head的生成是差不多的； $q^i$直接从通道维，平均拆分成两个，得到$q^{i,1},q^{i,2}$ 这两种方式，在最后结果上都差不多。至于为啥，后面会讲一下原因。 那么这里的图解使用第1个方式，先得到$q^{i,1}$,$k^{i,1}$,$v^{i,1}$。对$a^j$做同样的操作得到 ,对$a^j$做同样的操作得到$q^{j,1}$,$k^{j,1}$,$v^{j,1}$。这边需要注意的一点，$q^{i,1}$是要和$k^{j,1}$做矩阵乘法，而非$k^{j,2}$，一一对应。后面计算就和single-head一样了，最后得到$b^{i,1}$ 第二步，对$q^{i,2}$,$k^{i,2}$,$v^{i,2}$做一样的操作，得到$b^{i,2}$ 这里我们算出的$b^{i,1}$,$b^{i,2}$是同维度的，我们可以将其concat在一起，再通过一个$W^0$把他转成想要的维度。这也就不难理解，为什么说multi-head的两种生成方式是一样的，因为最终决定是输出维度的是$W^o$。我们可以将multi-head的过程看成是cnn中的隐藏层，multi-head的数量也就对应着Conv2D的filter数量，每一个head各司其职，提取不同的特征。 矩阵计算 我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 Z，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。 从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入X分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵Z。下图是 h=8 时候的情况，此时会得到 8 个输出矩阵Z。 得到 8 个输出矩阵$Z_1$到$Z_8$之后，Multi-Head Attention 将它们拼接在一起 (Concat)，然后传入一个Linear层，得到 Multi-Head Attention 最终的输出Z。 可以看到 Multi-Head Attention 输出的矩阵Z与其输入的矩阵X的维度是一样的。\n代码实现 import torch\rimport torch.nn.functional as F\rclass MultiHeadSelfAttention(torch.nn.Module):\rdef __init__(self, d_model, num_heads):\rsuper(MultiHeadSelfAttention, self).__init__()\rself.num_heads = num_heads\rself.d_model = d_model\rassert d_model % num_heads == 0 # 确保 d_model 可以被 num_heads 整除\r# 初始化 Q、K、V 矩阵和输出矩阵\rself.W_q = torch.nn.Linear(d_model, d_model)\rself.W_k = torch.nn.Linear(d_model, d_model)\rself.W_v = torch.nn.Linear(d_model, d_model)\rself.W_o = torch.nn.Linear(d_model, d_model)\rdef forward(self, x):\rbatch_size, seq_len, d_model = x.size()\r# 将输入 x 拆分成 num_heads 个头\rQ = self.W_q(x).view(batch_size, seq_len, self.num_heads, d_model // self.num_heads)\rK = self.W_k(x).view(batch_size, seq_len, self.num_heads, d_model // self.num_heads)\rV = self.W_v(x).view(batch_size, seq_len, self.num_heads, d_model // self.num_heads)\r# 对每个头进行 scaled dot-product attention\rattention_scores = torch.matmul(Q, K.transpose(1, 2)) / (d_model ** 0.5)\rattention_probs = F.softmax(attention_scores, dim=-1)\rattention_output = torch.matmul(attention_probs, V)\r# 将每个头的输出拼接起来\rattention_output = attention_output.view(batch_size, seq_len, d_model)\r# 经过线性变换得到最终的输出\routput = self.W_o(attention_output)\rreturn output\r# 为了测试\rd_model = 512 # 模型的维度\rnum_heads = 8 # 头的数量\rseq_len = 10 # 序列长度\rbatch_size = 4 # 批次大小\r# 创建一个随机输入张量\rx = torch.rand(batch_size, seq_len, d_model)\r# 创建 Multi-Head Self-Attention 模块并进行前向传播\rmultihead_attention = MultiHeadSelfAttention(d_model, num_heads)\routput = multihead_attention(x)\r# 打印输出张量的形状\rprint(\"Output shape:\", output.shape)",
    "description": "简介 下图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 Multi-Head Attention，是由多个 Self-Attention组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add \u0026 Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。\n因为 Self-Attention是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。 基础知识 向量的内积 向量的内积是什么，如何计算，最重要的，其几何意义是什么？\n内积的计算方法是将两个向量对应分量相乘，然后将结果相加。 内积的几何意义是非常重要的。在二维空间中，两个向量的内积等于两个向量的模（长度）之积乘以它们之间的夹角的余弦值。具体来说，如果 θ 是两个向量之间的夹角，则它们的内积为： $$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| |\\mathbf{b}| \\cos(\\theta)$$ 这个公式表明，内积可以用来衡量两个向量的相似程度。当两个向量的夹角为 0时(cos0=1)，它们的内积取得最大值，表示它们的方向相同；当夹角为 90时(cos90=0)，内积为 0，表示它们的方向垂直；当夹角为180(cos180=-1) 时，内积取得最小值，表示它们的方向相反。",
    "tags": [],
    "title": "Transformer模型详解03-Self-Attention（自注意力机制）",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_03/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "简介 Transformer 模型中的 Encoder 层主要负责将输入序列进行编码，将输入序列中的每个词或标记转换为其对应的向量表示，并且捕获输入序列中的语义和关系。\n具体来说，Transformer Encoder 层的作用包括：\n词嵌入（Word Embedding）：将输入序列中的每个词或标记映射为其对应的词嵌入向量。这些词嵌入向量包含了词语的语义信息，并且可以在模型中进行学习。\n位置编码（Positional Encoding）：因为 Transformer 模型不包含任何关于序列顺序的信息，为了将位置信息引入模型，需要添加位置编码。位置编码是一种特殊的向量，用于表示输入序列中每个词的位置信息，以便模型能够区分不同位置的词。\n多头自注意力机制（Multi-Head Self-Attention）：自注意力机制允许模型在处理每个词时，同时考虑到输入序列中所有其他词之间的关系。多头自注意力机制通过将输入进行多次线性变换并计算多组注意力分数，从而允许模型在不同的表示子空间中学习到不同的语义信息。\n残差连接（Residual Connection）：为了减轻梯度消失和加速训练，Transformer Encoder 层使用了残差连接。残差连接允许模型直接学习到输入序列的增量变换，而不是完全替代原始输入。\n层归一化（Layer Normalization）：在残差连接后应用层归一化，有助于提高模型的训练稳定性，加快训练速度。\nTransformer Encoder 层的主要作用是将输入序列转换为其对应的向量表示，并且捕获输入序列中的语义和位置信息，以便后续的模型能够更好地理解和处理输入序列。\n前面我们已经详解了三个点的计算过程，现在了解一下 Add \u0026 Norm 和 Feed Forward 部分。\n基础知识 归一化 归一化是将数据转换为具有统一尺度的过程，常用于机器学习、数据挖掘和统计分析中。归一化可以确保不同特征或变量之间具有相似的数值范围，有助于提高模型的性能和收敛速度。\n作用 让我用一个简单的例子来说明归一化的作用。\n假设你有一个数据集，其中包含两个特征：年龄和收入。年龄的范围是 0 到 100 岁，而收入的范围是 1000 到 100000 美元。这两个特征的范围差异很大。\n现在，你想要使用这些特征来训练一个机器学习模型，比如线性回归模型，来预测一个人是否会购买某种产品。由于特征的范围差异较大，这可能会导致某些问题：\n收入的范围比年龄大得多，这可能会使得模型过度关注收入而忽略年龄，因为收入的变化可能会对预测产生更大的影响。 模型可能会受到数值范围的影响，而不是特征本身的重要性。 这时候，归一化就可以派上用场了。通过归一化，你可以将不同特征的值缩放到相似的范围内，从而消除数值范围差异带来的影响。比如，你可以将年龄和收入都缩放到 0 到 1 之间的范围内，或者使用其他归一化方法，如标准化 (standardization)。\n通过归一化，你可以确保模型不会因为特征值的范围差异而偏向某个特定的特征，而是可以更平衡地利用所有的特征信息来进行预测。\n常用归一化 下面是几种常用的归一化方式及其公式：\nMin-Max 归一化： Min-Max 归一化将数据线性映射到一个指定的范围内，通常是 [0, 1] 或 [-1, 1]。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X - X_{\\text{min}}}}{{X_{\\text{max}} - X_{\\text{min}}}}]$$\n其中，$(X_{\\text{norm}})$ 是归一化后的数据，(X) 是原始数据，$(X_{\\text{min}})$ 和 $(X_{\\text{max}})$分别是数据的最小值和最大值。\nZ-Score 标准化： Z-Score 标准化将数据转换为均值为 0，标准差为 1 的正态分布。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X - \\mu}}{{\\sigma}}]$$\n其中，$(X_{\\text{norm}})$是归一化后的数据，$(X)$ 是原始数据，$\\mu$是数据的均值，$(\\sigma)$是数据的标准差。\nDecimal Scaling 归一化： Decimal Scaling 归一化将数据缩放到[-1,1]或者[0,1]的范围内，通过除以数据中的最大绝对值来实现。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X}}{{\\max(|X|)}}]$$\n其中，$(X_{\\text{norm}})$ 是归一化后的数据，$(X)$ 是原始数据，$(\\max(|X|))$ 是数据中的最大绝对值。\nRobust Scaling： Robust Scaling 是一种针对离群值鲁棒的归一化方法，通过除以数据的四分位距（IQR）来缩放数据。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X - Q_1}}{{Q_3 - Q_1}}]$$\n其中，$(X_{\\text{norm}})$ 是归一化后的数据，$(X)$是原始数据，$(Q_1)$ 是数据的第一四分位数（25th percentile），$(Q_3)$ 是数据的第三四分位数（75th percentile）。\n这些是常用的归一化方式，选择适合你的数据和模型的归一化方法可以提高模型的性能和稳定性。\n残差连接 残差连接（Residual Connection）是一种在深度神经网络中用于解决梯度消失和梯度爆炸问题的技术。它通过将输入直接添加到神经网络的某些层的输出中，从而允许梯度直接通过残差路径传播，减轻了梯度消失的问题，加速了训练过程。\n具体来说，假设我们有一个包含多个层的神经网络，每个层都由输入 $x$ 经过一些变换 $F(x)$得到输出 $H(x)$。传统的神经网络会直接将 $H(x)$ 作为下一层的输入，而残差连接则是将 $x$ 与 $H(x)$ 相加，即 $H(x)+x$，然后再输入到下一层。这样做可以使得网络学习到的变换是相对于输入的增量，而不是完全替代原始输入。\n残差连接的作用包括：\n缓解梯度消失：通过保留原始输入的信息，使得梯度可以更容易地传播到较浅层，从而减轻了梯度消失问题。 加速训练：残差连接可以使得神经网络更快地收敛，因为它减少了训练过程中的信息丢失。 提高模型性能：残差连接使得神经网络可以更深，更复杂，从而能够更好地捕捉输入数据的特征和模式。 举个例子，考虑一个包含残差连接的深度残差网络（Residual Network，ResNet）。在这个网络中，每个残差块都由两个或多个卷积层组成，其中第一个卷积层产生特征图 $H(x)$，而第二个卷积层则对 $H(x)$ 进行进一步变换。然后，原始输入 $x$ 被添加到 $H(x)$ 上，得到 $F(x)=H(x)+x$。这样，输出 $F(x)$ 就包含了相对于输入 $x$ 的增量，网络可以更轻松地学习到残差部分，从而更有效地优化模型。\nAdd \u0026 Norm Add \u0026 Norm 层由 Add 和 Norm 两部分组成，其计算公式如下： 第一个Add\u0026Norm中Add指 X+MultiHeadAttention(X)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 ResNet 中经常用到： Norm指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。\nFeed Forward Feed Forward 层比较简单，是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下。 $$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$ 也就是： 在这个公式中：\n$(X)$ 是输入的隐藏表示，维度为 $(d_{\\text{model}})$，是Add\u0026Norm输出； $(W_1)$ 和 $(W_2)$ 是权重矩阵，分别用于第一层和第二层的线性变换，维度分别为 $(d_{\\text{model}} \\times d_{\\text{ff}})$ 和 $(d_{\\text{ff}} \\times d_{\\text{model}})$； $(b_1)$ 和 $(b_2)$ 是偏置项； $(\\text{ReLU})$ 表示修正线性单元，是一种非线性激活函数，用于引入模型的非线性性。 Feed Forward 最终得到的输出矩阵的维度与X一致。\nFeed Forward 层在深度学习模型中具有重要意义，它主要有以下几个方面的作用：\n特征变换与组合： Feed Forward 层通过线性变换和非线性激活函数将输入数据进行特征变换和组合，使得模型能够学习到更高级、更复杂的特征表示。这有助于模型更好地理解数据的内在结构和规律。\n引入非线性： 非线性激活函数（如 ReLU、sigmoid、tanh 等）可以引入非线性变换，从而使得模型能够学习到非线性关系，提高模型的表达能力。如果没有非线性变换，多个线性变换的组合仍然只会得到线性变换，模型的表达能力将受到限制。\n增加模型的深度： Feed Forward 层通常是深度神经网络中的一个组成部分，通过堆叠多个 Feed Forward 层可以构建深度模型。深度模型能够学习到更多层次、更抽象的特征表示，从而提高模型的性能和泛化能力。\n提高模型的泛化能力： Feed Forward 层通过特征变换和非线性变换有助于模型学习到数据的高级抽象表示，这有助于提高模型对新样本的泛化能力，使得模型更好地适应未见过的数据。\n组成 Encoder 通过上面描述的 Multi-Head Attention, Feed Forward, Add \u0026 Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵$X_(nd)$, 并输出一个矩阵$O_(nd)$,通过多个 Encoder block 叠加就可以组成 Encoder。 第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是编码信息矩阵 C，这一矩阵后续会用到 Decoder 中。 代码实现 import torch\rimport torch.nn as nn\rimport torch.nn.functional as F\rclass TransformerEncoderLayer(nn.Module):\rdef __init__(self, d_model, n_heads, d_ff, dropout=0.1):\rsuper(TransformerEncoderLayer, self).__init__()\rself.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\rself.linear1 = nn.Linear(d_model, d_ff)\rself.linear2 = nn.Linear(d_ff, d_model)\rself.dropout = nn.Dropout(dropout)\rself.norm1 = nn.LayerNorm(d_model)\rself.norm2 = nn.LayerNorm(d_model)\rdef forward(self, src, src_mask=None):\r# Multi-head self-attention\rsrc2 = self.self_attn(src, src, src, attn_mask=src_mask)[0]\rsrc = src + self.dropout(src2)\rsrc = self.norm1(src)\r# Feed Forward Layer\rsrc2 = self.linear2(F.relu(self.linear1(src)))\rsrc = src + self.dropout(src2)\rsrc = self.norm2(src)\rreturn src\rclass TransformerEncoder(nn.Module):\rdef __init__(self, num_layers, d_model, n_heads, d_ff, dropout=0.1):\rsuper(TransformerEncoder, self).__init__()\rself.layers = nn.ModuleList([TransformerEncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\rdef forward(self, src, src_mask=None):\rfor layer in self.layers:\rsrc = layer(src, src_mask)\rreturn src",
    "description": "简介 Transformer 模型中的 Encoder 层主要负责将输入序列进行编码，将输入序列中的每个词或标记转换为其对应的向量表示，并且捕获输入序列中的语义和关系。\n具体来说，Transformer Encoder 层的作用包括：\n词嵌入（Word Embedding）：将输入序列中的每个词或标记映射为其对应的词嵌入向量。这些词嵌入向量包含了词语的语义信息，并且可以在模型中进行学习。\n位置编码（Positional Encoding）：因为 Transformer 模型不包含任何关于序列顺序的信息，为了将位置信息引入模型，需要添加位置编码。位置编码是一种特殊的向量，用于表示输入序列中每个词的位置信息，以便模型能够区分不同位置的词。\n多头自注意力机制（Multi-Head Self-Attention）：自注意力机制允许模型在处理每个词时，同时考虑到输入序列中所有其他词之间的关系。多头自注意力机制通过将输入进行多次线性变换并计算多组注意力分数，从而允许模型在不同的表示子空间中学习到不同的语义信息。\n残差连接（Residual Connection）：为了减轻梯度消失和加速训练，Transformer Encoder 层使用了残差连接。残差连接允许模型直接学习到输入序列的增量变换，而不是完全替代原始输入。\n层归一化（Layer Normalization）：在残差连接后应用层归一化，有助于提高模型的训练稳定性，加快训练速度。\nTransformer Encoder 层的主要作用是将输入序列转换为其对应的向量表示，并且捕获输入序列中的语义和位置信息，以便后续的模型能够更好地理解和处理输入序列。\n前面我们已经详解了三个点的计算过程，现在了解一下 Add \u0026 Norm 和 Feed Forward 部分。\n基础知识 归一化 归一化是将数据转换为具有统一尺度的过程，常用于机器学习、数据挖掘和统计分析中。归一化可以确保不同特征或变量之间具有相似的数值范围，有助于提高模型的性能和收敛速度。\n作用 让我用一个简单的例子来说明归一化的作用。\n假设你有一个数据集，其中包含两个特征：年龄和收入。年龄的范围是 0 到 100 岁，而收入的范围是 1000 到 100000 美元。这两个特征的范围差异很大。\n现在，你想要使用这些特征来训练一个机器学习模型，比如线性回归模型，来预测一个人是否会购买某种产品。由于特征的范围差异较大，这可能会导致某些问题：\n收入的范围比年龄大得多，这可能会使得模型过度关注收入而忽略年龄，因为收入的变化可能会对预测产生更大的影响。 模型可能会受到数值范围的影响，而不是特征本身的重要性。 这时候，归一化就可以派上用场了。通过归一化，你可以将不同特征的值缩放到相似的范围内，从而消除数值范围差异带来的影响。比如，你可以将年龄和收入都缩放到 0 到 1 之间的范围内，或者使用其他归一化方法，如标准化 (standardization)。\n通过归一化，你可以确保模型不会因为特征值的范围差异而偏向某个特定的特征，而是可以更平衡地利用所有的特征信息来进行预测。\n常用归一化 下面是几种常用的归一化方式及其公式：\nMin-Max 归一化： Min-Max 归一化将数据线性映射到一个指定的范围内，通常是 [0, 1] 或 [-1, 1]。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X - X_{\\text{min}}}}{{X_{\\text{max}} - X_{\\text{min}}}}]$$",
    "tags": [],
    "title": "Transformer模型详解04-Encoder 结构",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_04/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "@[toc]\n简介 Transformer 模型由编码器（Encoder）和解码器（Decoder）两部分组成。这里我会着重描述解码器的结构以及在预训练、输入输出和预测时的输入输出。\n解码器结构：\n自注意力层（Self-Attention Layers）：与编码器类似，解码器也包含多个自注意力层，用于在解码器端对输出序列的不同位置进行关注，解码器中的自注意力层被修改为接受一个遮盖（masking）向量，以便在计算注意力权重时将未来的信息屏蔽掉，只关注当前位置之前的信息。。\n编码器-解码器注意力层（Encoder-Decoder Attention Layers）：除了自注意力层外，解码器还包含编码器-解码器注意力层，用于将编码器端的信息与解码器端的信息进行交互，帮助解码器更好地理解输入序列。\n前馈神经网络（Feed-Forward Neural Networks）：与编码器一样，解码器也包含前馈神经网络层，用于对特征进行映射和转换。\n位置编码（Positional Encoding）：解码器也需要位置编码来将位置信息融入模型中，以便模型能够理解输入序列的顺序信息。\nDecoder在预训练、输入输出和预测时的输入输出：\n预训练：\n输入：在预训练期间，解码器的输入通常是由目标序列（target sequence）以及可选的编码器端输出的上下文信息组成。这些输入经过嵌入（embedding）和位置编码后，被送入解码器中。 输出：解码器预训练的目标是生成目标序列的下一个词的概率分布。因此，在每个时间步，解码器会生成一个预测概率分布，以便训练模型。 输入输出：\n输入：在进行输入输出（Inference）时，解码器的输入通常是由上一个时间步生成的词以及编码器端的上下文信息组成。这些输入通过嵌入和位置编码后，传递给解码器。 输出：解码器在每个时间步生成的输出通常是一个概率分布，用于预测下一个词的概率。根据应用场景，可以使用不同的策略（如贪婪搜索、束搜索等）来选择最终的输出序列。 预测：\n输入：在预测阶段，解码器的输入通常是由起始符号（如）以及编码器端的上下文信息组成。这些输入经过嵌入和位置编码后，传递给解码器。 输出：解码器生成的输出是一个概率分布，用于预测下一个词的概率。根据应用需求，可以根据生成的概率分布采样得到最终的预测结果。 结构 上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：\n包含两个 Multi-Head Attention 层。 第一个 Multi-Head Attention 层采用了 Masked 操作。 第二个 Multi-Head Attention 层的K, V矩阵使用 Encoder 的编码信息矩阵C进行计算，而Q使用上一个 Decoder block 的输出计算。 最后有一个 Softmax 层计算下一个翻译单词的概率。 先理解:自注意力的计算过程\n原理 第一个 Multi-Head Attention Decoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 “我有一只猫” 翻译成 “I have a cat” 为例，了解一下 Masked 操作。\n下面的描述中使用了类似 Teacher Forcing 的概念，不熟悉 Teacher Forcing 的童鞋可以参考以下上一篇文章Seq2Seq 模型详解。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 “” 预测出第一个单词为 “I”，然后根据输入 “ I” 预测下一个单词 “have”。 Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 ( I have a cat) 和对应输出 (I have a cat ) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 “ I have a cat \"。\n第一步：是 Decoder 的输入矩阵和 Mask 矩阵，输入矩阵包含 “ I have a cat” (0, 1, 2, 3, 4) 五个单词的表示向量，Mask 是一个 5×5 的矩阵。在 Mask 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。 第二步：接下来的操作和之前的 Self-Attention 一样，通过输入矩阵X计算得到Q,K,V矩阵。然后计算Q和$K^T$的乘积$QK^T$ 第三步：在得到 $QK^T$之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用Mask矩阵遮挡住每一个单词之后的信息，遮挡操作如下： 得到 Mask $QK^T$之后在 Mask$QK^T$上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。 第四步：使用 Mask $QK^T$与矩阵 V相乘，得到输出 Z，则单词 1 的输出向量 $Z_1$是只包含单词 1 信息的。 第五步：通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 $Z_i$，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出$Z_i$然后计算得到第一个 Multi-Head Attention 的输出Z，Z与输入X维度一样。\n第二个 Multi-Head Attention Decoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 K, V矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 Encoder 的编码信息矩阵 C 计算的。\n根据 Encoder 的输出 C计算得到 K, V，根据上一个 Decoder block 的输出 Z 计算 Q (如果是第一个 Decoder block 则使用输入矩阵 X 进行计算)，后续的计算方法与之前描述的一致。\n这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 Mask)。\n什么使用Encoder计算k,v decoder计算Q\n在 Transformer 模型的解码器中，使用了编码器的键（key）和值（value），而使用解码器的查询（query）。这种结构是为了充分利用编码器端对输入序列的理解，同时使得解码器端能够更好地根据自身生成的部分序列来做出决策。这种设计的物理意义可以从以下几个方面来理解：\n利用编码器的上下文信息：编码器对输入序列进行编码，生成了对输入序列全局理解的表示。因此，使用编码器的键和值可以提供丰富的上下文信息，帮助解码器更好地理解输入序列。\n解码器的自注意力：解码器的自注意力机制中，查询用于计算注意力权重，而键和值则用于构建注意力分布。使用解码器的查询意味着模型在计算注意力时更关注当前正在生成的部分序列，这有助于确保生成的序列在语法和语义上的连贯性。\n解耦编码器和解码器：使用不同的键、值和查询将编码器和解码器的功能分开，使得模型更具灵活性和泛化能力。解码器可以独立地根据当前正在生成的序列来调整自己的注意力，而不受编码器端信息的限制。\n总之，通过在解码器中使用编码器的键和值，以及使用解码器的查询，Transformer 模型能够更好地利用编码器端对输入序列的理解，并在解码器端根据当前正在生成的序列来做出决策，从而提高了生成序列的质量和连贯性。\nSoftmax 预测输出单词 Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下： Softmax 根据输出矩阵的每一行预测下一个单词： 这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。",
    "description": "@[toc]\n简介 Transformer 模型由编码器（Encoder）和解码器（Decoder）两部分组成。这里我会着重描述解码器的结构以及在预训练、输入输出和预测时的输入输出。\n解码器结构：\n自注意力层（Self-Attention Layers）：与编码器类似，解码器也包含多个自注意力层，用于在解码器端对输出序列的不同位置进行关注，解码器中的自注意力层被修改为接受一个遮盖（masking）向量，以便在计算注意力权重时将未来的信息屏蔽掉，只关注当前位置之前的信息。。\n编码器-解码器注意力层（Encoder-Decoder Attention Layers）：除了自注意力层外，解码器还包含编码器-解码器注意力层，用于将编码器端的信息与解码器端的信息进行交互，帮助解码器更好地理解输入序列。\n前馈神经网络（Feed-Forward Neural Networks）：与编码器一样，解码器也包含前馈神经网络层，用于对特征进行映射和转换。\n位置编码（Positional Encoding）：解码器也需要位置编码来将位置信息融入模型中，以便模型能够理解输入序列的顺序信息。\nDecoder在预训练、输入输出和预测时的输入输出：\n预训练：\n输入：在预训练期间，解码器的输入通常是由目标序列（target sequence）以及可选的编码器端输出的上下文信息组成。这些输入经过嵌入（embedding）和位置编码后，被送入解码器中。 输出：解码器预训练的目标是生成目标序列的下一个词的概率分布。因此，在每个时间步，解码器会生成一个预测概率分布，以便训练模型。 输入输出：\n输入：在进行输入输出（Inference）时，解码器的输入通常是由上一个时间步生成的词以及编码器端的上下文信息组成。这些输入通过嵌入和位置编码后，传递给解码器。 输出：解码器在每个时间步生成的输出通常是一个概率分布，用于预测下一个词的概率。根据应用场景，可以使用不同的策略（如贪婪搜索、束搜索等）来选择最终的输出序列。 预测：\n输入：在预测阶段，解码器的输入通常是由起始符号（如）以及编码器端的上下文信息组成。这些输入经过嵌入和位置编码后，传递给解码器。 输出：解码器生成的输出是一个概率分布，用于预测下一个词的概率。根据应用需求，可以根据生成的概率分布采样得到最终的预测结果。 结构 上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：\n包含两个 Multi-Head Attention 层。 第一个 Multi-Head Attention 层采用了 Masked 操作。 第二个 Multi-Head Attention 层的K, V矩阵使用 Encoder 的编码信息矩阵C进行计算，而Q使用上一个 Decoder block 的输出计算。 最后有一个 Softmax 层计算下一个翻译单词的概率。 先理解:自注意力的计算过程",
    "tags": [],
    "title": "Transformer模型详解05-Decoder 结构",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_05/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 插件开发 \u003e vscode插件",
    "content": "概述 Visual Studio Code（简称 VS Code）是一款由 Microsoft 开发的开源轻量级编辑器，支持跨平台（Windows、macOS、Linux）。\n其最大的优势之一是强大的插件系统，开发者可以通过编写扩展（Extension）来增强 VS Code 的功能，比如支持新的编程语言、代码提示、调试器、界面主题等。\nVS Code 插件的主要原理是：\n插件运行在独立的进程（Extension Host）中，不会阻塞编辑器主线程。 插件通过 VS Code 提供的 API 与编辑器进行交互，比如注册命令、添加菜单、修改编辑器行为等。 插件开发语言主要是 TypeScript 或 JavaScript，并基于 Node.js 生态。 安装 VS Code 安装 打开 VS Code 官方下载页面。 选择对应操作系统（Windows、macOS 或 Linux）。 按提示进行安装，安装完成后可以通过 code 命令（需要在安装时勾选“添加到 PATH”）在命令行中启动 VS Code。 插件开发环境安装 插件开发需要以下工具：\nyo（Yeoman 脚手架工具） generator-code（VS Code 插件项目生成器） vsce（VS Code Extension CLI，用于打包和发布插件） 安装步骤：\n# 安装 yo 和 generator-code npm install -g yo generator-code # 安装 vsce npm install -g @vscode/vsce 开发 生成代码 使用 Yeoman 脚手架生成插件项目：\nyo code 执行后会有交互式提示，例如：\n选择插件类型（TypeScript / JavaScript） 插件名称 描述 初始化 Git 仓库等 生成完成后，项目目录大致结构如下：\nmy-extension/\r├── .vscode/ # VS Code 调试配置\r├── src/ # 插件源码\r│ └── extension.ts # 插件入口文件\r├── package.json # 插件描述文件，配置命令、激活事件、依赖等\r├── tsconfig.json # TypeScript 配置（如果是 TS 项目）\r└── README.md # 插件说明文档 package.json：插件的核心配置文件，用来描述插件元信息和扩展点。 extension.ts：插件入口文件，负责注册命令和功能。 package.json 核心配置 package.json 是插件的描述文件，控制插件如何被 VS Code 加载。主要字段：\n{\r\"name\": \"my-extension\",\r\"displayName\": \"My Extension\",\r\"description\": \"一个简单的 VS Code 插件示例\",\r\"version\": \"0.0.1\",\r\"publisher\": \"your-name\",\r\"engines\": {\r\"vscode\": \"^1.80.0\"\r},\r\"activationEvents\": [\r\"onCommand:extension.helloWorld\"\r],\r\"main\": \"./out/extension.js\",\r\"contributes\": {\r\"commands\": [\r{\r\"command\": \"extension.helloWorld\",\r\"title\": \"Hello World\"\r}\r]\r},\r\"scripts\": {\r\"vscode:prepublish\": \"npm run compile\",\r\"compile\": \"tsc -p ./\",\r\"watch\": \"tsc -watch -p ./\",\r\"test\": \"npm run compile \u0026\u0026 node ./out/test/runTest.js\"\r},\r\"devDependencies\": {\r\"typescript\": \"^5.0.0\",\r\"vscode\": \"^1.1.37\"\r}\r} 核心字段说明：\nname：插件的唯一 ID（发布后不可更改）。 displayName：VS Code Marketplace 上显示的名称。 version：插件版本。 publisher：发布者名称（需与 Marketplace 发布者一致）。 engines.vscode：兼容的 VS Code 版本范围。 activationEvents：触发插件激活的事件（如 onCommand、onLanguage、*）。 main：插件的入口文件（一般是编译后的 extension.js）。 contributes：插件扩展点，例如命令、菜单、快捷键、配置等。 extension.ts 核心函数 extension.ts 是插件的入口文件，负责插件的生命周期和功能实现。\nimport * as vscode from 'vscode';\r/**\r* 插件被激活时调用\r* @param context 插件上下文对象，包含订阅、全局存储等\r*/\rexport function activate(context: vscode.ExtensionContext) {\rconsole.log('插件已激活！');\r// 注册命令\rlet disposable = vscode.commands.registerCommand('extension.helloWorld', () =\u003e {\rvscode.window.showInformationMessage('Hello World from My Extension!');\r});\r// 将命令注册到插件上下文，确保插件卸载时清理资源\rcontext.subscriptions.push(disposable);\r}\r/**\r* 插件被停用时调用\r* 通常用于清理资源、保存数据\r*/\rexport function deactivate() {} 核心点解释：\nactivate：插件激活时执行（如首次运行命令、打开特定文件类型）。 deactivate：插件停用时执行，用于清理资源。 vscode.commands.registerCommand：注册一个命令（命令 ID 必须和 package.json 中一致）。 vscode.window.showInformationMessage：在 VS Code 界面右下角弹出提示消息。 context.subscriptions：插件上下文，保存所有注册的资源，确保在插件停用时能正确释放。 Hello World 示例 编辑 src/extension.ts，添加一个最简单的命令： import * as vscode from 'vscode';\rexport function activate(context: vscode.ExtensionContext) {\rconsole.log('插件已激活！');\rlet disposable = vscode.commands.registerCommand('extension.helloWorld', () =\u003e {\rvscode.window.showInformationMessage('Hello World from My Extension!');\r});\rcontext.subscriptions.push(disposable);\r}\rexport function deactivate() {} 在 package.json 中配置命令： {\r\"contributes\": {\r\"commands\": [\r{\r\"command\": \"extension.helloWorld\",\r\"title\": \"Hello World\"\r}\r]\r}\r} 运行调试： 按 F5 启动调试，会打开一个新的 VS Code 窗口（Extension Development Host）。 打开命令面板（Ctrl+Shift+P / Cmd+Shift+P），输入并运行 Hello World。 会弹出消息 “Hello World from My Extension!\"。 拓展介绍 VS Code 插件 API 非常丰富，常见扩展能力包括：\n编辑器扩展：代码高亮、自动补全、格式化器。\nUI 扩展：状态栏、活动栏、侧边栏视图。\n调试扩展：调试适配器，用于支持新的调试语言。\n文件系统扩展：实现虚拟文件系统。\n常见配置示例（在 package.json 中添加）：\n1. 命令（Commands） 命令是最常见的扩展方式，用户可以在命令面板（Ctrl+Shift+P）或绑定快捷键来触发。\n配置（package.json）：\n{ \"contributes\": { \"commands\": [ { \"command\": \"extension.helloWorld\", \"title\": \"Hello World\" } ] } } 实现（extension.ts）：\nvscode.commands.registerCommand('extension.helloWorld', () =\u003e {\rvscode.window.showInformationMessage('Hello World!');\r}); 2. 菜单（Menus） 可以把命令挂载到编辑器右键菜单、资源管理器右键菜单等位置。\n配置（package.json）：\n{\r\"contributes\": {\r\"commands\": [\r{\r\"command\": \"extension.helloWorld\",\r\"title\": \"hello\"\r}，\r\"menus\": {\r\"editor/context\": [\r{\r\"command\": \"extension.helloWorld\",\r\"when\": \"editorLangId == javascript\",\r\"group\": \"navigation\"\r}\r]\r}\r}\r} 说明：\neditor/context 表示编辑器内右键菜单。 when 条件限制了命令只在 JavaScript 文件中出现。 group 决定菜单项分组（navigation = 导航相关）。 菜单本身没有名字，只能通过命令 title 来显示，菜单本省command会关联到commands的命令通过command的title显示菜单名称。 菜单位置由 menus 的 key 决定，比如：\n菜单位置 key:\r`editor/context` 编辑器右键菜单\r`editor/title` 编辑器标题栏按钮\r`editor/title/context` 编辑器标题栏右键菜单\r`explorer/context` 资源管理器右键菜单\r`commandPalette` 命令面板（Ctrl+Shift+P）\r`view/title` 视图面板标题栏按钮\r`scm/title` 版本控制标题栏按钮 3. 快捷键（Keybindings） 可以为命令绑定快捷键。\n配置（package.json）：\n{\r\"contributes\": {\r\"keybindings\": [\r{\r\"command\": \"extension.helloWorld\",\r\"key\": \"ctrl+alt+h\",\r\"when\": \"editorTextFocus\"\r}\r]\r}\r} 说明：\nkey：快捷键组合。 when：触发条件，这里是“编辑器有焦点时”。 4. 状态栏（Status Bar Items） 可以在底部状态栏添加一个按钮。\n实现（extension.ts）：\nlet statusBar = vscode.window.createStatusBarItem(vscode.StatusBarAlignment.Right, 100);\rstatusBar.text = \"$(smiley) Hello\";\rstatusBar.command = \"extension.helloWorld\";\rstatusBar.show();\rcontext.subscriptions.push(statusBar); 说明：\ncreateStatusBarItem 用于创建状态栏元素。 text 可以包含图标（如 $(smiley)）。 command 绑定点击事件。 5. 侧边栏视图（Views） 可以在活动栏（左侧竖栏）添加一个自定义视图。\n配置（package.json）：\n{\r\"contributes\": {\r\"views\": {\r\"explorer\": [\r{\r\"id\": \"mySidebar\",\r\"name\": \"My Sidebar\"\r}\r]\r}\r}\r} 实现（extension.ts）：\nclass MyTreeDataProvider implements vscode.TreeDataProvider\u003cvscode.TreeItem\u003e {\rgetTreeItem(element: vscode.TreeItem): vscode.TreeItem {\rreturn element;\r}\rgetChildren(): vscode.TreeItem[] {\rreturn [\rnew vscode.TreeItem(\"Item 1\"),\rnew vscode.TreeItem(\"Item 2\")\r];\r}\r}\rvscode.window.registerTreeDataProvider(\"mySidebar\", new MyTreeDataProvider()); 说明：\n在 资源管理器面板 添加一个新视图 “My Sidebar”。\n用 TreeDataProvider 动态提供数据。\n6. 编辑器装饰（Decorations） 可以给代码添加背景色、高亮、提示信息等。\n实现（extension.ts）：\nconst decorationType = vscode.window.createTextEditorDecorationType({\rbackgroundColor: \"rgba(255,0,0,0.3)\"\r});\rconst editor = vscode.window.activeTextEditor;\rif (editor) {\rconst range = new vscode.Range(0, 0, 0, 5);\reditor.setDecorations(decorationType, [range]);\r} 说明：\ncreateTextEditorDecorationType 定义样式。 setDecorations 应用到代码范围。 7. 语言支持（Language Features） 可以扩展某种语言的代码补全、悬浮提示等。\n配置（package.json）：\n{\r\"contributes\": {\r\"languages\": [\r{\r\"id\": \"mylang\",\r\"aliases\": [\"MyLang\"],\r\"extensions\": [\".mlg\"],\r\"configuration\": \"./language-configuration.json\"\r}\r]\r}\r} 实现补全（extension.ts）：\nvscode.languages.registerCompletionItemProvider(\"mylang\", {\rprovideCompletionItems(document, position) {\rreturn [new vscode.CompletionItem(\"helloWorld\", vscode.CompletionItemKind.Keyword)];\r}\r}); 说明：\nlanguages 定义新语言（这里是 .mlg 后缀）。 registerCompletionItemProvider 提供自动补全。 8. 配置（Configuration） 插件可以在 VS Code 设置里增加配置项。\n配置（package.json）：\n{\r\"contributes\": {\r\"configuration\": {\r\"title\": \"My Extension\",\r\"properties\": {\r\"myExtension.enableFeature\": {\r\"type\": \"boolean\",\r\"default\": true,\r\"description\": \"是否启用我的功能\"\r},\r\"myExtension.apiEndpoint\": {\r\"type\": \"string\",\r\"default\": \"https://api.example.com\",\r\"description\": \"API 接口地址\"\r}\r}\r}\r}\r} 读取配置（extension.ts）：\nconst config = vscode.workspace.getConfiguration(\"myExtension\");\rconst enable = config.get(\"enableFeature\", true);\rconst api = config.get(\"apiEndpoint\", \"\"); 9. 文件系统监听（File System Watcher） 可以监听文件变化事件。\n实现（extension.ts）：\nconst watcher = vscode.workspace.createFileSystemWatcher(\"**/*.js\");\rwatcher.onDidChange(uri =\u003e console.log(\"修改: \" + uri.fsPath));\rwatcher.onDidCreate(uri =\u003e console.log(\"创建: \" + uri.fsPath));\rwatcher.onDidDelete(uri =\u003e console.log(\"删除: \" + uri.fsPath));\rcontext.subscriptions.push(watcher); 10. 任务（Tasks） 可以让插件在 VS Code 的“任务运行器”中提供任务。\n配置（package.json）：\n{\r\"contributes\": {\r\"taskDefinitions\": [\r{\r\"type\": \"myTask\",\r\"required\": [\"taskName\"],\r\"properties\": {\r\"taskName\": {\r\"type\": \"string\",\r\"description\": \"任务名称\"\r}\r}\r}\r]\r}\r} 实现（extension.ts）：\nvscode.tasks.registerTaskProvider(\"myTask\", {\rprovideTasks: () =\u003e {\rreturn [new vscode.Task(\r{ type: \"myTask\", taskName: \"sayHello\" },\rvscode.TaskScope.Workspace,\r\"sayHello\",\r\"myTask\",\rnew vscode.ShellExecution(\"echo Hello from task!\")\r)];\r},\rresolveTask: () =\u003e undefined\r}); 发布 打包插件 使用 vsce 打包插件：\n# 在插件项目根目录执行\rvsce package 执行成功后，会生成一个 .vsix 文件，例如：\nmy-extension-0.0.1.vsix 安装插件：\ncode --install-extension my-extension-0.0.1.vsix 或者到vscode插件中心右侧… install from vsix选择本地文件。\n发布到 VS Code Marketplace 前往 Azure DevOps 创建 Publisher。\n使用 vsce login \u003cpublisher-name\u003e 登录，并输入 Personal Access Token。\n发布插件：\nvsce publish 或者指定版本号：\nvsce publish minor 发布成功后，你的插件就会出现在 Visual Studio Marketplace 上，供所有用户下载。",
    "description": "概述 Visual Studio Code（简称 VS Code）是一款由 Microsoft 开发的开源轻量级编辑器，支持跨平台（Windows、macOS、Linux）。\n其最大的优势之一是强大的插件系统，开发者可以通过编写扩展（Extension）来增强 VS Code 的功能，比如支持新的编程语言、代码提示、调试器、界面主题等。\nVS Code 插件的主要原理是：\n插件运行在独立的进程（Extension Host）中，不会阻塞编辑器主线程。 插件通过 VS Code 提供的 API 与编辑器进行交互，比如注册命令、添加菜单、修改编辑器行为等。 插件开发语言主要是 TypeScript 或 JavaScript，并基于 Node.js 生态。 安装 VS Code 安装 打开 VS Code 官方下载页面。 选择对应操作系统（Windows、macOS 或 Linux）。 按提示进行安装，安装完成后可以通过 code 命令（需要在安装时勾选“添加到 PATH”）在命令行中启动 VS Code。 插件开发环境安装 插件开发需要以下工具：\nyo（Yeoman 脚手架工具） generator-code（VS Code 插件项目生成器） vsce（VS Code Extension CLI，用于打包和发布插件） 安装步骤：\n# 安装 yo 和 generator-code npm install -g yo generator-code # 安装 vsce npm install -g @vscode/vsce 开发 生成代码 使用 Yeoman 脚手架生成插件项目：",
    "tags": [],
    "title": "vscode插件开发教程",
    "uri": "/docs/programming/plugins/vscode/vscode_cross/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 计算机视觉 \u003e 工具与框架",
    "content": "@[toc]\nPython OpenCV 入门指南 OpenCV是一个强大的计算机视觉库，它可以用于处理图像和视频数据，以及进行目标检测和跟踪等任务。，将学会如何使用Python编写OpenCV代码来进行基础和进阶的图像处理和分析。\n学习OpenCV可以帮助你掌握基本的图像处理技术，包括图像读取和处理、阈值处理、形态学函数、模板匹配、滤波器、图形处理、视频处理和人脸检测等方面的内容。这些技术都是计算机视觉和图像处理领域的基本内容，也是卷积神经网络的基础。通过学习OpenCV，你可以更好地理解卷积神经网络的工作原理和应用。同时，OpenCV也是一个非常流行的图像处理库，掌握它可以帮助你更好地处理和分析图像数据。\n参考书籍：Python Opencv从入门到精通\n安装OpenCV 在开始编写OpenCV代码之前，我们需要先安装OpenCV库。我们可以通过pip包管理器来安装：\npip install opencv-python\n你可以使用conda或者micromamba来安装虚拟环境,安装好notebook环境\n打印opencv版本\nimport cv2\rprint(\"OpenCV version:\")\rprint(cv2.__version__) 输出 OpenCV version: 4.7.0\n基础篇 图像读取和显示 在开始处理图像之前，我们需要学习如何读取和显示图像。下面的代码演示了如何使用OpenCV库读取和显示图像：\nimport cv2 读取图像 img = cv2.imread('image.jpg') 显示图像 opencv显示\ncv2.imshow('Image', img)\rcv2.waitKey(0)\rcv2.destroyAllWindows()` 在上面的代码中，我们首先使用cv2.imread()函数读取了一个名为image.jpg的图像文件。然后，我们使用cv2.imshow()函数来显示这个图像，并使用cv2.waitKey()和cv2.destroyAllWindows()函数来等待用户按下任意键，然后关闭显示窗口。\n注意如果是使用notebook执行waitKey(0)显示，会存在第二次运行无法显示的问题，可以cv2.waitKey(3)设置指定时间自动结束，\nimport matplotlib\rimport matplotlib.pyplot as plt\rmatplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为黑体\rmatplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\rimg = cv2.imread('image.jpg')\rrgbimg = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #opencv像素顺序是bgr\rplt.title(title)\rplt.imshow(img, cmap=cmap)\rplt.show() 裁剪图像 从左上角坐标为(200, 100)的位置开始，裁剪一个宽为400像素、高为400像素的矩形区域。\ncropped = img[100:500, 200:600] 像素操作 在OpenCV中，图像可以表示为三维的数组，其中每个元素都是表示像素值的数字。图像数组的维度取决于图像的大小和通道数。对于一个大小为 $height×width$ 的彩色图像，它的数组形状为 (height,width,3)其中3表示三个颜色通道，即BGR。BGR是指蓝色、绿色、红色三个通道，这是因为在OpenCV中图像的颜色通道排列顺序是B、G、R。\n要访问和修改图像中的像素值，可以使用numpy数组的索引方式，例如：\nimport numpy as np\rimg = cv2.imread(\"image.jpg\")\r# 获取图像宽高\rheight, width = img.shape[:2]\r# 获取某个像素的BGR值\r# 在OpenCV中，通常使用img[y,x]的方式来访问图像的像素值，其中y是像素的行坐标，x是像素的列坐标。因此，在你# 提到的img[20, 100]中，20是y坐标，100是x坐标。\rb, g, r = img[20, 100]\r# 设置某个像素的BGR值\rimg[100, 100] = (255, 255, 255)\r# 获取某个通道的所有像素值\rblue_channel = img[:, :, 0]\rgreen_channel = img[:, :, 1]\rred_channel = img[:, :, 2]\r# 修改某个通道的所有像素值\rimg[:, :, 0] = 0 # 将蓝色通道设为0` 注意在OpenCV中，通道顺序是BGR而不是RGB。\n色彩空间与通道 OpenCV支持多种色彩空间，比如RGB、HSV、YCrCb、Lab等。不同的色彩空间对应着不同的通道，比如RGB色彩空间有3个通道，分别是红色、绿色、蓝色通道。为了进行图像处理，我们通常需要对图像的色彩空间和通道进行转换。 下面是一些OpenCV常用的色彩空间和通道转换函数：\ncv2.cvtColor(src, code[, dst[, dstCn]])：将图像从一个色彩空间转换到另一个色彩空间。其中，src是输入图像，code是色彩空间转换代码，dst是输出图像，dstCn是输出图像的通道数。 比如，将BGR格式的图像转换为灰度图像：\nimg = cv2.imread('test.jpg')\rgray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.split(src[, mv])：将多通道图像分离成单通道图像。其中，src是输入图像，mv是输出单通道图像的列表。 比如，将BGR格式的图像分离成三个通道：\nimg = cv2.imread('test.jpg')\rb, g, r = cv2.split(img) cv2.merge(mv[, dst])：将多个单通道图像合并成一个多通道图像。其中，mv是单通道图像的列表，dst是输出的多通道图像。 比如，将三个单通道图像合并成BGR格式的图像：\nb = cv2.imread('test_b.jpg', cv2.IMREAD_GRAYSCALE)\rg = cv2.imread('test_g.jpg', cv2.IMREAD_GRAYSCALE)\rr = cv2.imread('test_r.jpg', cv2.IMREAD_GRAYSCALE)\rimg = cv2.merge([b, g, r]) cv2.addWeighted(src1, alpha, src2, beta, gamma[, dst])：将两个图像按照一定比例进行融合。其中，src1和src2是两个输入图像，alpha和beta是两个图像的权重，gamma是亮度调整值，dst是输出的融合后的图像。 比如，将两个灰度图像按照1:2的比例融合：\nimg1 = cv2.imread('test1.jpg', cv2.IMREAD_GRAYSCALE)\rimg2 = cv2.imread('test2.jpg', cv2.IMREAD_GRAYSCALE)\rimg = cv2.addWeighted(img1, 1, img2, 2, 0) BGR色彩空间是基于三基色而言的，三基色指的是红色、绿色和蓝色。 而HSV色彩空间则是基于色调、饱和度和亮度而言的。 其中，色调（H）是指光的颜色，例如，彩虹中的赤、橙、黄、绿、青、蓝、紫分别表示不同的色调，在OpenCV中，色调在区间[0, 180]内取值。例如，代表红色、黄色、绿色和蓝色的色调值分别为0、30、60和120。 饱和度（S）是指色彩的深浅。在OpenCV中，饱和度在区间[0, 255]内取值。当饱和度为0时，图像将变为灰度图像。 亮度（V）是指光的明暗。与饱和度相同，在OpenCV中，亮度在区间[0, 255]内取值。亮度值越大，图像越亮；当亮度值为0时，图像呈纯黑色\n图像几何变换 OpenCV提供了许多基础的图像变换函数，可以用于调整图像的大小、旋转、平移、裁剪等操作。下面的代码演示了如何使用这些函数：\n缩放图像 将img图像对象缩小了一半并赋值给了resized\nresized = cv2.resize(img, (int(img.shape[1]/2), int(img.shape[0]/2))) 仿射变换 仿射变换是一种仅在二维平面中发生的几何变形，变换之后的图像仍然可以保持直线的“平直性”和“平行性”，也就是说原来的直线变换之后还是直线，平行线变换之后还是平行线。常见的仿射变换效果如图所示，包含平移、旋转和倾斜。 OpenCV通过cv2. warpAffine()方法实现仿射变换效果，其语法如下：\ndst = cv2.warpAffine(src, M, dsize, flags, borderMode, borderValue) 参数说明： - src：原始图像。 - M：一个2行3列的矩阵，根据此矩阵的值变换原图中的像素位置。 - dsize：输出图像的尺寸大小。 - flags：可选参数，插值方式，建议使用默认值。 - borderMode：可选参数，边界类型，建议使用默认值。 - borderValue：可选参数，边界值，默认为0，建议使用默认值。 返回值说明：\n- dst：经过反射变换后输出图像。 M也被叫作仿射矩阵，实际上就是一个2×3的列表，其格式如下：\nM = [[a, b, c],[d, e, f]]\r图像做何种仿射变换，完全取决于M的值，仿射变换输出的图像按照以下公式进行计算：\n新x = 原x × a + 原y × b + c\r新y = 原x × d + 原y × e + f\r原x和原y表示原始图像中像素的横坐标和纵坐标，新x与新y表示同一个像素经过仿射变换后在新图像中的横坐标和纵坐标。\n平移图像 平移就是让图像中的所有像素同时沿着水平或垂直方向移动。实现这种效果只需要将M的值按照以下格式进行设置： M = [[1, 0, 水平移动的距离],[0, 1, 垂直移动的距离]] 原始图像的像素就会按照以下公式进行变换： 新x = 原x × 1 + 原y × 0 + 水平移动的距离 = 原x + 水平移动的距离 新y = 原x × 0 + 原y × 1 + 垂直移动的距离 = 原y + 垂直移动的距离\nM = np.float32([[1, 0, 100], [0, 1, 50]])\rtranslated = cv2.warpAffine(img, M, (img.shape[1], img.shape[0])) 旋转图像 首先获取了img图像对象的行数和列数，并分别赋值给了rows和cols变量。接着，使用cv2.getRotationMatrix2D函数，生成了一个旋转矩阵M， 其中第一个参数是旋转中心点的坐标，这里是图像中心点(cols/2, rows/2)； 第二个参数是旋转的角度，这里是30度； 第三个参数是旋转后的缩放比例，这里是1，表示不进行缩放。最后，使用cv2.warpAffine函数将原始图像对象img按照旋转矩阵M进行旋转，并将结果赋值给rotated。warpAffine函数的第一个参数是需要被旋转的原始图像对象，第二个参数是旋转矩阵，第三个参数是输出图像的尺寸大小，这里使用了原始图像的宽高。函数返回的是旋转后的图像对象rotated。\nrows, cols = img.shape[:2]\rM = cv2.getRotationMatrix2D((cols/2, rows/2), 30, 1)\rrotated = cv2.warpAffine(img, M, (cols, rows)) 倾斜图像 OpenCV需要定位图像的3个点来计算倾斜效果，3个点的位置如图所示，这3个点分别是“左上角”点A、“右上角”点B和“左下角”点C。OpenCV会根据这3个点的位置变化来计算其他像素的位置变化。因为要保证图像的“平直性”和“平行性”，所以不需要“右下角”的点做第4个参数，右下角这个点的位置根据A、B、C 3点的变化自动计算得出。 “平直性”是指图像中的直线在经过仿射变换之后仍然是直线。“平行性”是指图像中的平行线在经过仿射变换之后仍然是平行线。 让图像倾斜也是需要通过M矩阵实现的，但得出这个矩阵需要做很复杂的运算，于是OpenCV提供了getAffineTransform()方法来自动计算倾斜图像的M矩阵。getRotationMatrix2D()方法的语法如下： M = cv2.getAffineTransform(src, dst) 参数说明：\n- src：原图3个点坐标，格式为3行2列的32位浮点数列表，例如：[[0, 1], [1, 0], [1, 1]]。 - dst：倾斜图像的3个点坐标，格式与src一样。 返回值说明：\n- M：getAffineTransform()方法计算出的仿射矩阵。 rows,cols=len(image),len(image[0])\rsrc=np.float32([[0,0],[cols-1,0],[0,rows-1]])\rdst=np.float32([[0,50],[cols-1,0],[0,rows-1]])\rM=cv2.getAffineTransform(src,dst)\rdestImg =cv2.warpAffine(image,M=M,dsize=(len(image[0]),len(image)))\rplt.title(\"图像倾斜\")\rplt.imshow(cv2.cvtColor(destImg, cv2.COLOR_BGR2RGB))\rplt.show() 透视图像 如果说仿射是让图像在二维平面中变形，那么透视就是让图像在三维空间中变形。从不同的角度观察物体，会看到不同的变形画面，例如，矩形会变成不规则的四边形，直角会变成锐角或钝角，圆形会变成椭圆，等等。这种变形之后的画面就是透视图。\n从图像的底部观察图），眼睛距离图像底部较近，所以图像底部宽度不变，但眼睛距离图像顶部较远，图像顶部宽度就会等比缩小，于是观察者就会看到所示的透视效果。 OpenCV中需要通过定位图像的4个点计算透视效果，4个点的位置如图7.16所示。OpenCV根据这4个点的位置变化来计算其他像素的位置变化。透视效果不能保证图像的“平直性”和“平行性”。 warpPerspective()方法也需要通过M矩阵计算透视效果，但得出这个矩阵需要做很复杂的运算，于是OpenCV提供了getPerspectiveTransform()方法自动计算M矩阵。getPerspectiveTransform()方法的语法如下：\nM = cv2.getPerspectiveTransform(src, dst,)\r参数说明：\n- src：原图4个点坐标，格式为4行2列的32位浮点数列表，例如：[[0, 0], [1, 0], [0, 1],[1, 1]]。 - dst：透视图的4个点坐标，格式与src一样。 返回值说明：\n- M：getPerspectiveTransform()方法计算出的仿射矩阵。\nrows=len(image)\rcols=len(image[0])\rM=cv2.getPerspectiveTransform(np.array([[0,0],[cols-1,0],[0,rows-1],[cols-1,rows-1]],dtype=np.float32),\rnp.array([[100,0],[cols-1-100,0],[0,rows-1],[cols-1,rows-1]],dtype=np.float32)\r)\rdImag=cv2.warpPerspective(image,M,(cols,rows))\rplt.imshow(dImag)\rplt.title(\"透视\")\rplt.show() 阈值处理 阈值是图像处理中一个很重要的概念，类似一个“像素值的标准线”。所有像素值都与这条“标准线”进行比较，最后得到3种结果：像素值比阈值大、像素值比阈值小或像素值等于阈值。程序根据这些结果将所有像素进行分组，然后对某一组像素进行“加深”或“变淡”操作，使得整个图像的轮廓更加鲜明，更容易被计算机或肉眼识别。 阈值处理函数 图像处理的过程中，阈值的使用使得图像的像素值更单一，进而使得图像的效果更简单。首先，把一幅彩色图像转换为灰度图像，这样图像的像素值的取值范围即可简化为0~255。然后，通过阈值使得转换后的灰度图像呈现出只有纯黑色和纯白色的视觉效果。例如，当阈值为127时，把小于127的所有像素值都转换为0（即纯黑色），把大于127的所有像素值都转换为255（即纯白色）。虽然会丢失一些灰度细节，但是会更明显地保留灰度图像主体的轮廓。\n阈值处理在计算机视觉技术中占有十分重要的位置，它是很多高级算法的底层处理逻辑之一。因为二值图像会忽略细节，放大特征，而很多高级算法要根据物体的轮廓来分析物体特征，所以二值图像非常适合做复杂的识别运算。在进行识别运算之前，应先将图像转为灰度图像，再进行二值化处理，这样就得到了算法所需要的物体（大致）轮廓图像。\nOpenCV提供的threshold()方法用于对图像进行阈值处理，threshold()方法的语法如下：\nretval, dst = cv2.threshold(src, thresh, maxval, type)\r参数说明：\n- src：被处理的图像，可以是多通道图像。 - thresh：阈值，阈值在125～150取值的效果最好。 - maxval：阈值处理采用的最大值。 - type：阈值处理类型。常用类型和含义。\n返回值说明： - retval：处理时采用的阈值。 - dst：经过阈值处理后的图像。\n在OpenCV中，阈值处理类型有以下几种，以及对应的枚举值：\nTHRESH_BINARY：二值化阈值处理，将大于阈值的像素设置为最大值，小于等于阈值的像素设置为0。枚举值为0。 THRESH_BINARY_INV：反二值化阈值处理，将小于阈值的像素设置为最大值，大于等于阈值的像素设置为0。枚举值为1。 THRESH_TRUNC：截断阈值处理，将大于阈值的像素设置为该阈值，小于等于阈值的像素不变。枚举值为2。 THRESH_TOZERO：阈值处理为0，将小于阈值的像素设置为0，大于等于阈值的像素不变。枚举值为3。 THRESH_TOZERO_INV：反阈值处理为0，将大于阈值的像素设置为0，小于等于阈值的像素不变。枚举值为4。 二值化处理 二值化处理也叫二值化阈值处理，该处理让图像仅保留两种像素值，或者说所有像素都只能从两种值中取值。\n进行二值化处理时，每一个像素值都会与阈值进行比较，将大于阈值的像素值变为最大值，将小于或等于阈值的像素值变为0，计算公式如下：\nif 像素值 \u003c= 阈值: 像素值 = 0\rif 像素值 \u003e 阈值: 像素值 = 最大值 通常二值化处理是使用255作为最大值，因为灰度图像中255表示纯白色,0表示黑色，能够很清晰地与纯黑色进行区分，所以灰度图像经过二值化处理后呈现“非黑即白”的效果。\nimport matplotlib\rimport matplotlib.pyplot as plt\rimport cv2\rgrayimage=cv2.imread(\"../../images/demo1.png\",0) #直接读取灰度图\r_,dst=cv2.threshold(grayimage,127,255,cv2.THRESH_BINARY) plt.subplot(121)\rplt.imshow(grayimage,cmap=\"gray\")\rplt.title(\"灰度图\")\rplt.subplot(122)\rplt.imshow(dst,cmap=\"gray\")\rplt.title(\"二值化图\")\rplt.show() 注意像素值越大表示越白，越小越黑\n反二值化处理 反二值化处理也叫反二值化阈值处理，其结果为二值化处理的相反结果。将大于阈值的像素值变为0，将小于或等于阈值的像素值变为最大值。原图像中白色的部分变成黑色，黑色的部分变成白色。计算公式如下：\nif 像素值 \u003c= 阈值: 像素值 = 最大值\rif 像素值 \u003e 阈值: 像素值 = 0 代码\n_,dst=cv2.threshold(grayimage,127,255,cv2.THRESH_BINARY_INV)\rplt.subplot(121)\rplt.imshow(grayimage,cmap=\"gray\")\rplt.title(\"灰度图\")\rplt.subplot(122)\rplt.imshow(dst,cmap=\"gray\")\rplt.title(\"二值化图\")\rplt.show() 如果ocr一般都是通过反二值化突出文字，显示出黑底白字，然后膨胀（因为白色是比较大的数字）。 零处理 低于阈值零处理 低于阈值零处理也叫低阈值零处理，该处理将低于或等于阈值的像素值变为0，大于阈值的像素值保持原值，计算公式如下：\nif 像素值 \u003c= 阈值: 像素值 = 0\rif 像素值 \u003e 阈值: 像素值 = 原值\r_,dst=cv2.threshold(grayimage,127,255,cv2.THRESH_TOZERO)\rplt.subplot(121)\rplt.imshow(grayimage,cmap=\"gray\")\rplt.title(\"灰度图\")\rplt.subplot(122)\rplt.imshow(dst,cmap=\"gray\")\rplt.title(\"低于阈值零[置黑]处理\")\rplt.show() 超出阈值零处理 超出阈值零处理也叫超阈值零处理，该处理将大于阈值的像素值变为0，小于或等于阈值的像素值保持原值。计算公式如下：\nif 像素值 \u003c= 阈值: 像素值 = 原值\rif 像素值 \u003e 阈值: 像素值 = 0\r_,dst=cv2.threshold(grayimage,127,255,cv2.THRESH_TOZERO_INV)\rplt.subplot(121)\rplt.imshow(grayimage,cmap=\"gray\")\rplt.title(\"灰度图\")\rplt.subplot(122)\rplt.imshow(dst,cmap=\"gray\")\rplt.title(\"超阈值零[置黑]处理\")\rplt.show() 截断处理 截断处理也叫截断阈值处理，该处理将图像中大于阈值的像素值变为和阈值一样的值，小于或等于阈值的像素保持原值，其公式如下：\nif 像素 \u003c= 阈值: 像素 = 原值\rif 像素 \u003e 阈值: 像素 = 阈值\r_,dst=cv2.threshold(grayimage,127,255,cv2.THRESH_TRUNC)\rplt.subplot(121)\rplt.imshow(grayimage,cmap=\"gray\")\rplt.title(\"灰度图\")\rplt.subplot(122)\rplt.imshow(dst,cmap=\"gray\")\rplt.title(\"截断阈值处理\")\rplt.show() 自适应处理 OpenCV提供了一种改进的阈值处理技术：图像中的不同区域使用不同的阈值。把这种改进的阈值处理技术称作自适应阈值处理也称自适应处理，自适应阈值是根据图像中某一正方形区域内的所有像素值按照指定的算法计算得到的。与前面讲解的5种阈值处理类型相比，自适应处理能更好地处理明暗分布不均的图像，获得更简单的图像效果。\nOpenCV提供了adaptiveThresHold()方法对图像进行自适应处理，adaptiveThresHold()方法的语法如下： dst = cv2.adaptiveThreshold(src, maxValue, adaptiveMethod, thresholdType, blockSize, C) 参数说明：\nsrc：被处理的图像。需要注意的是，该图像需是灰度图像。 maxValue：阈值处理采用的最大值。 adaptiveMethod：自适应阈值的计算方法。自适应阈值的计算方法及其含义如表8.2所示。 自适应阈值的计算方法及其含义 ADAPTIVE_THRESH_MEAN_C:对一个正方形的区域所有像素平均加权。 ADAPTIVE_THRESH_GAUSSIAN_C：根据高斯函数按照像素与中心点的距离对一个正方形区域内的所有像素加权计算。 thresholdType：阈值处理类型；需要注意的是，阈值处理类型需是cv2.THRESH_BINARY或cv2.THRESH_BINARY_INV中的一个。 blockSize：一个正方形区域的大小。例如，5指的是5×5的区域。 C：常量。阈值等于均值或者加权值减去这个常量。 返回值说明： dst：经过阈值处理后的图像。 自适应处理保留了图像中更多的细节信息，更明显地保留了灰度图像主体的轮廓。 plt.subplot(221)\rplt.imshow(grayimage,cmap=\"gray\")\rplt.title(\"灰度图\")\rplt.subplot(222)\rmeanImg=cv2.adaptiveThreshold(grayimage,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,5,3)\rplt.imshow(meanImg,cmap=\"gray\")\rplt.title(\"ADAPTIVE_THRESH_MEAN_C图\")\rplt.show()\rplt.subplot(223)\rguassImg=cv2.adaptiveThreshold(grayimage,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,5,3)\rplt.imshow(guassImg,cmap=\"gray\")\rplt.title(\"ADAPTIVE_THRESH_MEAN_C图\")\rplt.show() Otsu方法 前面5种阈值处理类型的过程中，每个实例设置的阈值都是127，并不是通过算法计算得到的。对于有些图像，当阈值被设置为127时，得到的效果并不好，这时就需要一个个去尝试，直到找到最合适的阈值。\n逐个寻找最合适的阈值不仅工作量大，而且效率低。为此，OpenCV提供了Otsu方法。Otsu方法能够遍历所有可能的阈值，从中找到最合适的阈值。\nOtsu方法的语法与threshold()方法的语法基本一致，只不过在为type传递参数时，要多传递一个参数，即cv2.THRESH_OTSU。cv2.THRESH_OTSU的作用就是实现Otsu方法的阈值处理。Otsu方法的语法如下： retval, dst = cv2.threshold(src, thresh, maxval, type) 参数说明：\n- src：被处理的图像。需要注意的是，该图像需是灰度图像。 - thresh：阈值，且要把阈值设置为0。 - maxval：阈值处理采用的最大值，即255。 - type：阈值处理类型。除在表8.1中选择一种阈值处理类型外，还要多传递一个参数，即cv2.THRESH_OTSU。例如，cv2.THRESH_BINARY+cv2.THRESH_OTSU。 返回值说明：\n- retval：由Otsu方法计算得到并使用的最合适的阈值。 - dst：经过阈值处理后的图像。\nplt.subplot(221)\rplt.imshow(grayimage,cmap=\"gray\")\rplt.title(\"灰度图\")\rplt.subplot(222)\r_,dst=cv2.threshold(grayimage,127,255,cv2.THRESH_BINARY)\rplt.imshow(dst,cmap=\"gray\")\rplt.title(\"二值化图\")\rplt.show()\rplt.subplot(223)\r_,ostuImg=cv2.threshold(grayimage,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\rplt.imshow(ostuImg,cmap=\"gray\")\rplt.title(\"OTSU图\")\rplt.show() 进阶篇 模板匹配 模板是被查找目标的图像，查找模板在原始图像中的哪个位置的过程就叫模板匹配。OpenCV提供的matchTemplate()方法就是模板匹配方法，其语法如下：\nresult = cv2.matchTemplate(image, templ, method, mask)\r参数说明：\nimage：原始图像。 templ：模板图像，尺寸必须小于或等于原始图像。 method：匹配的方法，可用参数值如表10.1所示。 mask：可选参数。掩模，只有cv2.TM_SQDIFF和cv2.TM_CCORR_NORMED支持此参数，建议采用默认值。 返回值说明： result：计算得出的匹配结果。如果原始图像的宽、高分别为W、H，模板图像的宽、高分别为w、h，result就是一个W-w+1列、H-h+1行的32位浮点型数组。数组中每一个浮点数都是原始图像中对应像素位置的匹配结果，其含义需要根据method参数来解读。 在模板匹配的计算过程中，模板会在原始图像中移动。模板与重叠区域内的像素逐个对比，最后将对比的结果保存在模板左上角像素点索引位置对应的数组位置中 OpenCV的matchTemplate函数是用来在一幅图像中寻找另一幅图像的匹配的。在匹配过程中，可以选择不同的匹配方法，也就是method参数。常用的method参数有以下几种： - cv2.TM_SQDIFF：平方差匹配法，最简单的匹配方法，计算平方差和，值越小越匹配。 - cv2.TM_SQDIFF_NORMED：标准平方差匹配法，同样计算平方差和，但是会对结果进行标准化处理。注意使用距离来计算被匹配图像一定要小于原图像 - cv2.TM_CCORR：相关性匹配法，#，值越大越匹配。 - cv2.TM_CCORR_NORMED：标准相关性匹配法，对结果进行标准化处理,返回值越大表示匹配程度越高，越小表示匹配程度越低。 该参数使用的是归一化相关系数匹配模式， 返回的是匹配图像和模板图像之间的相关系数，取值范围在 0 到 1 之间，1 表示完美匹配，0 表示没有匹配。 - cv2.TM_CCOEFF：相关系数匹配法，计算两个图像的相关系数，值越大越匹配。 - cv2.TM_CCOEFF_NORMED：标准相关系数匹配法，对结果进行标准化处理。\n假设原图 被匹配图 #多目标匹配\rimage=cv2.imread(\"./images/2.jpg\");\rgrayImg=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\rmatchImg=cv2.imread(\"./images/2_match_1.jpg\",0);\rheight,width=matchImg.shape\rresult=cv2.matchTemplate(grayImg,matchImg,cv2.TM_CCORR_NORMED)\rshowImage=image.copy()\rfor y in range(len(result)):\rfor x in range(len(result[y])):\rif result[y][x]\u003e0.999:\rcv2.rectangle(showImage, (x,y), (x + width, y + height), (255, 0, 0), 1)\rplt.imshow(showImage,cmap=\"gray\") 匹配到结果 滤波器 在尽量保留原图像信息的情况下，去除图像内噪声、降低细节层次信息等一系列过程，叫作图像的平滑处理（或图像的模糊处理）。实现平滑处理最常用的工具就是滤波器。通过调节滤波器的参数，可以控制图像的平滑程度。OpenCV提供了种类丰富的滤波器，每种滤波器使用的算法均不同，但都能对图像中的像素值进行微调，让图像呈现平滑效果。本章将介绍均值滤波器、中值滤波器、高斯滤波器和双边滤波器的使用方法。\n可能会出现这样一种像素，该像素与周围像素的差别非常大，导致从视觉上就能看出该像素无法与周围像素组成可识别的图像信息，降低了整个图像的质量。这种“格格不入”的像素就是图像的噪声。如果图像中的噪声都是随机的纯黑像素或者纯白像素，这样的噪声称作“椒盐噪声”或“盐噪声”。例如如图7.1所示的就是一幅只有噪声的图像，常称为“雪花点”。\n均值滤波器 以一个像素为核心，其周围像素可以组成一个n行n列（简称n×n）的矩阵，这样的矩阵结构在滤波操作中被称为“滤波核”。矩阵的行、列数决定了滤波核的大小，滤波核大小为3×3，包含9个像素；图滤波核大小为5×5，包含25个像素。 均值滤波器（也称为低通滤波器）可以把图像中的每一个像素都当成滤波核的核心，然后计算核内所有像素的平均值，最后让核心像素值等于这个平均值。 OpenCV将均值滤波器封装成blur()方法，其语法如下：\ndst = cv2.blur(src, ksize, anchor, borderType)\r参数说明：\nsrc：被处理的图像。 ksize：滤波核大小，其格式为(高度，宽度)，建议使用如(3, 3)、(5, 5)、(7, 7)等宽、高相等的奇数边长。滤波核越大，处理之后的图像就越模糊。 anchor：可选参数，滤波核的锚点，建议采用默认值，可以自动计算锚点。 borderType：可选参数，边界样式，建议采用默认值。 返回值说明： dst：经过均值滤波处理之后的图像。 import matplotlib.pyplot as plt\rimport matplotlib\rimport cv2\rmatplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为黑体\rmatplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\rimage=cv2.imread(\"./images/1.png\");\rplt.subplot(221)\rplt.title(\"原始图\")\rplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\rplt.subplot(222)\rplt.title(\"滤波核(3,3)\")\rplt.imshow(cv2.cvtColor(cv2.blur(image,(3,3)), cv2.COLOR_BGR2RGB))\rplt.subplot(223)\rplt.title(\"滤波核(5,5)\")\rplt.imshow(cv2.cvtColor(cv2.blur(image,(5,5)), cv2.COLOR_BGR2RGB))\rplt.subplot(224)\rplt.title(\"滤波核(9,9)\")\rplt.imshow(cv2.cvtColor(cv2.blur(image,(9,9)), cv2.COLOR_BGR2RGB)) 中值滤波器 中值滤波器的原理与均值滤波器非常相似，唯一的不同就是不计算像素的平均值，而是将所有像素值排序，把最中间的像素值取出，赋值给核心像素。 OpenCV将中值滤波器封装成medianBlur()方法，其语法如下：\ndst = cv2.medianBlur(src, ksize)\r参数说明：\nsrc：被处理的图像。 ksize：滤波核的边长，必须是大于1的奇数，如3、5、7等。该方法根据此边长自动创建一个正方形的滤波核。 返回值说明： st：经过中值滤波处理之后的图像。 import matplotlib.pyplot as plt\rimport matplotlib\rimport cv2\rmatplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为黑体\rmatplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\rimage=cv2.imread(\"./images/1.png\");\rplt.subplot(221)\rplt.title(\"原始图\")\rplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\rplt.subplot(222)\rplt.title(\"滤波核(3,3)\")\rplt.imshow(cv2.cvtColor(cv2.medianBlur(image,3), cv2.COLOR_BGR2RGB))\rplt.subplot(223)\rplt.title(\"滤波核(5,5)\")\rplt.imshow(cv2.cvtColor(cv2.medianBlur(image,5), cv2.COLOR_BGR2RGB))\rplt.subplot(224)\rplt.title(\"滤波核(9,9)\")\rplt.imshow(cv2.cvtColor(cv2.medianBlur(image,9), cv2.COLOR_BGR2RGB)) 高斯滤波器 高斯滤波也被称为高斯模糊或高斯平滑，是目前应用最广泛的平滑处理算法。高斯滤波可以很好地在降低图片噪声、细节层次的同时保留更多的图像信息，经过处理的图像呈现“磨砂玻璃”的滤镜效果。 进行均值滤波处理时，核心周围每个像素的权重都是均等的，也就是每个像素都同样重要，所以计算平均值即可。但在高斯滤波中，越靠近核心的像素权重越大，越远离核心的像素权重越小，例如5×5大小的高斯滤波卷积核的权重示意图如图11.8所示。像素权重不同不能取平均值，要从权重大的像素中取较多的信息，从权重小的像素中取较少的信息。简单概括就是“离谁更近，跟谁更像”。\n高斯滤波的计算过程涉及卷积运算，会有一个与滤波核大小相等的卷积核。本节仅以3×3的滤波核为例，简单地描述一下高斯滤波的计算过程。\n卷积核中保存的值就是核所覆盖区域的权重值，其遵循图11.8的规律。卷积核中所有权重值相加的结果为1。例如，3×3的卷积核可以是如图11.9所示的值。随着核大小、σ标准差的变化，卷积核中的值也会发生较大变化，图11.9仅是一种最简单的情况。 import matplotlib.pyplot as plt\rimport matplotlib\rimport cv2\rmatplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为黑体\rmatplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\rimage=cv2.imread(\"./images/1.png\");\rplt.subplot(221)\rplt.title(\"原始图\")\rplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\rplt.subplot(222)\rplt.title(\"滤波核(3,3)\")\rplt.imshow(cv2.cvtColor(cv2.GaussianBlur(image,(3,3),0,0), cv2.COLOR_BGR2RGB))\rplt.subplot(223)\rplt.title(\"滤波核(5,5)\")\rplt.imshow(cv2.cvtColor(cv2.GaussianBlur(image,(5,5),0,0), cv2.COLOR_BGR2RGB))\rplt.subplot(224)\rplt.title(\"滤波核(9,9)\")\rplt.imshow(cv2.cvtColor(cv2.GaussianBlur(image,(9,9),0,0), cv2.COLOR_BGR2RGB)) 双边滤波器 不管是均值滤波、中值滤波还是高斯滤波，都会使整幅图像变得平滑，图像中的边界会变得模糊不清。双边滤波是一种在平滑处理过程中可以有效保护边界信息的滤波操作方法。\n双边滤波器自动判断滤波核处于“平坦”区域还是“边缘”区域：如果滤波核处于“平坦”区域，则会使用类似高斯滤波的算法进行滤波；如果滤波核处于“边缘”区域，则加大“边缘”像素的权重，尽可能地让这些像素值保持不变。 import matplotlib.pyplot as plt\rimport matplotlib\rimport cv2\rmatplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为黑体\rmatplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\rimage=cv2.imread(\"./images/1.png\");\rplt.subplot(221)\rplt.title(\"原始图\")\rplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\rplt.subplot(222)\rplt.title(\"高斯滤波核(15,15)\")\rplt.imshow(cv2.cvtColor(cv2.GaussianBlur(image,(15,15),0,0), cv2.COLOR_BGR2RGB))\rplt.subplot(223)\rplt.title(\"双边滤波核(15,15)\")\rplt.imshow(cv2.cvtColor(cv2.bilateralFilter(image,15,120,100), cv2.COLOR_BGR2RGB)) 形态学运算 腐蚀和膨胀是形态学的基础操作，除了开运算和闭运算以外，形态学中还有几种比较有特点的运算。OpenCV提供了一个morphologyEx()形态学方法，包含所有常用的运算，其语法如下： dst = cv2.morphologyEx(src, op, kernel, anchor, iterations, borderType, borderValue) 参数说明：\nsrc：原始图像。 op：操作类型，具体值如表12.1所示。 具体枚举值如下： MORPH_ERODE：腐蚀操作 MORPH_DILATE：膨胀操作 MORPH_OPEN：开运算 MORPH_CLOSE：闭运算 MORPH_GRADIENT：形态学梯度 MORPH_TOPHAT：顶帽操作 MORPH_BLACKHAT：黑帽操作 kernel：操作过程中使用的核。 anchor：可选参数，核的锚点位置。 iterations：可选参数，迭代次数，默认值为1。 borderType：可选参数，边界样式，建议默认。 borderValue：可选参数，边界值，建议默认。 返回值说明： dst：操作之后得到的图像。 腐蚀 腐蚀操作可以让图像沿着自己的边界向内收缩。OpenCV通过“核”来实现收缩计算。“核”的英文名为kernel，在形态学中可以理解为“由n个像素组成的像素块”，像素块包含一个核心（核心通常在中央位置，也可以定义在其他位置）。像素块在图像的边缘移动，在移动过程中，核会将图像边缘那些与核重合但又没有越过核心的像素点都抹除，效果类似图12.1所示的过程，就像削土豆皮一样，将图像一层一层地“削薄”。 OpenCV将腐蚀操作封装成erode()方法，该方法的语法如下：\ndst = cv2.erode(src, kernel, anchor, iterations, borderType, borderValue)\r参数说明：\nsrc：原始图像。 kernel：腐蚀使用的核。 anchor：可选参数，核的锚点位置。 iterations：可选参数，腐蚀操作的迭代次数，默认值为1。 borderType：可选参数，边界样式，建议默认。 borderValue：可选参数，边界值，建议默认。 返回值说明： dst：经过腐蚀之后的图像。 图像经过腐蚀操作之后，可以抹除一些外部的细节，如图12.2所示是一个卡通小蜘蛛，如果用一个5×5的像素块作为核对小蜘蛛进行腐蚀操作，可以得到如图12.3所示的结果。小蜘蛛的腿被当成外部细节抹除了，同时小蜘蛛的眼睛变大了，因为核从内部也“削”了一圈。 腐蚀之后 import matplotlib.pyplot as plt\rimport matplotlib\rimport cv2\rimport numpy as np\rmatplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为黑体\rmatplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\rimage=cv2.imread(\"./images/1.jpg\");\rplt.subplot(221)\rplt.title(\"原始图\")\rplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\rplt.subplot(222)\rplt.title(\"腐蚀\")\rzeroArray=np.ones((3,3))\rplt.imshow(cv2.erode(image, zeroArray))\rplt.subplot(223)\rplt.title(\"腐蚀\")\rdst = cv2.morphologyEx(image, cv2.MORPH_ERODE, zeroArray) #也可以使用这个形态学方法，效果和erode一致\rplt.imshow(dst) 膨胀 膨胀操作与腐蚀操作相反，膨胀操作可以让图像沿着自己的边界向内扩张。同样是通过核来计算，当核在图像的边缘移动时，核会将图像边缘填补新的像素，效果类似图12.6所示的过程，就像在一面墙上反反复复地涂水泥，让墙变得越来越厚。 OpenCV将膨胀操作封装成dilate()方法，该方法的语法如下： dst = cv2.dilate(src, kernel, anchor, iterations, borderType, borderValue) 参数说明：\nsrc：原始图像。 kernel：膨胀使用的核。 anchor：可选参数，核的锚点位置。 iterations：可选参数，腐蚀操作的迭代次数，默认值为1。 borderType：可选参数，边界样式，建议默认。 borderValue：可选参数，边界值，建议默认。 返回值说明： dst：经过膨胀之后的图像。 图像经过膨胀操作之后，可以放大一些外部的细节，如图12.7（a）所示的卡通小蜘蛛，如果用一个5×5的像素块作为核对小蜘蛛进行膨胀操作，可以得到如图12.7（b）所示的结果，小蜘蛛不仅腿变粗了，而且连眼睛都胖没了。 matplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为黑体\rmatplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\rimage=cv2.imread(\"./images/2.jpg\");\rplt.subplot(221)\rplt.title(\"原始图\")\rplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\rplt.subplot(222)\rplt.title(\"膨胀\")\rzeroArray=np.ones((9,9))\rplt.imshow(cv2.dilate(image, zeroArray))\rplt.subplot(223)\rplt.title(\"膨胀\")\rdst = cv2.morphologyEx(image, cv2.MORPH_DILATE, zeroArray) #也可以使用这个形态学方法，效果和erode一致\rplt.imshow(dst) 开运算 开运算是将图像先进行腐蚀操作，再进行膨胀操作。开运算可以用来抹除图像外部的细节（或者噪声）。 import matplotlib.pyplot as plt\rimport matplotlib\rimport cv2\rimport numpy as np\rmatplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为黑体\rmatplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\rimage=cv2.imread(\"./images/2.jpg\");\rplt.subplot(221)\rplt.title(\"原始图\")\rplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\rplt.subplot(222)\rplt.title(\"开运算\")\rzeroArray=np.ones((5,5))\r#腐蚀掉噪音，然后在膨胀特点\rdest=cv2.erode(image, zeroArray)\rdest=cv2.dilate(dest, zeroArray)\rplt.imshow(dest)\rplt.subplot(223)\rplt.title(\"开运算\")\rdst = cv2.morphologyEx(image, cv2.MORPH_OPEN, zeroArray) #也可以使用这个形态学方法，效果和erode一致\rplt.imshow(dst) 效果图 闭运算 闭运算是将图像先进行膨胀操作，再进行腐蚀操作。闭运算可以抹除图像内部的细节（或者噪声）。 梯度运算 这里的梯度是指图像梯度，可以简单地理解为像素的变化程度。如果几个连续的像素，其像素值跨度越大，则梯度值越大。\n梯度运算的运算过程如图12.15所示，让原图的膨胀图减原图的腐蚀图。因为膨胀图比原图大，腐蚀图比原图小，利用腐蚀图将膨胀图掏空，就得到了原图的轮廓图。 顶帽运算 顶帽运算的运算过程如图12.17所示，让原图减原图的开运算图。因为开运算抹除图像的外部细节，“有外部细节”的图像减去“无外部细节”的图像，得到的结果就只剩外部细节了，所以经过顶帽运算之后，小蜘蛛就只剩蜘蛛腿了。 黑帽运算 黑帽运算的运算过程如图12.19所示，让原图的闭运算图减去原图。因为闭运算抹除图像的内部细节，“无内部细节”的图像减去“有内部细节”的图像，得到的结果就只剩内部细节了，所以经过黑帽运算之后，小蜘蛛就只剩下斑点、花纹和眼睛了。 图形检测 图像轮廓 轮廓是指图像中图形或物体的外边缘线条。简单的几何图形轮廓是由平滑的线构成的，容易识别，但不规则图形的轮廓可能由许多个点构成，识别起来比较困难。\nOpenCV提供的findContours()方法可以通过计算图像梯度来判断图像的边缘，然后将边缘的点封装成数组返回。findContours()方法的语法如下：\ncontours, hierarchy = cv2.findContours(image, mode, methode)\r参数说明：\nimage：被检测的图像，必须是8位单通道二值图像。如果原始图像是彩色图像，必须转为灰度图像，并经过二值化处理。 mode：轮廓的检索模式，具体值如表所示。 cv2.RETR_EXTERNAL: 只检索外部轮廓。 cv2.RETR_LIST: 检索所有轮廓，并将其存储在列表中。 cv2.RETR_CCOMP: 检索所有轮廓，并将它们组织为两级层次结构。在顶层中，只有外部轮廓，而在第二层中，有内部轮廓。如果内部轮廓还有孔，则将其视为第三级。 cv2.RETR_TREE: 检索所有轮廓，并将它们组织为完整的层次结构树。 methode：检测轮廓时使用的方法，具体值如表13.2所示。 cv2.CHAIN_APPROX_NONE: 存储所有的轮廓点，相邻的两个轮廓点的像素位置差不超过 1。 cv2.CHAIN_APPROX_SIMPLE: 压缩水平、竖直和对角线方向上的冗余点，仅保留相邻的端点，如一个矩形轮廓只需存储其四个顶点。 cv2.CHAIN_APPROX_TC89_L1 或 cv2.CHAIN_APPROX_TC89_KCOS: 应用 Teh-Chin 链逼近算法中的一种，可以进一步减少轮廓的点数，但需要更长的计算时间。 返回值说明：\nontours：检测出的所有轮廓，list类型，每一个元素都是某个轮廓的像素坐标数组。 hierarchy：轮廓之间的层次关系。 通过findContours()方法找到图像轮廓后，为了方便开发人员观测，最好能把轮廓画出来，于是OpenCV提供了drawContours()方法用来绘制这些轮廓。drawContours()方法的语法如下：\nimage = cv2.drawContours(image, contours, contourIdx, color, thickness, lineTypee, hierarchy, maxLevel, offse) 参数说明：\nmage：被绘制轮廓的原始图像，可以是多通道图像。 contours：findContours()方法得出的轮廓列表。 contourIdx：绘制轮廓的索引，如果为-1则绘制所有轮廓。 color：绘制颜色，使用BGR格式。 thickness：可选参数，画笔的粗细程度，如果该值为-1则绘制实心轮廓。 lineTypee：可选参数，绘制轮廓的线型。 hierarchy：可选参数，findContours()方法得出的层次关系。 maxLevel：可选参数，绘制轮廓的层次深度，最深绘制第maxLevel层。 offse：可选参数，偏移量，可以改变绘制结果的位置。 返回值说明：\nimage：同参数中的image，执行后原始图中就包含绘制的轮廓了，可以不使用此返回值保存结果。 matplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为黑体\rmatplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\rimg=cv2.imread(\"./images/2.jpg\")\rgrayImg=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\rplt.title(\"灰度图\")\rplt.subplot(221)\rplt.imshow(grayImg,cmap=\"gray\")\r#二值化\r_,dst=cv2.threshold(grayImg,127,255,cv2.THRESH_BINARY)\rcontours, hierarchy = cv2.findContours(dst, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\rplt.subplot(222)\rplt.imshow(cv2.drawContours(img.copy(), contours, 2, (0, 0, 255), 5))\r\"\"\"\r矩形包围框是指图像轮廓的最小矩形边界。OpenCV提供的boundingRect()方法可以自动计算轮廓最小矩形边界的坐标、宽和高。boundingRect()方法的语法如下：\rretval = cv2.boundingRect (array)\r参数说明：\rarray：轮廓数组。\r返回值说明：\rretval：元组类型，包含4个整数值，分别是最小矩形包围框的：左上角顶点的横坐标、左上角顶点的纵坐标、矩形的宽和高。所以也可以写成x, y, w, h = cv2.boundingRect (array)的形式。\r\"\"\"\rx,y,w,h = cv2.boundingRect (contours[2])\rprint(x,y,w,h)\rdstImg=img.copy()\rplt.subplot(223)\rcv2.rectangle(dstImg,(x,y),(x+w,y+h),(0,0,255),2)\rplt.imshow(cv2.cvtColor(dstImg, cv2.COLOR_BGR2RGB))\rplt.show() 轮廓拟合 拟合是指将平面上的一系列点，用一条光滑的曲线连接起来。轮廓的拟合就是将凹凸不平的轮廓用平整的几何图形体现出来。本节将介绍如何按照轮廓绘制矩形包围框和圆形包围框。\n矩形包围框 矩形包围框是指图像轮廓的最小矩形边界。OpenCV提供的boundingRect()方法可以自动计算轮廓最小矩形边界的坐标、宽和高。boundingRect()方法的语法如下： retval = cv2.boundingRect (array) 参数说明：\nrray：轮廓数组。 返回值说明：\nretval：元组类型，包含4个整数值，分别是最小矩形包围框的：左上角顶点的横坐标、左上角顶点的纵坐标、矩形的宽和高。所以也可以写成x, y, w, h = cv2.boundingRect (array)的形式。 同上面图像轮廓的例子\n圆形包围框 圆形包围框与矩形包围框一样，是图像轮廓的最小圆形边界。OpenCV提供的minEnclosingCircle ()方法可以自动计算轮廓最小圆形边界的圆心和半径。minEnclosingCircle()方法的语法如下： center, radius = cv2.minEnclosingCircle(points) 参数说明：\npoints：轮廓数组。 返回值说明：\nenter：元组类型，包含2个浮点值，是最小圆形包围框圆心的横坐标和纵坐标。 radius：浮点类型，最小圆形包围框的半径。 效果 多边形包围框 cv2.approxPolyDP 函数是 OpenCV 中针对轮廓近似的函数，其可以将轮廓中的点根据一定的精度要求进行近似，从而化简轮廓的点数，方便后续处理。 该函数的语法如下：\nepsilon = cv2.arcLength(curve, closed)\rapprox = cv2.approxPolyDP(curve, epsilon, closed) 其中，curve 表示输入的轮廓，epsilon 表示近似精度，closed 表示轮廓是否闭合。函数返回一个近似的轮廓。 cv2.approxPolyDP 函数的工作原理是通过 Douglas-Peucker 算法来实现的。该算法的基本思想是：在轮廓中找到一条最长的线段，将其作为轮廓的近似线段，并将轮廓分成两个部分。然后对这两个部分递归进行处理，直到满足精度要求为止。经过这样的处理，得到的轮廓点数将会大大减少，但轮廓的形状仍能得到保留。\n需要注意的是，epsilon 的值越小，得到的近似轮廓点数就越多，但轮廓的形状精度也就越高；反之，epsilon 的值越大，得到的轮廓点数就越少，但轮廓的形状精度也就越低。因此，选择适当的 epsilon 值对于轮廓近似的效果是非常重要的。\nimg=cv2.imread(\"./images/2.jpg\")\rgrayImg=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r_,dst=cv2.threshold(grayImg,127,255,cv2.THRESH_BINARY)\rcontours, hierarchy = cv2.findContours(dst, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\r#近似轮廓\r# 对每个轮廓进行近似\rfor i,e in enumerate([0.01,0.05,0.1,1]):\rimg1=img.copy()\rfor cnt in contours:\repsilon = e * cv2.arcLength(cnt, True) #获取轮廓的周长\rapprox = cv2.approxPolyDP(cnt, epsilon, True)\r# 绘制近似的轮廓\rcv2.drawContours(img1, [approx], 0, (0, 255, 0), 3)\rplt.subplot(int(\"22\"+str(i+1)))\rplt.title(e)\rplt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\rplt.show() 凸包 之前介绍了矩形包围框和圆形包围框，这2种包围框虽然已经逼近了图形的边缘，但这种包围框为了保持几何形状，与图形的真实轮廓贴合度较差。如果能找出图形最外层的端点，将这些端点连接起来，就可以围出一个包围图形的最小包围框，这种包围框叫凸包。\n凸包是最逼近轮廓的多边形，凸包的每一处都是凸出来的，也就是任意3个点组成的内角均小于180°。例如，图13.12就是凸包，而图13.13就不是凸包。 OpenCV提供的convexHull()方法可以自动找出轮廓的凸包，该方法的语法如下：\nhull = cv2.convexHull(points, clockwise, returnPoints) 参数说明：\noints：轮廓数组。 clockwise：可选参数，布尔类型。当该值为True时，凸包中的点按顺时针排列，为False时按逆时针排列。 returnPoints：可选参数，布尔类型。当该值为True时返回点坐标，为False时返回点索引。默认值为True。 返回值说明：\nhull：凸包的点阵数组 img=cv2.imread(\"./images/2.jpg\")\rgrayImg=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r_,dst=cv2.threshold(grayImg,127,255,cv2.THRESH_BINARY)\rcontours, hierarchy = cv2.findContours(dst, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\r# 根据轮廓面积从大到小排序\rsorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\rhull=cv2.convexHull(sorted_contours[0])\rcv2.polylines(img1,[hull],True,(0,0,255),2)\rplt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\rplt.show() Canny边缘检测 Canny边缘检测算法是John F. Canny于1986年开发的一个多级边缘检测算法，该算法根据像素的梯度变化寻找图像边缘，最终可以绘制十分精细的二值边缘图像。\nOpenCV将Canny边缘检测算法封装在Canny()方法中，该方法的语法如下：\nedges = cv2.Canny(image, threshold1, threshold2, apertureSize, L2gradient) 参数说明：\nmage：检测的原始图像。 threshold1：计算过程中使用的第一个阈值，可以是最小阈值，也可以是最大阈值，通常用来设置最小阈值。 threshold2：计算过程中使用的第二个阈值，通常用来设置最大阈值。 apertureSize：可选参数，Sobel算子的孔径大小。 L2gradient：可选参数，计算图像梯度的标识，默认值为False。值为True时采用更精准的算法进行计算。 返回值说明：\ndges：计算后得出的边缘图像，是一个二值灰度图像。 import cv2\rimport matplotlib.pyplot as plt\rimport matplotlib\rmatplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为黑体\rmatplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\rimg=cv2.imread(\"./images/1.jpg\")\rplt.subplot(221)\rplt.title(\"原始图\")\rplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\r#二值化\rr1=cv2.Canny(img,10,50)\rplt.subplot(222)\rplt.title(\"Canny\")\rplt.imshow(cv2.cvtColor(r1, cv2.COLOR_BGR2RGB)) 霍夫直线 霍夫变换是一种特征检测，通过算法识别图像的特征，从而判断图像中的特殊形状，例如直线和圆。\n直线检测 霍夫直线变换是通过霍夫坐标系的直线与笛卡儿坐标系的点之间的映射关系来判断图像中的点是否构成直线。OpenCV将此算法封装成两个方法，分别是cv2.HoughLines()和cv2.HoughLinesP()，前者用于检测无限延长的直线，后者用于检测线段 HoughLinesP()方法名称最后有一个大写的P，该方法只能检测二值灰度图像，也就是只有两种像素值的黑白图像。该方法最后把找出的所有线段的两个端点坐标保存成一个数组。\nHoughLinesP()方法的语法如下：\nlines = cv2.HoughLinesP(image, rho, theta, threshold, minLineLength, maxLineGap)\r参数说明：\nmage：检测的原始图像。 rho：检测直线使用的半径步长，值为1时，表示检测所有可能的半径步长。 theta：搜索直线的角度，值为π/180°时，表示检测所有角度。 threshold：阈值，该值越小，检测出的直线就越多。 minLineLength：线段的最小长度，小于该长度的线段不记录到结果中。 maxLineGap：线段之间的最小距离。 返回值说明： lines：一个数组，元素为所有检测出的线段，每条线段是一个数组，代表线段两个端点的横、纵坐标，格式为[[[x1, y1, x2, y2], [x1, y1, x2, y2]]]。 import cv2\rimport matplotlib.pyplot as plt\rimport matplotlib\rimport numpy as np\rfrom utils import common\rdef show(dilate, title, cmap=None, debug=False):\rif debug:\rplt.title(title)\rplt.imshow(dilate, cmap=cmap)\rplt.show()\rmatplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为黑体\rmatplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\rimg=cv2.imread(\"./images/1.jpg\")\rgrayImg=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\rcommon.show(grayImg,\"原图\",cmap=\"gray\",debug=True)\redges = cv2.Canny(grayImg, 20, 40)\rcommon.show(edges,\"边缘检测图\",cmap=\"gray\",debug=True)\rlines = cv2.HoughLinesP(edges, 1, np.pi/180, 15, 100, 18)\rimg1=img.copy()\rfor line in lines:\rx1,y1,x2,y2=line[0]\rcv2.line(img1,(x1,y1),(x2,y2),(0,0,255),2)\rcommon.show(img1,\"直线\",cmap=\"gray\",debug=True) 圆环检测 霍夫圆环变换的原理与霍夫直线变换类似。OpenCV提供的HoughCircles()方法用于检测图像中的圆环，该方法在检测过程中进行两轮筛选：第一轮筛选找出可能是圆的圆心坐标，第二轮筛选计算这些圆心坐标可能对应的半径长度。该方法最后将圆心坐标和半径封装成一个浮点型数组。\nHoughCircles()方法的语法如下：\ncircles = cv2.HoughCircles(image, method, dp, minDist, param1, param2, minRadius, maxRadius)\r参数说明：\nmage：检测的原始图像。 method：检测方法，OpenCV 4.0.0及以前版本仅提供了cv2.HOUGH_GRADIENT作为唯一可用方法。 dp：累加器分辨率与原始图像分辨率之比的倒数。值为1时，累加器与原始图像具有相同的分辨率；值为2时，累加器的分辨率为原始图像的1/2。通常使用1作为参数。 minDist：圆心之间的最小距离。 param1：可选参数，Canny边缘检测使用的最大阈值。 param2：可选参数，检测圆环结果的投票数。第一轮筛选时投票数超过该值的圆环才会进入第二轮筛选。值越大，检测出的圆环越少，但越精准。 minRadius：可选参数，圆环的最小半径。 maxRadius：可选参数，圆环的最大半径。 返回值说明：\ncircles：一个数组，元素为所有检测出的圆环，每个圆环也是一个数组，内容为圆心的横、纵坐标和半径长度，格式为：[[[x1 ,y1, r1], [x2 ,y2, r2]]]。 视频处理 OpenCV不仅能够处理图像，还能够处理视频。视频是由大量的图像构成的，这些图像以固定的时间间隔从视频中获取。这样，就能够使用图像处理的方法对这些图像进行处理，进而达到处理视频的目的。要处理视频，需要先对视频进行读取、显示和保存等相关操作。为此，OpenCV提供了VideoCapture类和VideoWriter类的相关方法。 VideoCapture类提供了构造方法VideoCapture()，用于完成摄像头的初始化工作。VideoCapture()的语法 apture = cv2.VideoCapture(index|视频文件路径) 参数说明：\nvideo：要打开的视频。 filename：打开视频的文件名。例如，公司宣传.avi等。 import cv2\r# 打开视频文件\rcap = cv2.VideoCapture('video.avi')\rwhile True:\r# 读取视频帧\rret, frame = cap.read()\r# 如果视频结束或者读取失败，退出循环\rif not ret:\rbreak\r# 显示当前帧\rcv2.imshow('frame', frame)\r# 等待按键输入\rkey = cv2.waitKey(1) \u0026 0xFF\r# 如果按下 'q' 键，退出循环\rif key == ord('q'):\rbreak\r# 释放资源\rcap.release()\rcv2.destroyAllWindows() 人脸检测 人脸识别是基于人的脸部特征信息进行身份识别的一种生物识别技术，也是计算机视觉重点发展的技术。机器学习算法诞生之后，计算机可以通过摄像头等输入设备自动分析图像中包含的内容信息，随着技术的不断发展，现在已经有了多种人脸识别的算法。本章将介绍OpenCV自带的多种图像跟踪技术和3种人脸识别技术的用法。 级联分类器 将一系列简单的分类器按照一定顺序级联到一起就构成了级联分类器，使用级联分类器的程序可以通过一系列简单的判断来对样本进行识别。例如，依次满足“有6条腿”“有翅膀”“有头、胸、腹”这3个条件的样本就可以被初步判断为昆虫，但如果任何一个条件不满足，则不会被认为是昆虫\nOpenCV提供了一些已经训练好的级联分类器，这些级联分类器以XML文件的方式保存在以下路径中： …\\Python\\Lib\\site-packages\\cv2\\data\n我的window在：D:/condaenv/tensorflowcpu/Library/etc/haarcascades/ OpenCV实现人脸检测需要做两步操作：加载级联分类器和使用分类器识别图像。这两步操作都有对应的方法。\n首先是加载级联分类器，OpenCV通过CascadeClassifier()方法创建了分类器对象，其语法如下： \u003cCascadeClassifier object\u003e = cv2.CascadeClassifier(filename) 参数说明：\nfilename：级联分类器的XML文件名。 返回值说明： object：分类器对象。 然后使用已经创建好的分类器对图像进行识别，这个过程需要调用分类器对象的detectMultiScale()方法，其语法如下：\nobjects = cascade.detectMultiScale(image, scaleFactor, minNeighbors, flags, minSize, maxSize) 对象说明：\ncascade：已有的分类器对象。 参数说明：\nimage：待分析的图像。 scaleFactor：可选参数，扫描图像时的缩放比例。 minNeighbors：可选参数，每个候选区域至少保留多少个检测结果才可以判定为人脸。该值越大，分析的误差越小。 flags：可选参数，旧版本OpenCV的参数，建议使用默认值。 minSize：可选参数，最小的目标尺寸。 maxSize：可选参数，最大的目标尺寸。 返回值说明： objects：捕捉到的目标区域数组，数组中每一个元素都是一个目标区域，每一个目标区域都包含4个值，分别是：左上角点横坐标、左上角点纵坐标、区域宽、区域高。object的格式为：[[244　203　111　111]　[432　81　133　133]]。 原图： #%%\rimport cv2\rimport matplotlib.pyplot as plot\r#加载人脸模型\rxml_dir=\"D:/condaenv/tensorflowcpu/Library/etc/haarcascades/\"\r# 加载人脸检测器\rface_cascade = cv2.CascadeClassifier(xml_dir+'haarcascade_frontalface_alt2.xml')\rprint(face_cascade)\r# 读取要处理的图片\rimg = cv2.imread('../images/people.png')\r# 转换为灰度图像\rgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r# 检测人脸\rfaces = face_cascade.detectMultiScale(gray)\r# 在图像中框出人脸\rfor (x, y, w, h) in faces:\rcv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\r# 显示结果\rplot.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\rplot.show()",
    "description": "@[toc]\nPython OpenCV 入门指南 OpenCV是一个强大的计算机视觉库，它可以用于处理图像和视频数据，以及进行目标检测和跟踪等任务。，将学会如何使用Python编写OpenCV代码来进行基础和进阶的图像处理和分析。\n学习OpenCV可以帮助你掌握基本的图像处理技术，包括图像读取和处理、阈值处理、形态学函数、模板匹配、滤波器、图形处理、视频处理和人脸检测等方面的内容。这些技术都是计算机视觉和图像处理领域的基本内容，也是卷积神经网络的基础。通过学习OpenCV，你可以更好地理解卷积神经网络的工作原理和应用。同时，OpenCV也是一个非常流行的图像处理库，掌握它可以帮助你更好地处理和分析图像数据。\n参考书籍：Python Opencv从入门到精通\n安装OpenCV 在开始编写OpenCV代码之前，我们需要先安装OpenCV库。我们可以通过pip包管理器来安装：\npip install opencv-python\n你可以使用conda或者micromamba来安装虚拟环境,安装好notebook环境\n打印opencv版本\nimport cv2\rprint(\"OpenCV version:\")\rprint(cv2.__version__) 输出 OpenCV version: 4.7.0\n基础篇 图像读取和显示 在开始处理图像之前，我们需要学习如何读取和显示图像。下面的代码演示了如何使用OpenCV库读取和显示图像：\nimport cv2 读取图像 img = cv2.imread('image.jpg') 显示图像 opencv显示\ncv2.imshow('Image', img)\rcv2.waitKey(0)\rcv2.destroyAllWindows()` 在上面的代码中，我们首先使用cv2.imread()函数读取了一个名为image.jpg的图像文件。然后，我们使用cv2.imshow()函数来显示这个图像，并使用cv2.waitKey()和cv2.destroyAllWindows()函数来等待用户按下任意键，然后关闭显示窗口。\n注意如果是使用notebook执行waitKey(0)显示，会存在第二次运行无法显示的问题，可以cv2.waitKey(3)设置指定时间自动结束，\nimport matplotlib\rimport matplotlib.pyplot as plt\rmatplotlib.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为黑体\rmatplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题\rimg = cv2.imread('image.jpg')\rrgbimg = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #opencv像素顺序是bgr\rplt.title(title)\rplt.imshow(img, cmap=cmap)\rplt.show() 裁剪图像 从左上角坐标为(200, 100)的位置开始，裁剪一个宽为400像素、高为400像素的矩形区域。",
    "tags": [],
    "title": "图像处理实战01-OpenCV 入门指南",
    "uri": "/docs/programming/ai/computer_vision/tools/action_01_opencv/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 计算机视觉 \u003e 应用案例",
    "content": "yolov5 YOLOv5 是一种目标检测算法，它是 YOLO (You Only Look Once) 系列算法的最新版本。YOLOv5 采用了一种新的架构，它包括一个基于 CSPNet (Cross Stage Partial Network) 的主干网络以及一系列改进的技巧，如多尺度训练、数据增强、网络混合精度训练等，从而实现了更快的检测速度和更好的检测精度。\nYOLOv5 支持多种类型的目标检测任务，如物体检测、人脸检测、车辆检测等，可以应用于各种实际场景，如智能安防、自动驾驶、机器人视觉等。同时，YOLOv5 还提供了预训练的模型和开源代码，方便开发者进行模型的训练和应用。\ngithub地址：https://github.com/ultralytics/yolov5/blob/master/README.zh-CN.md 官网：https://ultralytics.com/\n发展历程 YOLO（You Only Look Once）是一系列的目标检测模型，由Joseph Redmon等人开发。以下是YOLO系列的发展历程：\nYOLOv1：于2015年首次提出，是YOLO系列的第一个版本。YOLOv1通过将目标检测任务转化为回归问题，将图像划分为网格并预测每个网格的边界框和类别概率。然而，YOLOv1存在定位不准确和对小目标敏感的问题。\nYOLOv2（YOLO9000）：于2016年提出，是YOLO系列的第二个版本。YOLOv2通过引入Darknet-19网络结构、使用anchor boxes和多尺度预测来改进检测性能。同时，YOLOv2还引入了目标类别的语义分割，可以检测更多类别的目标。\nYOLOv3：于2018年提出，是YOLO系列的第三个版本。YOLOv3针对YOLOv2存在的问题进行了改进，引入了多尺度预测、使用FPN结构和使用更小的anchor boxes等技术，提高了检测精度和对小目标的检测能力。\nYOLOv4：于2020年提出，是YOLO系列的第四个版本。YOLOv4在YOLOv3的基础上引入了一系列改进，包括CSPDarknet53作为主干网络、使用SAM和PANet模块来提取特征、使用YOLOv3和YOLOv4的预训练权重进行初始化等，提高了检测性能和速度。\nYOLOv5：于2020年提出，是YOLO系列的第五个版本。YOLOv5采用了轻量化的网络结构，提高了检测的速度，并引入了一些新功能，如YOLOv5-seg分割模型、Paddle Paddle导出功能、YOLOv5 AutoCache自动缓存功能和Comet日志记录和可视化集成功能。\n总体而言，YOLO系列模型通过不断的改进和优化，提高了目标检测的性能和速度，并在计算机视觉领域取得了重要的突破。\nyolov8 YOLOv8是YOLO系列模型的一个变种，它在YOLOv5的基础上进行了改进和优化。YOLOv8模型包含了检测（Detect）、分割（Segment）和姿态估计（Pose）、跟踪（Track）以及分类（Classify）等功能。下面是对这些功能的简要说明：\n检测（Detect）：YOLOv8模型能够对图像或视频中的目标进行实时的物体检测。它通过预测目标的边界框和类别信息来完成检测任务。\n分割（Segment）：YOLOv8模型还支持目标分割的功能，即将图像中的每个像素进行分类，将不同的目标区域进行分割。这个功能可以用于识别图像中的不同物体，并进行更精确的定位和分析。\n姿态估计（Pose）：YOLOv8模型还可以对检测到的目标进行姿态估计，即推断目标在三维空间中的姿态信息。这对于一些需要了解目标的方向和位置的应用非常有用，比如人体姿态分析、机器人导航等。\n跟踪（Track）：YOLOv8模型还具有目标跟踪的功能，即在视频中连续追踪相同目标的位置和轨迹。这对于视频监控、自动驾驶等应用非常重要。\n分类（Classify）：除了目标检测和分割功能之外，YOLOv8模型还可以对检测到的目标进行分类，即给出目标的类别信息。这对于了解目标的属性和进行更细粒度的分析非常重要。\n总而言之，YOLOv8模型综合了多种功能，包括检测、分割、姿态估计、跟踪和分类等，使其具备了更广泛的应用领域和更强大的功能。 github地址：https://github.com/ultralytics/ultralytics\nv5入门示例 安装 克隆 repo，并要求在 Python\u003e=3.7.0 环境中安装 requirements.txt ，且要求 PyTorch\u003e=1.7 。\nmicromamba create prefix=d:/python380 python=3.8 #创建3.8的虚拟环境\rmicromamba activate d:/python380\rgit clone https://github.com/ultralytics/yolov5 # clone\rcd yolov5\rpip install -r requirements.txt # install 源代码目录结构\nyolov5/\r├── data/ # 数据集配置目录\r│ ├── coco.yaml # COCO数据集配置文件，里面有数据集的下载地址和加载的python脚本\r│ ├──ImageNet.yaml # ImageNet数据集\r│ ├── custom.yaml # 自定义数据集配置文件\r│ └── ... # 其他数据集配置文件\r├── models/ # 模型定义目录\r│ ├── common.py # 通用函数和类定义\r│ ├── experimental.py # 实验性模型定义\r│ ├── export.py # 导出模型为ONNX的脚本\r│ ├── models.py # YOLOv5模型定义\r│ ├── yolo.py # YOLO类定义\r│ └── ... # 其他模型定义文件\r├── utils/ # 实用工具目录\r│ ├── autoanchor.py # 自动锚框生成工具\r│ ├── datasets.py # 数据集处理工具\r│ ├── general.py # 通用实用函数\r│ ├── google_utils.py # Google云平台工具\r│ ├── loss.py # 损失函数定义\r│ ├── metrics.py # 评估指标定义\r│ ├── torch_utils.py # PyTorch工具\r│ ├── wandb_logging.py # WandB日志记录工具\r│ └── ... # 其他实用工具文件\r├── runs/ # 训练和预测的结果输出目录\r│ ├── detect # 使用detect.py训练后输出目录，输出的目录是[ex自增数字]\r│ ├── train # 使用detect.py训练后输出目录，输出的目录是[ex自增数字],包含了训练好的模型和测试集效果\r├── weights/ # 预训练模型权重目录\r├── .gitignore # Git忽略文件配置\r├── Dockerfile # Docker容器构建文件\r├── LICENSE # 许可证文件\r├── README.md # 项目说明文档\r├── requirements.txt # 项目依赖包列表\r├── train.py # 训练脚本\r├── detect.py # 预测脚本\r├── export.py # 导出YOLOv5 PyTorch model to 其他格式\r├── hubconf.py # hubconf.py文件是用于定义模型和数据集的Python模块\r└── ... # 其他源代码文件 这里通过yolov5可以下载到很多常用的训练数据集，而且很轻松的找到下载地址,如ImageNet, coco128等，不用自己辛苦的找了\n模型下载 下载地址：https://github.com/ultralytics/yolov5/releases\nv6.1 这里的版本是v6.1是yolov5的子版本号\nPretrained Checkpoints Pretrained Checkpoints 是预训练权重文件的一种称呼。在深度学习中，预训练权重是指在大规模数据集上通过无监督学习或有监督学习得到的模型参数。这些参数通常可以被用来初始化一个新的模型，从而加速模型训练并提高模型的性能。\nPretrained Checkpoints 是指已经训练好的预训练权重文件，可以用来初始化一个新的模型，并继续训练这个模型以适应新的任务或数据集。这种方法被称为迁移学习，可以大大提高模型的训练效率和泛化能力。在计算机视觉领域，常见的预训练网络包括 VGG、ResNet、Inception、MobileNet 等。\n模型概述 以下模型列的解释\n列名 解释 Model 模型的名称 size(pixels) 输入图像的大小（以像素为单位） mAPval0.5:0.95 在验证集上的平均精确度（mean Average Precision），考虑所有IOU阈值从0.5到0.95的情况，准确率是% mAPval0.5 在验证集上的平均精确度，只考虑IOU阈值为0.5的情况 Speed CPU b1(ms) 在CPU上使用batch size为1时的推理速度（以毫秒为单位） Speed V100 b1(ms) 在NVIDIA V100 GPU上使用batch size为1时的推理速度（以毫秒为单位） Speed V100 b32(ms) 在NVIDIA V100 GPU上使用batch size为32时的推理速度（以毫秒为单位） params (M) 模型的参数量（以百万为单位） FLOPs @640 (B) 在输入图像大小为640时，模型的浮点运算次数（以十亿为单位） Model size(pixels) mAPval0.5:0.95 mAPval0.5 Speed CPU b1(ms) Speed V100 b1(ms) Speed V100 b32(ms) params (M) FLOPs @640 (B) YOLOv5n 640 28.0 45.7 45 6.3 0.6 1.9 4.5 YOLOv5s 640 37.4 56.8 98 6.4 0.9 7.2 16.5 YOLOv5m 640 45.4 64.1 224 8.2 1.7 21.2 49.0 YOLOv5l 640 49.0 67.3 430 10.1 2.7 46.5 109.1 YOLOv5x 640 50.7 68.9 766 12.1 4.8 86.7 205.7 YOLOv5n6 1280 36.0 54.4 153 8.1 2.1 3.2 4.6 YOLOv5s6 1280 44.8 63.7 385 8.2 3.6 12.6 16.8 YOLOv5m6 1280 51.3 69.3 887 11.1 6.8 35.7 50.0 YOLOv5l6 1280 53.7 71.3 1784 15.8 10.5 76.8 111.4 v7.0 新的YOLOv5 v7.0实例分割模型是世界上最快、最准确的，超过了所有当前的SOTA基准。我们使它们非常简单易用，可以轻松进行训练、验证和部署。 这个版本中的主要目标是引入与我们现有的目标检测模型类似的超级简单的YOLOv5分割工作流程。 重要更新\n分割模型 ⭐ 新增：第一次提供了SOTA YOLOv5-seg COCO预训练的分割模型（由@glenn-jocher、@AyushExel和@Laughing-q开发的#9052） Paddle Paddle导出：使用python export.py –include paddle 可以将任何YOLOv5模型（cls、seg、det）导出为Paddle格式（由@glenn-jocher开发的#9459） YOLOv5 AutoCache：使用python train.py –cache ram 现在会扫描可用内存并与预测的数据集RAM使用量进行比较。这降低了缓存风险，并应该有助于提高数据集缓存功能的使用率，从而显著加快训练速度。（由@glenn-jocher开发的#10027） Comet日志记录和可视化集成：永久免费，Comet可以保存YOLOv5模型，恢复训练，并进行交互式可视化和调试预测。（由@DN6开发的#9232） Model size (pixels) mAPbox50-95 mAPmask50-95 Train time 300 epochsA100 (hours) Speed ONNX CPU(ms) Speed TRT A100(ms) params (M) FLOPs @640(B) YOLOv5n-seg 640 27.6 23.4 80:17 62.7 1.2 2.0 7.1 YOLOv5s-seg 640 37.6 31.7 88:16 173.3 1.4 7.6 26.4 YOLOv5m-seg 640 45.0 37.1 108:36 427.0 2.2 22.0 70.8 YOLOv5l-seg 640 49.0 39.9 66:43 (2x) 857.4 2.9 47.9 147.7 YOLOv5x-seg 640 50.7 41.4 62:56 (3x) 1579.2 4.5 88.8 265.7 我这里选择一个V6.1模型yolov5n6.pt 将模型丢到yolov5项目根目录即可 预测 因为预训练模型，已经有检测某些类别能力，我们可以看下data/coco.yml中names可以看到总共有80个类别 在yolov5中可以使用./detect.py脚本来进行目标物品检测。 以下是对\"./detect.py\"脚本中常见参数的详细解释：\n--source：指定输入源，可以是图像路径、视频文件路径或摄像头索引（默认为当前目录data/images，里面就两张图片）。\n--weights：指定模型权重文件的路径。可以是本地路径或PaddleHub模型中心的模型名称，默认是当前目录的yolov5s.pt。\n--data：指定要使用的数据集的配置文件。数据集的配置文件包含了数据集的路径、类别标签、训练集、验证集和测试集的划分等信息,默认data/coco128.yaml，选填。\n--img-size：指定输入图像的尺寸，格式为\",\"，例如\"640,480\"。默认为640x640。\n--conf-thres：目标置信度阈值，范围为0到1。超过该阈值的目标将被保留，默认为0.25。\n--iou-thres：NMS（非极大值抑制）的IoU（交并比）阈值，范围为0到1。重叠度大于该阈值的目标将被合并，默认为0.45。\n--max-det：每个图像中最多检测的目标数，默认为100。\n--device：指定使用的设备，可以是\"cpu\"或\"cuda\"。默认为\"cpu\"。\n--view-img：在检测过程中显示图像窗口。\n--save-txt：保存检测结果的txt文件。\n--save-conf：保存检测结果的置信度。\n--save-crop：保存检测结果的裁剪图像。\n--half：使用半精度浮点数进行推理。\n这些参数可以根据您的需求进行调整，以获得最佳的检测结果。您可以在运行脚本时使用--help参数查看更多参数选项和说明。\n执行命令预测\npython ./detect.py --source ./data/images --weight ./yolov5n6.pt 执行结果\n(D:\\condaenv\\yolov5) D:\\code1\\yolov5-master\\yolov5-master\u003epython ./detect.py --source ./data/images --weight ./yolov5n6.pt\rdetect: weights=['./yolov5n6.pt'], source=./data/images, data=data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=Fal\rse, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\rYOLOv5 2023-5-30 Python-3.8.16 torch-2.0.1+cpu CPU\rFusing layers...\rYOLOv5n6 summary: 280 layers, 3239884 parameters, 0 gradients\rimage 1/2 D:\\code1\\yolov5-master\\yolov5-master\\data\\images\\bus.jpg: 640x512 4 persons, 1 bus, 211.9ms\rimage 2/2 D:\\code1\\yolov5-master\\yolov5-master\\data\\images\\zidane.jpg: 384x640 3 persons, 1 tie, 152.9ms\rSpeed: 1.0ms pre-process, 182.4ms inference, 3.0ms NMS per image at shape (1, 3, 640, 640)\rResults saved to runs\\detect\\exp8 找到runs\\detect\\exp8 打开目录查看分类图片 训练模型 参考自官网：https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/#before-you-start\n准备数据集 创建数据集yaml COCO128是一个小型教程数据集的例子，由COCO train2017中的前128张图像组成。这128张图像同时用于训练和验证，以验证我们的训练流程能够过拟合。data/coco128.yaml是数据集配置文件，定义了以下内容： 1）数据集根目录路径以及训练/验证/测试图像目录的相对路径（或包含图像路径的*.txt文件）； 2）类别名称字典。\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\rpath: ../datasets/coco128 # dataset root dir\rtrain: images/train2017 # train images (relative to 'path') 128 images\rval: images/train2017 # val images (relative to 'path') 128 images\rtest: # test images (optional)\r# Classes (80 COCO classes)\rnames:\r0: person\r1: bicycle\r2: car\r...\r77: teddy bear\r78: hair drier\r79: toothbrush\r# Download script/URL (optional)\rdownload: https://ultralytics.com/assets/coco128.zip https://ultralytics.com/assets/coco128.zip下载后，目录结构如下 我这里用来训练判断一个身份证的正反面，我在项目根目录新建一个idcard目录，下面在建一个mul目录，这个目录只是用来训练不同的身份证信息用来区分的，我们的所有数据集都在mul目录\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\rpath: ./idcard/mul # dataset root dir\rtrain: images # train images val: images # val images\rtest: images # test images # Classes\rnames:\r0: idcard_z #表示身份证正面\r1: idcard_f #表示身份证反面 注意这里yolov5回自动找path下的train目录在加上你的images作为图片的目录 比如真正的训练目录是：./idcard/mul/train/images，images的同级目录下会有个labels目录是标注 验证集的目录是：./idcard/mul/val/images 测试集的目录是：./idcard/test/val/images 一般来说，常见的做法是将数据集划分为训练集、验证集和测试集，比如将数据划分为70%的训练集、15%的验证集和15%的测试集。这种比例通常适用于较小的数据集。对于较大的数据集，可以考虑增加验证集和测试集的比例。\n创建labels 在使用注释工具（labelme,lableimg）为图像标注后，将标签导出为YOLO格式，每个图像对应一个*.txt文件（如果图像中没有对象，则不需要*.txt文件）。*.txt文件的规范如下：\n每个对象占据一行 每行的格式为：类别 x中心点 y中心点 宽度 高度。 框的坐标必须使用归一化的xywh格式（范围在0-1之间）。如果您的框的坐标是以像素为单位的，则需要将x中心点和宽度除以图像宽度，并将y中心点和高度除以图像高度。 类别编号从零开始（索引为0），和数据集yaml的names索引对应。 这里建议使用labelimg标注\npip install labelimg -i https://pypi.tuna.tsinghua.edu.cn/simple 切换到当前环境输入labelimg ，输入labelimage命令打开 选择open dir选择你的需要标记的图片目录(idcard/mul/train/images目录)，Change Save Dir选择你的idcard/mul/train/labels目录,选择YOLO格式 打开了图片后，需要一张一张图片的标记，常用的操作步骤是：\n按w唤起一个矩形框，选择你要选择的目标，选择后，弹出label，注意要先标注一个data.yaml中索引为0的，然后是1的，后面在弹出是可以选择的。 标准完成后ctrl+s保存。 按键盘d键切换到下一张图片，继续按w矩形框标注，知道所有图片完成。 在你的labels目录下会有个classes.txt，看下他的顺序是否和data.yaml一致，如果不一致，不要调整classes.txt,调整data.yaml保持一致就行。\n训练 我这里准备了差不多350个标注好的图片，训练后识别率98%。 使用train.py执行\n# --weight是指定初始的权重，可以用它来fine tuning调整训练你自己的模型。\rpython train.py --batch-size 4 --epochs 10 --data .\\idcard\\mul\\idcard.yaml --weight .\\yolov5n6.pt 执行完成后，runs\\trains\\expn\\weights\\best.pt就是训练好的模型，可以使用之前的detect.py指定这个模型来预测下\npython ./detect.py --source .\\idcard\\mul\\test\\images --weight .\\runs\\train\\exp3\\weights\\best.pt 查看runs\\detect\\expn\\下的预测图片 模型应用 我们需要在我们的应用使用生成好的best.pt模型可以使用torch.hub\n#使用我们本地之前用于训练的yolov5-master，我有把best.pt拷贝到当前目录\rmodel = torch.hub.load('D:\\\\code1\\\\yolov5-master\\\\yolov5-master', 'custom', path='./best.pt', source='local') # local repo\r#print(model)\r# 读取图像\rimg = cv2.imread('../images/zm.jpg')\r# 进行预测\rresults = model(img)\rresultLabel=[]\r# 解析预测结果\rfor result in results.xyxy[0]:\rx1, y1, x2, y2, conf, cls = result.tolist()\rif conf \u003e 0.5:\r# 绘制边框和标签\rcv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\rcv2.putText(img, f\"{model.names[int(cls)]} {conf:.2f}\", (int(x1), int(y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\rresultLabel.append(model.names[int(cls)])\r# 显示图像\rprint(\"预测的结果是\",resultLabel)\rplt.imshow(img)\rplt.show() 这是官方提供在线的版本调用，但是程序会自动去下载ultralytics/yolov5包和yolov5s模型，速度很慢\nimport torch\r# Model\rmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\") # or yolov5n - yolov5x6, custom\r# Images\rimg = \"https://ultralytics.com/images/zidane.jpg\" # or file, Path, PIL, OpenCV, numpy, list\r# Inference\rresults = model(img)\r# Results\rresults.print() # or .show(), .save(), .crop(), .pandas(), etc.",
    "description": "yolov5 YOLOv5 是一种目标检测算法，它是 YOLO (You Only Look Once) 系列算法的最新版本。YOLOv5 采用了一种新的架构，它包括一个基于 CSPNet (Cross Stage Partial Network) 的主干网络以及一系列改进的技巧，如多尺度训练、数据增强、网络混合精度训练等，从而实现了更快的检测速度和更好的检测精度。\nYOLOv5 支持多种类型的目标检测任务，如物体检测、人脸检测、车辆检测等，可以应用于各种实际场景，如智能安防、自动驾驶、机器人视觉等。同时，YOLOv5 还提供了预训练的模型和开源代码，方便开发者进行模型的训练和应用。\ngithub地址：https://github.com/ultralytics/yolov5/blob/master/README.zh-CN.md 官网：https://ultralytics.com/\n发展历程 YOLO（You Only Look Once）是一系列的目标检测模型，由Joseph Redmon等人开发。以下是YOLO系列的发展历程：\nYOLOv1：于2015年首次提出，是YOLO系列的第一个版本。YOLOv1通过将目标检测任务转化为回归问题，将图像划分为网格并预测每个网格的边界框和类别概率。然而，YOLOv1存在定位不准确和对小目标敏感的问题。\nYOLOv2（YOLO9000）：于2016年提出，是YOLO系列的第二个版本。YOLOv2通过引入Darknet-19网络结构、使用anchor boxes和多尺度预测来改进检测性能。同时，YOLOv2还引入了目标类别的语义分割，可以检测更多类别的目标。\nYOLOv3：于2018年提出，是YOLO系列的第三个版本。YOLOv3针对YOLOv2存在的问题进行了改进，引入了多尺度预测、使用FPN结构和使用更小的anchor boxes等技术，提高了检测精度和对小目标的检测能力。\nYOLOv4：于2020年提出，是YOLO系列的第四个版本。YOLOv4在YOLOv3的基础上引入了一系列改进，包括CSPDarknet53作为主干网络、使用SAM和PANet模块来提取特征、使用YOLOv3和YOLOv4的预训练权重进行初始化等，提高了检测性能和速度。\nYOLOv5：于2020年提出，是YOLO系列的第五个版本。YOLOv5采用了轻量化的网络结构，提高了检测的速度，并引入了一些新功能，如YOLOv5-seg分割模型、Paddle Paddle导出功能、YOLOv5 AutoCache自动缓存功能和Comet日志记录和可视化集成功能。\n总体而言，YOLO系列模型通过不断的改进和优化，提高了目标检测的性能和速度，并在计算机视觉领域取得了重要的突破。\nyolov8 YOLOv8是YOLO系列模型的一个变种，它在YOLOv5的基础上进行了改进和优化。YOLOv8模型包含了检测（Detect）、分割（Segment）和姿态估计（Pose）、跟踪（Track）以及分类（Classify）等功能。下面是对这些功能的简要说明：\n检测（Detect）：YOLOv8模型能够对图像或视频中的目标进行实时的物体检测。它通过预测目标的边界框和类别信息来完成检测任务。\n分割（Segment）：YOLOv8模型还支持目标分割的功能，即将图像中的每个像素进行分类，将不同的目标区域进行分割。这个功能可以用于识别图像中的不同物体，并进行更精确的定位和分析。\n姿态估计（Pose）：YOLOv8模型还可以对检测到的目标进行姿态估计，即推断目标在三维空间中的姿态信息。这对于一些需要了解目标的方向和位置的应用非常有用，比如人体姿态分析、机器人导航等。\n跟踪（Track）：YOLOv8模型还具有目标跟踪的功能，即在视频中连续追踪相同目标的位置和轨迹。这对于视频监控、自动驾驶等应用非常重要。\n分类（Classify）：除了目标检测和分割功能之外，YOLOv8模型还可以对检测到的目标进行分类，即给出目标的类别信息。这对于了解目标的属性和进行更细粒度的分析非常重要。\n总而言之，YOLOv8模型综合了多种功能，包括检测、分割、姿态估计、跟踪和分类等，使其具备了更广泛的应用领域和更强大的功能。 github地址：https://github.com/ultralytics/ultralytics\nv5入门示例 安装 克隆 repo，并要求在 Python\u003e=3.7.0 环境中安装 requirements.txt ，且要求 PyTorch\u003e=1.7 。\nmicromamba create prefix=d:/python380 python=3.8 #创建3.8的虚拟环境\rmicromamba activate d:/python380\rgit clone https://github.com/ultralytics/yolov5 # clone\rcd yolov5\rpip install -r requirements.txt # install 源代码目录结构",
    "tags": [],
    "title": "图像处理实战02-yolov5目标检测",
    "uri": "/docs/programming/ai/computer_vision/applications/action_02_yolov5/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 框架学习",
    "content": "@[toc]\n简介 TensorFlow是一种端到端开源机器学习平台，它提供了一个全面而灵活的生态系统，包含各种工具、库和社区资源，能够助力研究人员推动先进机器学习技术的发展。在TensorFlow机器学习框架下，开发者能够轻松地构建和部署由机器学习提供支持的应用。[2]\nKeras是一个高层次神经网络 API，适用于快速构建原型、高级研究和生产。它作为TensorFlow的一个接口，可以兼容多种深度学习框架。Keras 的核心数据结构是 model，一种组织网络层的方式。最简单的模型是 Sequential 顺序模型，它由多个网络层线性堆叠。对于更复杂的结构，你应该使用 Keras 函数式 API，它允许构建任意的神经网络图。 Keras最开始是为研究人员开发的，其目的在于快速实验，具有相同的代码可以在CPU或GPU上无缝切换运行的特点，同时也具有用户友好的API，方便快速地开发深度学习模型的原型。 Keras使用类sklearn的api接口来调用tensorflow，从sklearn机器学习中切换过来，更加容易上手。 Tenforflow2.0后直接内置可keras。\n运行硬件 TensorFlow支持在CPU和GPU上运行。GPU（图形处理单元）是一种专门用于加速计算的硬件，它可以大大提高深度学习模型的训练速度。相对而言，CPU（中央处理器）的每个核心具有更强大的处理能力，但它们的数量通常非常有限，因此在处理大数据时它们表现不佳。\nTensorFlow GPU和CPU的主要区别在于如何使用硬件来处理计算任务，以及处理速度的差异。在CPU上，TensorFlow利用所有可用的CPU内核并将任务分配给它们，这可能需要几分钟或几小时来完成。在GPU上，TensorFlow使用CUDA（Compute Unified Device Architecture）技术来利用GPU进行并行计算并加速训练过程，因为GPU拥有数百到数千个小型核心，这比CPU的几十个核心要多得多。这使得TensorFlow能够在GPU上实现更快的训练速度和更高的吞吐量，尤其是在处理大规模的深度学习任务时。\n另外需要注意的是，如果你的计算机没有安装专门的GPU，则无法使用TensorFlow GPU。在这种情况下，TensorFlow会使用CPU作为默认选项，但是训练过程会比在GPU上慢得多。因此，如果你需要进行大量的深度学习训练任务，建议使用具有至少一张GPU的计算机来加速训练。\n总之，TensorFlow GPU和CPU之间的区别在于它们的硬件架构、并行计算能力以及处理速度等方面。当进行大规模的深度学习训练时，使用GPU可以显著提高训练速度和吞吐量，而对于较小的任务或者没有专门GPU的计算机，则应该使用CPU。\ncuda和cuddn CUDA（Compute Unified Device Architecture）是一种由NVIDIA公司开发的并行计算平台和编程模型，它允许开发人员使用标准C/C++语言编写基于GPU的高性能应用程序。CUDA包括一个可编程的内核语言（CUDA C/C++），一个并行计算库（CUDA Toolkit），以及驱动程序和硬件架构，支持对NVIDIA GPU进行高性能并行计算。与CPU相比，GPU在并行处理任务时的性能要高得多，因此CUDA被广泛用于深度学习、科学计算和高性能计算等领域。[2]\ncuDNN（CUDA Deep Neural Network library）是NVIDIA CUDA的一个加速库，它提供了一组高度优化的本地函数，用于加速深度神经网络模型的训练和推理。cuDNN主要用于卷积神经网络（CNNs）和递归神经网络（RNNs）等深度学习模型的优化，从而实现更快的训练和推理速度。cuDNN支持多种深度学习框架，包括TensorFlow，PyTorch和Caffe等。[1]\n因此，CUDA是一种GPU计算平台和编程模型，cuDNN是其中一个加速库，专门用于加速深度学习模型的训练和推理。这两个技术结合起来，可以实现对GPU的高性能并行计算和深度学习模型的优化，从而提高深度学习任务的整体性能。\n不同的tensorflow版本需要不同的cuda和cuddn版本，google官网可查看 https://tensorflow.google.cn/install/source_windows?hl=en 如果电脑有gpu建议安装tensorflow-gpu,如果电脑没有gpu安装性能较差的tensorflow\n打开任务管理器-性能,查看你是否支持gpu 从这里我们可以看到我的cpu是英伟达(nvidia)的gtx1050 如果电脑已经安装显卡驱动，cuda肯定是自带的，我们可以使用nvidia-smi命令查看 tensorflow安装。 tensorflow版本 tensorflow-gpu需要至少4GB的GPU显存才能运行，并且在训练模型时需要大量的计算资源。而tensorflow-cpu则是专门为CPU设计的版本，能够在CPU上高效地运行，同时不需要GPU显存。一般pc电脑的环境中，建议使用tensorflow-cpu会更加适合。当然，如果你未来有升级GPU的计划，可以考虑使用tensorflow-gpu。\n安装Anaconda 首先我们安装Anaconda，教程参考： 打开命令行 同时这里建议使用anaconda的替代方案mamba，因为conda包多了之后安装超级慢，会让你崩溃的，mamba基本匹配pip的速度，官网。\n在window上打开powershell，执行web命令下载压缩包\nmkdir d:\\green\\mamba \u0026\u0026 cd d:\\green\\mamba\rInvoke-Webrequest -URI https://micro.mamba.pm/api/micromamba/win-64/latest -OutFile micromamba.tar.bz2\rtar xf micromamba.tar.bz2 比如我的下载这里，直接将解压路径添加到path环境变量中 其他命令就和conda一样了，比如\nmicromamba env list\rmicomamba activate base 创建python环境 在其他盘创建一个环境，假设使用python3.7版本\nconda create --prefix=d:\\condaenv\\env_name python=3.7 micromamba create --prefix=d:\\condaenv\\env_name python=3.7 -c conda-forge 切换\nactivate d:\\condaenv\\env_name\r(d:\\condaenv\\env_name) C:\\Users\\liaomin\u003epython --version\rPython 3.7.4 安装tensorflow-gpu 通过版本关系图 可以确定我们可以使用python3.7版本安装tensorflow-gpu 2.0.0以上所有版本，我们选择一个2.6.0\n#设置镜像源\rconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\rconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\rconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/\r#安装tensorflow-gpu\rconda install tensorflow-gpu==2.6.0\r#或者（建议用mamba）\rmicomamba install tensorflow-gpu==2.6.0\r#如果是cpu版本直接\rmicomamba install tensorflow==2.6.0 安装完成我们先不急着安装cuddn和cuda可以先写个helloworld测试下\npycharm配置 配置conda环境 创建一个项目pure python项目，点击project interpreter 选择Existing interpreter,点击右侧的.. 选择conda environment 然后点击ok 在interpreter下拉框中选择刚新建的那个\n编写一段helloworld代码\nimport tensorflow as tf\r# 创建一个常量张量\rx = tf.constant([1, 2, 3])\r# 创建一个变量张量\ry = tf.Variable([3, 2, 1])\r# 计算两个张量的和\rz = tf.add(x, y)\r# 输出结果\rprint(z.numpy()) 运行，虽然能输出结果[4 4 4]，但是有红色警告\n2023-05-06 16:43:35.610912: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r#下面这段是安装好cuda后报的错。\r2023-05-06 17:37:38.727999: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\r2023-05-06 17:37:38.728345: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\rSkipping registering GPU devices...\r2023-05-06 17:37:38.729310: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX AVX2\rTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 显然缺少cuda和cudnn，注意这里缺少cudart64_110.dll并不是说cuda就是110版本，实际你11.2安装后也是这个dll。\n配置juypternotebook 打开anaconda prompt，并且激活你新穿件的环境，安装jupyter\nconda install jupyter notebook\r#或者（建议用mamba）\rmicomamba install jupyter notebook 在pycharm中右键创建一个notebook 输入之前的helloword代码，选择运行或者调试 右侧输出结果\n安装cuda 通过版本关系图，我们知道tensorflow-gpu 2.6.0需要安装11.2的cuda\n如果是cpu版本无需安装\ncuda历史版本下载位置,下载对应版本安装 这里有三个版本选择最高的11.2.2 点击进入后 选择window版本 默认安装路径是C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA 打开电脑设置——\u003e系统——\u003e系统高级设置——\u003e环境变量——\u003e系统变量——\u003ePATH 将C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin加入到环境变量PATH中 cmd重新执行nvidia-smi，发现版本更新了 发现之前缺失的cudart64_110.dll确实在C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin 安装cudnn 通过版本关系图，我们知道tensorflow-gpu 2.6.0需要安装8.1的cudnn cudnn历史版本下载位置,下载对应版本8.1，下载cudnn需要注册nvidia 选择cudnn library for window(x86)点击下载 打开cudnn文件夹 将上述cudnn里面的文件移动或copy到cuda对应文件夹目录下即可！ 此时在运行helloworld程序使用gpu正常运行\n2023-05-06 19:01:23.403530: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX AVX2\rTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r2023-05-06 19:01:24.075663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1320 MB memory: -\u003e device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\r[4 4 4] 安装blas TensorFlow 是一个使用 C++ 编写的开源机器学习框架，它支持在 CPU 和 GPU 上运行计算。在 TensorFlow 中，BLAS（Basic Linear Algebra Subprograms）库是用于执行线性代数计算的关键库，例如矩阵乘法和向量加法。由于这些操作在机器学习中非常常见，因此 BLAS 库对于 TensorFlow 的性能和稳定性至关重要。 切换到你的python环境，使用以下命令来安装 BLAS 库：\nconda install blas\r#或者（建议用mamba）\rmicomamba install openblas 安装完成后，因为安装的是动态链接库最终仍然会安装在你的anaconda的base环境下，找到动态链接库的位置\nC:\\Users\\你的用户名\\anaconda3\\pkgs\\blas-2.116-blis\\Library\\bin 将上面的路径添加到环境变量PATH中，如果不添加tensorflow会报错找不到blas 执行以下代码测试\npython -c \"import tensorflow as tf; print(tf.linalg.matmul(tf.ones([1, 2]), tf.ones([2, 2])))\" 运行结果\n(d:\\condaenv\\env_name) C:\\Users\\liaomin\u003epython -c \"import tensorflow as tf; print(tf.linalg.matmul(tf.ones([1, 2]), tf.ones([2, 2])))\"\r2023-05-10 09:39:23.654612: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX AVX2\rTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r2023-05-10 09:39:24.372107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1320 MB memory: -\u003e device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\rtf.Tensor([[2. 2.]], shape=(1, 2), dtype=float32) 云服务器运行 云服务器提供高性能的CPU和GPU运算服务器，对个人开发者来说非常便宜一般几十块钱半个月的秒杀团很多，可以提供强大的计算能力。对于需要大量计算资源的TensorFlow程序，使用腾讯云服务器可以提高计算效率\n云服务器选择 我这里首选autodl（关机不扣费，gpu按小时收费，0.5-2块钱一个小时，而且数据集和镜像【chatglm镜像等】，cuda都安装好了）,其次是腾讯云（新用户有秒杀,15天60元还是划算的），对个人友好，可以微信登录，微信支付。 右侧的gpu服务器是gpu 8gb的适合个人学习来跑神经网络。 购买的时候选择系统为：unbuntu的tensorflow版本，我选择的是：TensorFlow 2.8.0 Ubuntu 18.04 GPU基础镜像（预装460驱动），不要选window自己安装环境，麻烦，因为我们的pycharm支持远程ssh编程，用服务器跑，结果显示在pycharm中。 重置你的ubuntu密码（系统默认的账号是ubuntu，不是root），主机会给你配个公网地址 pycharm配置 代码自动同步 我们在/homt/ubuntu用户目录下新建一个deeplearn目录用于映射本地代码 点击tools-deployment-configuation 选择sftp，输入账号密码测试连接 点击mappings目录映射本地目录和远程目录 确定后在享有右键deployments-\u003eupload 上传代码 点击tools-deployments-browe remote host查看远程目录是否上传（勾上Automatic upload(always)保存代码自动上传） 远程interpreter 点击File-Settings-Project（项目名）-Project interpreter add一个 输入密码后下一步进入配置python目录，我们可以使用shell登录到远程服务器执行\nubuntu@VM-0-5-ubuntu:~$ which python3\r/usr/local/miniconda3/bin//python3 在interpreter上输入python3的路径即可 确认项目选择了该interpreter 接下来打开神经网络代码,，代码右键运行，可看到运行是ssh运行的 执行后的模型实际上是在远程服务器的可以使用brwoer remote host右键下载下来覆盖本地",
    "description": "@[toc]\n简介 TensorFlow是一种端到端开源机器学习平台，它提供了一个全面而灵活的生态系统，包含各种工具、库和社区资源，能够助力研究人员推动先进机器学习技术的发展。在TensorFlow机器学习框架下，开发者能够轻松地构建和部署由机器学习提供支持的应用。[2]\nKeras是一个高层次神经网络 API，适用于快速构建原型、高级研究和生产。它作为TensorFlow的一个接口，可以兼容多种深度学习框架。Keras 的核心数据结构是 model，一种组织网络层的方式。最简单的模型是 Sequential 顺序模型，它由多个网络层线性堆叠。对于更复杂的结构，你应该使用 Keras 函数式 API，它允许构建任意的神经网络图。 Keras最开始是为研究人员开发的，其目的在于快速实验，具有相同的代码可以在CPU或GPU上无缝切换运行的特点，同时也具有用户友好的API，方便快速地开发深度学习模型的原型。 Keras使用类sklearn的api接口来调用tensorflow，从sklearn机器学习中切换过来，更加容易上手。 Tenforflow2.0后直接内置可keras。\n运行硬件 TensorFlow支持在CPU和GPU上运行。GPU（图形处理单元）是一种专门用于加速计算的硬件，它可以大大提高深度学习模型的训练速度。相对而言，CPU（中央处理器）的每个核心具有更强大的处理能力，但它们的数量通常非常有限，因此在处理大数据时它们表现不佳。\nTensorFlow GPU和CPU的主要区别在于如何使用硬件来处理计算任务，以及处理速度的差异。在CPU上，TensorFlow利用所有可用的CPU内核并将任务分配给它们，这可能需要几分钟或几小时来完成。在GPU上，TensorFlow使用CUDA（Compute Unified Device Architecture）技术来利用GPU进行并行计算并加速训练过程，因为GPU拥有数百到数千个小型核心，这比CPU的几十个核心要多得多。这使得TensorFlow能够在GPU上实现更快的训练速度和更高的吞吐量，尤其是在处理大规模的深度学习任务时。\n另外需要注意的是，如果你的计算机没有安装专门的GPU，则无法使用TensorFlow GPU。在这种情况下，TensorFlow会使用CPU作为默认选项，但是训练过程会比在GPU上慢得多。因此，如果你需要进行大量的深度学习训练任务，建议使用具有至少一张GPU的计算机来加速训练。\n总之，TensorFlow GPU和CPU之间的区别在于它们的硬件架构、并行计算能力以及处理速度等方面。当进行大规模的深度学习训练时，使用GPU可以显著提高训练速度和吞吐量，而对于较小的任务或者没有专门GPU的计算机，则应该使用CPU。\ncuda和cuddn CUDA（Compute Unified Device Architecture）是一种由NVIDIA公司开发的并行计算平台和编程模型，它允许开发人员使用标准C/C++语言编写基于GPU的高性能应用程序。CUDA包括一个可编程的内核语言（CUDA C/C++），一个并行计算库（CUDA Toolkit），以及驱动程序和硬件架构，支持对NVIDIA GPU进行高性能并行计算。与CPU相比，GPU在并行处理任务时的性能要高得多，因此CUDA被广泛用于深度学习、科学计算和高性能计算等领域。[2]\ncuDNN（CUDA Deep Neural Network library）是NVIDIA CUDA的一个加速库，它提供了一组高度优化的本地函数，用于加速深度神经网络模型的训练和推理。cuDNN主要用于卷积神经网络（CNNs）和递归神经网络（RNNs）等深度学习模型的优化，从而实现更快的训练和推理速度。cuDNN支持多种深度学习框架，包括TensorFlow，PyTorch和Caffe等。[1]\n因此，CUDA是一种GPU计算平台和编程模型，cuDNN是其中一个加速库，专门用于加速深度学习模型的训练和推理。这两个技术结合起来，可以实现对GPU的高性能并行计算和深度学习模型的优化，从而提高深度学习任务的整体性能。\n不同的tensorflow版本需要不同的cuda和cuddn版本，google官网可查看 https://tensorflow.google.cn/install/source_windows?hl=en 如果电脑有gpu建议安装tensorflow-gpu,如果电脑没有gpu安装性能较差的tensorflow\n打开任务管理器-性能,查看你是否支持gpu 从这里我们可以看到我的cpu是英伟达(nvidia)的gtx1050 如果电脑已经安装显卡驱动，cuda肯定是自带的，我们可以使用nvidia-smi命令查看",
    "tags": [],
    "title": "深度学习01-tensorflow开发环境搭建",
    "uri": "/docs/programming/ai/deep_learning/frameworks/dl_01_tensorflow/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 深度基础",
    "content": "神经网络 简介 神经网络是一种基于生物神经系统结构和功能特点而设计的人工神经网络模型，具有很强的自适应性和非线性映射能力。神经网络由多个神经元（或称节点）组成，这些神经元通过连接权重相互连接，构成多层的网络结构。每个神经元接收到来自其它神经元的信号，并将这些信号加权线性组合后通过激活函数进行非线性转换，最终输出给下一层神经元或输出层。\n学习机器学习后，学习神经网络可以帮助你更深入地理解模式识别和人工智能领域的基础知识。神经网络在很多领域都有广泛的应用，例如计算机视觉、自然语言处理、语音识别等。学习神经网络可以让你掌握这些领域中最前沿的技术，并且能够应用这些技术来解决具体的问题。同时，神经网络的学习方法和算法也是机器学习的重要组成部分，学习神经网络可以帮助你更好地理解机器学习的原理和技术，从而更好地应用机器学习来解决实际问题。\n学习路径 如果你已经学过机器学习，那么开始学习神经网络，可以从多层感知器（Multilayer Perceptron，简称 MLP）神经网络入手。 MLP 是最基本的神经网络模型之一，它的结构比较简单，易于理解和实现，同时又有很好的可扩展性和通用性，可以应用于分类、回归等多种任务。学习 MLP 之后，你可以进一步学习卷积神经网络（Convolutional Neural Networks，简称 CNN）和循环神经网络（Recurrent Neural Networks，简称 RNN），它们分别用于计算机视觉和自然语言处理等特定领域的问题。总之，建议先从 MLP 入手，逐渐深入学习其他类型的神经网络。\n分类 神经网络可以分为多种不同的类型，下面列举一些常见的神经网络类型：\n前馈神经网络（Feedforward Neural Network）：前馈神经网络是最基本的神经网络类型，也是深度学习中最常见的神经网络类型。它由若干个神经元按照一定的层次结构组成，每个神经元接收上一层的输出，产生本层的输出，从而实现信息的传递和处理。\n卷积神经网络（Convolutional Neural Network）：卷积神经网络是一种专门用于图像处理和计算机视觉任务的神经网络类型。它通过卷积和池化等操作，可以提取图像中的特征，从而实现图像分类、目标检测、图像分割等任务。\n循环神经网络（Recurrent Neural Network）：循环神经网络是一种能够处理序列数据的神经网络类型。它通过记忆单元和门控机制等方式，可以处理任意长度的序列数据，从而实现自然语言处理、语音识别等任务。\n自编码器（Autoencoder）：自编码器是一种无监督学习的神经网络类型，它的目标是将输入数据进行压缩和解压缩，从而实现特征提取和降维等任务。\n深度置信网络（Deep Belief Network）：深度置信网络是一种由多个受限玻尔兹曼机组成的神经网络类型。它可以通过逐层贪心预训练和微调等方式，实现高效的特征学习和分类任务。\n除了以上列举的几种神经网络类型，还有众多其他的神经网络类型，如反向传播神经网络、Hopfield网络、Boltzmann机等。不同的神经网络类型适用于不同的任务和数据类型，需要根据具体的问题选择合适的神经网络类型。\n多层感知器（MLP） MLP神经网络属于前馈神经网络（Feedforward Neural Network）的一种。在网络训练过程中，需要通过反向传播算法计算梯度，将误差从输出层反向传播回输入层，用于更新网络参数。这个过程中需要使用反向传播算法来计算梯度，并且在某些类型的神经网络中，例如循环神经网络（RNN），也存在反馈回路。除了MLP，其他常见的前馈神经网络包括卷积神经网络（CNN）和循环神经网络（RNN）等。\n神经网络认识 我们以一个简单的例子来认识神经网络，只是为了理解其中的一些概念。 我们已知四个数据点(1,1)(-1,1)(-1,-1)(1,-1)，这四个点分别对应I~IV象限（也就是数据属于的类别），如果这时候给我们一个新的坐标点（比如(2,2)），那么它应该属于哪个象限呢？（没错，当然是第I象限，但我们的任务是要让机器知道，机器不知道有象限这个东西啊，他只能根据历史数据的经验推断），如果机器只是知道一堆数据 比如(-2,3)属于2象限，机器就需要通过这些数据总结出一个特征，这个特征可能就是根据x和y坐标的正负来判断象限了。 两层神经网络 这里我们构建一个两层神经网络，理论上两层神经网络已经可以拟合任意函数。这个神经网络的结构如下图： 首先我们去掉途中难懂的东西 输入层 在我们的例子中，输入层是坐标值，例如（1,1），这是一个包含两个元素的数组，也可以看作是一个12的矩阵。输入层的元素维度与输入量的特征息息相关，如果输入的是一张3232像素的灰度图像，那么输入层的维度就是32*32。 因为整个神经网络的目的是为了训练出一个模型，所以输入的是历史数据，历史数据有一个确定的输出label，模型出来后，直接使用模型就可以分类出输入的数据的输出 这里输入的数据为:\n[\r[1,1],\r[-1,1],\r[-1,-1],\r[1,-1]\r] 从输入层到隐藏层 连接输入层和隐藏层的是W1和b1。由X计算得到H十分简单，就是矩阵运算： $H=wx+b$ 果你学过线性代数，对这个式子一定不陌生,可以理解为w是一个权重(权重越高，这个特征也就越重要)，b是一个偏置，如果有多个特征那么就有个w，还记得$w^T*x$。 如上图中所示，在设定隐藏层为50维（也可以理解成50个神经元）之后，矩阵H的大小为（450）的矩阵。 也就是说50个神经元就是一个矩阵50个特征，每一行就是他的w值，这里输入层总共两个维度，所有只有w1和w2，b值这里就不说了，假设为0 这里我们可以简化最终得到4*50的意义 从隐藏层到输出层 连接隐藏层和输出层的是W2和b2,输入就是隐藏层输入的H值。同样是通过矩阵运算进行的： $Y=w2H+b2$ 最终输出层，最终是4个象限 H是450的矩阵，输出层的w2矩阵就是个504,最终得到一个44的矩阵 这里不详细画图了，大概意义如下。\nH是4*50的矩阵其实是一个列是神经元50个，行是4个数据集的经过第一轮计算的输出值H，隐藏层的目的就是计算出一个H值。 输出层的 w矩阵是50*4，目的是为了将50个神经元压缩到4个输出特征，也就是每一个数据集在4个象限的概率。所以最终输出是4*4 激活层 通过上述两个线性方程的计算，我们就能得到最终的输出Y了，但是如果你还对线性代数的计算有印象的话，应该会知道：一系列线性方程的运算最终都可以用一个线性方程表示。也就是说，上述两个式子联立后可以用一个线性方程表达。对于两次神经网络是这样，就算网络深度加到100层，也依然是这样。这样的话神经网络就失去了意义。 所以这里要对网络注入灵魂：激活层。 简而言之，激活层是为矩阵运算的结果添加非线性的 具体为什么需要，请view：https://blog.csdn.net/liaomin416100569/article/details/130597944?spm=1001.2014.3001.5501\n激活层是神经网络中的一种层，其作用是在输入信号和输出信号之间添加一个非线性的转换函数，使得网络可以更好地学习和表示复杂的非线性关系。激活层的意义在于增加模型的非线性表达能力，使得神经网络可以更好地处理复杂的输入数据，例如图像、文本和语音等。激活函数的选择也非常重要，不同的激活函数具有不同的特点。 激活层常用的激活函数三种，分别是阶跃函数、Sigmoid和ReLU，如下图： 阶跃函数：当输入小于等于0时，输出0；当输入大于0时，输出1。 Sigmoid：当输入趋近于正无穷/负无穷时，输出无限接近于1/0。 ReLU：当输入小于0时，输出0；当输入大于0时，输出等于输入。 其中，阶跃函数输出值是跳变的，且只有二值，较少使用；Sigmoid函数在当x的绝对值较大时，曲线的斜率变化很小（梯度消失），并且计算较复杂；ReLU是当前较为常用的激活函数。\n激活函数具体是怎么计算的呢？ 假如经过公式H=X*W1+b1计算得到的H值为：(1,-2,3,-4,7…)，那么经过阶跃函数激活层后就会变为(1,0,1,0,1…)，经过ReLU激活层之后会变为(1,0,3,0,7…)。\n需要注意的是，每个隐藏层计算（矩阵线性运算）之后，都需要加一层激活层，要不然该层线性计算是没有意义的。 此时的神经网络变成了如下图所示的形式： 我们都知道（？）神经网络是分为“训练”和“使用”两个步骤的。如果是在“使用”的步骤，图4就已经完成整个过程了，在求得的Y（大小为4*4）矩阵中，当前样本数值最大的就代表着当前分类。\n但是对于用于“训练”的网络，上图还远远不够。起码当前的输出Y，还不够“漂亮”。\n输出的正规化 假设某个样本输出Y的值可能会是(3,1,0.1,0.5)这样的矩阵，诚然我们可以找到里边的最大值“3”，从而找到对应的分类为I，但是这并不直观。我们想让最终的输出为概率，也就是说可以生成像(90%,5%,2%,3%)这样的结果，这样做不仅可以找到最大概率的分类，而且可以知道各个分类计算的概率值。 具体是怎么计算的呢？ 计算公式如下： 简单来说分三步进行：（1）以e为底对所有元素求指数幂；（2）将所有指数幂求和；（3）分别将这些指数幂与该和做商。\n这样求出的结果中，所有元素的和一定为1，而每个元素可以代表概率值。\n我们将使用这个计算公式做输出结果正规化处理的层叫做“Softmax”层。此时的神经网络将变成如下图所示： 如何衡量输出的好坏 通过Softmax层之后，我们得到了I，II，III和IV这四个类别分别对应的概率，但是要注意，这是神经网络计算得到的概率值结果，而非真实的情况。\n比如，Softmax输出的结果是(90%,5%,3%,2%)，真实的结果是(100%,0,0,0)。虽然输出的结果可以正确分类，但是与真实结果之间是有差距的，一个优秀的网络对结果的预测要无限接近于100%，为此，我们需要将Softmax输出结果的好坏程度做一个“量化”。\n一种直观的解决方法，是用1减去Softmax输出的概率，比如1-90%=0.1。不过更为常用且巧妙的方法是，求对数的负数。\n还是用90%举例，对数的负数就是：-log0.9=0.046\n可以想见，概率越接近100%，该计算结果值越接近于0，说明结果越准确，该输出叫做“交叉熵损失（Cross Entropy Error）”。\n我们训练神经网络的目的，就是尽可能地减少这个“交叉熵损失”。 反向传播与参数优化 上面的过程其实就是神经网络的正向传播过程 ，一句话复习一下：神经网络的传播都是形如Y=WX+b的矩阵运算；为了给矩阵运算加入非线性，需要在隐藏层中加入激活层；输出层结果需要经过Softmax层处理为概率值，并通过交叉熵损失来量化当前网络的优劣。\n算出交叉熵损失后，就要开始反向传播了。其实反向传播就是一个参数优化的过程，优化对象就是网络中的所有W和b（因为其他所有参数都是确定的）。\n神经网络的神奇之处，就在于它可以自动做W和b的优化，在深度学习中，参数的数量有时会上亿，不过其优化的原理和我们这个两层神经网络是一样的。\n神经网络需要反复迭代。如上述例子中，第一次计算得到的概率是90%，交叉熵损失值是0.046；将该损失值反向传播，使W1,b1,W2,b2做相应微调；再做第二次运算，此时的概率可能就会提高到92%，相应地，损失值也会下降，然后再反向传播损失值，微调参数W1,b1,W2,b2。依次类推，损失值越来越小，直到我们满意为止。\n此时我们就得到了理想的W1,b1,W2,b2。\n具体参考 BP算法怎推导章节\n内容参考：https://zhuanlan.zhihu.com/p/65472471\n过拟合 Dropout是一种在神经网络中用于防止过拟合的技术。它是通过在训练期间随机将一些节点的输出设置为0来实现的。具体来说，每个节点有一定的概率被“关闭”，即其输出被设置为0。这样，节点之间的连接就会被随机断开，从而迫使网络学习更加鲁棒的特征，而不是依赖特定的节点或连接。这种随机性可以被看作是一种正则化技术，可以有效地防止过拟合。\n过拟合是指模型在训练数据上表现很好，但在测试数据上表现不佳的现象。这通常是由于模型过于复杂，而训练数据又过少或过于噪声导致的。通过使用Dropout技术，我们可以减少模型的复杂度，并使其更加适应不同的训练数据。这样，我们就可以更好地泛化模型，从而在测试数据上获得更好的表现。\n假设我们有一个二分类任务，需要从图像中识别猫和狗。我们使用卷积神经网络进行训练，但由于数据集较小，容易出现过拟合的问题。\n为了解决这个问题，我们可以在卷积神经网络中添加Dropout层。例如，我们可以在全连接层之前添加一个Dropout层，将其输出概率设置为0.5。这意味着在每个训练批次中，该层中的一半节点的输出将被随机设置为0。这样，网络就不会过于依赖特定节点或连接，并且可以更好地适应不同的训练数据。\nBP算法推导 定义 首先来一个反向传播算法的定义（转自维基百科）：反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。 该方法对网络中所有权重计算损失函数的梯度。 这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。（误差的反向传播）\n算法讲解 如果去问一下了解BP算法的人“BP算法怎推导？”，大概率得到的回答是“不就是链式求导法则嘛”，我觉得这种答案对于提问题的人来说没有任何帮助。BP的推导需要链式求导不错，但提问者往往想得到的是直观的回答，毕竟理解才是王道。直观的答案，非图解莫属了。 注：下图的确是反向传播算法，但不是深度学习中的backprop，不过backward的大体思想是一样的，毕竟误差没法从前往后计算啊。（在深度学习中操作的是计算图—Computational graph），如果暂时不理解上面那句话，你可以当我没说过，不要紧~（手动?）\n下面通过两组图来进行神经网络前向传播和反向传播算法的讲解，第一组图来自国外某网站，配图生动形象。如果对你来说，单纯的讲解理解起来比较费劲，那么可以参考第二组图——一个具体的前向传播和反向传播算法的例子。相信就算是刚刚入门的小白（只要有一点点高等数学基础知识），也一定可以理解反向传播算法！\n首先拿一个简单的三层神经网络来举例，如下： 每个神经元由两部分组成，第一部分（e）是输入值和权重系数乘积的和，第二部分（f(e)）是一个激活函数（非线性函数）的输出， y=f(e)即为某个神经元的输出，如下： 前向传播 第一层神经网络传播\n其中$w_{x1}1$表示x1对应第一个神经元的w值，$w_{x2}1$，表示x2对应对一个神经元的w值。\n第二层神经网络传播 第三层神经网络传播 反向传播 到这里为止，神经网络的前向传播已经完成，最后输出的y就是本次前向传播神经网络计算出来的结果（预测结果），但这个预测结果不一定是正确的，要和真实的标签（z）相比较，计算预测结果和真实标签的误差（$\\delta$），如下： 下面开始计算每个神经元的误差（$\\delta$） 计算第一层误差 下面开始利用反向传播的误差，计算各个神经元（权重）的导数，开始反向传播修改权重 计算第二次的w 计算第三层 到此为止，整个网络的前向，反向传播和权重更新已经完成\n具体实例 就算上面的所有东西你都看的迷迷糊糊，通过下面的例子，相信绝大多数人也能很轻松的理解BP算法。如图是一个简单的神经网络用来举例： 下面是前向（前馈）运算（激活函数为sigmoid）： 下面是反向传播（求网络误差对各个权重参数的梯度）：\n我们先来求最简单的，求误差E对w5的导数。首先明确这是一个“链式求导”过程，要求误差E对w5的导数，需要先求误差E对out o1的导数，再求out o1对net o1的导数，最后再求net o1对w5的导数，经过这个链式法则，我们就可以求出误差E对w5的导数（偏导），如下图所示： 导数（梯度）已经计算出来了，下面就是反向传播与参数更新过程： 上面的图已经很显然了，如果还看不懂真的得去闭门思过了（开玩笑~），耐心看一下上面的几张图，一定能看懂的。\n如果要想求误差E对w1的导数，误差E对w1的求导路径不止一条，这会稍微复杂一点，但换汤不换药，计算过程如下所示： bp推导参考：https://blog.csdn.net/ft_sunshine/article/details/90221691\ntensorflow实战 加载数据集 keras.datasets.mnist 是 Keras 框架内置的一个手写数字数据集，包含了 60,000 张训练图片和 10,000 张测试图片。每张图片都是 28x28 像素的灰度图像，每个像素的取值范围为 0 到 255。该数据集常用于机器学习领域中的图像分类和数字识别任务。\nkeras.datasets.mnist 的返回值是一个元组 (x_train, y_train), (x_test, y_test)，分别表示训练集和测试集。其中 x_train 和 x_test 分别是形状为 (60000, 28, 28) 和 (10000, 28, 28) 的 numpy 数组，表示图像数据。y_train 和 y_test 则是形状为 (60000,) 和 (10000,) 的 numpy 数组，表示对应的图像标签，即每张图片所代表的数字。 记载数据集，并绘制前20张图片\n#%%\rfrom tensorflow.keras.datasets import mnist\rimport matplotlib.pyplot as plt\r# 加载数据集\r(x_train, y_train), (x_test, y_test) = mnist.load_data()\rx_test_ori=x_test # 打印数据集信息\rprint('训练集图像数据形状：', x_train.shape)\rprint('训练集标签数据形状：', y_train.shape)\rprint('测试集图像数据形状：', x_test.shape)\rprint('测试集标签数据形状：', y_test.shape)\r# 绘制前20张训练集图像\rplt.figure(figsize=(10, 10))\rfor i in range(20):\rplt.subplot(5, 5, i+1)\rplt.xticks([])\rplt.yticks([])\rplt.grid(False)\rplt.imshow(x_train[i], cmap=plt.cm.binary)\rplt.xlabel(y_train[i])\rplt.show() 输出\n训练集图像数据形状： (60000, 28, 28)\r训练集标签数据形状： (60000,)\r测试集图像数据形状： (10000, 28, 28)\r测试集标签数据形状： (10000,) 默认图片下载路径在 ~/.keras/datasets ，window下：C:\\Users\\当前用户名.keras\\datasets,大小估计10MB左右。\n数据预处理 x_train = x_train.reshape(x_train.shape[0], 784).astype('float32') / 255\rx_test = x_test.reshape(x_test.shape[0], 784).astype('float32') / 255\ry_train = keras.utils.to_categorical(y_train, 10)\ry_test = keras.utils.to_categorical(y_test, 10) 在上面的代码中，我们将输入数据的维度从 28x28 转换为 784，因为我们处理的数据一般都是一个矩阵，一行代表一个数据样本，需要转换成784*1的数据，并将像素值的范围从 0-255 缩放到 0-1 之间。同时，我们将标签数据进行 one-hot 编码，将其转换为一个 10 维的向量，每个维度代表一个数字。\none-host编码 One-hot编码是一种将离散型变量转换为连续型变量的技术，在机器学习和深度学习中广泛应用。它将每个离散型变量的取值都编码为一个二进制位，其中只有一个二进制位为1，其余二进制位为0。举例说明如下：\n假设有一个离散型变量“颜色”，它的可能取值为“红色”、“黄色”和“蓝色”。我们可以将这三个取值编码为长度为3的二进制向量，如下所示：\n红色：[1, 0, 0]\n黄色：[0, 1, 0]\n蓝色：[0, 0, 1]\n这个编码方式就是one-hot编码。在机器学习中，我们可以使用这个编码方式来处理离散型变量，使其成为连续型变量，方便模型的学习和使用。\nkeras.utils.to_categorical() keras.utils.to_categorical()函数将整数型的类别标签转换成了独热编码（one-hot encoding）的形式。在独热编码中，每个类别标签被表示为一个长度等于类别总数的向量，其中该类别标签所对应的位置值为1，其余位置为0。\n对于手写数字识别任务，共有10个类别，即数字0到9，因此需要将标签向量转换为10维的独热编码。\n例如，如果原始标签为5，则转换后的独热编码为[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]，其中第6个位置（从0开始）的值为1，表示原始标签为5。\n这样做的目的是为了让神经网络更好地理解类别之间的差异和相似性，以便更准确地进行分类预测。\n构造多层感知器模型 我们使用 keras.Sequential 构建模型，该模型包含一个输入层、一个隐藏层和一个输出层。输入层的维度为 784（即每个图片的像素数），隐藏层包含 512 个神经元，激活函数为 ReLU，输出层包含 10 个神经元，激活函数为 softmax。同时，我们使用 Dropout 防止过拟合。\n# 构建模型\rmodel = tf.keras.Sequential([\rtf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\rtf.keras.layers.Dropout(0.2),\rtf.keras.layers.Dense(10, activation='softmax')\r]) tf.keras.Sequential keras.Sequential是Keras中的一个类，用于快速搭建神经网络模型。它提供了一个简单的方法来创建顺序模型，即一系列层按照顺序堆叠在一起的模型。在keras.Sequential中，可以通过添加层的方式来搭建神经网络。\nkeras.Sequential的定义如下：\nkeras.Sequential(\rlayers=None, name=None\r) 其中，layers是一个列表，包含了按照顺序堆叠在一起的层；name是模型的名称。 上面使用keras.Sequential创建简单神经网络的例子：\nmodel = tf.keras.Sequential([\rtf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\rtf.keras.layers.Dropout(0.2),\rtf.keras.layers.Dense(10, activation='softmax')\r]) 我们创建了一个包含三层的神经网络模型。第一层是一个全连接层，包含512个神经元，使用ReLU激活函数，输入形状为(784,)。第二层是一个Dropout层，第三层是一个全连接层包含10个神经元，使用Softmax激活函数。\nkeras.layers.Dense keras.layers.Dense是Keras中的一个类，用于创建全连接层。全连接层是神经网络中最基本的一种层，它的每一个神经元都与上一层的每一个神经元相连。keras.layers.Dense可以用于创建输入层、输出层和隐藏层。\nkeras.layers.Dense的定义如下：\nkeras.layers.Dense(\runits, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs\r) 其中，units表示该层的神经元数量；activation表示该层的激活函数；use_bias表示是否使用偏置；kernel_initializer表示权重矩阵的初始化方法；bias_initializer表示偏置向量的初始化方法；kernel_regularizer、bias_regularizer、activity_regularizer表示正则化项；kernel_constraint、bias_constraint表示约束项。\n下面是一个使用keras.layers.Dense创建全连接层的例子：\nimport tensorflow.keras as keras\rlayer = tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)), 在这个例子中，我们创建了一个包含512个神经元的全连接层。激活函数为ReLU，输入形状为(784,)，表示该层的输入数据是一个长度为784的向量。\nkeras.layers.Dense的一些常用参数和方法：\nunits：该层的神经元数量； activation：该层的激活函数； use_bias：是否使用偏置； kernel_initializer：权重矩阵的初始化方法； bias_initializer：偏置向量的初始化方法； kernel_regularizer、bias_regularizer、activity_regularizer：正则化项； kernel_constraint、bias_constraint：约束项； layer.get_weights()：获取该层的权重矩阵和偏置向量； layer.set_weights(weights)：设置该层的权重矩阵和偏置向量。 以上就是keras.layers.Dense的一些基本信息和使用方法。 keras.layers.Dropout tf.keras.layers.Dropout是一种在神经网络中应用的正则化方法，用于减少过拟合的影响。在训练期间，Dropout层会随机地将输入张量的一部分元素设置为0，从而强制网络学习更健壮的特征表示，防止过拟合。具体来说，Dropout层以一定的概率（通常为0.5）随机地将输入张量的一部分神经元输出设为0，这些被屏蔽的神经元将不会参与前向传播和反向传播。这样做可以强制网络在训练过程中学习到更多的特征，并且使得网络对于输入的微小变化更加稳健。\n在tf.keras.layers.Dropout中，可以设置一个rate参数，来控制屏蔽神经元的比例，即随机将输入张量的多少个元素置为0。具体来说，如果rate=0.5，则代表在训练过程中随机选取50%的神经元输出为0，而在测试过程中不会进行任何操作。同时，可以将tf.keras.layers.Dropout层放在神经网络的任何位置，通常放在全连接层之后，以减少过拟合的影响。\nDropout层的主要作用是减少过拟合的影响，从而提高模型的泛化能力。通过随机屏蔽部分神经元，Dropout层可以强制网络学习到更健壮的特征表示，并且使得网络对于输入的微小变化更加稳健。这样可以增加模型的鲁棒性，提高模型的泛化能力，从而使得模型在测试集上表现更好。\n需要注意的是，在测试过程中不应该使用Dropout层，因为测试过程需要对整个模型进行前向传播，而不是将部分神经元置为0。因此，在测试过程中，需要将所有的神经元都参与前向传播，以获得更准确的预测结果。为了解决这个问题，可以在训练过程中使用Dropout层，并在测试过程中关闭Dropout层，或者根据Dropout的特性对输出进行调整。\nkeras.layers.其他 keras.layers 模块提供了许多常见的神经网络层类，其中一些常用的层包括：\nDense：全连接层，每个输入节点都连接到输出节点 Conv2D：二维卷积层，对图像或其他二维输入进行卷积运算 MaxPooling2D：二维最大池化层，对输入进行下采样 Dropout：随机丢弃一部分输入节点，以减少过拟合 Flatten：将输入展平为一维张量 Activation：激活函数层，如ReLU、Sigmoid、Softmax等 BatchNormalization：批量归一化层，用于加速收敛和减少过拟合 Embedding：词嵌入层，将离散的词转换为连续的向量表示 LSTM：长短时记忆循环层，用于处理时间序列数据 GRU：门控循环单元层，用于处理时间序列数据 这些层可以通过组合或堆叠来构建复杂的神经网络模型。除了这些常用的层外，keras.layers 还提供了许多其他层，如 Conv1D、Conv3D、UpSampling2D、SeparableConv2D、GlobalMaxPooling2D、GlobalAveragePooling2D等等。可以根据具体的任务需求选择合适的层。\n模型编译 model.compile(optimizer='adam',\rloss='categorical_crossentropy',\rmetrics=['accuracy']) 这段代码是用来编译模型的，其中包含了三个参数：\noptimizer：优化器，用来控制模型的学习速率。在这里，我们使用了Adam优化器，这是目前被广泛使用的一种优化器，它可以自适应地调整学习速率。\nloss：损失函数，用来衡量模型在训练过程中的误差。在这里，我们使用了交叉熵损失函数，它适用于多分类问题，能够有效地衡量模型预测结果与真实标签之间的差异。\nmetrics：评价指标，用来评价模型的性能。在这里，我们使用了准确率作为评价指标，它可以衡量模型在测试集上的分类精度。\n总的来说，这段代码的作用是为模型指定优化器、损失函数和评价指标，以便在训练过程中使用。\n优化器 常用的优化器有以下几种：\n随机梯度下降（Stochastic Gradient Descent，SGD）：是最基础、最简单的优化器，通过不断迭代来寻找最优解，对应字符串为：‘sgd’。\nAdam：是目前最广泛使用的优化器之一，结合了Adagrad和RMSprop的优点，对应字符串为：‘adam。\nAdagrad：自适应地调整每个参数的学习率，适用于稀疏数据集，对应字符串为：‘adagrad。\nRMSprop：与Adagrad类似，但是对梯度的平方进行指数加权移动平均，能够更好地适应非平稳目标函数，对应字符串为：‘rmsprop。\nAdadelta：结合了Adagrad和RMSprop的优点，同时解决了Adagrad学习率下降快的问题，对应字符串为：‘adadelta。\nAdamax：是Adam的一种变体，使用了L∞范数来代替L2范数，对应字符串为：‘adamax。\nNadam：是Adam和Nesterov动量的结合体，能够更好地适应凸函数和非凸函数，对应字符串为：’nadam。\n以上是常用的优化器，每种优化器都有其优点和缺点，选择合适的优化器需要根据具体的场景和任务来进行选择。\n损失函数 常见的损失函数包括：\n均方误差（Mean Squared Error，MSE）：该损失函数常用于回归问题，计算预测值与真实值之间的差平方的平均值。 在Keras中的字符串表示为：‘mse’\n交叉熵损失函数（Cross Entropy Loss，CE）：该损失函数常用于分类问题，通过计算预测值和真实值之间的交叉熵来衡量模型的拟合能力。 在Keras中的字符串表示为：‘categorical_crossentropy’（用于多分类问题）或’binary_crossentropy’（用于二分类问题）。\n对数损失函数（Logarithmic Loss，LogLoss）：该损失函数常用于二分类问题，通过计算预测值和真实值之间的对数损失来衡量模型的拟合能力。 在Keras中的字符串表示为：‘binary_crossentropy’\nHinge损失函数：该损失函数常用于支持向量机（SVM）模型中，通过计算预测值和真实值之间的Hinge损失来衡量模型的拟合能力。 在Keras中的字符串表示为：‘hinge’\nHuber损失函数：该损失函数常用于回归问题，通过计算预测值和真实值之间的平滑L1损失来衡量模型的拟合能力。 在Keras中的字符串表示为：‘huber_loss’\n当然，这些只是常见的损失函数，还有其他的损失函数，比如Focal Loss等。\n评价指标 常见的评价指标包括：\n准确率（Accuracy）：该指标用于分类问题，表示模型正确分类的样本数占总样本数的比例。 在Keras中的字符串表示为：‘accuracy’\n精确率（Precision）：该指标用于分类问题，表示模型正确预测为正类的样本数占预测为正类的样本总数的比例。 在Keras中的字符串表示为：‘precision’\n召回率（Recall）：该指标用于分类问题，表示模型正确预测为正类的样本数占真实为正类的样本总数的比例。 在Keras中的字符串表示为：‘recall’\nF1分数（F1 Score）：该指标综合了精确率和召回率，是二者的调和平均数，可以更全面地评估模型的性能。 在Keras中的字符串表示为：‘f1_score’\n均方误差（Mean Squared Error，MSE）：该指标用于回归问题，表示模型预测值与真实值之间的平均平方误差。 在Keras中的字符串表示为：‘mse’\n平均绝对误差（Mean Absolute Error，MAE）：该指标用于回归问题，表示模型预测值与真实值之间的平均绝对误差。 在Keras中的字符串表示为：‘mae’\n当然，这些只是常见的评价指标，还有其他的评价指标，比如AUC等。\n模型训练 history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test)) 在上面的代码中，我们使用 model.fit 进行模型训练，设置了 10 个 epochs 和 128 个批次大小。同时，我们使用测试集进行模型验证。\n在每个epoch中，模型需要对整个训练数据集进行训练，而不是仅仅针对一个样本或一个batch进行训练。因此，在每个epoch中，模型需要对所有训练样本进行前向传播和反向传播，以计算出每个样本对应的误差和梯度，并使用这些梯度更新模型的权重参数。\n为了加快模型训练的速度，通常会将训练数据集分成多个batch，每个batch包含若干个样本。在每个epoch中，模型会将整个训练数据集分成多个batch，然后对每个batch进行前向传播和反向传播，以更新权重参数。因此，在每个epoch中，模型需要进行多次前向传播和反向传播，才能完成对整个训练数据集的训练。\n如果batch_size=128，在一次epochs中数据被拆成了128份，每一份都和512个神经元进行正向和反向传播进行梯度下降修正w和b，所以一次epochs，实际上进行了128次的梯度下降算法 如果设置成10个epoch，可以理解为128*10次梯度下降算法。 像手写数字识别的，数据进行两次epoch,进行256次梯度下降，准确率就达到97%了\n注意别6w个样本一批次处理，内存罩不住啊，可能执行1个epochs都要几个小时。\n模型评估 # 评估模型\rtest_loss, test_acc = model.evaluate(x_test, y_test)\rprint('Test accuracy:', test_acc)\r# 预测结果\rpredictions = model.predict(x_test) 在上面的代码中，我们使用 model.evaluate 对模型进行评估，并使用 model.predict 进行预测。\n如果运行过程中报错\nInternalError: Attempting to perform BLAS operation using StreamExecutor without BLAS support [[node sequential/dense/MatMul (defined at C:\\Users\\liaomin\\AppData\\Local\\Temp\\ipykernel_28392\\2909523142.py:25) ]] [Op:__inference_test_function_361]\n直接安装即可,gpu版本tensorflow需要blas库，cpu版本不需要\nconda install blas 模型预测 选择某个测试元素然后将图片显示出来，并使用模型预测\n# 预测第10个测试数据的结果\rpredictions = model.predict(np.array([x_test[9]]))\rprint(\"预测结果：\", np.argmax(predictions))\r# 绘制第10个测试数据的图形\rplt.imshow(x_test_ori[9], cmap=plt.cm.binary)\rplt.show() 完整运行 #%%\r#%%\rimport tensorflow as tf\rfrom tensorflow.keras.datasets import mnist\rimport numpy as np\rimport matplotlib.pyplot as plt\r# 加载数据集\r(x_train, y_train), (x_test, y_test) = mnist.load_data()\rx_test_ori=x_test\r# 数据预处理\rx_train = x_train.reshape(x_train.shape[0], 784).astype('float32') / 255\rx_test = x_test.reshape(x_test.shape[0], 784).astype('float32') / 255\ry_train = tf.keras.utils.to_categorical(y_train, 10)\ry_test = tf.keras.utils.to_categorical(y_test, 10)\r# 构建模型\rmodel = tf.keras.Sequential([\rtf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\rtf.keras.layers.Dropout(0.2),\rtf.keras.layers.Dense(10, activation='softmax')\r])\r# 编译模型\rmodel.compile(optimizer='adam',\rloss='categorical_crossentropy',\rmetrics=['accuracy'])\r# 训练模型\rhistory = model.fit(x_train, y_train, epochs=2, #这里为了节约时间，就两轮就差不多了97%正确率了，训练十次差不多0.98左右\rbatch_size=128, validation_data=(x_test, y_test))\r# 评估模型\rtest_loss, test_acc = model.evaluate(x_test, y_test)\rprint('Test accuracy:', test_acc)\r# 预测第10个测试数据的结果\rpredictions = model.predict(np.array([x_test[9]]))\rprint(\"预测结果：\", np.argmax(predictions))\r# 绘制第10个测试数据的图形\rplt.imshow(x_test_ori[9], cmap=plt.cm.binary)\rplt.show() 输出结果\nEpoch 1/2\r1/469 [..............................] - ETA: 2:17 - loss: 2.2535 - accuracy: 0.1562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r13/469 [..............................] - ETA: 1s - loss: 1.3687 - accuracy: 0.6358 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r25/469 [\u003e.............................] - ETA: 1s - loss: 0.9935 - accuracy: 0.7331\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r38/469 [=\u003e............................] - ETA: 1s - loss: 0.8118 - accuracy: 0.7775\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r52/469 [==\u003e...........................] - ETA: 1s - loss: 0.7005 - accuracy: 0.8057\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/469 [==\u003e...........................] - ETA: 1s - loss: 0.6644 - accuracy: 0.8137\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r70/469 [===\u003e..........................] - ETA: 1s - loss: 0.6148 - accuracy: 0.8256\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r83/469 [====\u003e.........................] - ETA: 1s - loss: 0.5690 - accuracy: 0.8376\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r97/469 [=====\u003e........................] - ETA: 1s - loss: 0.5308 - accuracy: 0.8479\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r111/469 [======\u003e.......................] - ETA: 1s - loss: 0.5008 - accuracy: 0.8562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r124/469 [======\u003e.......................] - ETA: 1s - loss: 0.4797 - accuracy: 0.8621\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r136/469 [=======\u003e......................] - ETA: 1s - loss: 0.4648 - accuracy: 0.8659\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r145/469 [========\u003e.....................] - ETA: 1s - loss: 0.4548 - accuracy: 0.8683\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r158/469 [=========\u003e....................] - ETA: 1s - loss: 0.4398 - accuracy: 0.8727\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r172/469 [==========\u003e...................] - ETA: 1s - loss: 0.4253 - accuracy: 0.8765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r185/469 [==========\u003e...................] - ETA: 1s - loss: 0.4145 - accuracy: 0.8802\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r199/469 [===========\u003e..................] - ETA: 1s - loss: 0.3999 - accuracy: 0.8843\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r212/469 [============\u003e.................] - ETA: 1s - loss: 0.3899 - accuracy: 0.8872\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r225/469 [=============\u003e................] - ETA: 1s - loss: 0.3804 - accuracy: 0.8902\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r238/469 [==============\u003e...............] - ETA: 0s - loss: 0.3700 - accuracy: 0.8933\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r252/469 [===============\u003e..............] - ETA: 0s - loss: 0.3619 - accuracy: 0.8951\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r266/469 [================\u003e.............] - ETA: 0s - loss: 0.3537 - accuracy: 0.8979\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r280/469 [================\u003e.............] - ETA: 0s - loss: 0.3460 - accuracy: 0.9001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r291/469 [=================\u003e............] - ETA: 0s - loss: 0.3401 - accuracy: 0.9018\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r303/469 [==================\u003e...........] - ETA: 0s - loss: 0.3350 - accuracy: 0.9032\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r316/469 [===================\u003e..........] - ETA: 0s - loss: 0.3298 - accuracy: 0.9046\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r330/469 [====================\u003e.........] - ETA: 0s - loss: 0.3250 - accuracy: 0.9061\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r343/469 [====================\u003e.........] - ETA: 0s - loss: 0.3198 - accuracy: 0.9073\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r357/469 [=====================\u003e........] - ETA: 0s - loss: 0.3132 - accuracy: 0.9090\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r368/469 [======================\u003e.......] - ETA: 0s - loss: 0.3088 - accuracy: 0.9103\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/469 [=======================\u003e......] - ETA: 0s - loss: 0.3051 - accuracy: 0.9115\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r395/469 [========================\u003e.....] - ETA: 0s - loss: 0.3017 - accuracy: 0.9125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r409/469 [=========================\u003e....] - ETA: 0s - loss: 0.2970 - accuracy: 0.9139\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r423/469 [==========================\u003e...] - ETA: 0s - loss: 0.2927 - accuracy: 0.9152\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r437/469 [==========================\u003e...] - ETA: 0s - loss: 0.2889 - accuracy: 0.9163\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r448/469 [===========================\u003e..] - ETA: 0s - loss: 0.2855 - accuracy: 0.9173\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r458/469 [============================\u003e.] - ETA: 0s - loss: 0.2827 - accuracy: 0.9181\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r469/469 [==============================] - 3s 5ms/step - loss: 0.2800 - accuracy: 0.9189 - val_loss: 0.1337 - val_accuracy: 0.9610\rEpoch 2/2\r1/469 [..............................] - ETA: 1s - loss: 0.1849 - accuracy: 0.9375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r13/469 [..............................] - ETA: 1s - loss: 0.1216 - accuracy: 0.9597\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r25/469 [\u003e.............................] - ETA: 1s - loss: 0.1333 - accuracy: 0.9584\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r37/469 [=\u003e............................] - ETA: 1s - loss: 0.1389 - accuracy: 0.9573\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r49/469 [==\u003e...........................] - ETA: 1s - loss: 0.1370 - accuracy: 0.9597\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r63/469 [===\u003e..........................] - ETA: 1s - loss: 0.1326 - accuracy: 0.9606\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r75/469 [===\u003e..........................] - ETA: 1s - loss: 0.1349 - accuracy: 0.9596\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r86/469 [====\u003e.........................] - ETA: 1s - loss: 0.1339 - accuracy: 0.9597\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r98/469 [=====\u003e........................] - ETA: 1s - loss: 0.1360 - accuracy: 0.9594\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r110/469 [======\u003e.......................] - ETA: 1s - loss: 0.1385 - accuracy: 0.9587\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r123/469 [======\u003e.......................] - ETA: 1s - loss: 0.1385 - accuracy: 0.9582\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r136/469 [=======\u003e......................] - ETA: 1s - loss: 0.1371 - accuracy: 0.9591\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r147/469 [========\u003e.....................] - ETA: 1s - loss: 0.1369 - accuracy: 0.9593\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r158/469 [=========\u003e....................] - ETA: 1s - loss: 0.1361 - accuracy: 0.9595\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r170/469 [=========\u003e....................] - ETA: 1s - loss: 0.1355 - accuracy: 0.9600\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r182/469 [==========\u003e...................] - ETA: 1s - loss: 0.1360 - accuracy: 0.9601\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r194/469 [===========\u003e..................] - ETA: 1s - loss: 0.1348 - accuracy: 0.9603\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r206/469 [============\u003e.................] - ETA: 1s - loss: 0.1330 - accuracy: 0.9610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r217/469 [============\u003e.................] - ETA: 1s - loss: 0.1322 - accuracy: 0.9610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r228/469 [=============\u003e................] - ETA: 1s - loss: 0.1304 - accuracy: 0.9617\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r240/469 [==============\u003e...............] - ETA: 1s - loss: 0.1301 - accuracy: 0.9616\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r252/469 [===============\u003e..............] - ETA: 0s - loss: 0.1290 - accuracy: 0.9618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r265/469 [===============\u003e..............] - ETA: 0s - loss: 0.1276 - accuracy: 0.9623\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r278/469 [================\u003e.............] - ETA: 0s - loss: 0.1264 - accuracy: 0.9625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r291/469 [=================\u003e............] - ETA: 0s - loss: 0.1253 - accuracy: 0.9629\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r304/469 [==================\u003e...........] - ETA: 0s - loss: 0.1246 - accuracy: 0.9632\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r318/469 [===================\u003e..........] - ETA: 0s - loss: 0.1235 - accuracy: 0.9635\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r332/469 [====================\u003e.........] - ETA: 0s - loss: 0.1230 - accuracy: 0.9636\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r345/469 [=====================\u003e........] - ETA: 0s - loss: 0.1220 - accuracy: 0.9641\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r358/469 [=====================\u003e........] - ETA: 0s - loss: 0.1214 - accuracy: 0.9643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r368/469 [======================\u003e.......] - ETA: 0s - loss: 0.1210 - accuracy: 0.9644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r380/469 [=======================\u003e......] - ETA: 0s - loss: 0.1209 - accuracy: 0.9643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r392/469 [========================\u003e.....] - ETA: 0s - loss: 0.1206 - accuracy: 0.9645\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r404/469 [========================\u003e.....] - ETA: 0s - loss: 0.1205 - accuracy: 0.9644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r415/469 [=========================\u003e....] - ETA: 0s - loss: 0.1206 - accuracy: 0.9643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r427/469 [==========================\u003e...] - ETA: 0s - loss: 0.1205 - accuracy: 0.9642\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r440/469 [===========================\u003e..] - ETA: 0s - loss: 0.1201 - accuracy: 0.9644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r452/469 [===========================\u003e..] - ETA: 0s - loss: 0.1201 - accuracy: 0.9643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r464/469 [============================\u003e.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r469/469 [==============================] - 2s 5ms/step - loss: 0.1198 - accuracy: 0.9644 - val_loss: 0.0944 - val_accuracy: 0.9716\r1/313 [..............................] - ETA: 8s - loss: 0.0785 - accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r19/313 [\u003e.............................] - ETA: 0s - loss: 0.0754 - accuracy: 0.9852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r36/313 [==\u003e...........................] - ETA: 0s - loss: 0.0956 - accuracy: 0.9705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r54/313 [====\u003e.........................] - ETA: 0s - loss: 0.1193 - accuracy: 0.9641\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r73/313 [=====\u003e........................] - ETA: 0s - loss: 0.1268 - accuracy: 0.9606\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r93/313 [=======\u003e......................] - ETA: 0s - loss: 0.1233 - accuracy: 0.9610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r112/313 [=========\u003e....................] - ETA: 0s - loss: 0.1196 - accuracy: 0.9637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/313 [===========\u003e..................] - ETA: 0s - loss: 0.1234 - accuracy: 0.9618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r151/313 [=============\u003e................] - ETA: 0s - loss: 0.1204 - accuracy: 0.9617\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r171/313 [===============\u003e..............] - ETA: 0s - loss: 0.1137 - accuracy: 0.9642\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r191/313 [=================\u003e............] - ETA: 0s - loss: 0.1163 - accuracy: 0.9638\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r210/313 [===================\u003e..........] - ETA: 0s - loss: 0.1119 - accuracy: 0.9658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r230/313 [=====================\u003e........] - ETA: 0s - loss: 0.1048 - accuracy: 0.9681\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r245/313 [======================\u003e.......] - ETA: 0s - loss: 0.1003 - accuracy: 0.9695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r263/313 [========================\u003e.....] - ETA: 0s - loss: 0.0979 - accuracy: 0.9699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r282/313 [==========================\u003e...] - ETA: 0s - loss: 0.0943 - accuracy: 0.9712\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r303/313 [============================\u003e.] - ETA: 0s - loss: 0.0911 - accuracy: 0.9723\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r313/313 [==============================] - 1s 3ms/step - loss: 0.0944 - accuracy: 0.9716\rTest accuracy: 0.9715999960899353 模型保存和加载 TensorFlow 提供了两种方式来保存和加载模型： 1.使用 tf.train.Checkpoint：\nimport tensorflow as tf\r# 定义模型\rmodel = tf.keras.models.Sequential([\rtf.keras.layers.Dense(10, activation='relu', input_shape=(784,)),\rtf.keras.layers.Dense(10, activation='softmax')\r])\r# 定义优化器和损失函数\rmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r# 训练模型\rmodel.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\r# 创建 Checkpoint 对象\rcheckpoint = tf.train.Checkpoint(model=model)\r# 保存模型\rcheckpoint.save('./model.ckpt')\r# 加载模型\rcheckpoint.restore('./model.ckpt') 2.使用 tf.keras.callbacks.ModelCheckpoint：\nimport tensorflow as tf\r# 定义模型\rmodel = tf.keras.models.Sequential([\rtf.keras.layers.Dense(10, activation='relu', input_shape=(784,)),\rtf.keras.layers.Dense(10, activation='softmax')\r])\r# 定义优化器和损失函数\rmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r# 定义 ModelCheckpoint 回调函数\rcheckpoint = tf.keras.callbacks.ModelCheckpoint('./model.h5', save_best_only=True, save_weights_only=False, monitor='val_loss')\r# 训练模型\rmodel.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), callbacks=[checkpoint])\r# 加载模型\rmodel = tf.keras.models.load_model('./model.h5') 第一种方式使用 tf.train.Checkpoint 对象保存和加载模型，可以保存模型的权重和优化器状态，还支持在训练过程中保存模型和恢复模型。第二种方式使用 tf.keras.callbacks.ModelCheckpoint 回调函数保存模型，可以指定保存最佳模型，同时可以选择保存模型的权重或整个模型。\n绘制ui手写数字识别 首先改写之前的tensorflow代码保存模型\n# 定义 ModelCheckpoint 回调函数\rcheckpoint = tf.keras.callbacks.ModelCheckpoint('./model.h5', save_best_only=True, save_weights_only=False, monitor='val_loss')\r# 训练模型\rhistory = model.fit(x_train, y_train, epochs=8, batch_size=128, validation_data=(x_test, y_test),\rcallbacks=[checkpoint]\r) 绘制ui，加载模型并预测显示在ui上\nimport tkinter as tk\rfrom PIL import Image, ImageDraw\rimport numpy as np;\rimport tensorflow as tf\rimport matplotlib.pyplot as plt\rfrom tensorflow.keras.datasets import mnist\r# 加载模型并进行数字识别\rdef recognize_digit():\r# 将画板图像转换成灰度图像，并将其大小调整为 28x28,注意要用convert,因为彩色图像是rgb是三维的，resize只是改变了rg，需要convert转换成灰度的二维\rimage_resized = np.array(image.resize((28, 28)).convert('L'))\r# 反转图像，因为灰度图像是黑底白字，但是我们训练的图片都是白底黑字，所以取反\rimage_resized = np.invert(image_resized)\r# plt.imshow(image_resized, cmap=plt.cm.binary)\r# plt.show()\r# 将图像转换为数字数组\rdata = image_resized.reshape(1, 784).astype('float32') / 255.0\r# 在这里添加您的识别代码\rmodel = tf.keras.models.load_model('./model.h5')\rpredictions = model.predict(np.array([data]))\rresult_label.configure(text=\"识别结果为：\" + str(np.argmax(predictions)))\r# 清空画板\rdef clear_canvas():\rdraw.rectangle((0, 0, 280, 280), fill=\"white\")\rcanvas.delete(\"all\")\r# 创建窗口\rwindow = tk.Tk()\rwindow.title(\"手写数字识别\")\rwindow.geometry(\"400x400\")\r# 创建画布\rcanvas = tk.Canvas(window, width=280, height=280, bg=\"white\")\rcanvas.grid(row=0, column=0, columnspan=2)\r# 创建清空画布按钮\rclear_button = tk.Button(window, text=\"清空画板\", command=clear_canvas)\rclear_button.grid(row=1, column=0)\r# 创建识别按钮\rrecognize_button = tk.Button(window, text=\"识别数字\", command=recognize_digit)\rrecognize_button.grid(row=1, column=1)\r# 创建识别结果标签\rresult_label = tk.Label(window, text=\"\")\rresult_label.grid(row=2, column=0, columnspan=2)\r# 创建画板图像\rimage = Image.new(\"RGB\", (280, 280), (255, 255, 255))\rdraw = ImageDraw.Draw(image)\r# 绑定画板事件\rdef on_mouse_down(event):\rglobal prev_x, prev_y\rprev_x, prev_y = event.x, event.y\rdef on_mouse_move(event):\rglobal prev_x, prev_y\rcanvas.create_line(prev_x, prev_y, event.x, event.y, width=20)\rdraw.line((prev_x, prev_y, event.x, event.y), fill=\"black\", width=20)\rprev_x, prev_y = event.x, event.y\rcanvas.bind(\"\u003cButton-1\u003e\", on_mouse_down)\rcanvas.bind(\"\u003cB1-Motion\u003e\", on_mouse_move)\r# 显示窗口\rwindow.mainloop() 程序效果 对于手写数字识别，正确率的高低不仅取决于模型的性能，还与数据的质量和多样性有关。在训练模型时，使用的数据集可能与实际应用场景中的数据存在差异，导致模型无法很好地泛化到新的、未知的数据上。\n此外，手写数字的识别难度还受到许多因素的影响，如书写的风格、字母大小、笔画粗细、书写方向等等。如果训练集中没有涵盖到这些因素，那么模型就可能无法准确地识别新的手写数字。因此，为了提高手写数字识别的准确率，需要使用更多、更丰富的数据集，并对模型进行调参和优化以提高其泛化能力。\n来看下下面这个例子，我们的mlp多层感知器貌似基本无能为力了",
    "description": "神经网络 简介 神经网络是一种基于生物神经系统结构和功能特点而设计的人工神经网络模型，具有很强的自适应性和非线性映射能力。神经网络由多个神经元（或称节点）组成，这些神经元通过连接权重相互连接，构成多层的网络结构。每个神经元接收到来自其它神经元的信号，并将这些信号加权线性组合后通过激活函数进行非线性转换，最终输出给下一层神经元或输出层。\n学习机器学习后，学习神经网络可以帮助你更深入地理解模式识别和人工智能领域的基础知识。神经网络在很多领域都有广泛的应用，例如计算机视觉、自然语言处理、语音识别等。学习神经网络可以让你掌握这些领域中最前沿的技术，并且能够应用这些技术来解决具体的问题。同时，神经网络的学习方法和算法也是机器学习的重要组成部分，学习神经网络可以帮助你更好地理解机器学习的原理和技术，从而更好地应用机器学习来解决实际问题。\n学习路径 如果你已经学过机器学习，那么开始学习神经网络，可以从多层感知器（Multilayer Perceptron，简称 MLP）神经网络入手。 MLP 是最基本的神经网络模型之一，它的结构比较简单，易于理解和实现，同时又有很好的可扩展性和通用性，可以应用于分类、回归等多种任务。学习 MLP 之后，你可以进一步学习卷积神经网络（Convolutional Neural Networks，简称 CNN）和循环神经网络（Recurrent Neural Networks，简称 RNN），它们分别用于计算机视觉和自然语言处理等特定领域的问题。总之，建议先从 MLP 入手，逐渐深入学习其他类型的神经网络。\n分类 神经网络可以分为多种不同的类型，下面列举一些常见的神经网络类型：\n前馈神经网络（Feedforward Neural Network）：前馈神经网络是最基本的神经网络类型，也是深度学习中最常见的神经网络类型。它由若干个神经元按照一定的层次结构组成，每个神经元接收上一层的输出，产生本层的输出，从而实现信息的传递和处理。\n卷积神经网络（Convolutional Neural Network）：卷积神经网络是一种专门用于图像处理和计算机视觉任务的神经网络类型。它通过卷积和池化等操作，可以提取图像中的特征，从而实现图像分类、目标检测、图像分割等任务。\n循环神经网络（Recurrent Neural Network）：循环神经网络是一种能够处理序列数据的神经网络类型。它通过记忆单元和门控机制等方式，可以处理任意长度的序列数据，从而实现自然语言处理、语音识别等任务。\n自编码器（Autoencoder）：自编码器是一种无监督学习的神经网络类型，它的目标是将输入数据进行压缩和解压缩，从而实现特征提取和降维等任务。\n深度置信网络（Deep Belief Network）：深度置信网络是一种由多个受限玻尔兹曼机组成的神经网络类型。它可以通过逐层贪心预训练和微调等方式，实现高效的特征学习和分类任务。\n除了以上列举的几种神经网络类型，还有众多其他的神经网络类型，如反向传播神经网络、Hopfield网络、Boltzmann机等。不同的神经网络类型适用于不同的任务和数据类型，需要根据具体的问题选择合适的神经网络类型。\n多层感知器（MLP） MLP神经网络属于前馈神经网络（Feedforward Neural Network）的一种。在网络训练过程中，需要通过反向传播算法计算梯度，将误差从输出层反向传播回输入层，用于更新网络参数。这个过程中需要使用反向传播算法来计算梯度，并且在某些类型的神经网络中，例如循环神经网络（RNN），也存在反馈回路。除了MLP，其他常见的前馈神经网络包括卷积神经网络（CNN）和循环神经网络（RNN）等。\n神经网络认识 我们以一个简单的例子来认识神经网络，只是为了理解其中的一些概念。 我们已知四个数据点(1,1)(-1,1)(-1,-1)(1,-1)，这四个点分别对应I~IV象限（也就是数据属于的类别），如果这时候给我们一个新的坐标点（比如(2,2)），那么它应该属于哪个象限呢？（没错，当然是第I象限，但我们的任务是要让机器知道，机器不知道有象限这个东西啊，他只能根据历史数据的经验推断），如果机器只是知道一堆数据 比如(-2,3)属于2象限，机器就需要通过这些数据总结出一个特征，这个特征可能就是根据x和y坐标的正负来判断象限了。 两层神经网络 这里我们构建一个两层神经网络，理论上两层神经网络已经可以拟合任意函数。这个神经网络的结构如下图： 首先我们去掉途中难懂的东西",
    "tags": [],
    "title": "深度学习02-神经网络(MLP多层感知器)",
    "uri": "/docs/programming/ai/deep_learning/basic/dl_02_mlp/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 卷积神经网络",
    "content": "简介 CNN，即卷积神经网络（Convolutional Neural Network），是一种常用于图像和视频处理的深度学习模型。与传统神经网络相比，CNN 有着更好的处理图像和序列数据的能力，因为它能够自动学习图像中的特征，并提取出最有用的信息。\nCNN 的一个核心特点是卷积操作，它可以在图像上进行滑动窗口的计算，通过滤波器（又称卷积核）和池化层（Max Pooling）来提取出图像的特征。卷积操作可以有效地减少权重数量，降低计算量，同时也能够保留图像的空间结构信息。池化层则可以在不改变特征图维度的前提下，减少计算量，提高模型的鲁棒性。\nCNN 的典型结构包括卷积层、池化层、全连接层等。同时，为了防止过拟合，CNN 还会加入一些正则化的技术，如 Dropout 和 L2 正则等。\nCNN 在图像分类、目标检测、语音识别等领域都有着广泛的应用。在图像分类任务中，CNN 的经典模型包括 LeNet-5、AlexNet、VGG 和 GoogleNet/Inception 等，这些模型的设计思想和网络结构都有所不同，但都对卷积神经网络的发展做出了重要贡献。\n发展历程 卷积神经网络（CNN）是一种深度学习模型，广泛应用于图像识别、计算机视觉等领域。在CNN的发展历程中，涌现出了许多经典的模型，下面简要介绍几个著名的模型。\nLeNet-5 LeNet-5是Yann LeCun等人于1998年提出的，是第一个被广泛应用的卷积神经网络模型。它主要用于手写数字识别，包含卷积层、池化层和全连接层。LeNet-5的设计使得它在MNIST手写数字识别任务上获得了很好的表现。它的特点是卷积核数量较少（6和16）以及参数量较少，第一层卷积层使用了6个大小为5×5的卷积核，第二层卷积层使用了16个大小为5×5的卷积核。这种设计可以有效地减少模型的参数量，但它是卷积神经网络的开山鼻祖，为后续模型奠定了基础。\nAlexNet AlexNet由Alex Krizhevsky等人于2012年提出，是第一个在ImageNet图像分类比赛中取得优异成绩的卷积神经网络模型。它采用了多个卷积层和池化层，使用了ReLU激活函数和Dropout正则化技术。AlexNet的设计使得它在ImageNet图像分类比赛中大幅领先于其他模型，从而引领了卷积神经网络的新一轮发展。它的特点是使用了大量卷积核（近6000个）、参数量较大，但在准确率和效率上都有很好的表现。\nVGG VGG由Karen Simonyan和Andrew Zisserman于2014年提出，其主要贡献是提出了使用更小的卷积核（3x3）来代替较大的卷积核。这种设计使得网络更深，而且参数量更少，从而提高了效率和准确率。VGG包含了16个或19个卷积层和池化层，这些层都采用了相同的卷积核大小和步长。VGG在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ResNet等模型提供了启示。\nGoogleNet/Inception GoogleNet由Google团队于2014年提出，其主要贡献是提出了Inception模块，可以在不增加参数量的情况下增加网络的深度和宽度。Inception模块采用了多个不同大小的卷积核和池化层来进行特征提取，然后将它们串联在一起，形成了一个模块。GoogleNet还使用了全局平均池化层来代替全连接层，从而进一步减少了参数量。GoogleNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ResNet、DenseNet等模型提供了启示。\nResNet ResNet由Microsoft Research Asia团队于2015年提出，其主要贡献是提出了残差学习，可以解决深度卷积神经网络的退化问题。退化问题指的是随着网络深度的增加，准确率反而下降的现象。残差学习通过引入跨层连接来将输入直接传递到输出，从而避免了信息的损失。ResNet包含了较深的网络结构（152层），但却获得了更好的准确率。ResNet的设计思想被后续的DenseNet、MobileNet等模型所继承。\nDenseNet DenseNet由Gao Huang等人于2017年提出，其主要贡献是提出了密集连接，可以增加网络的深度和宽度，从而提高了效率和准确率。密集连接指的是将每个层的输出都与后面所有层的输入相连，形成了一个密集的连接结构。这种设计使得网络更加紧凑，参数量更少，同时也可以提高特征的复用性。DenseNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ShuffleNet、EfficientNet等模型提供了启示。\nMobileNet MobileNet由Google团队于2017年提出，其主要贡献是提出了深度可分离卷积，可以在减少参数量的同时保持较好的准确率。深度可分离卷积指的是将卷积操作分为深度卷积和逐点卷积两步，从而减少了计算量和参数量。MobileNet采用了多个深度可分离卷积层和池化层，可以在移动设备等资源受限的环境下实现高效的图像分类和目标检测。MobileNet的设计思想被后续的ShuffleNet、EfficientNet等模型所继承。\nShuffleNet ShuffleNet由Microsoft Research Asia团队于2018年提出，其主要贡献是提出了通道重组和组卷积，可以在保持准确率的前提下大幅减少参数量和计算量。通道重组指的是将输入的通道分组并重新组合，从而让不同的组之间进行信息的交流。组卷积指的是将卷积操作分为组内卷积和组间卷积两步，从而减少了计算量和参数量。ShuffleNet采用了多个通道重组和组卷积层，可以在资源受限的环境下实现高效的图像分类和目标检测。\nEfficientNet EfficientNet由Google团队于2019年提出，其主要贡献是提出了网络缩放和复合系数，可以在保持准确率的前提下大幅减少参数量和计算量。网络缩放指的是同时缩放网络的深度、宽度和分辨率，从而在不改变模型结构的情况下进行优化。复合系数指的是将深度、宽度和分辨率的缩放系数进行组合，从而得到一个更加高效的模型。EfficientNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的模型优化提供了启示。\nRegNet RegNet由Facebook AI Research团队于2020年提出，其主要贡献是提出了网络结构的自适应规则，可以在保持准确率的前提下大幅减少参数量和计算量。自适应规则指的是通过搜索和优化来自动调整网络结构的超参数，从而得到一个更加高效的模型。RegNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的模型优化提供了启示。\n以上是几个著名的卷积神经网络模型，它们的设计思想和网络结构都有所不同，但都对卷积神经网络的发展做出了重要贡献。\n图解原理 卷积神经网络在图像识别中大放异彩，达到了前所未有的准确度，有着广泛的应用。接下来将以图像识别为例子，来介绍卷积神经网络的原理。\n案例 假设给定一张图（可能是字母X或者字母O），通过CNN即可识别出是X还是O，如下图所示，那怎么做到的呢 图像输入 如果采用经典的神经网络模型，则需要读取整幅图像作为神经网络模型的输入（即全连接的方式），当图像的尺寸越大时，其连接的参数将变得很多，从而导致计算量非常大。 而我们人类对外界的认知一般是从局部到全局，先对局部有感知的认识，再逐步对全体有认知，这是人类的认识模式。在图像中的空间联系也是类似，局部范围内的像素之间联系较为紧密，而距离较远的像素则相关性较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。这种模式就是卷积神经网络中降低参数数目的重要神器：局部感受野。 特征提取 如果字母X、字母O是固定不变的，那么最简单的方式就是图像之间的像素一一比对就行，但在现实生活中，字体都有着各个形态上的变化（例如手写文字识别），例如平移、缩放、旋转、微变形等等，如下图所示： 我们的目标是对于各种形态变化的X和O，都能通过CNN准确地识别出来，这就涉及到应该如何有效地提取特征，作为识别的关键因子。 回想前面讲到的“局部感受野”模式，对于CNN来说，它是一小块一小块地来进行比对，在两幅图像中大致相同的位置找到一些粗糙的特征（小块图像）进行匹配，相比起传统的整幅图逐一比对的方式，CNN的这种小块匹配方式能够更好的比较两幅图像之间的相似性。如下图： 以字母X为例，可以提取出三个重要特征（两个交叉线、一个对角线），如下图所示： 假如以像素值\"1\"代表白色，像素值\"-1\"代表黑色，则字母X的三个重要特征如下： 上面的特征提取是个假设，实际当有多张图作为输入时，卷积神经网络会对每张图进行特征提取，具体过程如下：输入图片经过第一个卷积层，卷积核会在图像上滑动，提取出一些低层次的特征，例如边缘、角点等。\n在卷积神经网络中，如果使用了多个不同的卷积核，那么每个卷积核的局部感受野大小是相同的，但是不同卷积核的权重是不同的，这样可以使得每个卷积核学习到不同的特征。\n举个例子，假设我们在卷积层中使用三个不同的卷积核，其中第一个卷积核的权重用于检测边缘，第二个卷积核的权重用于检测纹理特征，第三个卷积核的权重用于检测目标的形状。这三个卷积核的局部感受野大小都相同，但是由于它们的权重不同，因此每个卷积核可以学习到不同的特征。\n需要注意的是，卷积核的大小和步长也会影响到每个卷积核的局部感受野大小。如果卷积核的大小较大，那么它的局部感受野也会相应地变大；如果步长较大，那么卷积核每次滑动的距离也会相应地变大，从而影响到卷积核的局部感受野大小。\n比如卷积核 [-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]\n这个矩阵实际上是一个卷积核，也被称为Sobel滤波器。它可以用来检测图像中的垂直边缘。 在计算机视觉中，边缘是指图像中灰度值变化较大的区域。垂直边缘是指从图像的上部到下部或从下部到上部的灰度值变化。 卷积核的工作原理是将它与图像的像素进行卷积操作，从而提取图像的特征。在这个例子中，卷积核的中心元素是0，表示它与图像的中心像素无关。而卷积核的上面一行元素[-1, 0, 1]表示它与图像的上方像素进行卷积操作。同理，卷积核的下面一行元素[-1, 0, 1]表示它与图像的下方像素进行卷积操作。\n当卷积核与图像中的像素进行卷积操作时，如果图像中存在垂直边缘，那么卷积结果会显示出明显的变化。具体来说，在垂直边缘的一侧，卷积结果会得到较大的正值，而在垂直边缘的另一侧，卷积结果会得到较大的负值。这样，我们就可以通过阈值化卷积结果来识别图像中的垂直边缘，是负数的部分直接就归0了。\n举个例子，假设我们有一张图像，其中一部分是垂直边缘。我们将卷积核应用于这个图像的垂直边缘部分，卷积结果会显示出正值和负值，这样我们就可以通过阈值化卷积结果来提取垂直边缘的位置。\n希望这个例子可以帮助你理解为什么[-1, 0, 1]这个矩阵可以用来检测垂直边缘。 再比如 [[-0.1111, -0.1111, -0.1111], [-0.1111, 1.0000, -0.1111], [-0.1111, -0.1111, -0.1111]] 被称为拉普拉斯滤波器或者锐化滤波器。它可以用来增强图像中的边缘。 在这个矩阵中，中心元素1表示它与图像的中心像素有关。而周围的元素-0.1111表示它们与图像的周围像素有关。\n当卷积核与图像进行卷积操作时，中心像素的值会被放大，而周围像素的值会被抑制。这样，在图像的边缘部分，由于像素值的变化较大，卷积结果会显示出较大的正值和负值，从而增强了边缘的对比度。\n举个例子，假设我们有一张图像，其中包含一些边缘。我们将这个卷积核应用于图像，卷积结果会增强边缘的对比度，使得边缘更加清晰。\n因此，这个卷积核能够检测边缘，通过增强边缘的对比度，使得边缘更加明显。\n边缘 边缘是图像中像素灰度值变化明显的地方，通常表示图像中物体的边缘、轮廓或者纹理等信息。在图像处理和计算机视觉中，边缘检测是一种常用的技术，可以用来分割图像、提取特征等。 比如如下图 提取边缘的效果 角点 角点是图像中局部区域的特殊点，具有明显的角度变化。角点通常是由不同方向的边缘交汇处形成的，具有高斯曲率，是图像中的重要特征之一。在图像配准、物体跟踪、图像匹配等方面，角点检测也是一种常用的技术。常用的角点检测算法包括Harris角点检测、Shi-Tomasi角点检测等。 如下图 opencv OpenCV（Open Source Computer Vision Library）是一个开源的计算机视觉和机器学习软件库。它可以帮助开发人员快速构建计算机视觉应用程序，如图像处理、物体检测、人脸识别、视频分析等。\nOpenCV最初是由英特尔公司发起的，现已成为一个跨平台的开源项目，支持多种编程语言，包括C++、Python、Java等，可以在Windows、Linux、macOS等操作系统上运行。\n这里使用opencv来讲某张图片的边缘和角点提取出来，比如图片是 具体这里不细讲opencv，以后在开文讲解。 代码\n#%%\rimport cv2 #注意安装open-cv conda install open-cv\rimport numpy as np\rimport matplotlib.pyplot as plt\r# 读入lena图像\rimg = cv2.imread('d:/9.png')\r# 将BGR图像转换为RGB图像，便于matplotlib显示\rimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r# 将图像转换为灰度图像\rgray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\rgray_ori=gray\r# 使用Canny边缘检测函数检测图像的边缘\redges = cv2.Canny(gray, 100, 200)\r# 创建SIFT对象\rsift = cv2.xfeatures2d.SIFT_create()\r# 检测图像的特征点\rkeypoints = sift.detect(gray, None)\r# 在图像上绘制特征点\rimg_sift = cv2.drawKeypoints(img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\r# 检测图像的角点\rdst = cv2.cornerHarris(gray, 2, 3, 0.04)\r# 将角点标记为红色\rimg_corner = img.copy()\rimg_corner[dst \u003e 0.01 * dst.max()] = [255, 0, 0]\r# 创建一个Matplotlib窗口并显示图像及其各种特征\rplt.rcParams['font.family'] = 'SimHei'\rfig, axs = plt.subplots(2, 2, figsize=(10, 10))\raxs[0, 0].imshow(img)\raxs[0, 0].set_title('原始图像')\raxs[0, 1].imshow(edges, cmap='gray')\raxs[0, 1].set_title('边缘')\raxs[1, 0].imshow(img_sift)\r#SIFT的全称是Scale Invariant Feature Transform，尺度不变特征变换。具有旋转不变性、尺度不变性、亮度变化保持不变性，是一种非常稳定的局部特征。\raxs[1, 0].set_title('SIFT特征')\raxs[1, 1].imshow(img_corner)\raxs[1, 1].set_title('角点特征')\rplt.show() 输出效果 特征提取原理 请看完【卷积】章节后再来看这一段 常用的卷积核有以下几种：\n高斯滤波器：用于图像平滑处理，可以减少图像噪声。 高通滤波器：用于突出图像中的高频信息，例如边缘、角等。 低通滤波器：用于突出图像中的低频信息，例如模糊、平滑等。 Sobel滤波器：用于检测图像中的边缘信息。 Laplacian滤波器：用于增强图像的高频信息，例如边缘、细节等。 Scharr滤波器：与Sobel滤波器类似，但对边缘的响应更强。 Prewitt滤波器：与Sobel滤波器类似，但对边缘的响应更平滑。 这些卷积核可用于图像处理中的不同任务，例如边缘检测、图像平滑、图像增强等。您可以根据任务的不同选择适合的卷积核来处理图像。\n下面定义卷积核可以被看作是一个高通滤波器，因为它的中心像素被赋予了一个较大的权重，而周围像素的权重较小。这种权重分配使得卷积核能够检测出图像中的高频信息，例如边缘、角等。在卷积操作中，卷积核和图像中的每个像素点都进行相乘，并将结果加起来，这样可以得到一个新的像素值。如果卷积核中心像素周围的像素值与中心像素值之间的差异较大，那么卷积操作的结果将会比较大，这表明这个像素点可能是边缘点。因此，这个卷积核能够突出图像中的边缘信息。\nkernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) 有如下图片 使用opencv加载他，并用卷积核进行卷积\nimport cv2\rimport numpy as np\rfrom myutils.common import show,fillColor\r# 读取图片\rimg = cv2.imread('./images/z.png')\r# 将图像转换为灰度图像\rgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r# 定义卷积核\rkernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\r# kernel = np.array([[-1,-1,-1,-1,-1],[-1,-1,-1,-1,-1], [-1,-1,20,-1,-1],[-1,-1,-1,-1,-1], [-1,-1,-1,-1,-1]])\r# kernel = cv2.getGaussianKernel(5, 1)\r# 对灰度图像进行卷积操作，#注意如果-1 \u003c0的值会被归一化为0\redges = cv2.filter2D(gray, cv2.CV_32F, kernel)\rprint(edges[:][edges\u003c0])\r# 对卷积结果进行ReLU处理\redges_relu = np.maximum(0, edges)\rshow(img,'Original Image',cmap=\"gray\",debug=True) show(edges, 'Edges Image',cmap=\"gray\",debug=True)\rshow(edges_relu, 'Edges ReLU Image',cmap=\"gray\",debug=True)\rdef show(dilate, title, cmap=None, debug=False):\rif debug:\rplt.title(title)\rplt.imshow(dilate, cmap=cmap)\rplt.show() 为什么说卷积操作提取的是线性特征，而使用relu了 让我们以一个简单的例子来说明卷积操作本身并不能提取非线性特征。\n假设我们有一个输入矩阵X，它包含以下值：\nX = [[1, 2, 3],\r[4, 5, 6],\r[7, 8, 9]]\n现在，我们使用一个大小为2x2的卷积核K来对X进行卷积，卷积核的值如下：\nK = [[1, 1],\r[1, 1]]\n我们可以使用矩阵乘法来执行卷积操作。具体来说，我们将K矩阵翻转后，与X矩阵做点积操作，得到一个输出矩阵Y：\nY = K*X = [[12, 16],\r[24, 28]]\n可以看到，输出矩阵Y是输入矩阵X的线性组合，因此卷积操作本身只能提取输入矩阵X的线性特征，例如边缘和纹理等。\n但是，当我们使用非线性激活函数，例如ReLU激活函数，对输出矩阵Y进行处理时，就可以将线性特征转换为非线性特征。例如，当我们对Y应用ReLU函数时，得到的非线性特征是：\nReLU(Y) = [[12, 16],\r[24, 28]]\n因此，卷积操作本身只能提取输入矩阵的线性特征，但当与非线性激活函数结合使用时，可以提取非线性特征。\n卷积 那么这些特征又是怎么进行匹配计算呢？（不要跟我说是像素进行一一匹配的，汗！） 这时就要请出今天的重要嘉宾：卷积。那什么是卷积呢，不急，下面慢慢道来。 当给定一张新图时，CNN并不能准确地知道这些特征到底要匹配原图的哪些部分，所以它会在原图中把每一个可能的位置都进行尝试，相当于把这个feature（特征）变成了一个过滤器。这个用来匹配的过程就被称为卷积操作，这也是卷积神经网络名字的由来。 卷积的操作如下图所示： 黄色的部分就是一个卷积核，也就是上一张提取的特征 [[1,0,1] [0,1,0] [1,0,1]] 同图像中的每个可能的3*3图像进行计算(卷积相同位置相乘后相加/当前聚集矩阵个个数9)，计算的结果得到一个数放在当前被卷积的中心位置，最终会得到一个去掉最外层的新的矩阵，具体计算逻辑参考下文。\n在本案例中，要计算一个feature（特征）和其在原图上对应的某一小块的结果，只需将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可（注：也可不除以总个数的）。 如果两个像素点都是白色（值均为1），那么11 = 1，如果均为黑色，那么(-1)(-1) = 1，也就是说，每一对能够匹配上的像素，其相乘结果为1。类似地，任何不匹配的像素相乘结果为-1。具体过程如下（第一个、第二个……、最后一个像素的匹配结果）： 先将我们之前提取的三个特征中的一个拿来进行卷积 比如拿第一个特征和绿色框框圈起来的部分比较，完全一样 根据卷积的计算方式，第一块特征匹配后的卷积计算如下，结果为1 对于其它位置的匹配，也是类似（例如中间部分的匹配） 以此类推，对三个特征图像不断地重复着上述过程，通过每一个feature（特征）的卷积操作，会得到一个新的二维数组，称之为feature map（特征图）。其中的值，越接近1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。如下图所示： 可以看出，当图像尺寸增大时，其内部的加法、乘法和除法操作的次数会增加得很快，每一个filter的大小和filter的数目呈线性增长。由于有这么多因素的影响，很容易使得计算量变得相当庞大。\n池化(Pooling) 为了有效地减少计算量，CNN使用的另一个有效的工具被称为“池化(Pooling)”。池化就是将输入图像进行缩小，减少像素信息，只保留重要信息。 池化的操作也很简单，通常情况下，池化区域是22大小，然后按一定规则转换成相应的值，例如取这个池化区域内的最大值（max-pooling）、平均值（mean-pooling）等，以这个值作为结果的像素值。 下图显示了左上角22池化区域的max-pooling结果，取该区域的最大 max(0.77,-0.11,-0.11,1.00) ，作为池化后的结果，如下图： 池化区域往左，第二小块取大值max(0.11,0.33,-0.11,0.33)，作为池化后的结果，如下图： 其它区域也是类似，取区域内的最大值作为池化后的结果，最后经过池化后，结果如下： 对所有的feature map执行同样的操作，结果如下： 最大池化（max-pooling）保留了每一小块内的最大值，也就是相当于保留了这一块最佳的匹配结果（因为值越接近1表示匹配越好）。也就是说，它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。 通过加入池化层，图像缩小了，能很大程度上减少计算量，降低机器负载。\n激活函数ReLU (Rectified Linear Units) 常用的激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者ReLU常见于卷积层。 回顾一下前面讲的感知机，感知机在接收到各个输入，然后进行求和，再经过激活函数后输出。激活函数的作用是用来加入非线性因素，把卷积层输出结果做非线性映射。 在卷积神经网络中，激活函数一般使用ReLU(The Rectified Linear Unit，修正线性单元)，它的特点是收敛快，求梯度简单。计算公式也很简单，max(0,T)，即对于输入的负值，输出全为0，对于正值，则原样输出。 下面看一下本案例的ReLU激活函数操作过程： 第一个值，取max(0,0.77)，结果为0.77，如下图 第二个值，取max(0,-0.11)，结果为0，如下图 以此类推，经过ReLU激活函数后，结果如下： 对所有的feature map执行ReLU激活函数操作，结果如下： 深度神经网络 通过将上面所提到的卷积、激活函数、池化组合在一起，就变成下图： 通过加大网络的深度，增加更多的层，就得到了深度神经网络，如下图： 全连接层(Fully connected layers) 全连接层在整个卷积神经网络中起到“分类器”的作用，即通过卷积、激活函数、池化等深度网络后，再经过全连接层对结果进行识别分类。 首先将经过卷积、激活函数、池化的深度网络后的结果串起来，如下图所示： 由于神经网络是属于监督学习，在模型训练时，根据训练样本对模型进行训练，从而得到全连接层的权重（如预测字母X的所有连接的权重） 在利用该模型进行结果识别时，根据刚才提到的模型训练得出来的权重，以及经过前面的卷积、激活函数、池化等深度网络计算出来的结果，进行加权求和，得到各个结果的预测值，然后取值最大的作为识别的结果（如下图，最后计算出来字母X的识别值为0.92，字母O的识别值为0.51，则结果判定为X） 上述这个过程定义的操作为”全连接层“(Fully connected layers)，全连接层也可以有多个，如下图： 卷积神经网络（Convolutional Neural Networks） 将以上所有结果串起来后，就形成了一个“卷积神经网络”（CNN）结构，如下图所示： 最后，再回顾总结一下，卷积神经网络主要由两部分组成，一部分是特征提取（卷积、激活函数、池化），另一部分是分类识别（全连接层），下图便是著名的手写文字识别卷积神经网络结构图： 本章节内容参考：https://my.oschina.net/u/876354/blog/1620906\n卷积api Conv2D Conv2D是卷积神经网络中最核心的层之一，它是用于图像或其他二维数据的卷积处理的层。Conv2D的作用是将输入的二维图像或数据，通过卷积核进行一系列的卷积操作，从而提取出图像或数据中的特征。\nConv2D层的输入为一个tensor，该tensor的形状通常为(batch_size, height, width, channel)，其中batch_size表示输入数据的数量，height和width表示输入数据的高度和宽度，channel表示输入数据的通道数（如RGB图像的通道数为3）。\nConv2D层的输出也是一个tensor，表示经过卷积操作后得到的特征图。输出tensor的形状通常为(batch_size, conv_height, conv_width, filters)，其中conv_height和conv_width表示卷积核作用后得到的特征图的高度和宽度，filters表示卷积核的数量，即输出特征图的通道数。\n在卷积过程中，Conv2D层将卷积核作用于输入数据，通过逐个计算每个卷积核与输入数据的卷积操作，得到卷积后的输出特征图。在卷积过程中，卷积核的大小、步长、填充方式等参数都可以自由设置，以适应不同的应用场景。 在TensorFlow 2.0和Keras中，可以通过以下代码来创建一个Conv2D层：\nfrom tensorflow.keras.layers import Conv2D\rconv_layer = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(height, width, channel)) filters：卷积核的数量，也就是输出的特征图的个数。 kernel_size：卷积核的大小，可以是一个整数，表示正方形卷积核的边长，也可以是一个元组，表示长和宽不同的卷积核。 strides：步长，也就是卷积核在输入特征图上移动的距离。可以是一个整数，表示在两个相邻的卷积核之间的距离，也可以是一个元组，表示在长和宽方向上的步长不同。 padding：填充方式，可以是’same’或’valid’。‘same’表示输出特征图的大小和输入特征图的大小相同，需要在输入特征图的周围填充一些值；‘valid’表示不需要填充，输出特征图的大小会根据输入特征图和卷积核的大小而变化。 activation：激活函数，用于给特征图添加非线性变换。常见的激活函数有’relu’、‘sigmoid’、’tanh’等。 input_shape：输入特征图的形状，可以是一个三元组，表示高、宽和通道数。在第一层卷积层中需要指定该参数。 kernel_regularizer:在深度学习中，为了防止模型过拟合，通常会使用正则化技术对模型进行约束，其中一个常用的正则化方法是L2正则化。L2正则化是指在模型的损失函数中增加一个L2范数惩罚项，以限制模型权重的大小。 在Keras中，使用regularizers.l2(0.001)可以添加L2正则化惩罚项。其中，0.001是正则化参数，控制正则化强度的大小。正则化参数越大，惩罚项对权重的影响就越大，模型的复杂度就会降低，从而有效地防止过拟合。 具体来说，regularizers.l2(0.001)可以应用于神经网络中的任何权重矩阵，例如全连接层、卷积层等。在网络的定义中，我们可以在相应的层中使用kernel_regularizer参数来添加L2正则化。例如，在Keras中添加一个带有L2正则化的全连接层的代码如下所示： layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(28, 28, 1)), 卷积实例 取minist10张图，并且使用10个卷积核进行卷积，输出特征图，并显示图像， 因为每张图会生成10个卷积核，所以总共生成100张特征图。\n#%%\rimport tensorflow as tf\rimport matplotlib.pyplot as plt\rimport numpy as np\r# 加载mnist数据集\rmnist = tf.keras.datasets.mnist\r(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r# 取1张训练集图片\rimages = train_images[:10]\r# 将图片转换为float类型\rimages = images.astype('float32') / 255.0\r# 将图片reshape成4D张量，大小为(10, 28, 28, 1)，也就是第一个维度表示有10张图像，每张图像由28行、28列和1个# 通道(灰度)组成\rimages = np.expand_dims(images, axis=3)\r# 定义卷积核数量\rnum_filters = 10\r# 定义卷积层\rmodel = tf.keras.models.Sequential([\rtf.keras.layers.Conv2D(num_filters, (3, 3), activation='relu', input_shape=(28, 28, 1)),\r])\r# 计算卷积后的特征图\rfeatures = model.predict(images)\r# 绘制卷积后的特征图\rfig, axs = plt.subplots(nrows=num_filters, ncols=10, figsize=(10, num_filters))\rfor i in range(num_filters):\rfor j in range(10):\raxs[i][j].imshow(features[j, :, :, i], cmap='gray')\raxs[i][j].axis('off')\rplt.show() 输出 np.expand_dims函数用于在数组的指定轴上扩展维度。在这个例子中，images是一个形状为(10, 28, 28)的数组，表示10张28x28的灰度图像。但是，机器学习模型通常需要输入4维的数组，即(样本数, 图像高度, 图像宽度, 通道数)。因此，我们需要将images数组的最后一个维度(通道数)扩展一维，变成形状为(10, 28, 28, 1)的数组。 具体来说，axis=3表示在数组的第3个轴(从0开始计数)上扩展维度，它会在每张图像的最后一个维度上增加一个维度，从而将每张图像变成形状为(28, 28, 1)的三维数组。最终，images数组的形状变成了(10, 28, 28, 1)，表示有10张28x28的灰度图像，每张图像由一个通道组成。这样，images就可以作为输入传递给机器学习模型了。\n从上面的输出图片可以看出，有些卷积核的输出偏向于边缘，有些角点，有些纹理。\nMaxPooling2D keras.layers.MaxPooling2D((2, 2))是Keras中的一个层，它用于进行最大池化操作。 最大池化是一种常用的卷积神经网络操作，它可以在不改变图像尺寸的前提下，减少图像中的参数数量，从而减少计算量和内存消耗。最大池化操作将输入图像划分为不重叠的块，对每个块取最大值作为输出。在卷积神经网络中，最大池化通常跟卷积层交替使用，以提取图像的空间特征。\nMaxPooling2D层的参数是一个元组(2, 2)，表示池化窗口的大小为2x2。这意味着，输入图像会被划分为多个大小为2x2的块，对每个块取最大值作为输出。如果将池化窗口大小设置为(3, 3)，那么输入图像会被划分为多个大小为3x3的块，对每个块取最大值作为输出。\n总之，MaxPooling2D层可以帮助卷积神经网络提取图像的空间特征，同时减少计算量和内存消耗。\nFlatten keras.layers.Flatten()是Keras中的一个层，它用于将输入“平铺”成一维向量。\n在卷积神经网络中，通常会使用卷积层和池化层提取图像的特征，然后使用全连接层进行分类。全连接层的输入是一个一维向量，因此需要将之前的特征图“展平”为一维向量。这就是Flatten层的作用。\nFlatten层没有任何参数，它只是将输入张量按照顺序展开成一维向量。例如，如果输入张量的shape为(batch_size, 7, 7, 64)，则Flatten层的输出shape为(batch_size, 7764)。\n在搭建卷积神经网络时，通常会在卷积层和池化层之后添加一个Flatten层，将特征图展平成一维向量，然后再连接到全连接层进行分类。\nDense|Dropout 参考多层感知器\n手写数字识别 卷积mnist数据集 我们将加载MNIST数据集并进行预处理，将像素值缩放到0到1之间，并将数据集分为训练集和测试集。 这里数据处理详解参考多层感知器\nimport tensorflow as tf\rimport tensorflow.keras\rfrom tensorflow.keras import layers\rfrom tensorflow.keras.datasets import mnist\rfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\rfrom tensorflow.keras import regularizers\r(x_train, y_train), (x_test, y_test) = mnist.load_data()\rx_train = x_train.astype('float32') / 255.\rx_test = x_test.astype('float32') / 255.\rx_train = x_train[..., tf.newaxis]\rx_test = x_test[..., tf.newaxis]\rnum_classes = 10\ry_train = tf.keras.utils.to_categorical(y_train, num_classes)\ry_test = tf.keras.utils.to_categorical(y_test, num_classes) 接下来，我们将定义一个卷积神经网络模型。我们将使用两个卷积层和两个池化层，然后是两个全连接层和一个输出层。我们还将使用dropout和L2正则化来防止过拟合。\nmodel = tf.keras.Sequential([\rlayers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(28, 28, 1)),\rlayers.MaxPooling2D((2, 2)),\rlayers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\rlayers.MaxPooling2D((2, 2)),\rlayers.Flatten(),\rlayers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\rlayers.Dropout(0.5),\rlayers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\rlayers.Dropout(0.5),\rlayers.Dense(num_classes, activation='softmax')\r]) model.summary()是Keras中模型对象的一个方法，用于打印出模型的结构信息，包括每一层的名称、输出形状、参数数量等。这对于调试、优化模型以及理解模型结构都非常有用。\nmodel.summary() 然后，我们将对模型进行编译，并使用数据增强技术来进一步防止过拟合。数据增强技术将应用一系列随机变换，例如旋转、平移、缩放等，来生成新的训练样本。这样可以使模型更加鲁棒，并防止过拟合。\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\rdatagen = ImageDataGenerator(\rrotation_range=10,\rwidth_shift_range=0.1,\rheight_shift_range=0.1,\rzoom_range=0.1\r) 接下来，我们将使用训练集来训练模型，并使用测试集来评估模型的性能。\ndatagen.fit(x_train)\rbatch_size = 1024\repochs = 10\rcheckpoint = tf.keras.callbacks.ModelCheckpoint('./model.h5', save_best_only=True, save_weights_only=False, monitor='val_loss')\rhistory = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\repochs=epochs,\rvalidation_data=(x_test, y_test),\rsteps_per_epoch=len(x_train) // batch_size,callbacks=[checkpoint])\rscore = model.evaluate(x_test, y_test, verbose=0)\rprint('Test loss:', score[0])\rprint('Test accuracy:', score[1]) steps_per_epoch和batch_size 两个参数区别 batch_size 是指每个训练批次（batch）中包含的样本数。在深度学习中，通常会将训练集分成多个批次，每个批次中包含若干个样本。这样做的好处是可以利用矩阵运算加速计算，同时也可以在训练过程中随机打乱样本顺序以避免过拟合。\nsteps_per_epoch 是指在一个 epoch 中，模型需要训练的批次数。由于每个 epoch 中包含多个批次，因此需要设置 steps_per_epoch 来指定一个 epoch 中需要经过多少个批次。通常，steps_per_epoch 的值可以通过训练集大小和 batch_size 计算得到。例如，如果训练集大小为 1000，batch_size 为 32，那么一个 epoch 中就需要训练 1000 / 32 = 31 个批次，因此 steps_per_epoch 就应该设置为 31。\n需要注意的是，steps_per_epoch 不一定等于训练集大小除以 batch_size 的结果。如果训练集大小不能被 batch_size 整除，那么最后一个批次中可能会包含少于 batch_size 个样本。为了避免这种情况，可以使用向下取整操作 // 来计算 steps_per_epoch，确保每个 epoch 中都能够处理完整个训练集。\nfine-tuning Fine-tuning是指在已经训练好的模型上，针对特定任务或特定数据集进行微调，以达到更好的性能表现的方法。通常，我们会使用一个在大规模数据集上预训练好的模型，例如ImageNet等数据集，这个模型在训练过程中已经学到了很多通用的特征和模式。我们可以通过在这个模型的基础上进行微调，调整一些参数或者增加一些新的层，使得这个模型更适合新的任务或新的数据集。这种方法通常比从头开始训练一个模型更加高效，因为预训练模型已经具有很好的初始权重和特征提取能力。\nmnist-c数据集 MNIST-C是MNIST数据集的一个变体，它是加入了人工噪声的MNIST数据集。MNIST数据集是一个手写数字识别数据集，包含60,000个训练样本和10,000个测试样本，每个样本都是一个28 x 28像素的灰度图像。MNIST-C数据集是通过在MNIST数据集的图像上添加随机噪声来创建的，这些噪声包括模糊、扭曲、亮度变化等，从而使模型更有鲁棒性。\nMNIST-C数据集对于测试机器学习模型的稳健性非常有用，因为它可以测试模型对于不同类型的噪声的鲁棒性。MNIST-C数据集中的每个图像都包含一个标签，表示它所代表的数字。这些标签与MNIST数据集中的相应标签相同，因此您可以使用相同的训练和测试流程来训练和测试您的模型。\n下载该数据集，https://github.com/google-research/mnist-c/ ，这个github地址是源码地址，实际下载地址在readme中提及：https://zenodo.org/record/3239543#.ZF2rzXZByUl，下载后解压 这些文件夹里都是npy格式的numpy数组导出。 读取每个文件夹的前10张图片显示\n# 数据集的开源地址：https://github.com/google-research/mnist-c/\rimport os\rimport numpy as np\rimport matplotlib.pyplot as plt\r#加载数据集并打印每个子文件夹前10个数据集\rdata_root = './mnist_c'\rdirlist=os.listdir(data_root)\rfig, axs = plt.subplots(len(dirlist), 10, figsize=(10, 10))\rfor i, folder_name in enumerate(dirlist):\rfolder_path = os.path.join(data_root, folder_name)\rif os.path.isdir(folder_path):\rfile_path = os.path.join(folder_path, 'train_images.npy')\rdata = np.load(file_path)\rfor j in range(0,10):\raxs[i, j].imshow(data[j].reshape(28,28), cmap='gray')\raxs[i, j].axis('off')\rplt.tight_layout()\rplt.show() 输出 fine-tuning方法训练 假设我们开始试用试用minist的训练的模型位于./model.h5,我们需要加载该模型，然后试用该模型继续训练minist-c的数据。\n#%%\rimport os\rimport numpy as np\rimport tensorflow.keras as layers\rimport tensorflow as tf\rimport datetime\rTARGET_MODEL_DIR=\"./\"\rMODEL_NAME=\"model.h5\"\repochs_count=5\r\"\"\"\rjupyter打印的日志太大导致ipynb打开很慢，这里写个一模一样代码的py运行\r\"\"\"\rdef againTrain(x_train, y_train, x_test, y_test):\rtargetModel=os.path.join(TARGET_MODEL_DIR,MODEL_NAME)\r#记载CNN模型\rmodel=tf.keras.models.load_model(targetModel)\r\"\"\"\r在使用Fine-tuning方法微调预训练模型时，通常会冻结模型的前几层，只调整模型的后面几层，这是因为：\r1.预训练模型的前几层通常是针对原始数据集的通用特征提取器，这些特征对于不同的任务和数据集都是有用的，因此我们可以直接保留这些特征提取器，不需要进行微调。\r2.预训练模型的后几层通常是针对特定任务进行的微调，这些层的参数需要根据具体任务和数据集进行调整，以使模型更好地适应特定的任务和数据集。\r3.如果我们将整个模型的所有层都进行微调，会导致训练时间较长，而且可能会出现过拟合等问题。因此，冻结前几层可以有效地减少训练时间，并提高模型的泛化能力。\r总之，冻结模型的前几层可以节省计算资源和训练时间，同时还可以提高模型的泛化能力，使其更好地适应新的任务和数据集。\r\"\"\"\rmodel.layers[0].trainable = False\rmodel.layers[1].trainable = False\r# 对输入图像进行预处理\rx_train = x_train.reshape(-1, 28, 28, 1)\rx_train = x_train.astype('float32') / 255.0\rx_test = x_test.reshape(-1, 28, 28, 1)\rx_test = x_test.astype('float32') / 255.0\ry_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\ry_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\rnow = datetime.datetime.now() # 获取当前时间\rformat_time = now.strftime(\"%Y-%m-%d%H-%M-%S\") # 转换为指定格式\rcheckpoint = tf.keras.callbacks.ModelCheckpoint(targetModel, save_best_only=True, save_weights_only=False, monitor='val_loss')\r# 继续训练模型\rhistory = model.fit(x_train, y_train, batch_size=128, epochs=epochs_count, validation_data=(x_test, y_test),\rcallbacks=[checkpoint])\rtest_loss, test_acc = model.evaluate(x_test, y_test)\rprint('Test accuracy:', test_acc)\r\"\"\"\r传入mnist-c，数据会非常大加载数据很慢，这里每加载一份子目录就训练一次，节省内存开销。\r\"\"\"\rdef loadDataMnistC(data_root,func):\rdirlist=os.listdir(data_root)\rfor i, folder_name in enumerate(dirlist):\rfolder_path = os.path.join(data_root, folder_name)\rif os.path.isdir(folder_path):\rprint(\"开始读取：\"+folder_path)\rtrain_images = np.load(os.path.join(folder_path, 'train_images.npy'))\rtrain_labels = np.load(os.path.join(folder_path, 'train_labels.npy'))\rtest_images = np.load(os.path.join(folder_path, 'test_images.npy'))\rtest_labels = np.load(os.path.join(folder_path, 'test_labels.npy'))\rprint(\"开始训练：\"+folder_path)\rfunc(train_images,train_labels,test_images,test_labels)\rprint(\"训练完成：\"+folder_path)\r# 加载 MNIST-C 数据集\rdata_root = './mnist_c'\rmodel=None;\rloadDataMnistC(data_root,againTrain)\rprint(\"全部训练完成\") 这里每次读取一次某型，然后试用子文件夹训练又会写回到该模型，知道训练完成获取到最终的模型",
    "description": "简介 CNN，即卷积神经网络（Convolutional Neural Network），是一种常用于图像和视频处理的深度学习模型。与传统神经网络相比，CNN 有着更好的处理图像和序列数据的能力，因为它能够自动学习图像中的特征，并提取出最有用的信息。\nCNN 的一个核心特点是卷积操作，它可以在图像上进行滑动窗口的计算，通过滤波器（又称卷积核）和池化层（Max Pooling）来提取出图像的特征。卷积操作可以有效地减少权重数量，降低计算量，同时也能够保留图像的空间结构信息。池化层则可以在不改变特征图维度的前提下，减少计算量，提高模型的鲁棒性。\nCNN 的典型结构包括卷积层、池化层、全连接层等。同时，为了防止过拟合，CNN 还会加入一些正则化的技术，如 Dropout 和 L2 正则等。\nCNN 在图像分类、目标检测、语音识别等领域都有着广泛的应用。在图像分类任务中，CNN 的经典模型包括 LeNet-5、AlexNet、VGG 和 GoogleNet/Inception 等，这些模型的设计思想和网络结构都有所不同，但都对卷积神经网络的发展做出了重要贡献。\n发展历程 卷积神经网络（CNN）是一种深度学习模型，广泛应用于图像识别、计算机视觉等领域。在CNN的发展历程中，涌现出了许多经典的模型，下面简要介绍几个著名的模型。\nLeNet-5 LeNet-5是Yann LeCun等人于1998年提出的，是第一个被广泛应用的卷积神经网络模型。它主要用于手写数字识别，包含卷积层、池化层和全连接层。LeNet-5的设计使得它在MNIST手写数字识别任务上获得了很好的表现。它的特点是卷积核数量较少（6和16）以及参数量较少，第一层卷积层使用了6个大小为5×5的卷积核，第二层卷积层使用了16个大小为5×5的卷积核。这种设计可以有效地减少模型的参数量，但它是卷积神经网络的开山鼻祖，为后续模型奠定了基础。\nAlexNet AlexNet由Alex Krizhevsky等人于2012年提出，是第一个在ImageNet图像分类比赛中取得优异成绩的卷积神经网络模型。它采用了多个卷积层和池化层，使用了ReLU激活函数和Dropout正则化技术。AlexNet的设计使得它在ImageNet图像分类比赛中大幅领先于其他模型，从而引领了卷积神经网络的新一轮发展。它的特点是使用了大量卷积核（近6000个）、参数量较大，但在准确率和效率上都有很好的表现。\nVGG VGG由Karen Simonyan和Andrew Zisserman于2014年提出，其主要贡献是提出了使用更小的卷积核（3x3）来代替较大的卷积核。这种设计使得网络更深，而且参数量更少，从而提高了效率和准确率。VGG包含了16个或19个卷积层和池化层，这些层都采用了相同的卷积核大小和步长。VGG在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ResNet等模型提供了启示。\nGoogleNet/Inception GoogleNet由Google团队于2014年提出，其主要贡献是提出了Inception模块，可以在不增加参数量的情况下增加网络的深度和宽度。Inception模块采用了多个不同大小的卷积核和池化层来进行特征提取，然后将它们串联在一起，形成了一个模块。GoogleNet还使用了全局平均池化层来代替全连接层，从而进一步减少了参数量。GoogleNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ResNet、DenseNet等模型提供了启示。\nResNet ResNet由Microsoft Research Asia团队于2015年提出，其主要贡献是提出了残差学习，可以解决深度卷积神经网络的退化问题。退化问题指的是随着网络深度的增加，准确率反而下降的现象。残差学习通过引入跨层连接来将输入直接传递到输出，从而避免了信息的损失。ResNet包含了较深的网络结构（152层），但却获得了更好的准确率。ResNet的设计思想被后续的DenseNet、MobileNet等模型所继承。\nDenseNet DenseNet由Gao Huang等人于2017年提出，其主要贡献是提出了密集连接，可以增加网络的深度和宽度，从而提高了效率和准确率。密集连接指的是将每个层的输出都与后面所有层的输入相连，形成了一个密集的连接结构。这种设计使得网络更加紧凑，参数量更少，同时也可以提高特征的复用性。DenseNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ShuffleNet、EfficientNet等模型提供了启示。\nMobileNet MobileNet由Google团队于2017年提出，其主要贡献是提出了深度可分离卷积，可以在减少参数量的同时保持较好的准确率。深度可分离卷积指的是将卷积操作分为深度卷积和逐点卷积两步，从而减少了计算量和参数量。MobileNet采用了多个深度可分离卷积层和池化层，可以在移动设备等资源受限的环境下实现高效的图像分类和目标检测。MobileNet的设计思想被后续的ShuffleNet、EfficientNet等模型所继承。\nShuffleNet ShuffleNet由Microsoft Research Asia团队于2018年提出，其主要贡献是提出了通道重组和组卷积，可以在保持准确率的前提下大幅减少参数量和计算量。通道重组指的是将输入的通道分组并重新组合，从而让不同的组之间进行信息的交流。组卷积指的是将卷积操作分为组内卷积和组间卷积两步，从而减少了计算量和参数量。ShuffleNet采用了多个通道重组和组卷积层，可以在资源受限的环境下实现高效的图像分类和目标检测。\nEfficientNet EfficientNet由Google团队于2019年提出，其主要贡献是提出了网络缩放和复合系数，可以在保持准确率的前提下大幅减少参数量和计算量。网络缩放指的是同时缩放网络的深度、宽度和分辨率，从而在不改变模型结构的情况下进行优化。复合系数指的是将深度、宽度和分辨率的缩放系数进行组合，从而得到一个更加高效的模型。EfficientNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的模型优化提供了启示。\nRegNet RegNet由Facebook AI Research团队于2020年提出，其主要贡献是提出了网络结构的自适应规则，可以在保持准确率的前提下大幅减少参数量和计算量。自适应规则指的是通过搜索和优化来自动调整网络结构的超参数，从而得到一个更加高效的模型。RegNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的模型优化提供了启示。\n以上是几个著名的卷积神经网络模型，它们的设计思想和网络结构都有所不同，但都对卷积神经网络的发展做出了重要贡献。\n图解原理 卷积神经网络在图像识别中大放异彩，达到了前所未有的准确度，有着广泛的应用。接下来将以图像识别为例子，来介绍卷积神经网络的原理。\n案例 假设给定一张图（可能是字母X或者字母O），通过CNN即可识别出是X还是O，如下图所示，那怎么做到的呢 图像输入 如果采用经典的神经网络模型，则需要读取整幅图像作为神经网络模型的输入（即全连接的方式），当图像的尺寸越大时，其连接的参数将变得很多，从而导致计算量非常大。 而我们人类对外界的认知一般是从局部到全局，先对局部有感知的认识，再逐步对全体有认知，这是人类的认识模式。在图像中的空间联系也是类似，局部范围内的像素之间联系较为紧密，而距离较远的像素则相关性较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。这种模式就是卷积神经网络中降低参数数目的重要神器：局部感受野。",
    "tags": [],
    "title": "深度学习03-卷积神经网络(CNN)",
    "uri": "/docs/programming/ai/deep_learning/cnn/dl_03_cnn/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 卷积神经网络",
    "content": "简介 卷积神经网络（CNN）是深度学习中非常重要的一种网络结构，它可以处理图像、文本、语音等各种类型的数据。以下是CNN的前4个经典模型\nLeNet-5 LeNet-5是由Yann LeCun等人于1998年提出的，是第一个成功应用于手写数字识别的卷积神经网络。它由7层神经网络组成，包括2层卷积层、2层池化层和3层全连接层。其中，卷积层提取图像特征，池化层降低特征图的维度，全连接层将特征映射到对应的类别上。\nLeNet-5的主要特点是使用Sigmoid激活函数、平均池化和卷积层后没有使用零填充。它在手写数字识别、人脸识别等领域都有着广泛的应用。\nAlexNet AlexNet是由Alex Krizhevsky等人于2012年提出的，是第一个在大规模图像识别任务中取得显著成果的卷积神经网络。它由5层卷积层、3层全连接层和1层Softmax输出层组成，其中使用了ReLU激活函数、最大池化和Dropout技术。\nAlexNet的主要特点是使用了GPU加速训练、数据增强和随机化Dropout等技术，使得模型的泛化能力和鲁棒性得到了大幅提升。它在ImageNet大规模图像识别比赛中取得了远超其他模型的优异成绩。\nVGGNet VGGNet是由Karen Simonyan和Andrew Zisserman于2014年提出的，它是一个非常深的卷积神经网络，有16层或19层。VGGNet的每个卷积层都使用了3x3的卷积核和ReLU激活函数，使得它的网络结构非常清晰、易于理解。\nVGGNet的主要特点是使用了更深的网络结构、小卷积核和少量的参数，使得模型的特征提取能力得到了进一步提升。它在ImageNet比赛中也获得了非常好的成绩。\nGoogLeNet GoogLeNet是由Google团队于2014年提出的，它是一个非常深的卷积神经网络，有22层。它使用了一种称为Inception模块的结构，可以在保持网络深度的同时减少参数量。\nGoogLeNet的主要特点是使用了Inception模块、1x1卷积核和全局平均池化等技术，使得模型的计算复杂度得到了大幅降低。它在ImageNet比赛中获得了非常好的成绩，并且被广泛应用于其他领域。\nCNN回顾 回顾一下 CNN 的几个特点：局部感知、参数共享、池化。\n局部感知 人类对外界的认知一般是从局部到全局、从片面到全面，类似的，在机器识别图像时也没有必要把整张图像按像素全部都连接到神经网络中，在图像中也是局部周边的像素联系比较紧密，而距离较远的像素则相关性较弱，因此可以采用局部连接的模式（将图像分块连接，这样能大大减少模型的参数），如下图所示： 参数（权值）共享 每张自然图像（人物、山水、建筑等）都有其固有特性，也就是说，图像其中一部分的统计特性与其它部分是接近的。这也意味着这一部分学习的特征也能用在另一部分上，能使用同样的学习特征。因此，在局部连接中隐藏层的每一个神经元连接的局部图像的权值参数（例如 5×5），将这些权值参数共享给其它剩下的神经元使用，那么此时不管隐藏层有多少个神经元，需要训练的参数就是这个局部图像的权限参数（例如 5×5），也就是卷积核的大小，这样大大减少了训练参数。如下图 卷积核的权值是指每个卷积核中的参数，用于对输入数据进行卷积操作时，对每个位置的像素进行加权求和。在卷积神经网络中，同一层中的所有卷积核的权值是共享的，这意味着每个卷积核在不同位置上的权值是相同的。共享权值可以减少模型中需要学习的参数数量，从而降低了模型的复杂度，同时可以提高模型的泛化能力，因为共享权值可以使模型更加稳定，避免过度拟合。共享权值的实现方式是通过使用相同的卷积核对输入数据进行卷积操作。 池化 随着模型网络不断加深，卷积核越来越多，要训练的参数还是很多，而且直接拿卷积核提取的特征直接训练也容易出现过拟合的现象。回想一下，之所以对图像使用卷积提取特征是因为图像具有一种 “静态性” 的属性，因此，一个很自然的想法就是对不同位置区域提取出有代表性的特征（进行聚合统计，例如最大值、平均值等），这种聚合的操作就叫做池化，池化的过程通常也被称为特征映射的过程（特征降维），如下图：\nLeNet-5 概述 LeNet5 诞生于 1994 年，是最早的卷积神经网络之一， 由 Yann LeCun 完成，推动了深度学习领域的发展。在那时候，没有 GPU 帮助训练模型，甚至 CPU 的速度也很慢，因此，LeNet5 通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络也是最近大量神经网络架构的起点，给这个领域带来了许多灵感。 LeNet5 的网络结构示意图如下所示： LeNet5 由 7 层 CNN（不包含输入层）组成，上图中输入的原始图像大小是 32×32 像素，卷积层用 Ci 表示，子采样层（pooling，池化）用 Si 表示，全连接层用 Fi 表示。下面逐层介绍其作用和示意图上方的数字含义。\nC1 层（卷积层）：6@28×28 该层使用了 6 个卷积核，每个卷积核的大小为 5×5，这样就得到了 6 个 feature map（特征图）。 （1）特征图大小 每个卷积核（5×5）与原始的输入图像（32×32）进行卷积，这样得到的 feature map（特征图）大小为（32-5+1）×（32-5+1）= 28×28 卷积过程如下图所示（下图是4*4只是用于演示）： 卷积核与输入图像按卷积核大小逐个区域进行匹配计算，匹配后原始输入图像的尺寸将变小，因为边缘部分卷积核无法越出界，只能匹配一次，如上图，匹配计算后的尺寸变为 Cr×Cc=（Ir-Kr+1）×（Ic-Kc+1），其中 Cr、Cc，Ir、Ic，Kr、Kc 分别表示卷积后结果图像、输入图像、卷积核的行列大小。 其中Cr表示结果行row，Cc表示结果列column （2）参数个数 由于参数（权值）共享的原因，对于同个卷积核每个神经元均使用相同的参数，因此，参数个数为（5×5+1）×6= 156，其中 5×5 为卷积核参数，1 为偏置参数 （3）连接数 卷积后的图像大小为 28×28，因此每个特征图有 28×28 个神经元，每个卷积核参数为（5×5+1）×6，因此，该层的连接数为（5×5+1）×6×28×28=122304\nS2 层（下采样层，也称池化层）：6@14×14 （1）特征图大小 这一层主要是做池化或者特征映射（特征降维），池化单元为 2×2，因此，6 个特征图的大小经池化后即变为 14×14。回顾本文刚开始讲到的池化操作，池化单元之间没有重叠，在池化区域内进行聚合统计后得到新的特征值，因此经 2×2 池化后，每两行两列重新算出一个特征值出来，相当于图像大小减半，因此卷积后的 28×28 图像经 2×2 池化后就变为 14×14。 这一层的计算过程是：2×2 单元里的值相加，然后再乘以训练参数 w，再加上一个偏置参数 b（每一个特征图共享相同的 w 和 b)，然后取 sigmoid 值（S 函数：0-1 区间），作为对应的该单元的值。卷积操作与池化的示意图如下： （2）参数个数 S2 层由于每个特征图都共享相同的 w 和 b 这两个参数，因此需要 2×6=12 个参数 （3）连接数 下采样之后的图像大小为 14×14，因此 S2 层的每个特征图有 14×14 个神经元，每个池化单元连接数为 2×2+1（1 为偏置量），因此，该层的连接数为（2×2+1）×14×14×6 = 5880\nC3 层（卷积层）：16@10×10 C3 层有 16 个卷积核，卷积模板大小为 5×5。 （1）特征图大小 与 C1 层的分析类似，C3 层的特征图大小为（14-5+1）×（14-5+1）= 10×10 （2）参数个数 需要注意的是，C3 与 S2 并不是全连接而是部分连接，有些是 C3 连接到 S2 三层、有些四层、甚至达到 6 层，通过这种方式提取更多特征，连接的规则如下表所示： 例如第一列表示 C3 层的第 0 个特征图（feature map）只跟 S2 层的第 0、1 和 2 这三个 feature maps 相连接，计算过程为：用 3 个卷积模板分别与 S2 层的 3 个 feature maps 进行卷积，然后将卷积的结果相加求和，再加上一个偏置，再取 sigmoid 得出卷积后对应的 feature map 了。其它列也是类似（有些是 3 个卷积模板，有些是 4 个，有些是 6 个）。因此，C3 层的参数数目为（5×5×3+1）×6 +（5×5×4+1）×9 +5×5×6+1 = 1516\n（3）连接数 卷积后的特征图大小为 10×10，参数数量为 1516，因此连接数为 1516×10×10= 151600\nS4（下采样层，也称池化层）：16@5×5 （1）特征图大小 与 S2 的分析类似，池化单元大小为 2×2，因此，该层与 C3 一样共有 16 个特征图，每个特征图的大小为 5×5。 （2）参数数量 与 S2 的计算类似，所需要参数个数为 16×2 = 32 （3）连接数 连接数为（2×2+1）×5×5×16 = 2000\nC5 层（卷积层）：120 （1）特征图大小 该层有 120 个卷积核，每个卷积核的大小仍为 5×5，因此有 120 个特征图。由于 S4 层的大小为 5×5，而该层的卷积核大小也是 5×5，因此特征图大小为（5-5+1）×（5-5+1）= 1×1。这样该层就刚好变成了全连接，这只是巧合，如果原始输入的图像比较大，则该层就不是全连接了。 （2）参数个数 与前面的分析类似，本层的参数数目为 120×（5×5×16+1） = 48120 （3）连接数 由于该层的特征图大小刚好为 1×1，因此连接数为 48120×1×1=48120\nF6 层（全连接层）：84 1）特征图大小 F6 层有 84 个单元，之所以选这个数字的原因是来自于输出层的设计，对应于一个 7×12 的比特图，如下图所示，-1 表示白色，1 表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。 该层有 84 个特征图，特征图大小与 C5 一样都是 1×1，与 C5 层全连接。 （2）参数个数 由于是全连接，参数数量为（120+1）×84=10164。跟经典神经网络一样，F6 层计算输入向量和权重向量之间的点积，再加上一个偏置，然后将其传递给 sigmoid 函数得出结果。 （3）连接数 由于是全连接，连接数与参数数量一样，也是 10164。\nOUTPUT 层（输出层）：10 Output 层也是全连接层，共有 10 个节点，分别代表数字 0 到 9。如果第 i 个节点的值为 0，则表示网络识别的结果是数字 i。 （1）特征图大小 该层采用径向基函数（RBF）的网络连接方式，假设 x 是上一层的输入，y 是 RBF 的输出，则 RBF 输出的计算方式是： 上式中的 Wij 的值由 i 的比特图编码确定，i 从 0 到 9，j 取值从 0 到 7×12-1。RBF 输出的值越接近于 0，表示当前网络输入的识别结果与字符 i 越接近。\n（2）参数个数 由于是全连接，参数个数为 84×10=840 （3）连接数 由于是全连接，连接数与参数个数一样，也是 840\n通过以上介绍，已经了解了 LeNet 各层网络的结构、特征图大小、参数数量、连接数量等信息，下图是识别数字 3 的过程，可对照上面介绍各个层的功能进行一一回顾： 编程实现 import tensorflow as tf\rfrom tensorflow.keras import layers, models\rfrom tensorflow.keras.datasets import mnist\rimport matplotlib.pyplot as plt\rimport numpy as np\r#开启tensorflow支持numpy函数，astype是numpy的函数\rfrom tensorflow.python.ops.numpy_ops import np_config\rnp_config.enable_numpy_behavior()\r# 加载MNIST数据集\r(x_train, y_train), (x_test, y_test) = mnist.load_data()\rori_x_test1=x_test\r# 将图像从28*28转换成32*32\rx_train = tf.pad(x_train, [[0,0], [2,2], [2,2]], mode='constant')\rx_test = tf.pad(x_test, [[0,0], [2,2], [2,2]], mode='constant')\r# 将像素值缩放到0-1之间\rx_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\r# 定义Lenet-5模型\rmodel = models.Sequential([\r# 第一层卷积层，6个卷积核，大小为5*5，使用sigmoid激活函数\rlayers.Conv2D(6, (5, 5), activation='relu', input_shape=(32, 32, 1)),\r# 第一层池化层，大小为2*2\rlayers.MaxPooling2D((2, 2)),\r# 第二层卷积层，16个卷积核，大小为5*5，使用sigmoid激活函数\rlayers.Conv2D(16, (5, 5), activation='relu'),\r# 第二层池化层，大小为2*2\rlayers.MaxPooling2D((2, 2)),\r# 第三层卷积层，120个卷积核，大小为5*5，使用sigmoid激活函数\rlayers.Conv2D(120, (5, 5), activation='relu'),\r# 将卷积层的输出拉平\rlayers.Flatten(),\r# 第一层全连接层，84个节点，使用sigmoid激活函数\rlayers.Dense(84, activation='relu'),\r# 输出层，共10个节点，对应0-9十个数字，使用softmax激活函数\rlayers.Dense(10, activation='softmax')\r])\r# 编译模型\rmodel.compile(optimizer='adam',\rloss='sparse_categorical_crossentropy',\rmetrics=['accuracy'])\r# 训练模型\rmodel.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\rscore = model.evaluate(x_test, y_test, verbose=0)\rprint('Test loss:', score[0])\rprint('Test accuracy:', score[1])\r#取出其中一个测试数据进行测试\rtestdata = ori_x_test1[100]\rtestdata = testdata.reshape(-1,28,28)\rtestdata = tf.pad(testdata, [[0,0], [2,2], [2,2]], mode='constant')\rtestdata=testdata.reshape(-1, 32, 32, 1)\r# 将像素值缩放到0-1之间\rtestdata = testdata.astype('float32') / 255.0\rpredictions = model.predict(testdata)\rprint(\"预测结果：\", np.argmax(predictions))\r# 绘制第10个测试数据的图形\rplt.imshow(ori_x_test1[100], cmap=plt.cm.binary)\rplt.show() 输出： Test loss: 0.03826029598712921 Test accuracy: 0.9879999756813049 预测结果： 6 参考:https://my.oschina.net/u/876354/blog/1632862\nAlexNet 2012 年，Alex Krizhevsky、Ilya Sutskever 在多伦多大学 Geoff Hinton 的实验室设计出了一个深层的卷积神经网络 AlexNet，夺得了 2012 年 ImageNet LSVRC 的冠军，且准确率远超第二名（top5 错误率为 15.3%，第二名为 26.2%），引起了很大的轰动。AlexNet 可以说是具有历史意义的一个网络结构，在此之前，深度学习已经沉寂了很长时间，自 2012 年 AlexNet 诞生之后，后面的 ImageNet 冠军都是用卷积神经网络（CNN）来做的，并且层次越来越深，使得 CNN 成为在图像识别分类的核心算法模型，带来了深度学习的大爆发。 在本博客之前的文章中已经介绍过了卷积神经网络（CNN）的技术原理（大话卷积神经网络），也回顾过卷积神经网络（CNN）的三个重要特点（大话 CNN 经典模型：LeNet），有兴趣的同学可以打开链接重新回顾一下，在此就不再重复 CNN 基础知识的介绍了。下面将先介绍 AlexNet 的特点，然后再逐层分解解析 AlexNet 网络结构。\nAlexNet 模型的特点 AlexNet 之所以能够成功，跟这个模型设计的特点有关，主要有：\n使用了非线性激活函数：ReLU 防止过拟合的方法：Dropout，数据扩充（Data augmentation） 其他：多 GPU 实现，LRN 归一化层的使用 1、使用 ReLU 激活函数 传统的神经网络普遍使用 Sigmoid 或者 tanh 等非线性函数作为激励函数，然而它们容易出现梯度弥散或梯度饱和的情况。以 Sigmoid 函数为例，当输入的值非常大或者非常小的时候，这些神经元的梯度接近于 0（梯度饱和现象），如果输入的初始值很大的话，梯度在反向传播时因为需要乘上一个 Sigmoid 导数，会造成梯度越来越小，导致网络变的很难学习。（详见本公博客的文章：深度学习中常用的激励函数）。 在 AlexNet 中，使用了 ReLU （Rectified Linear Units）激励函数，该函数的公式为：f (x)=max (0,x)，当输入信号 \u003c 0 时，输出都是 0，当输入信号 \u003e 0 时，输出等于输入，如下图所示：\n使用 ReLU 替代 Sigmoid/tanh，由于 ReLU 是线性的，且导数始终为 1，计算量大大减少，收敛速度会比 Sigmoid/tanh 快很多，如下图所示： 2、数据扩充（Data augmentation）\n有一种观点认为神经网络是靠数据喂出来的，如果能够增加训练数据，提供海量数据进行训练，则能够有效提升算法的准确率，因为这样可以避免过拟合，从而可以进一步增大、加深网络结构。而当训练数据有限时，可以通过一些变换从已有的训练数据集中生成一些新的数据，以快速地扩充训练数据。 其中，最简单、通用的图像数据变形的方式：水平翻转图像，从原始图像中随机裁剪、平移变换，颜色、光照变换，如下图所示： AlexNet 在训练时，在数据扩充（data augmentation）这样处理： （1）随机裁剪，对 256×256 的图片进行随机裁剪到 224×224，然后进行水平翻转，相当于将样本数量增加了（（256-224）^2）×2=2048 倍； （2）测试的时候，对左上、右上、左下、右下、中间分别做了 5 次裁剪，然后翻转，共 10 个裁剪，之后对结果求平均。作者说，如果不做随机裁剪，大网络基本上都过拟合； （3）对 RGB 空间做 PCA（主成分分析），然后对主成分做一个（0, 0.1）的高斯扰动，也就是对颜色、光照作变换，结果使错误率又下降了 1%。\n3、重叠池化 (Overlapping Pooling) 一般的池化（Pooling）是不重叠的，池化区域的窗口大小与步长相同，如下图所示： 在 AlexNet 中使用的池化（Pooling）却是可重叠的，也就是说，在池化的时候，每次移动的步长小于池化的窗口长度。AlexNet 池化的大小为 3×3 的正方形，每次池化移动步长为 2，这样就会出现重叠。重叠池化可以避免过拟合，这个策略贡献了 0.3% 的 Top-5 错误率。 4、局部归一化（Local Response Normalization，简称 LRN） 在神经生物学有一个概念叫做 “侧抑制”（lateral inhibitio），指的是被激活的神经元抑制相邻神经元。归一化（normalization）的目的是 “抑制”，局部归一化就是借鉴了 “侧抑制” 的思想来实现局部抑制，尤其当使用 ReLU 时这种 “侧抑制” 很管用，因为 ReLU 的响应结果是无界的（可以非常大），所以需要归一化。使用局部归一化的方案有助于增加泛化能力。 LRN 的公式如下，核心思想就是利用临近的数据做归一化，这个策略贡献了 1.2% 的 Top-5 错误率。 5、Dropout 引入 Dropout 主要是为了防止过拟合。在神经网络中 Dropout 通过修改神经网络本身结构来实现，对于某一层的神经元，通过定义的概率将神经元置为 0，这个神经元就不参与前向和后向传播，就如同在网络中被删除了一样，同时保持输入层与输出层神经元的个数不变，然后按照神经网络的学习方法进行参数更新。在下一次迭代中，又重新随机删除一些神经元（置为 0），直至训练结束。 Dropout 应该算是 AlexNet 中一个很大的创新，以至于 “神经网络之父” Hinton 在后来很长一段时间里的演讲中都拿 Dropout 说事。Dropout 也可以看成是一种模型组合，每次生成的网络结构都不一样，通过组合多个模型的方式能够有效地减少过拟合，Dropout 只需要两倍的训练时间即可实现模型组合（类似取平均）的效果，非常高效。 如下图所示： 6、多 GPU 训练 AlexNet 当时使用了 GTX580 的 GPU 进行训练，由于单个 GTX 580 GPU 只有 3GB 内存，这限制了在其上训练的网络的最大规模，因此他们在每个 GPU 中放置一半核（或神经元），将网络分布在两个 GPU 上进行并行计算，大大加快了 AlexNet 的训练速度。\nAlexNet 网络结构的逐层解析 下图是 AlexNet 的网络结构图： AlexNet 网络结构共有 8 层，前面 5 层是卷积层，后面 3 层是全连接层，最后一个全连接层的输出传递给一个 1000 路的 softmax 层，对应 1000 个类标签的分布。 由于 AlexNet 采用了两个 GPU 进行训练，因此，该网络结构图由上下两部分组成，一个 GPU 运行图上方的层，另一个运行图下方的层，两个 GPU 只在特定的层通信。例如第二、四、五层卷积层的核只和同一个 GPU 上的前一层的核特征图相连，第三层卷积层和第二层所有的核特征图相连接，全连接层中的神经元和前一层中的所有神经元相连接。\n下面逐层解析 AlexNet 结构：\n第一层（卷积层） 该层的处理流程为：卷积 –\u003eReLU–\u003e 池化 –\u003e 归一化，流程图如下： （1）卷积 输入的原始图像大小为 224×224×3（RGB 图像），在训练时会经过预处理变为 227×227×3。在本层使用 96 个 11×11×3 的卷积核进行卷积计算，生成新的像素。由于采用了两个 GPU 并行运算，因此，网络结构图中上下两部分分别承担了 48 个卷积核的运算。 卷积核沿图像按一定的步长往 x 轴方向、y 轴方向移动计算卷积，然后生成新的特征图，其大小为：floor ((img_size - filter_size)/stride) +1 = new_feture_size，其中 floor 表示向下取整，img_size 为图像大小，filter_size 为核大小，stride 为步长，new_feture_size 为卷积后的特征图大小，这个公式表示图像尺寸减去卷积核尺寸除以步长，再加上被减去的核大小像素对应生成的一个像素，结果就是卷积后特征图的大小。 AlexNet 中本层的卷积移动步长是 4 个像素，卷积核经移动计算后生成的特征图大小为 (227-11)/4+1=55，即 55×55。 （2）ReLU 卷积后的 55×55 像素层经过 ReLU 单元的处理，生成激活像素层，尺寸仍为 2 组 55×55×48 的像素层数据。 （3）池化 RuLU 后的像素层再经过池化运算，池化运算的尺寸为 3×3，步长为 2，则池化后图像的尺寸为 (55-3)/2+1=27，即池化后像素的规模为 27×27×96 （4）归一化 池化后的像素层再进行归一化处理，归一化运算的尺寸为 5×5，归一化后的像素规模不变，仍为 27×27×96，这 96 层像素层被分为两组，每组 48 个像素层，分别在一个独立的 GPU 上进行运算。\n第二层（卷积层） 该层与第一层类似，处理流程为：卷积 –\u003eReLU–\u003e 池化 –\u003e 归一化，流程图如下： （1）卷积 第二层的输入数据为第一层输出的 27×27×96 的像素层（被分成两组 27×27×48 的像素层放在两个不同 GPU 中进行运算），为方便后续处理，在这里每幅像素层的上下左右边缘都被填充了 2 个像素（填充 0），即图像的大小变为 (27+2+2) ×(27+2+2)。第二层的卷积核大小为 5×5，移动步长为 1 个像素，跟第一层第（1）点的计算公式一样，经卷积核计算后的像素层大小变为 (27+2+2-5)/1+1=27，即卷积后大小为 27×27。 本层使用了 256 个 5×5×48 的卷积核，同样也是被分成两组，每组为 128 个，分给两个 GPU 进行卷积运算，结果生成两组 27×27×128 个卷积后的像素层。 （2）ReLU 这些像素层经过 ReLU 单元的处理，生成激活像素层，尺寸仍为两组 27×27×128 的像素层。 （3）池化 再经过池化运算的处理，池化运算的尺寸为 3×3，步长为 2，池化后图像的尺寸为 (57-3)/2+1=13，即池化后像素的规模为 2 组 13×13×128 的像素层 （4）归一化 然后再经归一化处理，归一化运算的尺度为 5×5，归一化后的像素层的规模为 2 组 13×13×128 的像素层，分别由 2 个 GPU 进行运算。\n第三层（卷积层） 第三层的处理流程为：卷积 –\u003eReLU （1）卷积 第三层输入数据为第二层输出的 2 组 13×13×128 的像素层，为便于后续处理，每幅像素层的上下左右边缘都填充 1 个像素，填充后变为 (13+1+1)×(13+1+1)×128，分布在两个 GPU 中进行运算。 这一层中每个 GPU 都有 192 个卷积核，每个卷积核的尺寸是 3×3×256。因此，每个 GPU 中的卷积核都能对 2 组 13×13×128 的像素层的所有数据进行卷积运算。如该层的结构图所示，两个 GPU 有通过交叉的虚线连接，也就是说每个 GPU 要处理来自前一层的所有 GPU 的输入。 本层卷积的步长是 1 个像素，经过卷积运算后的尺寸为 (13+1+1-3)/1+1=13，即每个 GPU 中共 13×13×192 个卷积核，2 个 GPU 中共有 13×13×384 个卷积后的像素层。 （2）ReLU 卷积后的像素层经过 ReLU 单元的处理，生成激活像素层，尺寸仍为 2 组 13×13×192 的像素层，分配给两组 GPU 处理。\n第四层（卷积层） 与第三层类似，第四层的处理流程为：卷积 –\u003eReLU 1）卷积 第四层输入数据为第三层输出的 2 组 13×13×192 的像素层，类似于第三层，为便于后续处理，每幅像素层的上下左右边缘都填充 1 个像素，填充后的尺寸变为 (13+1+1)×(13+1+1)×192，分布在两个 GPU 中进行运算。 这一层中每个 GPU 都有 192 个卷积核，每个卷积核的尺寸是 3×3×192（与第三层不同，第四层的 GPU 之间没有虚线连接，也即 GPU 之间没有通信）。卷积的移动步长是 1 个像素，经卷积运算后的尺寸为 (13+1+1-3)/1+1=13，每个 GPU 中有 13×13×192 个卷积核，2 个 GPU 卷积后生成 13×13×384 的像素层。 （2）ReLU 卷积后的像素层经过 ReLU 单元处理，生成激活像素层，尺寸仍为 2 组 13×13×192 像素层，分配给两个 GPU 处理。\n第五层（卷积层） 第五层的处理流程为：卷积 –\u003eReLU–\u003e 池化 （1）卷积 第五层输入数据为第四层输出的 2 组 13×13×192 的像素层，为便于后续处理，每幅像素层的上下左右边缘都填充 1 个像素，填充后的尺寸变为 (13+1+1)×(13+1+1) ，2 组像素层数据被送至 2 个不同的 GPU 中进行运算。 这一层中每个 GPU 都有 128 个卷积核，每个卷积核的尺寸是 3×3×192，卷积的步长是 1 个像素，经卷积后的尺寸为 (13+1+1-3)/1+1=13，每个 GPU 中有 13×13×128 个卷积核，2 个 GPU 卷积后生成 13×13×256 的像素层。 （2）ReLU 卷积后的像素层经过 ReLU 单元处理，生成激活像素层，尺寸仍为 2 组 13×13×128 像素层，由两个 GPU 分别处理。 （3）池化 2 组 13×13×128 像素层分别在 2 个不同 GPU 中进行池化运算处理，池化运算的尺寸为 3×3，步长为 2，池化后图像的尺寸为 (13-3)/2+1=6，即池化后像素的规模为两组 6×6×128 的像素层数据，共有 6×6×256 的像素层数据。\n第六层（全连接层） 第六层的处理流程为：卷积（全连接）–\u003eReLU–\u003eDropout （1）卷积（全连接） 第六层输入数据是第五层的输出，尺寸为 6×6×256。本层共有 4096 个卷积核，每个卷积核的尺寸为 6×6×256，由于卷积核的尺寸刚好与待处理特征图（输入）的尺寸相同，即卷积核中的每个系数只与特征图（输入）尺寸的一个像素值相乘，一一对应，因此，该层被称为全连接层。由于卷积核与特征图的尺寸相同，卷积运算后只有一个值，因此，卷积后的像素层尺寸为 4096×1×1，即有 4096 个神经元。 （2）ReLU 这 4096 个运算结果通过 ReLU 激活函数生成 4096 个值。 （3）Dropout 然后再通过 Dropout 运算，输出 4096 个结果值。\n第七层（全连接层） 第七层的处理流程为：全连接 –\u003eReLU–\u003eDropout 第六层输出的 4096 个数据与第七层的 4096 个神经元进行全连接，然后经 ReLU 进行处理后生成 4096 个数据，再经过 Dropout 处理后输出 4096 个数据。\n第八层（全连接层） 第八层的处理流程为：全连接 第七层输出的 4096 个数据与第八层的 1000 个神经元进行全连接，经过训练后输出 1000 个 float 型的值，这就是预测结果。\n以上就是关于 AlexNet 网络结构图的逐层解析了，看起来挺复杂的，下面是一个简图，看起来就清爽很多啊 通过前面的介绍，可以看出 AlexNet 的特点和创新之处，主要如下： 编程实现 下载imagenet数据集， Keras提供的keras.datasets模块可以用来直接加载ImageNet数据集。不过需要注意的是，ImageNet数据集非常大，包含数百万张高分辨率图像，因此通常需要使用分布式计算或者在GPU上进行训练。\nCIFAR-10数据集是一个常用的图像分类数据集，包含10个类别的图像，每个类别包含6000张32x32像素的彩色图像，总共60000张，其中50000张是用于训练，10000张是用于测试。这10个类别分别是：\n飞机（airplane） 汽车（automobile） 鸟类（bird） 猫（cat） 鹿（deer） 狗（dog） 青蛙（frog） 马（horse） 船（ship） 卡车（truck） 每个图像的标签是一个0到9之间的整数，对应上述10个类别中的一个。因此，我们可以使用这些标签来训练和测试图像分类模型。 你可以使用以下代码来加载这个小样本数据集：\nfrom tensorflow.keras.datasets import cifar10\r(x_train, y_train), (x_test, y_test) = cifar10.load_data()\rprint(x_train.shape) 执行后，日志里有一直在下载的过程，下载很慢，路径 https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 我们可以手动下载下来，重命名为：cifar-10-batches-py.tar.gz，然后上传到 ~/.keras/datasets目录即可（不用解压），程序会离线解压该文件，window下是：C:\\Users\\你的用户.keras\\datasets 再次运行输出 (50000, 32, 32, 3) 随机加载100张，看看效果\n# 随机选择100张图片进行显示\rindices = np.random.choice(len(x_train), size=100, replace=False)\rimages = x_train[indices]\rlabels = y_train[indices]\r# 绘制图片\rfig = plt.figure(figsize=(10, 10))\rfor i in range(10):\rfor j in range(10):\rindex = i * 10 + j\rax = fig.add_subplot(10, 10, index + 1)\rax.imshow(images[index])\rax.set_xticks([])\rax.set_yticks([])\rax.set_title(labels[index][0])\rplt.show() 显示 因为数据集总共有6万张，格式3232，使用alexnet模型进行计算，图像需要转换224224，rgb通道数3，每个像素都需要转换成float32，这样导致数gpu显存占用过大导致内存溢出， 需要占用显存=6000022422434＞＝３０ＧＢ， 所以需增量式进行训练\nimport tensorflow as tf\rfrom tensorflow.keras.datasets import cifar10\rfrom tensorflow.python.ops.numpy_ops import np_config\rnp_config.enable_numpy_behavior()\r(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\"\"\"\r在Python中，我们可以使用TensorFlow或Keras等深度学习框架来加载CIFAR-10数据集。为了有效地处理大量图像数据，我们可以使用生成器函数和yield语句来逐批加载数据。\r生成器函数是一个Python函数，它使用yield语句来产生一个序列的值。当函数执行到yield语句时，它会将当前的值返回给调用者，并暂停函数的执行。当函数再次被调用时，它会从上一次暂停的位置继续执行，并返回下一个值。\r\"\"\"\rdef cifar10_generator(x, y, batch_size):\r\"\"\"\rCIFAR-10 data generator.\r\"\"\"\rwhile True:\rfor i in range(0, len(x), batch_size):\rx_batch = x[i:i+batch_size]\ry_batch = y[i:i+batch_size]\rx_batch = tf.image.resize_with_pad(x_batch, target_height=224, target_width=224)\rx_batch = x_batch.astype('float32') / 255.0\ryield x_batch, y_batch\rfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\rdef alexnet(input_shape, num_classes):\rmodel = tf.keras.Sequential([\rConv2D(96, (11,11), strides=(4,4), activation='relu', input_shape=input_shape),\rMaxPooling2D(pool_size=(3,3), strides=(2,2)),\rConv2D(256, (5,5), strides=(1,1), padding='same', activation='relu'),\rMaxPooling2D(pool_size=(3,3), strides=(2,2)),\rConv2D(384, (3,3), strides=(1,1), padding='same', activation='relu'),\rConv2D(384, (3,3), strides=(1,1), padding='same', activation='relu'),\rConv2D(256, (3,3), strides=(1,1), padding='same', activation='relu'),\rMaxPooling2D(pool_size=(3,3), strides=(2,2)),\rFlatten(),\rDense(4096, activation='relu'),\rDropout(0.5),\rDense(4096, activation='relu'),\rDropout(0.5),\rDense(num_classes, activation='softmax')\r])\rreturn model\r# 定义一些超参数\rbatch_size = 256\repochs = 5\rlearning_rate = 0.001\r# 定义生成器\rtrain_generator = cifar10_generator(x_train, y_train, batch_size)\rtest_generator = cifar10_generator(x_test, y_test, batch_size)\r# 定义模型\rinput_shape = (224,224,3)\rnum_classes = 10\rmodel = alexnet(input_shape, num_classes)\r# 编译模型\roptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\rmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r# 定义 ModelCheckpoint 回调函数\rcheckpoint = tf.keras.callbacks.ModelCheckpoint('./AlexNet.h5', save_best_only=True, save_weights_only=False, monitor='val_loss')\r# 训练模型\rmodel.fit(train_generator,\repochs=epochs,\rsteps_per_epoch=len(x_train)//batch_size,\rvalidation_data=test_generator,\rvalidation_steps=len(x_test)//batch_size,\rcallbacks=[checkpoint]\r)\rtest_loss, test_acc = model.evaluate(test_generator, y_test)\rprint('Test accuracy:', test_acc) 预测结果和显示图像\n# 在这里添加您的识别代码\rmodel = tf.keras.models.load_model('./AlexNet.h5')\rsrcImage=x_test[105]\rp_test=np.array([srcImage])\rp_test = tf.image.resize_with_pad(p_test, target_height=224, target_width=224)\rp_test = p_test.astype('float32') / 255.0\rpredictions = model.predict(p_test)\rprint(\"识别结果为：\" + str(np.argmax(predictions)))\r# 绘制第10个测试数据的图形\rplt.imshow(srcImage, cmap=plt.cm.binary)\rplt.show() 输出：1 参考:https://my.oschina.net/u/876354/blog/1633143\nVGGNet 2014 年，牛津大学计算机视觉组（Visual Geometry Group）和 Google DeepMind 公司的研究员一起研发出了新的深度卷积神经网络：VGGNet，并取得了 ILSVRC2014 比赛分类项目的第二名（第一名是 GoogLeNet，也是同年提出的）和定位项目的第一名。 VGGNet 探索了卷积神经网络的深度与其性能之间的关系，成功地构筑了 16~19 层深的卷积神经网络，证明了增加网络的深度能够在一定程度上影响网络最终的性能，使错误率大幅下降，同时拓展性又很强，迁移到其它图片数据上的泛化性也非常好。到目前为止，VGG 仍然被用来提取图像特征。 VGGNet 可以看成是加深版本的 AlexNet，都是由卷积层、全连接层两大部分构成。\nVGG 的特点 先看一下 VGG 的结构图 1、结构简洁 VGG 由 5 层卷积层、3 层全连接层、softmax 输出层构成，层与层之间使用 max-pooling（最大化池）分开，所有隐层的激活单元都采用 ReLU 函数。 2、小卷积核和多卷积子层 VGG 使用多个较小卷积核（3x3）的卷积层代替一个卷积核较大的卷积层，一方面可以减少参数，另一方面相当于进行了更多的非线性映射，可以增加网络的拟合 / 表达能力。 小卷积核是 VGG 的一个重要特点，虽然 VGG 是在模仿 AlexNet 的网络结构，但没有采用 AlexNet 中比较大的卷积核尺寸（如 7x7），而是通过降低卷积核的大小（3x3），增加卷积子层数来达到同样的性能（VGG：从 1 到 4 卷积子层，AlexNet：1 子层）。 VGG 的作者认为两个 3x3 的卷积堆叠获得的感受野大小，相当一个 5x5 的卷积；而 3 个 3x3 卷积的堆叠获取到的感受野相当于一个 7x7 的卷积。这样可以增加非线性映射，也能很好地减少参数（例如 7x7 的参数为 49 个，而 3 个 3x3 的参数为 27），如下图所示： 3、小池化核 相比 AlexNet 的 3x3 的池化核，VGG 全部采用 2x2 的池化核。 4、通道数多 VGG 网络第一层的通道数为 64，后面每层都进行了翻倍，最多到 512 个通道，通道数的增加，使得更多的信息可以被提取出来。 5、层数更深、特征图更宽 由于卷积核专注于扩大通道数、池化专注于缩小宽和高，使得模型架构上更深更宽的同时，控制了计算量的增加规模。 6、全连接转卷积（测试阶段） 这也是 VGG 的一个特点，在网络测试阶段将训练阶段的三个全连接替换为三个卷积，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高为的输入，这在测试阶段很重要。 如本节第一个图所示，输入图像是 224x224x3，如果后面三个层都是全连接，那么在测试阶段就只能将测试的图像全部都要缩放大小到 224x224x3，才能符合后面全连接层的输入数量要求，这样就不便于测试工作的开展。 而 “全连接转卷积”，替换过程如下： 例如 7x7x512 的层要跟 4096 个神经元的层做全连接，则替换为对 7x7x512 的层作通道数为 4096、卷积核为 1x1 的卷积。 这个 “全连接转卷积” 的思路是 VGG 作者参考了 OverFeat 的工作思路，例如下图是 OverFeat 将全连接换成卷积后，则可以来处理任意分辨率（在整张图）上计算卷积，这就是无需对原图做重新缩放处理的优势。 VGG 的网络结构 下图是来自论文《Very Deep Convolutional Networks for Large-Scale Image Recognition》（基于甚深层卷积网络的大规模图像识别）的 VGG 网络结构，正是在这篇论文中提出了 VGG，如下图： 在这篇论文中分别使用了 A、A-LRN、B、C、D、E 这 6 种网络结构进行测试，这 6 种网络结构相似，都是由 5 层卷积层、3 层全连接层组成，其中区别在于每个卷积层的子层数量不同，从 A 至 E 依次增加（子层数量从 1 到 4），总的网络深度从 11 层到 19 层（添加的层以粗体显示），表格中的卷积层参数表示为 “conv⟨感受野大小⟩- 通道数⟩”，例如 con3-128，表示使用 3x3 的卷积核，通道数为 128。为了简洁起见，在表格中不显示 ReLU 激活功能。 其中，网络结构 D 就是著名的 VGG16，网络结构 E 就是著名的 VGG19。\n以网络结构 D（VGG16）为例，介绍其处理过程如下，请对比上面的表格和下方这张图，留意图中的数字变化，有助于理解 VGG16 的处理过程： 1、输入 224x224x3 的图片，经 64 个 3x3 的卷积核作两次卷积 + ReLU，卷积后的尺寸变为 224x224x64 2、作 max pooling（最大化池化），池化单元尺寸为 2x2（效果为图像尺寸减半），池化后的尺寸变为 112x112x64 3、经 128 个 3x3 的卷积核作两次卷积 + ReLU，尺寸变为 112x112x128 4、作 2x2 的 max pooling 池化，尺寸变为 56x56x128 5、经 256 个 3x3 的卷积核作三次卷积 + ReLU，尺寸变为 56x56x256 6、作 2x2 的 max pooling 池化，尺寸变为 28x28x256 7、经 512 个 3x3 的卷积核作三次卷积 + ReLU，尺寸变为 28x28x512 8、作 2x2 的 max pooling 池化，尺寸变为 14x14x512 9、经 512 个 3x3 的卷积核作三次卷积 + ReLU，尺寸变为 14x14x512 10、作 2x2 的 max pooling 池化，尺寸变为 7x7x512 11、与两层 1x1x4096，一层 1x1x1000 进行全连接 + ReLU（共三层） 12、通过 softmax 输出 1000 个预测结果\n以上就是 VGG16（网络结构 D）各层的处理过程，A、A-LRN、B、C、E 其它网络结构的处理过程也是类似，执行过程如下（以 VGG16 为例）： 从上面的过程可以看出 VGG 网络结构还是挺简洁的，都是由小卷积核、小池化核、ReLU 组合而成。其简化图如下（以 VGG16 为例）： A、A-LRN、B、C、D、E 这 6 种网络结构的深度虽然从 11 层增加至 19 层，但参数量变化不大，这是由于基本上都是采用了小卷积核（3x3，只有 9 个参数），这 6 种结构的参数数量（百万级）并未发生太大变化，这是因为在网络中，参数主要集中在全连接层。 经作者对 A、A-LRN、B、C、D、E 这 6 种网络结构进行单尺度的评估，错误率结果如下： 从上表可以看出：\n1、LRN 层无性能增益（A-LRN）\nVGG 作者通过网络 A-LRN 发现，AlexNet 曾经用到的 LRN 层（local response normalization，局部响应归一化）并没有带来性能的提升，因此在其它组的网络中均没再出现 LRN 层。\n2、随着深度增加，分类性能逐渐提高（A、B、C、D、E）\n从 11 层的 A 到 19 层的 E，网络深度增加对 top1 和 top5 的错误率下降很明显。\n3、多个小卷积核比单个大卷积核性能好（B）\nVGG 作者做了实验用 B 和自己一个不在实验组里的较浅网络比较，较浅网络用 conv5x5 来代替 B 的两个 conv3x3，结果显示多个小卷积核比单个大卷积核效果要好。\n最后进行个小结：\n1、通过增加深度能有效地提升性能；\n2、最佳模型：VGG16，从头到尾只有 3x3 卷积与 2x2 池化，简洁优美；\n3、卷积可代替全连接，可适应各种尺寸的图片\n编程实现 ILSVRC2014 数据集在image-net下载目前需要注册，并且需要审批比较麻烦，可以在阿里云天池数据集上下载ILSVRC2017版本（可以使用钉钉或者支付宝实名认证登录下，很多大型数据集都可以登录后直接下载），地址：https://tianchi.aliyun.com/dataset/92252，下载imagenet_object_localization_patched2019 (1).tar.gz，数据集大小155GB 由于数据集过大，我这里依然使用cifar10\nVGGNet和AlexNet都是深度神经网络模型，VGGNet比AlexNet更深，因此它需要更多的计算资源和时间来训练。具体来说，VGGNet有16层或19层，而AlexNet只有8层。这意味着VGGNet需要处理更多的参数和数据，需要更长的训练时间。此外，VGGNet使用了更小的卷积核，这也导致了更多的计算量。所以，VGGNet训练比AlexNet慢很多是很正常的。\nimport tensorflow as tf\rfrom tensorflow.keras.datasets import cifar10\rfrom tensorflow.python.ops.numpy_ops import np_config\rfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\rfrom tensorflow.keras.models import Sequential\rnp_config.enable_numpy_behavior()\r(x_train, y_train), (x_test, y_test) = cifar10.load_data()\rdef cifar10_generator(x, y, batch_size):\rwhile True:\rfor i in range(0, len(x), batch_size):\rx_batch = x[i:i+batch_size]\ry_batch = y[i:i+batch_size]\rx_batch = tf.image.resize_with_pad(x_batch, target_height=224, target_width=224)\rx_batch = x_batch.astype('float32') / 255.0\ryield x_batch, y_batch\rdef vggnet(input_shape, num_classes):\r# 定义VGGNet\rmodel = Sequential([\r# 第一层卷积和池化\rConv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape),\rConv2D(64, (3, 3), activation='relu', padding='same'),\rMaxPooling2D((2, 2)),\r# 第二层卷积和池化\rConv2D(128, (3, 3), activation='relu', padding='same'),\rConv2D(128, (3, 3), activation='relu', padding='same'),\rMaxPooling2D((2, 2)),\r# 第三层卷积和池化\rConv2D(256, (3, 3), activation='relu', padding='same'),\rConv2D(256, (3, 3), activation='relu', padding='same'),\rConv2D(256, (3, 3), activation='relu', padding='same'),\rMaxPooling2D((2, 2)),\r# 第四层卷积和池化\rConv2D(512, (3, 3), activation='relu', padding='same'),\rConv2D(512, (3, 3), activation='relu', padding='same'),\rConv2D(512, (3, 3), activation='relu', padding='same'),\rMaxPooling2D((2, 2)),\r# 第五层卷积和池化\rConv2D(512, (3, 3), activation='relu', padding='same'),\rConv2D(512, (3, 3), activation='relu', padding='same'),\rConv2D(512, (3, 3), activation='relu', padding='same'),\rMaxPooling2D((2, 2)),\r# 将输出的特征图展平，并连接全连接层\rFlatten(),\rDense(4096, activation='relu'),\rDense(4096, activation='relu'),\rDense(10, activation='softmax')\r])\rreturn model\r# 定义一些超参数\rbatch_size = 128\repochs = 5\rlearning_rate = 0.001\r# 定义生成器\rtrain_generator = cifar10_generator(x_train, y_train, batch_size)\rtest_generator = cifar10_generator(x_test, y_test, batch_size)\r# 定义模型\rinput_shape = (224,224,3)\rnum_classes = 10\rmodel = vggnet(input_shape, num_classes)\rmodel.summary()\r# 编译模型\roptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\rmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r# 定义 ModelCheckpoint 回调函数\rcheckpoint = tf.keras.callbacks.ModelCheckpoint('./VGGNet.h5', save_best_only=True, save_weights_only=False, monitor='val_loss')\r# 训练模型\rmodel.fit(train_generator,\repochs=epochs,\rsteps_per_epoch=len(x_train)//batch_size,\rvalidation_data=test_generator,\rvalidation_steps=len(x_test)//batch_size,\rcallbacks=[checkpoint]\r)\rtest_loss, test_acc = model.evaluate(test_generator, y_test)\rprint('Test accuracy:', test_acc) 参考:https://my.oschina.net/u/876354/blog/1634322\nGoogLeNet 2014 年，GoogLeNet 和 VGG 是当年 ImageNet 挑战赛 (ILSVRC14) 的双雄，GoogLeNet 获得了第一名、VGG 获得了第二名，这两类模型结构的共同特点是层次更深了。VGG 继承了 LeNet 以及 AlexNet 的一些框架结构，而 GoogLeNet 则做了更加大胆的网络结构尝试，虽然深度只有 22 层，但大小却比 AlexNet 和 VGG 小很多，GoogleNet 参数为 500 万个，AlexNet 参数个数是 GoogleNet 的 12 倍，VGGNet 参数又是 AlexNet 的 3 倍，因此在内存或计算资源有限时，GoogleNet 是比较好的选择；从模型结果来看，GoogLeNet 的性能却更加优越。\n小知识：GoogLeNet 是谷歌（Google）研究出来的深度网络结构，为什么不叫 “GoogleNet”，而叫 “GoogLeNet”，据说是为了向 “LeNet” 致敬，因此取名为 “GoogLeNet”\n那么，GoogLeNet 是如何进一步提升性能的呢？\n一般来说，提升网络性能最直接的办法就是增加网络深度和宽度，深度指网络层次数量、宽度指神经元数量。但这种方式存在以下问题：\n（1）参数太多，如果训练数据集有限，很容易产生过拟合；\n（2）网络越大、参数越多，计算复杂度越大，难以应用；\n（3）网络越深，容易出现梯度弥散问题（梯度越往后穿越容易消失），难以优化模型。\n所以，有人调侃 “深度学习” 其实是 “深度调参”。\n解决这些问题的方法当然就是在增加网络深度和宽度的同时减少参数，为了减少参数，自然就想到将全连接变成稀疏连接。但是在实现上，全连接变成稀疏连接后实际计算量并不会有质的提升，因为大部分硬件是针对密集矩阵计算优化的，稀疏矩阵虽然数据量少，但是计算所消耗的时间却很难减少。\n那么，有没有一种方法既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能。大量的文献表明可以将稀疏矩阵聚类为较为密集的子矩阵来提高计算性能，就如人类的大脑是可以看做是神经元的重复堆积，因此，GoogLeNet 团队提出了 Inception 网络结构，就是构造一种 “基础神经元” 结构，来搭建一个稀疏性、高计算性能的网络结构。 【问题来了】什么是 Inception 呢？ Inception 历经了 V1、V2、V3、V4 等多个版本的发展，不断趋于完善，下面一一进行介绍\nInception V1 通过设计一个稀疏网络结构，但是能够产生稠密的数据，既能增加神经网络表现，又能保证计算资源的使用效率。谷歌提出了最原始 Inception 的基本结构： 该结构将 CNN 中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积、池化后的尺寸相同，将通道相加），一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。 网络卷积层中的网络能够提取输入的每一个细节信息，同时 5x5 的滤波器也能够覆盖大部分接受层的的输入。还可以进行一个池化操作，以减少空间大小，降低过度拟合。在这些层之上，在每一个卷积层后都要做一个 ReLU 操作，以增加网络的非线性特征。 然而这个 Inception 原始版本，所有的卷积核都在上一层的所有输出上来做，而那个 5x5 的卷积核所需的计算量就太大了，造成了特征图的厚度很大，为了避免这种情况，在 3x3 前、5x5 前、max pooling 后分别加上了 1x1 的卷积核，以起到了降低特征图厚度的作用，这也就形成了 Inception v1 的网络结构，如下图所示： 1x1 的卷积核有什么用呢？ 1x1 卷积的主要目的是为了减少维度，还用于修正线性激活（ReLU）。比如，上一层的输出为 100x100x128，经过具有 256 个通道的 5x5 卷积层之后 (stride=1，pad=2)，输出数据为 100x100x256，其中，卷积层的参数为 128x5x5x256= 819200。而假如上一层输出先经过具有 32 个通道的 1x1 卷积层，再经过具有 256 个输出的 5x5 卷积层，那么输出数据仍为为 100x100x256，但卷积参数量已经减少为 128x1x1x32 + 32x5x5x256= 204800，大约减少了 4 倍。\n基于 Inception 构建了 GoogLeNet 的网络结构如下（共 22 层）： 对上图说明如下：\n（1）GoogLeNet 采用了模块化的结构（Inception 结构），方便增添和修改；\n（2）网络最后采用了 average pooling（平均池化）来代替全连接层，该想法来自 NIN（Network in Network），事实证明这样可以将准确率提高 0.6%。但是，实际在最后还是加了一个全连接层，主要是为了方便对输出进行灵活调整；\n（3）虽然移除了全连接，但是网络中依然使用了 Dropout ;\n（4）为了避免梯度消失，网络额外增加了 2 个辅助的 softmax 用于向前传导梯度（辅助分类器）。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的 softmax 会被去掉。\nGoogLeNet 的网络结构图细节如下： 注：上表中的 “#3x3 reduce”，“#5x5 reduce” 表示在 3x3，5x5 卷积操作之前使用了 1x1 卷积的数量。\nGoogLeNet 网络结构明细表解析如下：\n0、输入\n原始输入图像为 224x224x3，且都进行了零均值化的预处理操作（图像每个像素减去均值）。\n1、第一层（卷积层）\n使用 7x7 的卷积核（滑动步长 2，padding 为 3），64 通道，输出为 112x112x64，卷积后进行 ReLU 操作\n经过 3x3 的 max pooling（步长为 2），输出为 ((112 - 3+1)/2)+1=56，即 56x56x64，再进行 ReLU 操作\n2、第二层（卷积层）\n使用 3x3 的卷积核（滑动步长为 1，padding 为 1），192 通道，输出为 56x56x192，卷积后进行 ReLU 操作\n经过 3x3 的 max pooling（步长为 2），输出为 ((56 - 3+1)/2)+1=28，即 28x28x192，再进行 ReLU 操作\n3a、第三层（Inception 3a 层）\n分为四个分支，采用不同尺度的卷积核来进行处理\n（1）64 个 1x1 的卷积核，然后 RuLU，输出 28x28x64\n（2）96 个 1x1 的卷积核，作为 3x3 卷积核之前的降维，变成 28x28x96，然后进行 ReLU 计算，再进行 128 个 3x3 的卷积（padding 为 1），输出 28x28x128\n（3）16 个 1x1 的卷积核，作为 5x5 卷积核之前的降维，变成 28x28x16，进行 ReLU 计算后，再进行 32 个 5x5 的卷积（padding 为 2），输出 28x28x32\n（4）pool 层，使用 3x3 的核（padding 为 1），输出 28x28x192，然后进行 32 个 1x1 的卷积，输出 28x28x32。\n将四个结果进行连接，对这四部分输出结果的第三维并联，即 64+128+32+32=256，最终输出 28x28x256\n3b、第三层（Inception 3b 层）\n（1）128 个 1x1 的卷积核，然后 RuLU，输出 28x28x128\n（2）128 个 1x1 的卷积核，作为 3x3 卷积核之前的降维，变成 28x28x128，进行 ReLU，再进行 192 个 3x3 的卷积（padding 为 1），输出 28x28x192\n（3）32 个 1x1 的卷积核，作为 5x5 卷积核之前的降维，变成 28x28x32，进行 ReLU 计算后，再进行 96 个 5x5 的卷积（padding 为 2），输出 28x28x96\n（4）pool 层，使用 3x3 的核（padding 为 1），输出 28x28x256，然后进行 64 个 1x1 的卷积，输出 28x28x64。\n将四个结果进行连接，对这四部分输出结果的第三维并联，即 128+192+96+64=480，最终输出输出为 28x28x480\n第四层（4a,4b,4c,4d,4e）、第五层（5a,5b）……，与 3a、3b 类似，在此就不再重复。\n从 GoogLeNet 的实验结果来看，效果很明显，差错率比 MSRA、VGG 等模型都要低，对比结果如下表所示： Inception V2 GoogLeNet 凭借其优秀的表现，得到了很多研究人员的学习和使用，因此 GoogLeNet 团队又对其进行了进一步地发掘改进，产生了升级版本的 GoogLeNet。\nGoogLeNet 设计的初衷就是要又准又快，而如果只是单纯的堆叠网络虽然可以提高准确率，但是会导致计算效率有明显的下降，所以如何在不增加过多计算量的同时提高网络的表达能力就成为了一个问题。\nInception V2 版本的解决方案就是修改 Inception 的内部计算逻辑，提出了比较特殊的 “卷积” 计算结构。\n1、卷积分解（Factorizing Convolutions）\n大尺寸的卷积核可以带来更大的感受野，但也意味着会产生更多的参数，比如 5x5 卷积核的参数有 25 个，3x3 卷积核的参数有 9 个，前者是后者的 25/9=2.78 倍。因此，GoogLeNet 团队提出可以用 2 个连续的 3x3 卷积层组成的小网络来代替单个的 5x5 卷积层，即在保持感受野范围的同时又减少了参数量，如下图： 那么这种替代方案会造成表达能力的下降吗？通过大量实验表明，并不会造成表达缺失。 可以看出，大卷积核完全可以由一系列的 3x3 卷积核来替代，那能不能再分解得更小一点呢？GoogLeNet 团队考虑了 nx1 的卷积核，如下图所示，用 3 个 3x1 取代 3x3 卷积： 因此，任意 nxn 的卷积都可以通过 1xn 卷积后接 nx1 卷积来替代。GoogLeNet 团队发现在网络的前期使用这种分解效果并不好，在中度大小的特征图（feature map）上使用效果才会更好（特征图大小建议在 12 到 20 之间）。 2、降低特征图大小\n一般情况下，如果想让图像缩小，可以有如下两种方式： 先池化再作 Inception 卷积，或者先作 Inception 卷积再作池化。但是方法一（左图）先作 pooling（池化）会导致特征表示遇到瓶颈（特征缺失），方法二（右图）是正常的缩小，但计算量很大。为了同时保持特征表示且降低计算量，将网络结构改为下图，使用两个并行化的模块来降低计算量（卷积、池化并行执行，再进行合并） 使用 Inception V2 作改进版的 GoogLeNet，网络结构图如下： 注：上表中的 Figure 5 指没有进化的 Inception，Figure 6 是指小卷积版的 Inception（用 3x3 卷积核代替 5x5 卷积核），Figure 7 是指不对称版的 Inception（用 1xn、nx1 卷积核代替 nxn 卷积核）。\n经实验，模型结果与旧的 GoogleNet 相比有较大提升，如下表所示： Inception V3 Inception V3 一个最重要的改进是分解（Factorization），将 7x7 分解成两个一维的卷积（1x7,7x1），3x3 也是一样（1x3,3x1），这样的好处，既可以加速计算，又可以将 1 个卷积拆成 2 个卷积，使得网络深度进一步增加，增加了网络的非线性（每增加一层都要进行 ReLU）。 另外，网络输入从 224x224 变为了 299x299。\nInception V4 Inception V4 研究了 Inception 模块与残差连接的结合。ResNet 结构大大地加深了网络深度，还极大地提升了训练速度，同时性能也有提升（ResNet 的技术原理介绍见本博客之前的文章：大话深度残差网络 ResNet）。 Inception V4 主要利用残差连接（Residual Connection）来改进 V3 结构，得到 Inception-ResNet-v1，Inception-ResNet-v2，Inception-v4 网络。 ResNet 的残差结构如下： 将该结构与 Inception 相结合，变成下图： 通过 20 个类似的模块组合，Inception-ResNet 构建如下： 编程实现 后续补充\n参考:https://my.oschina.net/u/876354/blog/1637819",
    "description": "简介 卷积神经网络（CNN）是深度学习中非常重要的一种网络结构，它可以处理图像、文本、语音等各种类型的数据。以下是CNN的前4个经典模型\nLeNet-5 LeNet-5是由Yann LeCun等人于1998年提出的，是第一个成功应用于手写数字识别的卷积神经网络。它由7层神经网络组成，包括2层卷积层、2层池化层和3层全连接层。其中，卷积层提取图像特征，池化层降低特征图的维度，全连接层将特征映射到对应的类别上。\nLeNet-5的主要特点是使用Sigmoid激活函数、平均池化和卷积层后没有使用零填充。它在手写数字识别、人脸识别等领域都有着广泛的应用。\nAlexNet AlexNet是由Alex Krizhevsky等人于2012年提出的，是第一个在大规模图像识别任务中取得显著成果的卷积神经网络。它由5层卷积层、3层全连接层和1层Softmax输出层组成，其中使用了ReLU激活函数、最大池化和Dropout技术。\nAlexNet的主要特点是使用了GPU加速训练、数据增强和随机化Dropout等技术，使得模型的泛化能力和鲁棒性得到了大幅提升。它在ImageNet大规模图像识别比赛中取得了远超其他模型的优异成绩。\nVGGNet VGGNet是由Karen Simonyan和Andrew Zisserman于2014年提出的，它是一个非常深的卷积神经网络，有16层或19层。VGGNet的每个卷积层都使用了3x3的卷积核和ReLU激活函数，使得它的网络结构非常清晰、易于理解。\nVGGNet的主要特点是使用了更深的网络结构、小卷积核和少量的参数，使得模型的特征提取能力得到了进一步提升。它在ImageNet比赛中也获得了非常好的成绩。\nGoogLeNet GoogLeNet是由Google团队于2014年提出的，它是一个非常深的卷积神经网络，有22层。它使用了一种称为Inception模块的结构，可以在保持网络深度的同时减少参数量。\nGoogLeNet的主要特点是使用了Inception模块、1x1卷积核和全局平均池化等技术，使得模型的计算复杂度得到了大幅降低。它在ImageNet比赛中获得了非常好的成绩，并且被广泛应用于其他领域。\nCNN回顾 回顾一下 CNN 的几个特点：局部感知、参数共享、池化。\n局部感知 人类对外界的认知一般是从局部到全局、从片面到全面，类似的，在机器识别图像时也没有必要把整张图像按像素全部都连接到神经网络中，在图像中也是局部周边的像素联系比较紧密，而距离较远的像素则相关性较弱，因此可以采用局部连接的模式（将图像分块连接，这样能大大减少模型的参数），如下图所示： 参数（权值）共享 每张自然图像（人物、山水、建筑等）都有其固有特性，也就是说，图像其中一部分的统计特性与其它部分是接近的。这也意味着这一部分学习的特征也能用在另一部分上，能使用同样的学习特征。因此，在局部连接中隐藏层的每一个神经元连接的局部图像的权值参数（例如 5×5），将这些权值参数共享给其它剩下的神经元使用，那么此时不管隐藏层有多少个神经元，需要训练的参数就是这个局部图像的权限参数（例如 5×5），也就是卷积核的大小，这样大大减少了训练参数。如下图 卷积核的权值是指每个卷积核中的参数，用于对输入数据进行卷积操作时，对每个位置的像素进行加权求和。在卷积神经网络中，同一层中的所有卷积核的权值是共享的，这意味着每个卷积核在不同位置上的权值是相同的。共享权值可以减少模型中需要学习的参数数量，从而降低了模型的复杂度，同时可以提高模型的泛化能力，因为共享权值可以使模型更加稳定，避免过度拟合。共享权值的实现方式是通过使用相同的卷积核对输入数据进行卷积操作。 池化 随着模型网络不断加深，卷积核越来越多，要训练的参数还是很多，而且直接拿卷积核提取的特征直接训练也容易出现过拟合的现象。回想一下，之所以对图像使用卷积提取特征是因为图像具有一种 “静态性” 的属性，因此，一个很自然的想法就是对不同位置区域提取出有代表性的特征（进行聚合统计，例如最大值、平均值等），这种聚合的操作就叫做池化，池化的过程通常也被称为特征映射的过程（特征降维），如下图：\nLeNet-5 概述 LeNet5 诞生于 1994 年，是最早的卷积神经网络之一， 由 Yann LeCun 完成，推动了深度学习领域的发展。在那时候，没有 GPU 帮助训练模型，甚至 CPU 的速度也很慢，因此，LeNet5 通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络也是最近大量神经网络架构的起点，给这个领域带来了许多灵感。 LeNet5 的网络结构示意图如下所示： LeNet5 由 7 层 CNN（不包含输入层）组成，上图中输入的原始图像大小是 32×32 像素，卷积层用 Ci 表示，子采样层（pooling，池化）用 Si 表示，全连接层用 Fi 表示。下面逐层介绍其作用和示意图上方的数字含义。",
    "tags": [],
    "title": "深度学习04-CNN经典模型",
    "uri": "/docs/programming/ai/deep_learning/cnn/dl_04_cnn_models/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 循环神经网络",
    "content": "@[toc]\n概述 循环神经网络（Recurrent Neural Network，RNN）是一种具有循环连接的神经网络结构，被广泛应用于自然语言处理、语音识别、时序数据分析等任务中。相较于传统神经网络，RNN的主要特点在于它可以处理序列数据，能够捕捉到序列中的时序信息。\nRNN的基本单元是一个循环单元（Recurrent Unit），它接收一个输入和一个来自上一个时间步的隐藏状态，并输出当前时间步的隐藏状态。在传统的RNN中，循环单元通常使用tanh或ReLU等激活函数。\n基本循环神经网络 原理 基本的 循环神经网络，结构由 输入层、一个隐藏层和输出层 组成。\n$x$是输入向量，$o$是输出向量，$s$表示隐藏层的值；$U$是输入层到隐藏层的权重矩阵，$V$是隐藏层到输出层的权重矩阵。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入$x$，还取决于上一次隐藏层的值$s$。权重矩阵W就是隐藏层上一次的值作为这一次的输入的权重。 将上图的基本RNN结构在时间维度展开(RNN是一个链式结构，每个时间片使用的是相同的参数,t表示t时刻)： 现在看上去就会清楚许多，这个网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。关键一点是$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t−1}$。 公式1：$s_t=f(U∗x_t+W∗s_{t−1}+B1)$ 公式2：$o_t=g(V∗s_t+B2)$\n式1是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次隐藏层值$S_{t−1}$作为这一次的输入的权重矩阵，f是激活函数。 式2是输出层的计算公式，V是输出层的权重矩阵，g是激活函数,B1,B2是偏置假设为0。 隐含层有两个输入，第一是U与$x_t$向量的乘积，第二是上一隐含层输出的状态$s_t−1$和W的乘积。等于上一个时刻计算的$s_t−1$需要缓存一下，在本次输入$x_t$一起计算，共同输出最后的$o_t$。\n如果反复把式1带入式2，我们将得到： 从上面可以看出，循环神经网络的输出值ot，是受前面历次输入值、、、、、、、、$x_t$、$x_{t−1}$、$x_{t−2}$、$x_{t−3}$、…影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。这样其实不好，因为如果太前面的值和后面的值已经没有关系了，循环神经网络还考虑前面的值的话，就会影响后面值的判断。\n上面是整个单向单层NN的前向传播过程\n为了更快理解输入x输入格式下面使用nlp中Word Embedding讲解下。\nWord Embedding 首先我们需要对输入文本x进行编码，使之成为计算机可以读懂的语言，在编码时，我们期望句子之间保持词语间的相似行，词的向量表示是进行机器学习和深度学习的基础。\nword embedding的一个基本思路就是，我们把一个词映射到语义空间的一个点，把一个词映射到低维的稠密空间，这样的映射使得语义上比较相似的词，他在语义空间的距离也比较近，如果两个词的关系不是很接近，那么在语义空间中向量也会比较远。\n如上图英语和西班牙语映射到语义空间，语义相同的数字他们在语义空间分布的位置是相同的 简单回顾一下word embedding,对于nlp来说，我们输入的是一个个离散的符号，对于神经网络来说，它处理的都是向量或者矩阵。所以第一步，我们需要把一个词编码成向量。最简单的就是one-hot的表示方法。如下图所示： python代码（one-hot），比如\nimport numpy as np\rword_array = ['apple', 'kiwi', 'mango']\rword_dict = {'apple': 0, 'banana': 1, 'orange': 2, 'grape': 3, 'melon': 4, 'peach': 5, 'pear': 6, 'kiwi': 7, 'plum': 8, 'mango': 9}\r# 创建一个全为0的矩阵\rone_hot_matrix = np.zeros((len(word_array), len(word_dict)))\r# 对每个单词进行one-hot编码\rfor i, word in enumerate(word_array):\rword_index = word_dict[word]\rone_hot_matrix[i, word_index] = 1\rprint(one_hot_matrix) 输出:\n[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] #这就是apple的one-hot编码\r[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] #这就是kiwi的one-hot编码\r[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] #这就是mango的one-hot编码 行表示每个单词，列表示语料库，每个列对应一个语料单词，也就是特征列\n虽然one-hot编码是一种简单有效的特征表示方法，但它也存在一些缺点：\n高维度表示：使用one-hot编码时，每个特征都需要创建一个很大的稀疏向量，维度与特征的唯一值数量相等。这会导致高维度的输入数据，增加了计算和存储的开销。特别是在处理具有大量离散特征的问题时，会导致非常庞大的特征空间。\n维度独立性：one-hot编码将每个特征都表示为独立的二进制特征，没有考虑到特征之间的相关性和语义关系。这可能会导致模型难以捕捉到特征之间的相互作用和关联性，从而影响了模型的性能。\n无法处理未知特征：one-hot编码要求特征的唯一值在训练集中都出现过，否则会出现问题。如果在测试集或实际应用中遇到了未在训练集中出现的特征值，就无法进行one-hot编码，这可能导致模型无法处理这些未知特征。\n特征稀疏性：由于one-hot编码的特征向量是稀疏的，大部分元素都是0，这会导致数据稀疏性增加，对于一些算法（如线性模型）可能会带来一些问题。\n综上所述，尽管one-hot编码在某些情况下是一种简单有效的特征表示方法，但它也存在一些缺点，特别是在处理高维度离散特征、考虑特征间关系和处理未知特征值时可能会遇到问题。\n使用nn.Embedding替代one-hot编码的原因主要有两点：\n维度灵活性：使用one-hot编码时，每个特征都需要创建一个很大的稀疏向量，维度与特征的唯一值数量相等。这会导致高维度的输入，增加了计算和存储的开销。而使用嵌入（embedding）可以将离散特征映射为低维度的连续向量表示，减少了存储和计算的成本。\n语义关系和相似性：嵌入向量可以捕捉到特征之间的语义关系和相似性。例如，在自然语言处理任务中，使用嵌入向量可以将单词映射为连续的向量表示，使得具有相似语义含义的单词在嵌入空间中距离较近。这样的特性可以帮助模型更好地理解和学习特征之间的关系，提升模型的性能。\n因此，使用nn.Embedding替代one-hot编码可以提高模型的效率和性能，特别是在处理高维度的离散特征时。\n好的，我们来看一个简单的例子来手推nn.embedding的两个参数的作用。\n假设我们有一个句子分类任务，我们的输入是一个句子，每个单词都是一个特征。我们有5个不同的单词，分别是[“I”, “love”, “deep”, “learning”, “!” ]。\n我们可以使用nn.embedding来将这些单词映射为嵌入向量（在坐标系中有一个位置指向了这个单词）。假设我们将每个单词嵌入为一个3维的向量。这里，num_embeddings为5，表示我们有5个不同的单词；embedding_dim为3，表示每个单词嵌入为一个3维的向量。\n我们可以用下面的表格来表示每个单词的嵌入向量：\n单词 嵌入向量 “I” [0.1, 0.2, 0.3] “love” [0.4, 0.5, 0.6] “deep” [0.7, 0.8, 0.9] “learning” [0.2, 0.3, 0.4] “!” [0.5, 0.6, 0.7] 通过nn.embedding，我们可以将句子中的每个单词转换为对应的嵌入向量。例如，句子\"I love deep learning!“可以转换为以下嵌入向量序列：\n[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [0.2, 0.3, 0.4], [0.5, 0.6, 0.7]]\n这样，我们就可以将离散的单词特征转换为连续的嵌入向量，在深度学习模型中使用。\n以下是pytorch(python入门)的使用\n# 创建词汇表\rvocab = {\"I\": 0, \"love\": 1, \"deep\": 2, \"learning\": 3, \"!\": 4}\rstrings=[\"I\", \"love\", \"deep\", \"learning\", \"!\" ]\r# 将字符串序列转换为整数索引序列\rinput = t.LongTensor([vocab[word] for word in strings])\r#注意第一个参数是词汇表的个数，并不是输入单词的长度，你在这里就算填100也不影响最终的输出维度，这个输入值影响的是算出来的行向量值\r#nn.Embedding模块会随机初始化嵌入矩阵。在深度学习中，模型参数通常会使用随机初始化的方法来开始训练，以便模型能够在训练过程中学习到合适的参数值。\r#在nn.Embedding中，嵌入矩阵的每个元素都会被随机初始化为一个小的随机值，这些值将作为模型在训练过程中学习的可训练参数，可以使用manual_seed固定。\rt.manual_seed(1234)\rembedding=nn.Embedding(len(vocab),3)\rprint(embedding(input)) 输出结果为： tensor([[-0.1117, -0.4966, 0.1631], [-0.8817, 0.0539, 0.6684], [-0.0597, -0.4675, -0.2153], [ 0.8840, -0.7584, -0.3689], [-0.3424, -1.4020, 0.3206]], grad_fn=)\n注意Embedding第一个参数不是输入的字符的长度，而是词汇表的长度，比如有词汇表 {“I”: 0, “love”: 1, “deep”: 2, “learning”: 3, “!”: 4}，而输入input可能是：i love，此时应该传入的是5而不是2，因为预测最后隐藏层需要做个全连接用来预测当前输入单词对于整个词汇表的所有单词的概率。\nnn.Embedding 是一个简单的查找表，它的计算过程非常直接和简单。让我们来看一个具体的例子来说明它的计算过程。\n假设我们有一个词汇表包含以下单词：\n\"apple\" - 索引为0\r\"banana\" - 索引为1\r\"orange\" - 索引为2\r\"grape\" - 索引为3 我们还假设我们正在构建一个3维的词嵌入矩阵，所以每个单词会映射到一个3维向量。现在，让我们看看如何计算单词 “banana” 的向量坐标。 初始化词嵌入矩阵 初始化词嵌入矩阵的过程通常是随机的，其中每个单词的向量都会被初始化为随机的数值。这是因为在大多数情况下，我们不会有关于单词向量的先验信息，因此随机初始化是一个常见的做法。\n在实际中，词嵌入矩阵的初始化可以采用不同的方法，其中最常见的是以下两种：\n均匀分布初始化：每个单词的向量从一个均匀分布中随机抽样，通常在[-1, 1]或者[0, 1]之间。 正态分布初始化：每个单词的向量从一个正态分布（高斯分布）中随机抽样，通常具有均值为0和标准差为1的标准正态分布。 在 PyTorch 中，默认情况下，nn.Embedding 层的权重（即词嵌入矩阵）会在初始化时使用均匀分布或者正态分布进行随机初始化，具体取决于所选择的初始化方法。\n假设我们的初始化词嵌入矩阵如下所示（这里只是一个示例）：\n[\r[0.1, 0.2, 0.3] [0.4, 0.5, 0.6] [0.7, 0.8, 0.9] [1.0, 1.1, 1.2] ] 计算 “banana” 的向量坐标：由于 “banana” 的索引为1，因此我们只需找到词嵌入矩阵中的第二行，即：\n[\r[0.4, 0.5, 0.6]\r] 所以 “banana” 的向量坐标为 [0.4, 0.5, 0.6]。\n这就是 nn.Embedding 计算单词向量坐标的过程。它简单地根据单词的索引来查找词嵌入矩阵中对应的行。\nnn.Embedding 层在体现语义相关性方面的能力来自于它的训练过程和所使用的语料库。虽然 nn.Embedding 本身只是一个简单的查找表，它将每个单词映射到一个固定长度的向量，但是这些向量在训练过程中会根据模型任务的损失函数进行调整，以使得模型在语义上更加相似的单词在向量空间中更加接近。\n具体来说，当使用诸如语言模型、机器翻译、文本分类等任务进行端到端的训练时，nn.Embedding 层会根据模型的输出和损失函数的反馈进行调整，以最大程度地提高模型在任务上的性能。在这个过程中，如果两个单词在语义上相似（如 “apple” 和 “orange”），它们的词嵌入向量在向量空间中也会更加接近，以使得模型能够更好地捕捉它们之间的语义关系。\n另外，训练过程中使用的语料库也对词嵌入向量的语义相关性产生影响。如果语料库足够大且涵盖了各种语言使用情况，那么词嵌入向量往往能够更好地捕捉单词之间的语义关系。\n总的来说，nn.Embedding 层体现语义相关性的能力主要来自于两个方面：一是在训练过程中根据任务反馈调整词嵌入向量，使得语义相似的单词在向量空间中更加接近；二是在训练过程中使用的语料库的质量和规模。\npytorch rnn 以下是pytorch使用rnn最简单的一个例子，用来熟悉pytorch rnn 注意pytorch的rnn并不处理隐藏层到输出层的逻辑，他只是关注隐藏层的输出结果，如果需要将隐藏层转换为结果输出，可以在添加一个全连接层即可，这里暂不关注这部分\n#%%\rimport torch\rimport torch.nn as nn\r# 定义输入数据\rinput_size = 10 # 输入特征的维度\rsequence_length = 5 # 时间步个数\rbatch_size = 3 # 批次大小\r# 创建随机输入数据\r#输入数据的维度为(sequence_length, batch_size, input_size)，表示有sequence_length个时间步，\r#每个时间步有batch_size个样本，每个样本的特征维度为input_size。\rinput_data = torch.randn(sequence_length, batch_size, input_size)\rprint(\"输入数据\",input_data)\r# 定义RNN模型\r# 定义RNN模型时，我们指定了输入特征的维度input_size、隐藏层的维度hidden_size、隐藏层的层数num_layers等参数。\r# batch_first=False表示输入数据的维度中批次大小是否在第一个维度，我们在第二个维度上。\rrnn = nn.RNN(input_size, hidden_size=20, num_layers=1, batch_first=False)\r\"\"\"\r在前向传播过程中，我们将输入数据传递给RNN模型，并得到输出张量output和最后一个时间步的隐藏状态hidden。\r输出张量的大小为(sequence_length, batch_size, hidden_size)，表示每个时间步的隐藏层输出。\r最后一个时间步的隐藏状态的大小为(num_layers, batch_size, hidden_size)。\r\"\"\"\r# 前向传播，第二个参数h0未传递，默认为0\routput, hidden = rnn(input_data)\rprint(\"最后一个隐藏层\",hidden.shape)\rprint(\"输出所有隐藏层\",output.shape)\r# 打印每个隐藏层的权重和偏置项\r# weight_ih表示输入到隐藏层的权重，weight_hh表示隐藏层到隐藏层的权重，注意这里使出是转置的结果。\r# bias_ih表示输入到隐藏层的偏置，bias_hh表示隐藏层到隐藏层的偏置。\rfor name, param in rnn.named_parameters():\rif 'weight' in name or 'bias' in name:\rprint(name, param.data) 输出\n最后一个隐藏层 torch.Size([1, 3, 20])\r输出所有隐藏层 torch.Size([5, 3, 20]) 权重为什么是10行20列参数卷积神经网络的原理 数据最外层的行的长度决定了前向传播时间序列的个数。 这个input_size是输入数据的维度，比如一个单词转换为one-hot后列就是字典的特征长度 这个hidden_size是隐藏层神经元的个数也就是最终隐藏层输入的特征数。 num_layer中是堆叠的多层隐藏层。\n常见的结构 RNN（循环神经网络）常用的结果类型包括单输入单输出、单输入多输出、多输入多输出和多输入单输出。下面我将详细解释每种结果类型以及它们的应用场景。\n单输入单输出（Single Input Single Output，SISO）：这是最常见的RNN结果类型，输入是一个序列，输出是一个单一的预测值。例如，给定一段文本，预测下一个词语；给定一段时间序列数据，预测下一个时间步的值。这种结果类型适用于许多序列预测任务，如语言模型、时间序列预测等。 举个例子，假设我们要预测房屋价格，可能会使用多个特征，如房屋的面积、卧室数量、浴室数量等。这样，我们可以将这些特征组合成一个特征向量作为模型的输入，而模型的输出则是预测的房屋价格。因此，线性回归可以用来解决多特征到单个输出的问题，因此被称为单输入单输出模型。\n单输入多输出（Single Input Multiple Output，SIMO）：这种结果类型中，输入是一个序列，但输出是多个预测值。例如，给定一段文本，同时预测下一个词语和该词语的词性标签；给定一段音频信号，同时预测语音情感和说话者身份。这种结果类型适用于需要同时预测多个相关任务的情况。\n多输入多输出（Multiple Input Multiple Output，MIMO）：这种结果类型中，有多个输入序列和多个输出序列。例如，在机器翻译任务中，输入是源语言的句子序列，输出是目标语言的句子序列；在对话系统中，输入是用户的问题序列，输出是系统的回答序列。这种结果类型适用于需要处理多个输入和输出序列的任务，mimo有两种一种输入和输出个数相等和不相等。\n多输入单输出（Multiple Input Single Output，MISO）：这种结果类型中，有多个输入序列，但只有一个输出。例如，在图像描述生成任务中，输入是图像序列，输出是对图像的描述；在自动驾驶中，输入是多个传感器的数据序列，输出是车辆的控制命令。这种结果类型适用于需要将多个输入序列映射到单个输出序列的任务。\n线性回归是一种简单的机器学习模型，它的输入可以是多个特征，但是输出只有一个。这里的“单输入单输出”是指模型的输入是一个向量（多个特征的组合），输出是一个标量（一个预测值）。在线性回归中，我们通过对输入特征进行线性组合，得到一个预测值。因此，尽管输入可以是多个元素，但输出只有一个。\n双向循环神经网络 普通的RNN只能依据之前时刻的时序信息来预测下一时刻的输出，但在有些问题中，当前时刻的输出不仅和之前的状态有关，还可能和未来的状态有关系。\n比如预测一句话中缺失的单词不仅需要根据前文来判断，还需要考虑它后面的内容，真正做到基于上下文判断。\nBRNN有两个RNN上下叠加在一起组成的，输出由这两个RNN的状态共同决定。 先对图片和公式中的符号集中说明，需要时方便查看：\n$h_t^1$表示t 时刻，Cell1 中从左到右获得的 memory(信息); $W^1,U^1$ 表示图中 Cell1 的可学习参数,W是隐藏层的参数U是输入层参数； $f_1$ 表示 Cell1 的激活函数； $h_t^2$ 表示 t 时刻，Cell2 中从右到左获得的 memory; $W^2$,$U^2$ 表示图中 Cell2 的可学习参数； $f_2$ 表示 Cell2 的激活函数； $V$ 是输出层的参数，可以理解为 MLP; $f_3$ 是输出层的激活函数； $y_t$ 是 t 时刻的输出值； 在图1-1中，对于 t 时刻的输入$x_t$ ，可以结合从左到右的 memory $h^1_{t-1}$ , 获得当前时刻的 memory $h^1_t$: 同理也可以结合从右到左的 memory $h^2_{t−1}$ , 获得当前时刻的 memory $h^2_t$: 然后将 $h^1_t$ 和 $h^2_t$ 首尾级联在一起通过输出层网络 $V$ 得到输出 $y_t$ : 这样对于任何一个时刻 t 可以看到从不同方向获得的 memory, 使模型更容易优化，加速了模型的收敛速度。\npytorch rnn 下面是一个使用PyTorch中nn.RNN模块实现双向RNN的最简单例子：\nimport torch\rimport torch.nn as nn\r# 定义输入数据\rinput_size = 10 # 输入特征的维度\rsequence_length = 5 # 时间步个数\rbatch_size = 3 # 批次大小\r# 创建随机输入数据\rinput_data = torch.randn(sequence_length, batch_size, input_size)\r# 定义双向RNN模型\rrnn = nn.RNN(input_size, hidden_size=20, num_layers=1, batch_first=False, bidirectional=True)\r# 前向传播\routput, hidden = rnn(input_data)\r# 输出结果\rprint(\"输出张量大小：\", output.size())\rprint(\"最后一个时间步的隐藏状态大小：\", hidden.size()) 输出\n输出张量大小： torch.Size([5, 3, 40])\r最后一个时间步的隐藏状态大小： torch.Size([2, 3, 20]) 这个例子中，输入数据的维度和之前的例子相同。\n定义双向RNN模型时，我们在RNN模型的参数中设置bidirectional=True，表示我们希望构建一个双向RNN模型。\n在前向传播过程中，我们将输入数据传递给双向RNN模型，并得到输出张量output和最后一个时间步的隐藏状态hidden。输出张量的大小为(sequence_length, batch_size, hidden_sizenum_directions)，其中num_directions为2，表示正向和反向两个方向。最后一个时间步的隐藏状态的大小为(num_layersnum_directions, batch_size, hidden_size)。\n双向RNN可以同时利用过去和未来的信息，可以更好地捕捉到时间序列数据中的特征。你可以根据需要调整输入数据的大小、RNN模型的参数等进行实验。\n双向RNN的输出通常是正向和反向隐藏状态的组合，它们被存储在一个数组中。具体来说，如果使用PyTorch中的nn.RNN模块实现双向RNN，输出张量的形状将是(sequence_length,batch_size, hidden_size * 2)，其中hidden_size * 2表示正向和反向隐藏状态的大小之和。这个输出张量包含了每个时间步上正向和反向隐藏状态的信息，可以在后续的任务中使用。 双向rnn的最后的隐藏层大小是(2,, batch_size, hidden_size)\nDeep RNN(多层 RNN) 前文我们介绍的 RNN，是数据在时间维度上的变换。不论时间维度多长，只有一个 RNN 模块, 即只有一组待学习参数 (W, U)，属于单层 RNN。deep RNN 也叫做多层 RNN，顾名思义它由多个 RNN 级联组成，是输入数据在空间维度上变换。如图, 这是 L 层的 RNN 架构。每一层是一个单独的RNN，共有L个RNN。 在每一层的水平方向，只有一组可学习参数，如第 $l$ 层的参数$W^lU^l$。水平方向是数据沿着时间维度变换，变换机制与单个 RNN 的机制一致,具体参考式上一篇文章。在每个时刻 t 的垂直方向，共有 L 组可学习参数( $W^i,U^i$) i = 1, 2, …, L。在第 $l$ 层的第 t 时刻 Cell 的输入数据来自 2 个方向：一个是来自上一层的输出 $h^{l−1}t$ : 一个是来自第 $l$ 层，$t − 1$ 时刻的 memory 数据 $h^l{t−1}$ : 所以 Cell 的输出 $h^l_t$： 本质上，Deep RNN 在单个 RNN 的基础上，将当前时刻的输入修改为上层的输出。这样 RNN 便完成了空间上的数据变换。额外提一下：DeepRNN的每一层也可以是一个双向RNN。\npytorch rnn 下面是一个使用nn.RNN模块实现多层RNN的最简单例子：\nimport torch\rimport torch.nn as nn\r# 定义输入数据和参数\rinput_size = 5\rhidden_size = 10\rnum_layers = 2\rbatch_size = 3\rsequence_length = 4\r# 创建输入张量\rinput_tensor = torch.randn(sequence_length, batch_size, input_size)\r# 创建多层RNN模型\rrnn = nn.RNN(input_size, hidden_size, num_layers)\r# 前向传播\routput, hidden = rnn(input_tensor)\r# 打印输出张量和隐藏状态的大小\rprint(\"Output shape:\", output.shape)\rprint(\"Hidden state shape:\", hidden.shape) 在上面的例子中，我们首先定义了输入数据的维度、RNN模型的参数（输入大小、隐藏状态大小和层数），以及批次大小和序列长度。然后，我们创建了一个输入张量，其形状为(sequence_length, batch_size, input_size)。接下来，我们使用nn.RNN模块创建一个多层RNN模型，其中包含两层。最后，我们通过将输入张量传递给RNN模型的前向方法来进行前向传播，并打印输出张量和隐藏状态的大小。\n请注意，输出张量的形状为(sequence_length, batch_size, hidden_size)，其中sequence_length和batch_size保持不变，hidden_size是隐藏状态的大小。隐藏状态的形状为(num_layers, batch_size, hidden_size)，其中num_layers是RNN模型的层数。\nRNN缺点 梯度爆炸和消失问题 实践中前面介绍的几种RNNs并不能很好的处理较长的序列，RNN在训练中很容易发生梯度爆炸和梯度消失，这导致梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。\n通常来说，梯度爆炸更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。\n梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：\n1、合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。\n2、使用relu代替sigmoid和tanh作为激活函数。。\n3、使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法\n短期记忆 假如需要判断用户的说话意图（问天气、问时间、设置闹钟…），用户说了一句“what time is it？”我们需要先对这句话进行分词： 然后按照顺序输入 RNN ，我们先将 “what”作为 RNN 的输入，得到输出「01」 然后，我们按照顺序，将“time”输入到 RNN 网络，得到输出「02」。\n这个过程我们可以看到，输入 “time” 的时候，前面 “what” 的输出也产生了影响（隐藏层中有一半是黑色的）。 以此类推，前面所有的输入都对未来的输出产生了影响，大家可以看到圆形隐藏层中包含了前面所有的颜色。如下图所示： 当我们判断意图的时候，只需要最后一层的输出「05」，如下图所示： RNN 的缺点也比较明显 通过上面的例子，我们已经发现，短期的记忆影响较大（如橙色区域），但是长期的记忆影响就很小（如黑色和绿色区域），这就是 RNN 存在的短期记忆问题。\nRNN 有短期记忆问题，无法处理很长的输入序列 训练 RNN 需要投入极大的成本 RNN 的优化算法 LSTM – 长短期记忆网络 RNN 是一种死板的逻辑，越晚的输入影响越大，越早的输入影响越小，且无法改变这个逻辑。 LSTM 做的最大的改变就是打破了这个死板的逻辑，而改用了一套灵活了逻辑——只保留重要的信息。 简单说就是：抓重点！ 举个例子，我们先快速的阅读下面这段话： 当我们快速阅读完之后，可能只会记住下面几个重点： LSTM 类似上面的划重点，他可以保留较长序列数据中的「重要信息」，忽略不重要的信息。这样就解决了 RNN 短期记忆的问题。\n原理 原始RNN的隐藏层只有一个状态，即h，它对于短期的输入非常敏感。那么如果我们再增加一个门（gate）机制用于控制特征的流通和损失，即c，让它来保存长期的状态，这就是长短时记忆网络(Long Short Term Memory，LSTM)。 新增加的状态c，称为单元状态。我们把LSTM按照时间维度展开： 其中图像上的标识$\\sigma$标识使用sigmod激活到[0-1],$\\tanh$激活到[-1,1] ⨀ 是一个数学符号，表示逐元素乘积（element-wise product）或哈达玛积（Hadamard product）。当两个相同维度的矩阵、向量或张量进行逐元素相乘时，可以使用 ⨀ 符号来表示。\n例如，对于两个向量 [a1,a2,a3] ⨀ [b1, b2, b3]=[a1b1,a2b2,a3*b3]，它们的逐元素乘积可以表示 可以看到在t时刻，\nLSTM的输入有三个：当前时刻网络的输出值$x_t$、上一时刻LSTM的输出值$h_{t−1}$、以及上一时刻的记忆单元向量$c_{t−1}$；\nLSTM的输出有两个：当前时刻LSTM输出值$h_t$、当前时刻的隐藏状态向量$h_t$、和当前时刻的记忆单元状态向量$c_t$。\n注意：记忆单元c在LSTM 层内部结束工作，不向其他层输出。LSTM的输出仅有隐藏状态向量h。\nLSTM 的关键是单元状态，即贯穿图表顶部的水平线，有点像传送带。这一部分一般叫做单元状态（cell state）它自始至终存在于LSTM的整个链式系统中。 遗忘门 $f_t$叫做遗忘门，表示$C_{t−1}$的哪些特征被用于计算$C_t$。$f_t$是一个向量，向量的每个元素均位于(01)范围内。通常我们使用 sigmoid 作为激活函数，sigmoid 的输出是一个介于于(01)区间内的值，但是当你观察一个训练好的LSTM时，你会发现门的值绝大多数都非常接近0或者1，其余的值少之又少。 输入门 $C_t$ 表示单元状态更新值，由输入数据$x_t$和隐节点$h_{t−1}$经由一个神经网络层得到，单元状态更新值的激活函数通常使用tanh。 $i_t$叫做输入门，同 $f_t$ 一样也是一个元素介于(0~1)区间内的向量，同样由$x_t$和$h_{t−1}$经由sigmoid激活函数计算而成 输出门 最后，为了计算预测值$y^t$和生成下个时间片完整的输入，我们需要计算隐节点的输出 $h_t$。 lstm写诗 首先我们研究下pytorch中lstm的用法 单层lstm\nsequence_length =3\rbatch_size =2\rinput_size =4\r#这里如果是输入比如[张三,李四，王五]，一般实际使用需要通过embedding后生成一个[时间步是3，批量1（这里是1，但是如果是真实数据集可能有分批处理，就是实际的批次值）,3（三个值的坐标表示一个张三或者李四）]\rinput=t.randn(sequence_length,batch_size,input_size)\rlstmModel=nn.LSTM(input_size,3,1)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, (h, c) =lstmModel(input)\r#因为是3个时间步，每个时间步都有一个隐藏层，每个隐藏层都有2条数据，隐藏层的维度是3，最终(3,2,3)\rprint(\"LSTM隐藏层输出的维度\",output.shape)\r#\rprint(\"LSTM隐藏层最后一个时间步输出的维度\",h.shape)\rprint(\"LSTM隐藏层最后一个时间步细胞状态\",c.shape) 输出\nLSTM隐藏层输出的维度 torch.Size([3, 2, 3])\rLSTM隐藏层最后一个时间步输出的维度 torch.Size([1, 2, 3])\rLSTM隐藏层最后一个时间步细胞状态 torch.Size([1, 2, 3]) 双层lstm\nsequence_length =3\rbatch_size =2\rinput_size =4\rinput=t.randn(sequence_length,batch_size,input_size)\rlstmModel=nn.LSTM(input_size,3,num_layers=2)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, (h, c) =lstmModel(input)\rprint(\"2层LSTM隐藏层输出的维度\",output.shape)\rprint(\"2层LSTM隐藏层最后一个时间步输出的维度\",h.shape)\rprint(\"2层LSTM隐藏层最后一个时间步细胞状态\",c.shape) 输出： 2层LSTM隐藏层输出的维度 torch.Size([3, 2, 3]) 2层LSTM隐藏层最后一个时间步输出的维度 torch.Size([2, 2, 3]) 2层LSTM隐藏层最后一个时间步细胞状态 torch.Size([2, 2, 3])\n2层的话输出的是最后一层的隐藏层的输出，h，c是一个时间步就有两层的隐藏层和记忆细胞\n开始写诗的例子 这是项目的目录结构 加载数据 实验数据来自Github上中文爱好者收集的5万多首唐诗，作者在此基础上进行了一些数据处理，由于数据处理很耗时间，且不是pytorch学习的重点，这里省略。作者提供了一个numpy的压缩包tang.npz，下载地址 数据具体结构可参考，以下代码main部分\nfrom torch.utils.data import Dataset,DataLoader\rimport numpy as np\rclass PoetryDataset(Dataset):\rdef __init__(self,root):\rself.data=np.load(root, allow_pickle=True)\rdef __len__(self):\rreturn len(self.data[\"data\"])\rdef __getitem__(self, index):\rreturn self.data[\"data\"][index]\rdef getData(self):\rreturn self.data[\"data\"],self.data[\"ix2word\"].item(),self.data[\"word2ix\"].item()\rif __name__==\"__main__\":\rdatas=PoetryDataset(\"./tang.npz\").data\r# data是一个57580 * 125的numpy数组，即总共有57580首诗歌，每首诗歌长度为125个字符（不足125补空格，超过125的丢弃）\rprint(datas[\"data\"].shape)\r#这里都字符已经转换成了索引\rprint(datas[\"data\"][0])\r# 使用item将numpy转换为字典类型，ix2word存储这下标对应的字,比如{0: '憁', 1: '耀'}\rix2word = datas['ix2word'].item()\rprint(ix2word)\r# word2ix存储这字对应的小标，比如{'憁': 0, '耀': 1}\rword2ix = datas['word2ix'].item()\rprint(word2ix)\r# 将某一首古诗转换为索引表示,转换后：[5272, 4236, 3286, 6933, 6010, 7066, 774, 4167, 2018, 70, 3951]\rstr=\"床前明月光，疑是地上霜\"\rprint([word2ix[i] for i in str])\r#将第一首古诗打印出来\rprint([ix2word[i] for i in datas[\"data\"][0]]) 定义模型 import torch.nn as nn\rclass Net(nn.Module):\r\"\"\"\r:param vocab_size 表示输入单词的格式\r:param embedding_dim 表示将一个单词映射到embedding_dim维度空间\r:param hidden_dim 表示lstm输出隐藏层的维度\r\"\"\"\rdef __init__(self, vocab_size, embedding_dim, hidden_dim):\rsuper(Net, self).__init__()\rself.hidden_dim = hidden_dim\r#Embedding层，将单词映射成vocab_size行embedding_dim列的矩阵，一行的坐标代表第一行的词\rself.embeddings = nn.Embedding(vocab_size, embedding_dim)\r#两层lstm，输入词向量的维度和隐藏层维度\rself.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=2, batch_first=False)\r#最后将隐藏层的维度转换为词汇表的维度\rself.linear1 = nn.Linear(self.hidden_dim, vocab_size)\rdef forward(self, input, hidden=None):\r#获取输入的数据的时间步和批次数\rseq_len, batch_size = input.size()\r#如果没有传入上一个时间的隐藏值，初始一个，注意是2层\rif hidden is None:\rh_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\rc_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\relse:\rh_0, c_0 = hidden\r#将输入的数据embeddings为（input行数,embedding_dim）\rembeds = self.embeddings(input) # (seq_len, batch_size, embedding_dim), (1,1,128)\routput, hidden = self.lstm(embeds, (h_0, c_0)) #(seq_len, batch_size, hidden_dim), (1,1,256)\routput = self.linear1(output.view(seq_len*batch_size, -1)) # ((seq_len * batch_size),hidden_dim), (1,256) → (1,8293)\rreturn output, hidden 训练 下述代码：input, target = (data[:-1, :]), (data[1:, :])解释:\n在使用LSTM进行词预测时，输入和标签的设置是为了将输入序列和目标序列对齐。\n在语言模型中，我们希望根据前面的单词来预测下一个单词。因此，输入序列是前面的单词，而目标序列是下一个单词。\n考虑以下例子： 假设我们有一个句子：“I love deep learning.” 我们可以将其分解为以下形式的输入和目标序列： 输入序列：[“I”, “love”, “deep”] 目标序列：[“love”, “deep”, “learning”]\n在这个例子中，输入序列是前面的单词[“I”, “love”, “deep”]，而目标序列是相应的下一个单词[“love”, “deep”, “learning”]。\n在代码中，data是一个包含所有单词的数据集，其中每一行代表一个单词。将data切片为input和target时，我们使用data[:-1, :]作为输入序列，即除了最后一个单词。而data[1:, :]作为目标序列，即从第二个单词开始。\n这样设置输入和目标序列的目的是为了将输入和标签对齐，使得模型可以根据前面的单词来预测下一个单词。\nimport fire\rimport torch.nn as nn\rimport torch as t\rfrom data.dataset import PoetryDataset\rfrom models.model import Net\rnum_epochs=5\rdata_root=\"./data/tang.npz\"\rbatch_size=10\rdef train(**kwargs):\rdatasets=PoetryDataset(data_root)\rdata,ix2word,word2ix=datasets.getData()\rlenData=len(data)\rdata = t.from_numpy(data)\rdataloader = t.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=1)\r#总共有8293的词。模型定义：vocab_size, embedding_dim, hidden_dim = 8293 * 128 * 256\rmodel=Net(len(word2ix),128,256)\r#定义损失函数\rcriterion = nn.CrossEntropyLoss()\rmodel=model.cuda()\roptimizer = t.optim.Adam(model.parameters(), lr=1e-3)\riteri=0\rfilename = \"example.txt\"\rtotalIter=lenData*num_epochs/batch_size\rfor epoch in range(num_epochs): # 最大迭代次数为8\rfor i, data in enumerate(dataloader): # 一批次数据 128*125\rdata = data.long().transpose(0,1).contiguous() .cuda()\roptimizer.zero_grad()\rinput, target = (data[:-1, :]), (data[1:, :])\routput, _ = model(input)\rloss = criterion(output, target.view(-1)) # torch.Size([15872, 8293]), torch.Size([15872])\rloss.backward()\roptimizer.step()\riteri+=1\rif(iteri%500==0):\rprint(str(iteri+1)+\"/\"+str(totalIter)+\"epoch\")\rif (1 + i) % 1000 == 0: # 每575个batch可视化一次\rwith open(filename, \"a\") as file:\rfile.write(str(i) + ':' + generate(model, '床前明月光', ix2word, word2ix)+\"\\n\")\rt.save(model.state_dict(), './checkpoints/model_poet_2.pth')\rdef generate(model, start_words, ix2word, word2ix): # 给定几个词，根据这几个词生成一首完整的诗歌\rtxt = []\rfor word in start_words:\rtxt.append(word)\rinput = t.Tensor([word2ix['\u003cSTART\u003e']]).view(1,1).long() # tensor([8291.]) → tensor([[8291.]]) → tensor([[8291]])\rinput = input.cuda()\rhidden = None\rnum = len(txt)\rfor i in range(48): # 最大生成长度\routput, hidden = model(input, hidden)\rif i \u003c num:\rw = txt[i]\rinput = (input.data.new([word2ix[w]])).view(1, 1)\relse:\rtop_index = output.data[0].topk(1)[1][0]\rw = ix2word[top_index.item()]\rtxt.append(w)\rinput = (input.data.new([top_index])).view(1, 1)\rif w == '\u003cEOP\u003e':\rbreak\rreturn ''.join(txt)\rif __name__==\"__main__\":\rfire.Fire() 5epoch，10batch，普通pc，GTX1050,2GB显存，训练时间30分钟。 50epoch 128batch colab免费gpu 16GB显存，训练时间1小时\n测试 def test():\rdatasets = PoetryDataset(data_root)\rdata, ix2word, word2ix = datasets.getData()\rmodle = Net(len(word2ix), 128, 256) # 模型定义：vocab_size, embedding_dim, hidden_dim —— 8293 * 128 * 256\rif t.cuda.is_available() == True:\rmodle.cuda()\rmodle.load_state_dict(t.load('./checkpoints/model_poet_2.pth'))\rmodle.eval()\rname = input(\"请输入您的开头：\")\rtxt = generate(modle, name, ix2word, word2ix)\rprint(txt) 由于才训练了5epoch效果不是太好，可视化loss后可多次epoch看看效果，还有个问题，如果输入不变，生成的结果就是相同的，所以这个可能需要一个噪声干扰。 5epoch版本效果\n(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\\案例\\生成古诗\\tang\u003epython main.py test\r请输入您的开头：唧唧复唧唧\r唧唧复唧唧，不知何所如？君不见此地，不如此中生。一朝一杯酒，一日相追寻。一朝一杯酒，一醉一相逢。\r(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\\案例\\生成古诗\\tang\u003epython main.py test\r请输入您的开头：我儿小谦谦\r我儿小谦谦，不是天地间。有时有所用，不是无为名。有时有所用，不是无生源。有时有所用，不是无生源。 50epoch版本效果\n(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\\案例\\生成古诗\\tang\u003epython main.py test\r请输入您的开头：我家小谦谦\r我家小谦谦，今古何为郎。我生不相识，我心不可忘。我来不我见，我亦不得尝。君今不我见，我亦不足伤。\r(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\\案例\\生成古诗\\tang\u003epython main.py test\r请输入您的开头：床前明月光\r床前明月光，上客不可见。玉楼金阁深，玉瑟风光紧。玉指滴芭蕉，飘飘出罗幕。玉堂无尘埃，玉节凌风雷。\r(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\\案例\\生成古诗\\tang\u003epython main.py test\r请输入您的开头：唧唧复唧唧\r唧唧复唧唧，胡儿女卿侯。妾本邯郸道，相逢两不游。妾心不可再，妾意不能休。妾本不相见，妾心如有钩。 GRU Gated Recurrent Unit – GRU 是 LSTM 的一个变体。他保留了 LSTM 划重点，遗忘不重要信息的特点，在long-term 传播的时候也不会被丢失。\nLSTM 的参数太多，计算需要很长时间。因此，最近业界又提出了 GRU（Gated RecurrentUnit，门控循环单元）。GRU 保留了 LSTM使用门的理念，但是减少了参数，缩短了计算时间。\n相对于 LSTM 使用隐藏状态和记忆单元两条线，GRU只使用隐藏状态。异同点如下： GRU的计算图 GRU计算图，σ节点和tanh节点有专用的权重，节点内部进行仿射变换（“1−”节点输入x，输出1 − x） GRU 中进行的计算由上述 4 个式子表示（这里 xt和 ht−1 都是行向量），如图所示，GRU 没有记忆单元，只有一个隐藏状态h在时间方向上传播。这里使用r和z共两个门（LSTM 使用 3 个门），r称为 reset 门，z称为 update 门。\nr（reset门）**决定在多大程度上“忽略”过去的隐藏状态。根据公式2.3，如果r是 0，则新的隐藏状态h~仅取决于输入$x_t$。也就是说，此时过去的隐藏状态将完全被忽略。\nz（update门）**是更新隐藏状态的门，它扮演了 LSTM 的 forget 门和input 门两个角色。公式2.4 的(1−z)⊙$h_{t−1}$部分充当 forget 门的功能，从过去的隐藏状态中删除应该被遗忘的信息。z⊙$h^~$的部分充当 input 门的功能，对新增的信息进行加权。",
    "description": "@[toc]\n概述 循环神经网络（Recurrent Neural Network，RNN）是一种具有循环连接的神经网络结构，被广泛应用于自然语言处理、语音识别、时序数据分析等任务中。相较于传统神经网络，RNN的主要特点在于它可以处理序列数据，能够捕捉到序列中的时序信息。\nRNN的基本单元是一个循环单元（Recurrent Unit），它接收一个输入和一个来自上一个时间步的隐藏状态，并输出当前时间步的隐藏状态。在传统的RNN中，循环单元通常使用tanh或ReLU等激活函数。\n基本循环神经网络 原理 基本的 循环神经网络，结构由 输入层、一个隐藏层和输出层 组成。\n$x$是输入向量，$o$是输出向量，$s$表示隐藏层的值；$U$是输入层到隐藏层的权重矩阵，$V$是隐藏层到输出层的权重矩阵。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入$x$，还取决于上一次隐藏层的值$s$。权重矩阵W就是隐藏层上一次的值作为这一次的输入的权重。 将上图的基本RNN结构在时间维度展开(RNN是一个链式结构，每个时间片使用的是相同的参数,t表示t时刻)： 现在看上去就会清楚许多，这个网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。关键一点是$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t−1}$。 公式1：$s_t=f(U∗x_t+W∗s_{t−1}+B1)$ 公式2：$o_t=g(V∗s_t+B2)$\n式1是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次隐藏层值$S_{t−1}$作为这一次的输入的权重矩阵，f是激活函数。 式2是输出层的计算公式，V是输出层的权重矩阵，g是激活函数,B1,B2是偏置假设为0。 隐含层有两个输入，第一是U与$x_t$向量的乘积，第二是上一隐含层输出的状态$s_t−1$和W的乘积。等于上一个时刻计算的$s_t−1$需要缓存一下，在本次输入$x_t$一起计算，共同输出最后的$o_t$。\n如果反复把式1带入式2，我们将得到： 从上面可以看出，循环神经网络的输出值ot，是受前面历次输入值、、、、、、、、$x_t$、$x_{t−1}$、$x_{t−2}$、$x_{t−3}$、…影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。这样其实不好，因为如果太前面的值和后面的值已经没有关系了，循环神经网络还考虑前面的值的话，就会影响后面值的判断。\n上面是整个单向单层NN的前向传播过程\n为了更快理解输入x输入格式下面使用nlp中Word Embedding讲解下。\nWord Embedding 首先我们需要对输入文本x进行编码，使之成为计算机可以读懂的语言，在编码时，我们期望句子之间保持词语间的相似行，词的向量表示是进行机器学习和深度学习的基础。\nword embedding的一个基本思路就是，我们把一个词映射到语义空间的一个点，把一个词映射到低维的稠密空间，这样的映射使得语义上比较相似的词，他在语义空间的距离也比较近，如果两个词的关系不是很接近，那么在语义空间中向量也会比较远。\n如上图英语和西班牙语映射到语义空间，语义相同的数字他们在语义空间分布的位置是相同的 简单回顾一下word embedding,对于nlp来说，我们输入的是一个个离散的符号，对于神经网络来说，它处理的都是向量或者矩阵。所以第一步，我们需要把一个词编码成向量。最简单的就是one-hot的表示方法。如下图所示： python代码（one-hot），比如",
    "tags": [],
    "title": "深度学习05-RNN循环神经网络",
    "uri": "/docs/programming/ai/deep_learning/rnn/dl_05_rnn/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 框架学习",
    "content": "概述 PyTorch是一个基于Python的开源机器学习框架，由Facebook的人工智能研究团队开发并维护。它提供了丰富的工具和接口，用于构建和训练深度神经网络模型。\nPyTorch的主要特点和优势包括：\n动态图：PyTorch使用动态图机制，即在运行时构建计算图。这使得模型的构建和调试更加直观和灵活，能够更好地处理复杂的计算流程和动态控制流。\n简洁明了：PyTorch的API设计简洁明了，易于学习和使用。它提供了一系列高级接口，使得模型的构建、训练和评估变得更加简单和高效。\n强大的GPU加速支持：PyTorch能够利用GPU进行张量运算和模型训练，从而加快计算速度。它提供了简单易用的接口，使得在GPU上进行加速变得更加方便。\n灵活扩展：PyTorch支持自定义操作符和扩展，使得用户可以方便地实现和使用自己的模型组件和功能。\n相比之下，TensorFlow是由Google开发的另一个流行的深度学习框架。与PyTorch相比，TensorFlow的主要特点和优势包括：\n静态图：TensorFlow使用静态图机制，即在编译时构建计算图。这使得TensorFlow在模型运行时能够进行更多的优化和性能提升，适用于大规模的、计算密集型的任务。\n跨平台支持：TensorFlow可以在多种硬件和操作系统上运行，并且具有广泛的部署支持。它提供了TensorFlow Serving、TensorFlow Lite和TensorFlow.js等工具，使得模型的部署和移植更加方便。\n分布式训练支持：TensorFlow提供了分布式训练的功能，可以在多个设备和计算节点上进行模型训练，从而加快训练速度。\n生态系统和社区：TensorFlow具有庞大的生态系统和活跃的社区，提供了丰富的资源和支持，包括模型库、教程和论坛等。\n总的来说，PyTorch和TensorFlow都是优秀的深度学习框架，各有其特点和适用场景。PyTorch适合于快速原型开发、动态计算流程和小规模任务，而TensorFlow适合于大规模、计算密集型的任务和分布式训练。选择哪个框架取决于具体的需求和个人偏好。\n对于初学接触神经网络，建议先学pytorch，它提供的api接近理论概念，有动态图，方便调试，适合做研究使用，，由于最近chargpt的大火，Hugging Face的transforms是使用PyTorch的。Hugging Face是一个提供自然语言处理（NLP）模型和工具的平台，他们的Transformers库主要基于PyTorch实现，他的入门pytorch必须要有基础。这个库提供了一系列用于数据预处理和后处理的函数，可以方便地对文本数据进行转换和处理。\n环境准备 安装cuda和cudnn 一般pc电脑或者服务器都有nvida显卡，可以通过nvidia-smi命令查看。 其中python环境（3.8+版本），cuda和cudnn安装请参考：https://blog.csdn.net/liaomin416100569/article/details/130532993 安装后可以看到我的cuda version是11.2\n安装pytorch 考虑到版本向下兼容，不一定非要下载cuda=11.2对应的那个版本的torch，或许低于这个版本就可以。所以我就选择下载cuda11.1的版本。 以下是pytorch对应的稳定版的网址下载链接，可以根据需要找到对应的torch版本下载。cu版本就是gpu版本，不带cu的是cpu版本，https://download.pytorch.org/whl/torch_stable.html，搜索cu111 直接选择\npip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html 编写测试代码\nimport torch\rprint(torch.__version__)\r#cuda是否可用，如果返回True，表示正常可用gpu\rprint(torch.cuda.is_available())\rprint(torch.cuda.device_count())\rx1=torch.rand(5,3)\r#把x1转换gpu0的tensor\rx1=x1.cuda(0)\rprint(x1) 测试运行\n1.9.1+cu111\rTrue 1\rtensor([[0.5761, 0.7046, 0.2004],\r[0.6030, 0.3285, 0.5602],\r[0.6852, 0.6602, 0.0033],\r[0.4213, 0.7174, 0.0591],\r[0.5276, 0.4181, 0.8665]], device='cuda:0') 使用colab 如果自己沒有gpu的環境，可以使用cpu進行學習，但是學到模型训练还是要gpu，如果有外网环境，可以考虑使用google提供的colab，主要是免费，gpu能给到16GB，系统磁盘100gb，googledrive15gb,非常良心了，注意：colab不支持和本地pycharm远程调试，模型比较大时磁盘是个大问题，根据Colab的官方规定，每个用户每天可以使用Colab资源的总时间为12小时。这意味着，一旦你的Colab会话运行了12小时，你将无法继续使用Colab的计算资源。当然，你可以重新启动一个新的Colab会话来继续使用。\n申请colab 首先你先需要申请一个googledrive（类似百度网盘），准备一个gmail邮箱就可以申请，申请完成后默认有15GB的存储空间，如果需要更多就需要购买了。 在我的云端云盘右侧空白的地方点击邮件，关联更多应用，搜索colab安装，安装后右键就会多了 一个google colaboratory，点击进去，会弹出一个notebook的开发环境。 可以点击左侧的文件夹，自动分配一个计算资源 查看分配的机器 点击目录..可以进入到系统根目录,点击工具栏第三个图标挂载googledrive 在content notepadbook中执行命令查看资源信息，和右侧的资源坐下对比，可以确认系统是ubuntu，内存是12.7gb，磁盘107gb，没有gpu 你的notepad文件内容，默认是新建在googledrive的根目录下，你可以双击文件直接进入notebook 挂载googledrive 挂载完成后/content多了一个drive目录，MyDrive内容就和googledrive是一致的。 colab申请gpu 点击右上角的view resouce 选择change runtime type,选择免费的GPU或者TPU 再次使用nvdia-smi确认 google的另一款免费的实验免费gpu是kaggle，也可以注册使用，比colab更简单方便，同时可直接引用其他开源模型。\nnotebook语法 python语法 可直接在快中执行python语法。 使用！执行shell命令。 比如使用 !bash进入一个交互shell行，exit退出 可以使用 ！任意shell命令执行， 基础 张量 在PyTorch中，除了张量（Tensor）之外，还有很多其他的数据类型和类。以下是一些常见的PyTorch数据类型和类：\nTensor（张量）：张量是PyTorch的核心数据结构，类似于数组，可以存储和操作多维数据。\nVariable（变量）：Variable是对张量的封装，用于自动求导。\nnn.Module：nn.Module是PyTorch中用于构建神经网络模型的基类，可以包含多个层和操作。\nnn.Parameter：nn.Parameter是Variable的子类，用于定义模型中需要进行学习的参数。\nDataLoader：DataLoader是一个用于加载数据的实用类，可以方便地对数据进行批量处理和迭代。\nOptimizer（优化器）：优化器是用于更新模型参数的算法，例如SGD、Adam等。\nLoss Function（损失函数）：损失函数用于衡量模型预测结果与真实标签之间的差异，例如交叉熵损失、均方误差等。\n这些是PyTorch中常用的一些数据类型和类，它们提供了丰富的功能来支持深度学习任务的实现和训练。\n定义 import torch as t\rimport numpy as np\r#构建5*3数组,只是分配了空间未初始化\rresult=t.Tensor(5,3)\rprint(result)\r#这里产生个0-1之间的tensor张量，并且初始化\rx1=t.rand(5,3)\ry1=t.rand(5,3)\rprint(x1)\rprint(x1.size())\rresult=x1+y1\rprint(result) numpy转换 #产生5个1的一维数组tensor转换成numpy\rprint(t.ones(5).numpy())\r#将numpy数组转换为tensor\rprint(t.from_numpy(np.array([2,2,2,]))) 数学函数 随机数 下面是一些常见的PyTorch函数，可以用于生成随机数：\ntorch.randn(size, dtype=None, device=None) - 从标准正态分布中返回随机样本。返回一个具有给定大小的张量，其中每个元素独立地从标准正态分布中抽取。可以通过指定size参数来指定张量的形状。 例子：\nx = torch.randn(3, 3)\rprint(x) torch.rand(size, dtype=None, device=None) - 从均匀分布中返回随机样本。返回一个具有给定大小的张量，其中每个元素独立地从一个均匀分布上抽取。可以通过指定size参数来指定张量的形状。 例子：\nx = torch.rand(3, 3)\rprint(x)` torch.randint(low, high, size, dtype=None, device=None) - 从离散均匀分布中返回随机整数。返回一个具有给定大小的张量，其中每个元素独立地从一个均匀分布上抽取。可以通过指定low和high参数来指定取值范围。 例子：\nx = torch.randint(0, 10, (3, 3))\rprint(x) torch.normal(mean, std, size, dtype=None, device=None) - 从正态分布中返回随机样本。返回一个具有给定大小的张量，其中每个元素独立地从一个正态分布中抽取。可以通过指定mean和std参数来指定正态分布的均值和标准差。 例子：\nx = torch.normal(0, 1, (3, 3))\rprint(x) 这些函数可以帮助您在PyTorch中生成随机数。请根据您的需求选择适当的函数。\n计算函数 常用的数学计算函数 当然，下面是PyTorch中一些常用的数学函数的清单，每个都附有简短的描述和一个调用小例子：\ntorch.abs(input): 返回输入张量的绝对值。示例：torch.abs(torch.tensor([-1, 2, -3]))。 torch.sqrt(input): 返回输入张量的平方根。示例：torch.sqrt(torch.tensor([4, 9, 16]))。 torch.exp(input): 计算输入张量的指数函数。示例：torch.exp(torch.tensor([1, 2, 3]))。 torch.log(input): 计算输入张量的自然对数。示例：torch.log(torch.tensor([1, 10, 100]))。 torch.sin(input): 计算输入张量的正弦值。示例：torch.sin(torch.tensor([0, math.pi/2, math.pi]))。 torch.cos(input): 计算输入张量的余弦值。示例：torch.cos(torch.tensor([0, math.pi/2, math.pi]))。 torch.tan(input): 计算输入张量的正切值。示例：torch.tan(torch.tensor([0, math.pi/4, math.pi/2]))。 torch.sigmoid(input): 计算输入张量的Sigmoid函数。示例：torch.sigmoid(torch.tensor([0, 1, 2]))。 torch.relu(input): 应用ReLU激活函数，即max(0, input)。示例：torch.relu(torch.tensor([-1, 0, 1]))。 torch.softmax(input, dim): 计算输入张量在指定维度上的Softmax函数。示例：torch.softmax(torch.tensor([[1, 2], [3, 4]]), dim=1)。 torch.mean(input): 计算输入张量的均值。示例：torch.mean(torch.tensor([1, 2, 3]))。 torch.sum(input): 计算输入张量的总和。示例：torch.sum(torch.tensor([1, 2, 3]))。 torch.max(input): 返回输入张量中的最大值。示例：torch.max(torch.tensor([1, 2, 3]))。 torch.min(input): 返回输入张量中的最小值。示例：torch.min(torch.tensor([1, 2, 3]))。 torch.argmax(input): 返回输入张量中最大值的索引。示例：torch.argmax(torch.tensor([1, 2, 3]))。 torch.argmin(input): 返回输入张量中最小值的索引。示例：torch.argmin(torch.tensor([1, 2, 3]))。 torch.sort(input): 对输入张量进行排序。示例：torch.sort(torch.tensor([3, 1, 2]))。 torch.clamp(input, min, max): 将输入张量的值限制在指定范围内。示例：torch.clamp(torch.tensor([1, 2, 3]), min=2, max=3)。 torch.round(input): 对输入张量进行四舍五入。示例：torch.round(torch.tensor([1.1, 2.4, 3.6]))。 torch.floor(input): 向下取整，返回不大于输入张量的最大整数。示例：torch.floor(torch.tensor([1.1, 2.4, 3.6]))。 矩阵处理函数 以下是PyTorch中常用的20个矩阵处理函数的清单及其描述：\ntorch.mm(): 计算两个矩阵的乘积。 示例：torch.mm(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5], [6]]))返回tensor([[17], [39]])\ntorch.matmul(): 计算两个张量的矩阵乘积。 示例：torch.matmul(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5], [6]]))返回tensor([[17], [39]])\ntorch.transpose(): 返回输入张量的转置。 示例：torch.transpose(torch.tensor([[1, 2], [3, 4]]), 0, 1)返回tensor([[1, 3], [2, 4]])\ntorch.mm(): 计算一个矩阵和一个向量的乘积。 示例：torch.mm(torch.tensor([[1, 2], [3, 4]]), torch.tensor([5, 6]))返回tensor([17, 39])\ntorch.trace(): 返回矩阵的迹。 示例：torch.trace(torch.tensor([[1, 2], [3, 4]]))返回tensor(5)\ntorch.det(): 计算矩阵的行列式。 示例：torch.det(torch.tensor([[1, 2], [3, 4]]))返回tensor(-2)\ntorch.svd(): 对矩阵进行奇异值分解。 示例：torch.svd(torch.tensor([[1, 2], [3, 4]]))返回(tensor([[-0.4046, -0.9145], [-0.9145, 0.4046]]), tensor([5.4645, 0.3650]), tensor([[-0.5760, -0.8174], [-0.8174, 0.5760]]))\ntorch.eig(): 计算矩阵的特征值和特征向量。 示例：torch.eig(torch.tensor([[1, 2], [3, 4]]))返回(tensor([[0.3723, 0.0000], [5.6277, 0.0000]]), tensor([]))\ntorch.inverse(): 计算矩阵的逆。 示例：torch.inverse(torch.tensor([[1, 2], [3, 4]]))返回tensor([[-2.0000, 1.0000], [ 1.5000, -0.5000]])\ntorch.diag(): 返回矩阵的对角线元素。 示例：torch.diag(torch.tensor([[1, 2], [3, 4]]))返回tensor([1, 4])\ntorch.diag_embed(): 将一维张量转化为对角矩阵。 示例：torch.diag_embed(torch.tensor([1, 2, 3]))返回tensor([[[1, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 2, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 3]]])\ntorch.einsum(): 执行爱因斯坦求和约定。 示例：torch.einsum(‘ij,jk-\u003eik’, torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))返回tensor([[19, 22], [43, 50]])\ntorch.flatten(): 对输入张量进行扁平化操作。 示例：torch.flatten(torch.tensor([[1, 2], [3, 4]]))返回tensor([1, 2, 3, 4])\ntorch.cat(): 沿指定维度拼接张量。 示例：torch.cat((torch.tensor([[1, 2]]), torch.tensor([[3, 4]])), dim=0)返回tensor([[1, 2], [3, 4]])\ntorch.stack(): 沿新维度拼接张量。 示例：torch.stack((torch.tensor([1, 2]), torch.tensor([3, 4])), dim=0)返回tensor([[1, 2], [3, 4]])\ntorch.split(): 沿指定维度分割张量。 示例：torch.split(torch.tensor([[1, 2, 3, 4]]), 2, dim=1)返回(tensor([[1, 2]]), tensor([[3, 4]]))\ntorch.chunk(): 将张量分割成指定数量的块。 示例：torch.chunk(torch.tensor([[1, 2, 3, 4]]), 2, dim=1)返回(tensor([[1, 2]]), tensor([[3, 4]]))\ntorch.reshape(): 改变张量的形状。 示例：torch.reshape(torch.tensor([[1, 2, 3, 4]]), (2, 2))返回tensor([[1, 2], [3, 4]])\ntorch.squeeze(): 压缩张量中尺寸为1的维度。 示例：torch.squeeze(torch.tensor([[[1], [2]]]))返回tensor([1, 2])\ntorch.unsqueeze(): 在指定位置插入尺寸为1的新维度。 示例：torch.unsqueeze(torch.tensor([1, 2]), dim=1)返回tensor([[1], [2]]\ntorch.view是PyTorch中的一个函数，用于改变张量的形状，即对张量进行重塑操作。它的作用类似于NumPy中的reshape函数。 x = torch.tensor([1, 2, 3, 4, 5, 6]) y = x.view(2, 3)\ntorch.permute函数是PyTorch中的一个函数，用于重新排列张量的维度顺序。它的作用是交换或重新组织张量的维度。 在下述示例中，原始张量x的维度顺序为(2, 3, 4)，通过使用permute(2, 0, 1)，将维度顺序重新排列为(4, 2, 3)，得到了新的张量 也就是维度2换成函数索引0个维度,0维度的换成1,1维度的换成2\nimport torch\rx = torch.randn(2, 3, 4) # 创建一个形状为(2, 3, 4)的张量\rx_permuted = x.permute(2, 0, 1) # 将维度顺序重新排列为(4, 2, 3)\rprint(x_permuted.shape) # 输出: torch.Size([4, 2, 3]) 自动梯度 深度学习的算法本质上是通过反向传播求导数，PyTorch的Autograd模块实现了此功能。在Tensor上的所有操作，Autograd都能为它们自动提供微分，避免手动计算导数的复杂过程。\n在PyTorch中，Tensor和Variable都可以求梯度，但是它们有一些区别。\n在旧版本的PyTorch中，Variable是一个Tensor的封装，它包含了Tensor的数据以及关于这个Tensor的梯度信息。在新版本的PyTorch中，Variable已经被弃用，官方建议直接使用Tensor。\nPyTorch中的Tensor对象有一个属性.requires_grad，默认为False。当你将其设置为True时，表示希望计算这个Tensor的梯度。在进行反向传播计算梯度时，所有具有.requires_grad=True的Tensor都会被保留梯度信息。\n当你使用Tensor进行计算时，可以调用.backward()方法来计算相对于这个Tensor的梯度。梯度信息会保存在.grad属性中。\n所以，Variable的作用可以用Tensor的.requires_grad属性来代替，而且在新版本的PyTorch中，官方建议直接使用Tensor进行梯度计算。 Variable和Tensor主要包含三个属性。\ndata：保存计算后结果对应的的Tensor。 grad：保存data对应的梯度，是Tensor，它和data的形状一样。 grad fn：指向一个Function对象，这个Function用来反向传播计算输入的梯度,requires_grad=True x=Variable(t.from_numpy(np.array([[1,2],[2,4]],dtype=float)),requires_grad=True)\rprint(\"张量x=\",x)\ry=x.sum()\rprint(\"输出y\",y)\rprint(\"输出y的梯度\",y.grad) #注意结果是y，所以y是没有梯度的，y进行反向传播，可以求导x的导数\rprint(\"y的反向梯度函数\",y.grad_fn)\rprint(\"y的数据\",y.data)\r# 因为y=x[0][0]+x[0][1]+x[1][0]++x[1][1],可以认为四个数是四个变量，比如求每个变量的导数\r# 假设是y=x1+x2+x3+x4 x1是自变量，x1的导数就是1，同理x2的导数也是1，最后就得到了4个1\r# 注意每个点都有个梯度\ry.backward() #反向传播计算梯度\rprint(x.grad) 输出\n张量x= tensor([[1., 2.],\r[2., 4.]], dtype=torch.float64, requires_grad=True)\r输出y tensor(9., dtype=torch.float64, grad_fn=\u003cSumBackward0\u003e)\r输出y的梯度 None\ry的反向梯度函数 \u003cSumBackward0 object at 0x0000025F8F2C8C10\u003e\ry的数据 tensor(9., dtype=torch.float64)\rx的梯度 tensor([[1., 1.],\r[1., 1.]], dtype=torch.float64) 案例 案例1:计算$x^2*e^x$导数\n#计算x**2*e^x导数\r#dx=2*x*e^x+x**2*e^x\r#定义fx的函数逻辑\rdef f(x):\rreturn x**2*t.exp(x)\r#我们预先知道他的梯度函数是\rdef graddx(x):\rreturn 2*x*t.exp(x)+x**2*t.exp(x)\r#生成一个3*3随机矩阵，求梯度\rx=Variable(t.rand(3,3),requires_grad=True)\rprint(graddx(x))\r#使用反向传播求梯度\ry=f(x)\ry.backward(t.ones(y.size()))\rprint(x.grad) 案例2: 使用自动梯度梯度下降拟合最佳直线\n#使用autograd计算梯度，来实现线性回归\rimport torch as t\rfrom torch.autograd import Variable as V\rimport matplotlib.pyplot as plot\rt.manual_seed(42)\r# 使用自动梯度实现线性回归\rx=t.randn(100,1)\ry=3*x+2+t.randn(100,1) #实际值上加上一些随机噪点\rplot.plot(x,y,'.')\rplot.show()\rw=V(t.randn(1,1),requires_grad=True)\rb=V(t.randn(1),requires_grad=True)\rdef fx(x):\rreturn t.mm(x,w)+b\r#损失函数 def lossf(y_pre,y):\rreturn t.mean((y_pre-y)**2)\r#训练100次，100次梯度下降，计算到最小损失时的w和b\rw_gra_last,b_gra_last=0,0\rfor epoch in range(100):\ry_pre=fx(x)\rloss=lossf(y_pre,y)\rloss.backward()\rw_gra=w.grad.data\rb_gra=b.grad.data\rw_gra_last=w_gra.clone()\rb_gra_last=b_gra.clone()\r#如果梯度小于某个值直接退出\rif t.abs(w_gra)\u003c=1e-8 and t.abs(b_gra)\u003c=1e-8:\rbreak;\rlearn_rate=0.01\r#注意w.sub_是不行的因为w是requires_grad=True，需要后面的参数都是设置为：requires_grad=True\r#所以只能是更新他的data\rw.data.sub_(w_gra*learn_rate)\rb.data.sub_(b_gra*learn_rate)\r#注意梯度清零，否则会累加\rw.grad.data.zero_()\rb.grad.data.zero_()\r# w_gra_last是张量，item输出标量\rprint(epoch,w_gra_last.item(),b_gra_last.item()) y_pre=fx(x) plot.plot(x,y,'.')\rplot.plot(x.data.numpy(),y_pre.data.numpy())\rplot.show() 计算图 PyTorch的计算图是一种用于描述计算操作的有向无环图(Directed Acyclic Graph, DAG)。在PyTorch中，计算图是动态的，它会随着代码的执行而构建。\n计算图的主要作用是记录和管理计算操作的流程，以便进行自动微分和梯度优化。通过构建计算图，PyTorch能够追踪和记录所有的计算操作，从而实现自动求导。这使得在深度学习中，我们可以方便地进行反向传播和优化模型的参数。\n使用计算图的好处有：\n自动求导：PyTorch可以根据计算图自动生成反向传播所需的梯度计算代码，简化了手动求导的过程。 动态图灵活性：计算图是动态构建的，可以根据需要进行动态修改和调整，使得模型的结构和计算过程更加灵活和可变。 可视化和调试：计算图可以可视化，帮助我们理解和调试模型的运行过程，更好地理解和解释模型的行为。 总之，PyTorch的计算图是一种强大的工具，它为我们提供了灵活、高效的自动求导功能，使得深度学习模型的训练和优化更加方便和快捷。\n#打印计算图\rimport torch\rfrom torchviz import make_dot\r# 定义一个简单的计算图\rw= torch.randn(1, requires_grad=True)\rb= torch.randn(1, requires_grad=True)\rx = torch.randn(1, requires_grad=True)\ry = w*x + b\r# 使用make_dot函数绘制计算图,图上的数字只是代表数据的维度\rdot = make_dot(y, params={'x': x, 'w': w, 'b': b}, show_attrs=True, show_saved=True)\rdot.render(filename='compute_graph', format='png') 当前运行的目录出现一个compute_graph.png 剖析下反向求导的过程 如表达式z=wx+b可分解为y=wx和z=y+b，其计算图如图3-5所示，图中的MUL和ADD都是算子，w、x、b为变量。 如上有向无环图中，X 和b是叶子节点（leaf node），这些节点通常由用户自己创建，不依赖于其他变量。z称为根节点，是计算图的最终目标。利用链式法则很容易求得各个叶子节点的梯度。 而有了计算图，上述链式求导即可利用计算图的反向传播自动完成，其过程如图所示。 torchvision模块 torchvision是PyTorch的一个扩展库，提供了许多用于计算机视觉任务的实用函数和预训练模型。它包含了常用的数据集、数据转换、模型架构和图像处理方法等功能。\ntorchvision的主要特点包括：\n数据集：torchvision提供了许多常用的计算机视觉数据集，例如MNIST、CIFAR-10、ImageNet等。这些数据集可以方便地用于训练和测试模型。\n数据转换：torchvision提供了一系列用于数据预处理和增强的转换函数，例如对图像进行裁剪、缩放、翻转、归一化等操作。这些转换函数可以灵活地应用于数据集中的样本，以满足模型训练的需求。\n预训练模型：torchvision中集成了一些经典的计算机视觉模型，例如AlexNet、VGG、ResNet等。这些预训练模型可以直接用于特定任务的迁移学习，也可以作为基准模型进行性能比较。\n图像处理：torchvision还提供了一些常用的图像处理方法，例如图像滤波、边缘检测、颜色转换等。这些方法可以用于图像处理和增强的任务。\n总之，torchvision为PyTorch提供了丰富的计算机视觉功能和工具，可以极大地简化计算机视觉任务的开发和实现过程。\nTransforms torchvision提供了一些用于数据增强的常用transforms，如随机裁剪、翻转、旋转、归一化等。这些transforms可以在数据加载时应用于图像，以提高模型的泛化能力和鲁棒性。 以下是torch中所有的transforms：\nCompose: 将多个transforms组合在一起。 ToTensor: 将PIL图像或NumPy数组转换为张量。 ToPILImage: 将张量转换为PIL图像对象。 Normalize: 标准化张量，将每个通道的值减去均值，然后除以标准差。 Resize: 调整图像的大小。 CenterCrop: 中心裁剪图像的一部分。 RandomCrop: 随机裁剪图像的一部分。 RandomResizedCrop: 随机裁剪并调整大小图像。 FiveCrop: 对图像进行五个不同位置的裁剪。 TenCrop: 对图像进行十个不同位置的裁剪。 RandomHorizontalFlip: 随机水平翻转图像。 RandomVerticalFlip: 随机垂直翻转图像。 RandomRotation: 随机旋转图像。 RandomAffine: 随机仿射变换图像。 ColorJitter: 随机调整图像的亮度、对比度、饱和度和色调。 RandomGrayscale: 随机将图像转换为灰度图像。 RandomErasing: 随机擦除图像的一部分。 RandomChoice: 随机选择一个transform进行应用。 RandomApply: 随机应用一个transform。 RandomOrder: 随机打乱transforms的顺序。 这是torch中所有的transforms，你可以根据需要选择适合的transforms来处理图像数据。 下面是一些常用的transforms功能和示例代码：\nResize：调整图像大小 from PIL import Image\r# 定义一个Resize变换，将图像调整为指定大小\rresize = transforms.Resize((256, 256))\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行Resize变换\rresized_image = resize(image) ToTensor：将图像转换为Tensor类型 from PIL import Image\r# 定义一个ToTensor变换，将图像转换为Tensor类型\rto_tensor = transforms.ToTensor()\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行ToTensor变换\rtensor_image = to_tensor(image) Normalize：对图像进行归一化 from PIL import Image\r# 定义一个Normalize变换，将图像进行归一化\rnormalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行Normalize变换\rnormalized_image = normalize(image) transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) 的作用是将输入数据标准化到均值为0，标准差为1的范围内，而不是将值标准化到-1和1之间。标准化的目的是为了使数据具有相似的尺度，以便更好地进行模型训练和优化。 对于给定的某个点(100, 150, 200)，标准化的过程如下： 1.计算每个通道的均值：(100 + 150 + 200) / 3 = 150 2.计算每个通道的标准差：sqrt(((100-150)^2 + (150-150)^2 + (200-150)^2) / 3) = sqrt((2500 + 0 + 2500) / 3) ≈ 50 3.对每个通道的值进行标准化：(100-150)/50 = -1, (150-150)/50 = 0, (200-150)/50 = 1 所以，标准化后的点为(-1, 0, 1)。 需要注意的是，这只是一个简单的例子，实际上在计算标准差时使用的是整个数据集的均值和标准差，而不是单个点的均值和标准差。\nRandomCrop：随机裁剪图像 from PIL import Image\r# 定义一个RandomCrop变换，随机裁剪图像\rrandom_crop = transforms.RandomCrop((224, 224))\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行RandomCrop变换\rcropped_image = random_crop(image) RandomHorizontalFlip：随机水平翻转图像 from PIL import Image\r# 定义一个RandomHorizontalFlip变换，随机水平翻转图像\rrandom_horizontal_flip = transforms.RandomHorizontalFlip(p=0.5)\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行RandomHorizontalFlip变换\rflipped_image = random_horizontal_flip(image) 通过使用transforms模块中的这些函数，我们可以方便地对图像进行预处理和增强，以便于在训练模型时使用。需要注意的是，transforms函数通常需要作为参数传递给torchvision.transforms.Compose函数，以便将多个transforms组合在一起应用到图像上，如：\ntransform = transforms.Compose([\rtransforms.ToTensor(),\rtransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\r]) 更详细的图像增强处理例子参考：https://github.com/lzeqian/deeplearn/blob/master/learn_rnn/pytorch/4.nn%E6%A8%A1%E5%9D%97/3.%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86.ipynb\nDataSet torchvision库中提供了许多常用的计算机视觉数据集。以下是torchvision库中支持的一些常见数据集的列表：\nMNIST：手写数字图片数据集。 FashionMNIST：时尚商品图片数据集。 CIFAR10：包含10个类别的彩色图片数据集。 CIFAR100：包含100个细分类别的彩色图片数据集。 SVHN：包含数字图片的街景数据集。 ImageNet：包含超过100万个物体类别的彩色图片数据集。 COCO：包含多个物体类别的彩色图片数据集，用于目标检测和图像分割任务。 除了上述数据集，torchvision还提供了一些辅助函数和类，用于加载和预处理数据集，如DataLoader、ImageFolder等。\nDataLoader 以下是对CIFAR10的加载例子\nimport torchvision.datasets as datasets\rimport torchvision.transforms as transforms\rfrom torch.utils.data import Dataset,DataLoader\rimport torch as t\rimport numpy as np\r#加载训练数据50000条\rtrain_dataset=datasets.CIFAR10(root=\"./data\",train=True,transform=transforms.ToTensor(),download=True)\r#测试数据集10000条\rtest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\r#打印数据集的维度\rprint(train_dataset.data.shape,test_dataset.data.shape)\r#打印数据集的标签\rprint(len(train_dataset.targets))\r#torchvision.datasets.cifar.CIFAR10\rprint(type(train_dataset))\r#torchvision.datasets.vision.VisionDataset\rprint(type(train_dataset).__bases__) 注意datasets.CIFAR10在root指定的目录没有数据集会自动下载，如果下载很慢，可以将控制台打印的路径下载下来丢到./data目录即可离线加载。\nDataLoader是PyTorch中用于数据加载的实用工具类。它可以将自定义的数据集包装成一个可迭代的数据加载器，方便进行批处理、洗牌和并行加载等操作。以下是DataLoader的一些常用参数的详细解释：\ndataset：要加载的数据集。可以是继承自torch.utils.data.Dataset的自定义数据集类的实例，也可以是已有的PyTorch数据集类（如torchvision.datasets.ImageFolder）的实例。\nbatch_size：每个批次中样本的数量。默认值为1。通常会根据模型和设备的内存情况选择合适的批量大小。\nshuffle：是否在每个epoch开始前对数据进行洗牌（随机打乱顺序）。默认值为False。洗牌可以提高训练的随机性，有助于模型更好地学习数据中的模式。\nsampler：用于定义数据采样策略的采样器。如果指定了sampler，则忽略shuffle参数。常用的采样器包括torch.utils.data.RandomSampler（随机采样）和torch.utils.data.SequentialSampler（顺序采样）。\nbatch_sampler：用于定义批次级别的数据采样策略的采样器。如果指定了batch_sampler，则忽略batch_size、shuffle和sampler参数。常用的批次采样器包括torch.utils.data.BatchSampler。\nnum_workers：用于数据加载的子进程数量。默认值为0，表示在主进程中加载数据。可以根据计算机的CPU核心数和数据加载的性能需求来选择合适的数值。\ncollate_fn：用于将样本列表转换为批次张量的函数。默认情况下使用默认的collate函数，它假定样本是Tensor或Numpy数组，并将它们堆叠成批次。如果数据集返回的样本具有不同的类型或形状，可以自定义collate函数来处理。\npin_memory：是否将数据加载到CUDA固定内存中。默认值为False。当使用GPU进行训练时，设置pin_memory为True可以加速数据传输，但会占用额外的内存。\ndrop_last：如果数据集的大小不能被批次大小整除，是否丢弃最后一个不完整的批次。默认值为False。在训练过程中，通常会设置为True，以避免不完整的批次导致的错误。\ntimeout：数据加载器在等待数据时的超时时间（以秒为单位）。默认值为0，表示无超时限制。如果数据加载时间较长，可以设置一个较大的超时时间。\nworker_init_fn：用于每个数据加载器子进程的初始化函数。可以用来设置每个子进程的随机种子或其他初始化操作。\n这些参数可以根据具体的需求进行调整和配置，以实现更高效、方便的数据加载 DataLoader会将加载的数据集转换为（批量，通道，高度，宽度）的形式。在PyTorch中，图像数据一般采用CHW（通道，高度，宽度）的顺序。而DataLoader则会将加载的图像数据转换为（批量，通道，高度，宽度）的形式， 其中批量表示一次加载的图像数量。这样的数据形式符合PyTorch中卷积神经网络的输入要求。 torchvision.datasets.vision.VisionDataset复杂处理这些。\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\rtest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\r# 使用数据加载器进行迭代,一批次64条，64条一个循环\rfor batch in train_loader:\rinput_data, labels = batch\rprint(input_data.shape)\rbreak; 输出：torch.Size([64, 3, 32, 32])\n自定义数据集 自己创建的数据集没有做任何维度的转换。\nclass MyDs(Dataset):\rdef __init__(self,data,label):\rself.data=data\rself.label=label\rdef __len__(self):\rreturn len(self.data)\rdef __getitem__(self, index):\rreturn self.data[index],self.label[index]\rds=MyDs([1,2,3,4],[0,1,1,1])\rdsLoader=DataLoader(ds,batch_size=2,shuffle=True)\rfor input,label in dsLoader: #四条数据分成了2批，循环两次\rprint(input,label) nn模块 nn.Module nn.Module是PyTorch中所有神经网络模块的基类。它是构建自定义神经网络模块的核心组件，提供了一些基本功能和属性。\n下面是nn.Module的一些重要属性和方法：\nparameters()：返回模块中需要训练的参数的迭代器。 named_parameters()：返回模块中需要训练的参数及其名称的迭代器。 children()：返回模块中所有子模块的迭代器。 named_children()：返回模块中所有子模块及其名称的迭代器。 to(device)：将模块移动到指定的设备（如CPU或GPU）。 train()：将模块设置为训练模式，启用BatchNorm和Dropout等层的训练行为。 eval()：将模块设置为评估模式，禁用BatchNorm和Dropout等层的训练行为。 forward(input)：定义模块的前向传播逻辑，接收输入并返回输出。 此外，nn.Module还提供了一些方法用于模块的初始化和参数管理：\n__init__()：构造函数，用于初始化模块的参数和子模块。 zero_grad()：将模块中所有参数的梯度置零。 apply(fn)：递归地对模块和子模块应用指定的函数。 state_dict()：返回模块的当前状态字典，包含所有参数和缓冲区。 load_state_dict(state_dict)：加载给定的状态字典，用于恢复模块的参数和缓冲区。 通过继承nn.Module类，可以方便地构建自定义的神经网络模块，并使用PyTorch提供的许多功能来管理模块的参数、状态和计算逻辑。\n使用module自定义一个全连接层\nimport torch as t;\rimport torch.nn as nn\rclass Linear(nn.Module):\rdef __init__(self,input_feature,out_feature):\rnn.Module.__init__(self)\r#nn.Prameter是自动算梯度的\rself.w=nn.Parameter(t.randn(input_feature,out_feature))\rself.b=nn.Parameter(t.randn(out_feature))\rdef forward(self,x):\rreturn x.mm(self.w)+self.b\rlayer=Linear(4,1)\rrtn=layer(t.randn(3,4))\rrtn.backward(t.ones(rtn.size())) # 计算梯度\rprint(layer.w.grad) # 获取w的梯度\rprint(layer.b.grad) # 获取b的梯度 CNN 在神经网络处理中，图片矩阵的通道通常是在宽高之前。这种表示方式被称为“通道优先”（channel-first）或“NCHW”表示法。在这种表示法中，矩阵的维度顺序为（批量大小，通道数，高度，宽度）。\n例如，对于一个RGB彩色图像，它的矩阵表示将具有维度顺序为（1，3，H，W），其中1是批量大小（表示一次处理的图像数量，就是行数），3是通道数（表示RGB三个通道），H是图像的高度，W是图像的宽度，pytorch使用这种方式。\n另一种表示方式是“宽高优先”（channel-last）或“NHWC”表示法，其中矩阵的维度顺序为（批量大小，高度，宽度，通道数）。但是，通道优先的表示法更常见，因为它与卷积操作的计算方式更契合，tensorflow使用这种方式。\n图像处理层 PyTorch提供了一系列用于图像处理的层和函数。以下是一些常用的图像处理层：\nnn.Conv2d：卷积层，用于提取图像中的特征。 nn.MaxPool2d：最大池化层，用于降低特征图的空间维度。 nn.AvgPool2d：均值池化层，用于降低特征图的空间维度 nn.BatchNorm2d：批量归一化层，用于加速训练并提高模型的鲁棒性。 nn.ReLU：ReLU激活函数层，用于引入非线性性。 nn.Linear：全连接层，用于将卷积层的输出映射到最终的分类或回归结果。 nn.Dropout2d：二维Dropout层，用于减少过拟合。 nn.Upsample：上采样层，用于增加特征图的空间维度。 nn.Softmax：Softmax函数层，用于多类别分类问题中的概率计算。 除了这些层，PyTorch还提供了一些用于图像处理的函数，例如卷积操作torch.nn.functional.conv2d，池化操作torch.nn.functional.max_pool2d，以及其他常用的图像处理函数如裁剪、旋转、缩放等。\n这些层和函数可以用来构建卷积神经网络（CNN）等图像处理模型,torch.nn.functional只是用于计算结果而nn包的函数可以用于计算梯度，如果在构建神经网络时必须用nn包。\n卷积神经网络的各层的概念请参考：https://blog.csdn.net/liaomin416100569/article/details/130597944?spm=1001.2014.3001.5501\nnn.Conv2d nn.Conv是PyTorch中用于定义卷积层的类，它的参数如下：\nin_channels：输入张量的通道数。 out_channels：卷积层输出的通道数，也是卷积核的数量。 kernel_size：卷积核的大小，可以是一个整数或一个元组，如(3, 3)。 stride：卷积操作的步长，默认为1。 padding：在输入张量的边缘周围填充0的层数，默认为0。 dilation：卷积核元素之间的间隔，默认为1。 groups：将输入张量分成几组进行卷积，默认为1。 bias：是否使用偏置项，默认为True。 以下是一个示例：\nimport torch.nn as nn\r# 创建一个卷积层\rconv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\r## 打印卷积层的参数\rprint(conv) 输出结果如下：\nConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n上述代码创建了一个输入通道数为3，输出通道数（神经元个数）为64的卷积层，卷积核大小为3x3，步长为1，填充层数为1。\nin_channels代表输入张量的通道数，也可以理解为输入张量的维度。在卷积神经网络中，输入张量的维度通常是指图像的通道数。例如，对于RGB图像，通道数为3，因为图像由红、绿、蓝三个通道组成。对于灰度图像，通道数为1，因为图像只有一个通道。\n在使用nn.Conv创建卷积层时，需要根据输入张量的通道数来设置in_channels参数，以确保卷积层与输入张量的维度匹配。\nConv2d的步长（stride）参数表示卷积核在进行滑动时的步幅大小。步长的作用是控制输出特征图的尺寸。具体来说，如果步长为1， 则卷积核每次滑动一个像素；如果步长为2，则卷积核每次滑动两个像素，以此类推。 步长的两个维度分别表示在图像的行方向和列方向上的步幅大小。在您提供的示例中，步长为(1, 1)，表示卷积核在图像的行和列方向上每次滑动一个像素。\nConv2d的padding参数表示在输入图像的周围添加填充（padding）的大小。填充的作用是在卷积操作中保持输出特征图的尺寸与输入特征图的尺寸相同，或者根据需要进行调整。\n#################学习卷积\rimport torchvision.datasets as datasets\rimport torchvision.transforms as transforms\rimport matplotlib.pyplot as plt\rimport torch.nn as nn\rimport torch as t\r#预处理模块\rfrom PIL import Image\rimage=Image.open(\"./images/2023_6_30.jpg\")\r# plt.imshow(image)\r# plt.show()\r\"\"\"\r这是一个用于边缘检测的卷积核。在这个卷积核中，中心元素是1，\r表示当前位置的像素值对边缘检测有贡献，而周围的元素都是-0.1111，\r表示对边缘检测没有贡献。这样的卷积核可以帮助我们提取图像中的垂直边缘特征。\r\"\"\"\rkernel=t.Tensor(\r[[-0.1111, -0.1111, -0.1111],\r[-0.1111, 1.0000, -0.1111],\r[-0.1111, -0.1111, -0.1111]],\r)\rkernel=t.ones(3,3)/-9\rkernel[1][1]=1\r#转换成灰度图，通道数变成1了\rimage=image.convert(\"L\")\r#转换成张量\rimageTensor=transforms.ToTensor()(image)\rprint(imageTensor.shape)\r#在第0个维度添加一个一维表示批次数据\rinput=imageTensor.unsqueeze(0)\rprint(\"输入形状\",input.shape)\rlayer=nn.Conv2d(1,1,(3,3),bias=False)\r# 定义输入张量shape为(batch_size, channels, height, width)\rlayer.weight.data=kernel.view(1,1,3,3)\routput=layer(input)\rplt.imshow(transforms.ToPILImage()(output.squeeze(0)),cmap=\"gray\")\rplt.show()\r#每个卷积核（3×3）与原始的输入图像（480×479）进行卷积，这样得到的 feature map（特征图）大小为（480-3+1）×（479-3+1）= 478×477\rprint(\"输出形状\",output.shape) 原始图 输出： torch.Size([1, 480, 479]) 输入形状 torch.Size([1, 1, 480, 479]) 输出形状 torch.Size([1, 1, 478, 477]) nn.AvgPool2d和nn.MaxPool2d 上面的卷积图在经过池化\nplt.rcParams['font.sans-serif'] = ['SimHei'] # 设置全局字体为SimHei\r#平均池化（AvgPool）\rpool=nn.AvgPool2d(kernel_size=2, stride=2)\r#池化层478×477经过(2,2)池化后=(478/2=239,477/2=238)\rpoolOuput=pool(output)\rprint(poolOuput.shape)\rplt.title(\"平均池化\")\rplt.imshow(transforms.ToPILImage()(poolOuput.squeeze(0)),cmap=\"gray\")\rplt.show()\r#最大化池\rpool=nn.MaxPool2d(kernel_size=2, stride=2)\r#池化层478×477经过(2,2)池化后=(478/2=239,477/2=238)\rpoolOuput=pool(output)\rprint(poolOuput.shape)\rplt.title(\"最大池化\")\rplt.imshow(transforms.ToPILImage()(poolOuput.squeeze(0)),cmap=\"gray\")\rplt.show() 输出 nn.Linear nn.Linear是PyTorch中用于定义线性变换的类。它是nn.Module的子类，用于构建神经网络的层。\nnn.Linear接受两个参数：in_features和out_features，分别表示输入特征的大小和输出特征的大小。它会自动创建一个可学习的权重矩阵，形状为( in_features，out_features)，以及一个可学习的偏置向量，形状为(out_features,)。\n#注意全连接是特征连接是是改变最后一维的特征数的，在pytorch图片批量处理后最后需要进行view操作来降低维度到二维。\rarr=t.randn((3,4)) print(arr)\rresult=nn.Linear(4,5)\r#全连接就是一个输入数据点乘(输入数据维度,输出数据维度)最后得到一个（输入数据行数，输出数据维度）的数组\rprint(result(arr)) nn.BatchNorm2d BatchNorm2d是用于对二维卷积层的输出进行批量归一化的操作。它的计算过程如下所示：\n假设输入的维度为 [batch_size, num_channels, height, width]，其中 batch_size 表示批量大小，num_channels 表示通道数，height 和 width 表示特征图的高度和宽度。\n计算每个通道的均值和方差：\n对于每个通道，计算当前批次中所有样本的特征图在该通道上的均值和方差。 均值的计算：mean = sum(x) / N，其中 x 是当前通道上的特征图值，N 是批次大小。 方差的计算：var = sum((x - mean)^2) / N。 对于每个通道，进行归一化：\n对于每个样本，在当前通道上，将特征图的值减去均值，然后除以标准差（方差的平方根），以实现归一化。 归一化后的特征图为：y = (x - mean) / sqrt(var + eps)，其中 eps 是一个很小的数，以避免除以零的情况。 对于每个通道，进行缩放和平移：\n对于每个归一化后的特征图，通过乘以一个可学习的缩放因子（scale）和加上一个可学习的平移因子（shift），对特征图进行缩放和平移。 缩放和平移后的特征图为：y = gamma * y + beta，其中 gamma 和 beta 是可学习的参数。 最后，BatchNorm2d操作的输出为归一化、缩放和平移后的特征图。\n这样做的好处是可以加快神经网络的训练速度，提高模型的收敛性和泛化能力，并减少对学习率的敏感性。\n\"\"\"\r具体来说，nn.BatchNorm2d是应用在卷积层之后、激活函数之前的操作，其目的是对每个特征通道的数据进行归一化。\r它通过对每个特征通道的数据进行标准化，使得数据的均值为0，方差为1。这样做的好处是可以防止梯度消失或爆炸的问题，\r并且有助于加速模型的收敛速度。\r除此之外，nn.BatchNorm2d还具有正则化的效果，可以减少模型的过拟合。它通过引入额外的可学习参数，实现了对每个特征通道的平移和缩放操作，以便网络可以自行学习数据的适当分布。\r\"\"\"\rarr=t.randint(0,10,(1,1,2,2)).float()#一批次一个通道，高是2，宽是2\rprint(arr)\rresult=nn.BatchNorm2d(num_features=1)\r#全连接就是一个输入数据点乘(输入数据维度,输出数据维度)最后得到一个（输入数据行数，输出数据维度）的数组\rprint(result(arr)) 输出：\ntensor([[[[9., 1.],\r[8., 9.]]]])\rtensor([[[[ 0.6727, -1.7191],\r[ 0.3737, 0.6727]]]], grad_fn=\u003cNativeBatchNormBackward\u003e) nn.Relu nn.ReLU是PyTorch中的一个激活函数，它将输入中的所有负值变为零，保持正值不变。具体来说，对于输入张量x，nn.ReLU函数的计算公式为：\nReLU(x) = max(0, x) 例子\narr=t.randint(0,10,(1,1,2,2)).float()#一批次一个通道，高是2，宽是2\r#首先进行归一化，归一化后会有负数的部分\rbatchNorm2d=nn.BatchNorm2d(num_features=1)\rresult=batchNorm2d(arr)\rprint(\"归一化\",result)\rrelu=nn.ReLU()\r#全连接就是一个输入数据点乘(输入数据维度,输出数据维度)最后得到一个（输入数据行数，输出数据维度）的数组\rprint(\"relu结果\",relu(result)) 输出：\n归一化 tensor([[[[ 0.2773, -1.3867],\r[ 1.3867, -0.2773]]]], grad_fn=\u003cNativeBatchNormBackward\u003e)\rrelu结果 tensor([[[[0.2773, 0.0000],\r[1.3867, 0.0000]]]], grad_fn=\u003cReluBackward0\u003e) nn.Dropout2d nn.Dropout2d会在训练过程中，对输入张量的每个通道的每个元素按照给定的概率进行丢弃。被丢弃的元素会被设置为零，而保留的元素则会按比例进行缩放，以保持期望值不变。 这种随机丢弃的操作有助于在训练过程中减少过拟合现象，增强模型的泛化能力。丢弃的概率可以通过nn.Dropout2d的参数进行控制。 需要注意的是，在测试过程中，所有的元素都会被保留，不会进行丢弃操作。nn.Dropout2d通常用于卷积神经网络中，可以放在卷积层或者全连接层之后，帮助网络更好地适应数据。 例子\narr=t.randint(0,10,(1,1,4,4)).float()#一批次一个通道，高是2，宽是2\rdrop=nn.Dropout2d()\rnewArr=drop(arr)\rprint(newArr) 输出\ntensor([[[[ 8., 8., 12., 12.],\r[10., 12., 6., 12.],\r[ 0., 4., 12., 16.],\r[ 4., 4., 18., 10.]]]]) nn.Softmax nn.Softmax是PyTorch中的一个函数，它用于计算softmax函数的输出。softmax函数通常用于多分类问题的神经网络中，它将原始的类别分数转化为概率分布。\n在PyTorch中，nn.Softmax可以被应用于一维或二维张量。对于一维张量，它会对张量中的每个元素进行softmax操作，并返回一个与输入张量相同形状的张量。对于二维张量，它会在指定维度上对每行进行softmax操作。\nsoftmax函数的计算公式如下：\n$softmax(x_i) = exp(x_i) / sum(exp(x_j))$\n其中，$x_i$是原始的类别分数，exp是指数函数，sum是对所有类别分数的求和。\nsoftmax函数的输出是一个概率分布，每个类别的概率值介于0和1之间，并且所有类别的概率之和为1。这样可以方便地用于多分类问题中，根据概率选择最可能的类别。\n在PyTorch中，可以使用nn.Softmax函数对网络的输出进行处理，以得到分类结果。 例子\narr=t.randint(0,10,(1, 1, 4, 4)).float()#一批次一个通道，高是2，宽是2\rprint(arr)\r#注意在哪个维度上的和等于1，比如一个4维的（维度从0开始），(1, 1, 4, 4)如果你从0维上，取出0维第一行数据/0维上所有数据行，因为只有一行所有永远都是1\r#如果是第3维上，总共有4个数据，也就是这四个数之和等于1\r#Softmax2D==nn.Softmax(dim=1)\rsoftmax2d=nn.Softmax(dim=3)\rnewArr=softmax2d(arr)\rprint(newArr)\rt.manual_seed(10)\rarr=t.randint(0,10,(1, 2, 4, 4)).float()#一批次2个通道，高是2，宽是2\rsoftmax2d=nn.Softmax2d()\rnewArr=softmax2d(arr)\rprint(arr)\rprint(newArr) 输出\ntensor([[[[7., 5., 2., 0.],\r[3., 0., 8., 1.],\r[6., 8., 8., 4.],\r[2., 6., 3., 5.]]]])\rtensor([[[[8.7490e-01, 1.1841e-01, 5.8950e-03, 7.9781e-04],\r[6.6846e-03, 3.3281e-04, 9.9208e-01, 9.0466e-04],\r[6.2840e-02, 4.6433e-01, 4.6433e-01, 8.5045e-03],\r[1.2755e-02, 6.9639e-01, 3.4671e-02, 2.5619e-01]]]])\rtensor([[[[7., 5., 2., 7.],\r[2., 5., 7., 2.],\r[1., 5., 6., 3.],\r[1., 0., 6., 3.]],\r[[4., 0., 6., 2.],\r[8., 9., 2., 0.],\r[9., 9., 4., 4.],\r[9., 4., 4., 5.]]]])\rtensor([[[[9.5257e-01, 9.9331e-01, 1.7986e-02, 9.9331e-01],\r[2.4726e-03, 1.7986e-02, 9.9331e-01, 8.8080e-01],\r[3.3535e-04, 1.7986e-02, 8.8080e-01, 2.6894e-01],\r[3.3535e-04, 1.7986e-02, 8.8080e-01, 1.1920e-01]],\r[[4.7426e-02, 6.6929e-03, 9.8201e-01, 6.6929e-03],\r[9.9753e-01, 9.8201e-01, 6.6929e-03, 1.1920e-01],\r[9.9966e-01, 9.8201e-01, 1.1920e-01, 7.3106e-01],\r[9.9966e-01, 9.8201e-01, 1.1920e-01, 8.8080e-01]]]]) nn.Sequential nn.Sequential和nn.ModuleList是PyTorch中用于组合神经网络模块的两种容器。\nnn.Sequential：\nnn.Sequential是一个按照顺序排列的容器，其中的模块按照它们被添加到容器中的顺序依次执行。 可以通过在Sequential对象的构造函数中传递模块列表来创建Sequential容器，或者通过.add_module()方法逐个添加模块。 nn.Sequential适用于简单的顺序模型，其中每个模块只有一个输入和一个输出。 nn.ModuleList：\nnn.ModuleList是一个可以包含任意数量模块的容器，模块之间没有特定的顺序。 可以通过在ModuleList对象的构造函数中传递模块列表来创建ModuleList容器，或者通过.append()方法逐个添加模块。 nn.ModuleList适用于自定义连接和复杂的模型结构，其中模块之间可能存在多个输入和输出。 总而言之，nn.Sequential适用于简单的顺序模型，而nn.ModuleList适用于自定义连接和复杂的模型结构。在实际使用中，可以根据模型的结构和需要选择合适的容器。 使用nn.Sequential自定义一个多层感知器\nimport torch as t\rimport torch.nn as nn\r#实现一个多层感知器,多层感知器（Multilayer Perceptron, MLP）的隐藏层的特征数就是神经元的个数\rclass MulPerceptron(nn.Module):\rdef __init__(self,input_feature,hidden_feature,out_feature):\rnn.Module.__init__(self)\r#Sequential会将上一层的输出作为下层的输入\rself.model=nn.Sequential(\rnn.Linear(input_feature,hidden_feature),\rnn.ReLU(),\rnn.Linear(hidden_feature,out_feature)\r)\rdef forward(self,x):\r#隐藏层进行一次全连接得到（行，hidden_feature）数据矩阵\rreturn self.model(x);\rmp=MulPerceptron(784,512,1)\rresult=mp(t.randn(200,784))\rprint(result) 最后输出(200,1)的结果。\n损失函数 PyTorch提供了一系列常用的损失函数，下面是其中一些常见的损失函数及其用法举例：\nnn.MSELoss（均方误差损失函数）：\n用于回归任务，计算预测值与真实值之间的均方误差。\nloss_fn = nn.MSELoss()\rloss = loss_fn(output, target)\nnn.CrossEntropyLoss（交叉熵损失函数）：\n用于多分类任务，计算预测类别与真实类别之间的交叉熵损失。\nloss_fn = nn.CrossEntropyLoss()\rloss = loss_fn(output, target)\nnn.BCELoss（二分类交叉熵损失函数）：\n用于二分类任务，计算预测概率与真实标签之间的二分类交叉熵损失。\nloss_fn = nn.BCELoss()\rloss = loss_fn(output, target)\nnn.BCEWithLogitsLoss（二分类交叉熵损失函数，结合了Sigmoid函数）：\n用于二分类任务，结合了Sigmoid函数的操作，可以在计算二分类交叉熵损失时避免数值稳定性问题。\nloss_fn = nn.BCEWithLogitsLoss()\rloss = loss_fn(output, target)\nnn.NLLLoss（负对数似然损失函数）：\n用于多分类任务，计算预测类别的负对数似然损失。\nloss_fn = nn.NLLLoss()\rloss = loss_fn(output, target)\n这只是一小部分PyTorch中提供的损失函数，还有其他损失函数可用于不同的任务和应用场景。你可以根据具体的需求选择合适的损失函数来进行模型训练和优化。\n均方误差 均方误差（Mean Squared Error，MSE）是一种常用的回归问题的损失函数。它衡量了预测值与真实值之间的差异的平方的平均值。\n对于给定的预测值和真实值，MSE的计算公式如下：\nMSE = (1/n) * Σ(y_pred - y_true)^2\n其中，n是样本数量，y_pred是预测值，y_true是真实值。\nMSE的值越小，表示预测值和真实值之间的差异越小，模型的性能越好。常用的优化算法，如梯度下降法，通过最小化MSE来调整模型的参数，以提高模型的准确性。\nx=t.randn(100,1)\ry=3*x+2+t.randn(100,1) #实际值上加上一些随机噪点\ry_pre=3*x+2\rplot.plot(x,y,'.')\rplot.plot(x,y_pre)\rplot.show()\r#使用均方误差计算损失值\rcriterion=nn.MSELoss()\rloss=criterion(y,y_pre)\rprint(loss) 交叉熵 下面是一个使用nn.CrossEntropyLoss的例子，并对交叉熵的计算过程进行详细解释：\nimport torch.nn as nn\r# 假设有4个样本，每个样本有3个类别的预测结果\r# 真实标签为[2, 1, 0, 2]\r# 预测结果为一个3维张量，每一维表示对应类别的预测概率\routputs = torch.tensor([[0.1, 0.2, 0.7],\r[0.6, 0.3, 0.1],\r[0.8, 0.1, 0.1],\r[0.3, 0.5, 0.2]])\rlabels = torch.tensor([2, 1, 0, 2])\r# 创建交叉熵损失函数\rloss_fn = nn.CrossEntropyLoss()\r# 计算损失\rloss = loss_fn(outputs, labels)\rprint(loss) 输出结果为：\ntensor(0.8025)\n交叉熵是一种常用的损失函数，用于衡量两个概率分布之间的相似性。在分类任务中，我们通常将模型的预测结果视为一个概率分布，其中每个类别都有一个对应的概率。\n在上面的例子中，我们有4个样本，每个样本有3个类别的预测结果。outputs是一个3维张量，每一维表示对应类别的预测概率。例如，outputs[0]表示第一个样本对三个类别的预测概率，分别为0.1、0.2和0.7。\nlabels是一个1维张量，表示每个样本的真实类别标签。例如，labels[0]表示第一个样本的真实类别标签为2。\n交叉熵损失函数的计算过程如下：\n首先，对于每个样本，我们需要根据预测概率和真实标签计算出对应类别的预测概率。\n在上面的例子中，对于第一个样本，预测概率为[0.1, 0.2, 0.7]，真实标签为2。我们只需要取出预测概率中对应真实标签的值，即0.7。\n接下来，我们对每个样本的预测概率进行对数转换，即计算每个预测概率的自然对数。\n在上面的例子中，对于第一个样本，对数转换后的预测概率为log(0.7)。\n然后，我们将对数转换后的预测概率求和，并除以样本的数量，得到平均交叉熵损失。\n在上面的例子中，我们有4个样本，因此将对数转换后的预测概率求和，并除以4，得到平均交叉熵损失。\n最后，我们将平均交叉熵损失作为模型的损失，并用于模型的训练和优化过程。在PyTorch中，我们可以使用nn.CrossEntropyLoss函数来计算交叉熵损失，它会自动进行softmax操作和对数转换的计算。\n优化器 torch.optim是PyTorch中用于优化算法的模块。它提供了各种优化算法的实现，用于更新神经网络模型的参数以最小化损失函数。\n在PyTorch中，我们通过创建一个优化器对象来使用torch.optim模块。该优化器对象将被用于更新神经网络模型的参数。\n以下是torch.optim模块中常用的优化算法：\nSGD (Stochastic Gradient Descent): 随机梯度下降算法是最基本的优化算法之一。它通过计算损失函数对参数的梯度，并根据学习率更新参数。可以通过torch.optim.SGD类来实现。\nAdam (Adaptive Moment Estimation): Adam是一种自适应的优化算法，它结合了Momentum和RMSprop的优点。它使用动量和平方梯度的指数加权移动平均来自适应地调整学习率。可以通过torch.optim.Adam类来实现。\nAdagrad (Adaptive Gradient): Adagrad是一种自适应的优化算法，它为每个参数分配一个学习率，并根据参数的历史梯度的平方和来自适应地调整学习率。可以通过torch.optim.Adagrad类来实现。\nRMSprop (Root Mean Square Propagation): RMSprop也是一种自适应的优化算法，它使用指数加权移动平均来自适应地调整学习率。它通过除以梯度的平方和的平方根来缩放学习率。可以通过torch.optim.RMSprop类来实现。\n这些优化算法都可以通过创建相应的优化器对象，并传递神经网络模型的参数和其他超参数来使用。例如，下面的代码演示了如何使用SGD优化算法(伪代码)：\nimport torch\rimport torch.optim as optim\r# 创建神经网络模型\rmodel = MyModel()\r# 创建优化器对象，学习率为0.001\roptimizer = optim.SGD(model.parameters(), lr=0.001)\r# 在每个训练迭代中，使用优化器更新模型的参数\roptimizer.zero_grad() # 清零梯度\routput = model(input) # 前向传播\rloss = criterion(output, target) # 计算损失\rloss.backward() # 反向传播\roptimizer.step() # 更新参数 在上面的代码中，model.parameters()返回神经网络模型的所有可学习参数，这些参数将被优化器更新。optimizer.zero_grad()方法用于将参数的梯度清零，loss.backward()方法用于计算梯度，optimizer.step()方法用于更新参数。\n除了上述常用的优化算法之外，torch.optim模块还提供了其他一些优化算法，如Adadelta、AdamW等。您可以根据自己的需求选择适合的优化算法来训练模型。\nLetnet5分类CIFAR10 #%%\rimport torch\rimport torch.nn as nn\rimport torch.optim as optim\rimport torchvision.transforms as transforms\rfrom torch.utils.data import DataLoader\rfrom torchvision.datasets import CIFAR10\r#定义LeNet5模型，模型计算过程参考：https://blog.csdn.net/liaomin416100569/article/details/130677530?spm=1001.2014.3001.5501\rclass LeNet5(nn.Module):\rdef __init__(self):\rsuper(LeNet5, self).__init__()\rself.features = nn.Sequential(\r#C1 层（卷积层）：6@28×28 该层使用了 6 个卷积核，每个卷积核的大小为 5×5，这样就得到了 6 个 feature map（特征图）。\rnn.Conv2d(3, 6, kernel_size=5),\rnn.ReLU(inplace=True),\r#S2 层（下采样层，也称池化层）：6@14×14,池化单元为 2×2，因此，6 个特征图的大小经池化后即变为 14×14\rnn.MaxPool2d(kernel_size=2, stride=2),\r#C3 层（卷积层）：16@10×10 C3 层有 16 个卷积核，卷积模板大小为 5×5 C3 层的特征图大小为（14-5+1）×（14-5+1）= 10×10。\rnn.Conv2d(6, 16, kernel_size=5),\rnn.ReLU(inplace=True),\r#S4（下采样层，也称池化层）：16@5×5,与 S2 的分析类似，池化单元大小为 2×2，因此，该层与 C3 一样共有 16 个特征图，每个特征图的大小为 5×5。\rnn.MaxPool2d(kernel_size=2, stride=2)\r)\rself.classifier = nn.Sequential(\r#LeNet-5模型中的C5层是一个全连接层。在LeNet-5模型中，前两个卷积层（C1和C3）之后是一个池化层（S2和S4），\r# 然后是一个全连接层（C5），最后是输出层（F6）。全连接层C5的输入是S4层的输出，它将这个输入展平为一个向量，\r# 并将其连接到输出层F6。因此，C5层是一个全连接层，而不是卷积层，这里和文中有些冲突。\r#C5 层（卷积层）：120 该层有 120 个卷积核，每个卷积核的大小仍为 5×5，因此有 120 个特征图,特征图大小为（5-5+1）×（5-5+1）= 1×1。这样该层就刚好变成了全连接\rnn.Linear(16 * 5 * 5, 120),\rnn.ReLU(inplace=True),\r#F6 层（全连接层）：84,该层有 84 个特征图，特征图大小与 C5 一样都是 1×1\rnn.Linear(120, 84),\rnn.ReLU(inplace=True),\r# OUTPUT 层（输出层）：10\rnn.Linear(84, 10)\r)\rdef forward(self, x):\rx = self.features(x)\r#张量x在第0个维度上的大小，因为第0个维度是数据批次数（行数），s4层后的维度是(批次数，16,5,5)\r#转换成2维就是(行数,16*5*5)，-1表示自动计算合并成最后一个维度\rx = x.view(x.size(0), -1)\rx = self.classifier(x)\rreturn x\rmodel = LeNet5()\r\"\"\"\rtransforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])是一种数据预处理操作，用于对图像数据进行归一化处理。这个操作会将每个像素的数值减去均值(mean)并除以标准差(std)。\r在这个例子中，mean=[0.5, 0.5, 0.5]表示将每个通道的像素值减去0.5，std=[0.5, 0.5, 0.5]表示将每个通道的像素值除以0.5。这样处理后，图像的像素值会在-1到1之间。\r归一化可以帮助提高模型的训练效果和稳定性，因为它可以使输入数据的分布更加接近标准正态分布。此外，对于不同的数据集，可能需要不同的均值和标准差进行归一化操作，以使数据的分布更加合理。\r在使用PyTorch的transforms.Normalize时，通常需要将其与其他数据预处理操作一起使用，例如transforms.ToTensor()将图像转换为张量。可以通过transforms.Compose将多个预处理操作组合在一起，形成一个数据预处理管道。\r\"\"\"\rtransform = transforms.Compose([\rtransforms.ToTensor(),\rtransforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\r])\r#下载训练集,data是数据数组，target是标签\rtrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\r#下载测试集\rtest_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\r#数据批处理和打乱，一次64条数据\rtrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\rtest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\r#使用交叉熵损失函数\rcriterion = nn.CrossEntropyLoss()\r#使用随机梯度下降法优化参数，梯度下降的学习率是0.001\roptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\r#判断是否有gpu如果有的话讲模型附加到cuda设备上\r#momentum参数通过累积之前的梯度信息，使得参数更新具有一定的惯性，从而在参数空间中更快地找到全局最优解或局部最优解。\rdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\rmodel.to(device)\r#模型对数据集进行10次epoch\rnum_epochs = 10\rfor epoch in range(num_epochs):\rmodel.train()\repoch_loss = 0.0\rfor images, labels in train_loader:\rimages = images.to(device)\rlabels = labels.to(device)\roptimizer.zero_grad()\routputs = model(images)\rloss = criterion(outputs, labels)\rloss.backward()\roptimizer.step()\repoch_loss += loss.item()\rprint(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\r\"\"\"\rmodel.eval()是PyTorch中用于将模型设置为评估模式的函数。当调用model.eval()时，模型的行为会发生变化，包括：\r1. Batch Normalization和Dropout等具有随机性的层会固定住，不再产生随机变化。\r2. 模型的参数不会被更新，即不会进行梯度计算和反向传播。\r3. 在推断阶段，模型会根据输入数据生成输出，而不会进行训练。\r通常，在测试或评估模型时，需要调用model.eval()来确保模型的行为与训练时保持一致。\r这样可以避免由于Batch Normalization和Dropout等层的随机性而导致结果不稳定。在调用model.train()之前，\r应该使用model.eval()将模型切换回训练模式,要将模型切换回训练模式，可以使用model.train()方法。\r\"\"\" model.eval()\rcorrect = 0\rtotal = 0\r#torch.no_grad()是一个上下文管理器，将其包裹的代码块中的所有操作都不会计算梯度。\r# 通常用于在不需要计算梯度的情况下进行推理或评估。\rwith torch.no_grad():\rfor images, labels in test_loader:\r#数据加载到显存\rimages = images.to(device)\rlabels = labels.to(device)\routputs = model(images)\r#获取输出数据中概率最高的那一个\r_, predicted = torch.max(outputs.data, 1)\r#总共数据行\rtotal += labels.size(0)\r#正确的数据行\rcorrect += (predicted == labels).sum().item()\raccuracy = 100 * correct / total\rprint(f\"Test Accuracy: {accuracy:.2f}%\") RNN nn.RNN nn.RNN是PyTorch中的一个循环神经网络模块，用于处理序列数据。下面是nn.RNN的常用参数和解释：\ninput_size：输入的特征维度。 hidden_size：隐藏层的特征维度。 num_layers：RNN的层数。 nonlinearity：激活函数，默认为\"tanh\"。可以是\"tanh\"、“relu\"等。 bias：是否使用偏置，默认为True。 batch_first：是否输入数据的第一个维度为batch大小，默认为False。 dropout：是否在输出层应用dropout操作，默认为0，即不使用dropout。 bidirectional：是否使用双向RNN，默认为False。 这些参数可以在创建nn.RNN时进行设置。例如：\nimport torch.nn as nn\rinput_size = 10\rhidden_size = 20\rnum_layers = 2\rrnn = nn.RNN(input_size, hidden_size, num_layers) 这样就创建了一个具有输入特征维度为10、隐藏层特征维度为20、2层的RNN模型。\n传入数据格式 nn.RNN的输入数据格式通常为三维张量，具体格式为：\n如果batch_first=False（默认值），则输入数据的形状为(sequence_length, batch_size, input_size)。 如果batch_first=True，则输入数据的形状为(batch_size, sequence_length, input_size)。 其中，\nsequence_length表示序列的长度，即时间步的数目。 batch_size表示每个batch的样本数量。 input_size表示输入特征的维度。 例如，假设我们有一个batch包含3个样本，每个样本的序列长度为4，输入特征维度为5，那么输入数据的形状可以是(4, 3, 5)或(3, 4, 5)。\n可以使用torch.randn()函数生成随机输入数据进行测试，例如：\nimport torch.nn as nn\rbatch_size = 3\rsequence_length = 4\rinput_size = 5\rinput_data = torch.randn(sequence_length, batch_size, input_size)\rrnn = nn.RNN(input_size, hidden_size, num_layers)\routput, hidden = rnn(input_data) 其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\n案例 \"\"\"\rPyTorch中实现了如今最常用的三种RNN：RNN（vanilla RNN）、LSTM和GRU。此外还有对应的三种RNNCell。\rRNN和RNNCell层的区别在于前者能够处理整个序列，而后者一次只处理序列中一个时间点的数据，\r前者封装更完备更易于使用，后者更具灵活性。RNN层可以通过组合调用RNNCell来实现。\r理论参考：https://blog.csdn.net/liaomin416100569/article/details/131380370?spm=1001.2014.3001.5501\r输入参数和RNN参数解释参考readme.md\r\"\"\"\rimport torch as t\rimport torch.nn as nn\r#注意默认（时间步，批次数，数据维度）\rsequence_length =3\rbatch_size =2\rinput_size =4\rinput=t.randn(sequence_length,batch_size,input_size)\rprint(\"输入数据\",input)\rrnnModel=nn.RNN(input_size,3,1)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, hidden=rnnModel(input)\rprint(\"RNN最后时间步隐藏层\",hidden)\rprint(\"RNN最后时间步隐藏层维度\",hidden.shape)\rprint(\"RNN所有隐藏层\",output)\rprint(\"RNN所有隐藏层维度\",output.shape) 输出：\n输入数据 tensor([[[ 0.5364, -0.5291, 0.3117, -0.0282],\r[-0.2012, 0.9933, 1.5328, -0.8234]],\r[[ 1.3270, -1.2367, 0.5925, 1.0894],\r[-1.8035, 0.3598, -0.4404, 0.4921]],\r[[-0.6487, -0.0487, -0.9728, 0.7563],\r[ 1.2929, 0.5146, 1.2296, 1.0124]]])\rRNN最后时间步隐藏层 tensor([[[0.2800, 0.8572, 0.3759],\r[0.5901, 0.4742, 0.9417]]], grad_fn=\u003cStackBackward\u003e)\rRNN最后时间步隐藏层维度 torch.Size([1, 2, 3])\rRNN所有隐藏层 tensor([[[ 0.5862, 0.7417, 0.8068],\r[ 0.9564, 0.5668, 0.6112]],\r[[-0.1729, 0.7310, 0.9879],\r[ 0.6202, 0.7824, 0.3075]],\r[[ 0.2800, 0.8572, 0.3759],\r[ 0.5901, 0.4742, 0.9417]]], grad_fn=\u003cStackBackward\u003e)\rRNN所有隐藏层维度 torch.Size([3, 2, 3]) nn.LSTM nn.LSTM是PyTorch中的一个循环神经网络模块，它基于长短期记忆（Long Short-Term Memory，LSTM）的架构。LSTM是一种特殊类型的循环神经网络，通过使用门控机制来解决传统循环神经网络中的梯度消失和梯度爆炸问题，从而能够更好地处理长期依赖关系。\nnn.LSTM的主要参数包括：\ninput_size：输入数据的特征维度。 hidden_size：隐藏层的维度，也是LSTM单元输出的维度。 num_layers：LSTM的层数，默认为1。 bias：是否使用偏置，默认为True。 batch_first：输入数据的维度顺序是否为(batch, seq, feature)，默认为False。 dropout：是否应用dropout，用于防止过拟合，默认为0，表示不使用dropout。 bidirectional：是否使用双向LSTM，默认为False。 nn.LSTM的输入数据格式通常是一个三维张量，具体格式取决于batch_first参数的设置。如果batch_first为False（默认值），输入数据的维度应为(seq_len, batch, input_size)，其中seq_len表示序列的长度，batch表示批次的大小，input_size表示输入数据的特征维度。如果batch_first为True，输入数据的维度应为(batch, seq_len, input_size)。\nnn.LSTM的前向传播过程会根据输入数据的时间步长和层数进行迭代计算，并返回最后一个时间步的输出以及最后一个时间步的隐藏状态和记忆细胞状态。这些输出可以用于下游任务，如分类或回归。\n使用nn.LSTM时，可以通过调整参数来适应不同的任务和数据。此外，还可以使用nn.LSTMCell来构建自定义的LSTM网络。\nnn.LSTM的返回值是一个元组，包含两个元素：output和(hidden_state, cell_state)。\noutput：表示LSTM模型的隐藏状态输出。它是一个元组，包含了模型在每个时间步的输出结果。具体来说，output的形状是(seq_len, batch, num_directions * hidden_size)，其中：\nseq_len表示输入序列的长度； batch表示输入数据的批次大小； num_directions表示LSTM模型的方向数，通常为1或2（双向LSTM）； hidden_size表示隐藏状态的维度。 (hidden_state, cell_state)：表示LSTM模型的最后一个时间步的隐藏状态和细胞状态。它们的形状都是(num_layers * num_directions, batch, hidden_size)，其中：\nnum_layers表示LSTM模型的层数； num_directions表示LSTM模型的方向数，通常为1或2（双向LSTM）； batch表示输入数据的批次大小； hidden_size表示隐藏状态的维度。 这两个返回值可以用于进一步的处理和分析，比如用于序列标注、语言建模等任务。 也就是hidden_state是output最后一个值，每个时间步都有一个cell_state 案例\nlstmModel=nn.LSTM(input_size,3,1)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, (h, c) =lstmModel(input)\rprint(\"LSTM隐藏层输出的维度\",output.shape)\rprint(\"LSTM隐藏层最后一个时间步输出的维度\",h.shape)\rprint(\"LSTM隐藏层最后一个时间步细胞状态\",c.shape) 输出\nLSTM隐藏层输出的维度 torch.Size([3, 2, 3])\rLSTM隐藏层最后一个时间步输出的维度 torch.Size([1, 2, 3])\rLSTM隐藏层最后一个时间步细胞状态 torch.Size([1, 2, 3]) nn.GRU nn.GRU是PyTorch中的一个循环神经网络（Recurrent Neural Network，RNN）模块，它实现了门控循环单元（Gated Recurrent Unit，GRU）的功能。GRU是一种用于处理序列数据的RNN变体，它具有比传统的循环神经网络更强大的建模能力。\nGRU通过引入两个门控机制，即更新门（Update Gate）和重置门（Reset Gate），来控制信息的流动。这些门控机制使得GRU能够学习长期依赖关系，并且在处理长序列时能够更好地捕捉到序列中的重要信息。\n在nn.GRU模块中，可以通过设置参数来定义GRU的输入维度、隐藏状态维度、层数等。以下是nn.GRU的一些常用参数：\ninput_size：输入的特征维度。 hidden_size：隐藏状态的维度。 num_layers：GRU的层数。 bias：是否使用偏置。 batch_first：如果为True，则输入数据的形状应为（batch_size，sequence_length，input_size）；如果为False，则输入数据的形状应为（sequence_length，batch_size，input_size）。 dropout：dropout比例，用于控制输入数据的随机丢弃比例。 bidirectional：是否使用双向GRU。 除了上述参数之外，nn.GRU还提供了其他一些方法和功能，如forward方法用于前向传播计算，reset_parameters方法用于重置模型的参数等。 案例\n# gru没有细胞状态\rgruModel=nn.GRU(input_size,3,1)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, h =gruModel(input)\rprint(\"GRU隐藏层输出的维度\",output.shape)\rprint(\"GRU隐藏层最后一个时间步输出的维度\",h.shape) 输出\nGRU隐藏层输出的维度 torch.Size([3, 2, 3])\rGRU隐藏层最后一个时间步输出的维度 torch.Size([1, 2, 3]) models checkpoints 在深度学习中，checkpoints是训练期间保存模型参数的文件。它们是在每个训练周期或某个特定时间间隔保存的，以便在训练过程中出现问题时可以恢复训练。通过保存checkpoints，我们可以在训练过程中随时停止并重新开始，而无需从头开始训练。\n“.pt\"是PyTorch中用于保存模型参数的文件扩展名。当我们训练一个模型时，我们可以将模型的参数保存在.pt文件中，以便以后在其他地方使用或加载到其他模型中。这些文件包含了模型在训练期间学到的权重和偏差等参数。在PyTorch中，我们可以使用torch.save()函数将模型参数保存为.pt文件，并使用torch.load()函数加载.pt文件中的参数。\nimport torch\rimport torch.nn as nn\r#模型的保存和加载\rmodel=nn.Linear(3,1)\r#修改权重和偏置后保存模型\rnew_weight = torch.tensor([[1.0, 2.0, 3.0]])\rnew_bias = torch.tensor([4.0])\rmodel.weight = nn.Parameter(new_weight)\rmodel.bias = nn.Parameter(new_bias)\rtorch.save(model.state_dict(),\"./model.pt\")\rnewModel=nn.Linear(3,1)\rprint(\"默认参数\",newModel.weight,newModel.bias)\rnewModel.load_state_dict(torch.load(\"./model.pt\"))\rprint(\"加载后\",newModel.weight,newModel.bias) 输出\n默认参数 Parameter containing:\rtensor([[-0.4357, -0.0781, 0.0136]], requires_grad=True) Parameter containing:\rtensor([-0.2013], requires_grad=True)\r加载后 Parameter containing:\rtensor([[1., 2., 3.]], requires_grad=True) Parameter containing:\rtensor([4.], requires_grad=True) 内置models 在PyTorch的torchvision.models模块中，提供了一些已经实现好的经典神经网络模型，包括：\nAlexNet VGG ResNet SqueezeNet DenseNet Inception GoogLeNet MobileNet ShuffleNet ResNeXt Wide ResNet MNASNet 这些模型可以通过torchvision.models模块的函数进行实例化，以便在自己的项目中使用。每个模型都有预训练的权重，也可以在自定义数据集上进行微调。你可以根据自己的需求选择适合的模型进行使用。\n以下使用models.resnet18分类datasets.CIFAR10\n#%%\rimport torch\rimport torch.nn as nn\rimport torch.optim as optim\rimport torchvision.transforms as transforms\rimport torchvision.datasets as datasets\rimport torchvision.models as models\r# 定义数据预处理\rtransform = transforms.Compose([\rtransforms.RandomCrop(32, padding=4),\rtransforms.RandomHorizontalFlip(),\rtransforms.ToTensor(),\rtransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\r])\r# 加载CIFAR-10数据集\rtrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\rtest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\r# 定义模型\r#参数pretrained表示是否加载预训练的权重。如果pretrained为True，那么模型将加载在ImageNet数据集上预训练的权重。\r# 这些预训练的权重可以提供更好的初始权重，有助于模型在其他任务上进行迁移学习。如果pretrained为False，\r# 则使用随机初始化的权重进行训练。\rmodel = models.resnet18(pretrained=False)\rnum_classes = 10\rmodel.fc = nn.Linear(512, num_classes)\r# 定义损失函数和优化器\rcriterion = nn.CrossEntropyLoss()\roptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\r# 训练模型\rbatch_size = 64\rtrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\rtest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\rnum_epochs = 10\rdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\rmodel.to(device)\rfor epoch in range(num_epochs):\rmodel.train()\repoch_loss = 0.0\rfor images, labels in train_loader:\rimages = images.to(device)\rlabels = labels.to(device)\r# 前向传播和计算损失\routputs = model(images)\rloss = criterion(outputs, labels)\r# 反向传播和优化\roptimizer.zero_grad()\rloss.backward()\roptimizer.step()\repoch_loss += loss.item()\rprint(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\r# 在测试集上评估模型\rmodel.eval()\rwith torch.no_grad():\rcorrect = 0\rtotal = 0\rfor images, labels in test_loader:\rimages = images.to(device)\rlabels = labels.to(device)\routputs = model(images)\r_, predicted = torch.max(outputs.data, 1)\rtotal += labels.size(0)\rcorrect += (predicted == labels).sum().item()\raccuracy = 100 * correct / total\rprint(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy}%')\rtorch.cuda.empty_cache() torch.hub torch.hub是PyTorch中一个用于加载预训练模型的工具。它提供了一个简单的接口，可以方便地从互联网上获取训练好的模型并加载到您的代码中使用。通过使用torch.hub，您可以轻松地使用各种预训练模型，如图像分类、目标检测、语义分割等模型。\ntorch.hub的使用非常简单，您只需要提供模型的命名空间和模型名称，它将自动下载并加载预训练模型。例如，要加载一个名为\"pytorch/vision\"的模型，您可以使用以下代码：\nimport torch\rmodel = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\n上述代码将下载并加载名为\"resnet50\"的预训练模型，并将其存储在model变量中。您可以使用model变量进行推理、特征提取等操作。\ntorch.hub还支持本地模型缓存，这意味着当您多次运行相同的代码时，它将自动从本地缓存中加载模型，而不是重新下载。这样可以提高代码的运行效率。\n总之，torch.hub是一个非常方便的工具，使您能够轻松地使用各种预训练模型，并将它们集成到您的代码中，从而加速您的深度学习项目开发。 官网模型搜索地址：https://pytorch.org/hub/research-models 以下是最火的6个model yolov5目标检测 实战使用yolov5目标检测,参考官方模型文档：https://pytorch.org/hub/ultralytics_yolov5/ 注意 Python\u003e=3.8 PyTorch\u003e=1.7 安装ultralytics\npip install -U ultralytics 编写程序\n#%%\rimport torch\r# Model，加载模型中的参数\rmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\r# Images\rimgs = ['https://ultralytics.com/images/zidane.jpg'] # batch of images\r# Inference\rresults = model(imgs)\r# Results\rresults.print()\rresults.save() # or .show()\rresults.xyxy[0] # img1 predictions (tensor) 会在当前运行的目录上生成一个runs/detect/exp/zidane.jpg 生成动漫图像 github项目地址：https://github.com/bryandlee/animegan2-pytorch 可以将人物图像转换为动漫效果。\nfrom PIL import Image\rimport torch\rfrom matplotlib import pyplot\rmodel = torch.hub.load(\"bryandlee/animegan2-pytorch:main\", \"generator\",pretrained=\"celeba_distill\").eval()\rface2paint = torch.hub.load(\"bryandlee/animegan2-pytorch:main\", \"face2paint\", size=512)\rimage=Image.open(\"./images/lyf.png\")\rout = face2paint(model, image)\rpyplot.imshow(out)\rpyplot.show() 原始图像 转换后 可视化监控 在 TensorFlow 中，最常使用的可视化工具是Tensorboard ,TensorboardX 工具使得 PyTorch 也享受到 Tensorboard 的便捷功能。 pytorch1.8之后已经包含了tensorboardx工具，在torch.utils.tensorboard包中。 FaceBook 也为 PyTorch 开发了一款交互式可视化工具 Visdom，它可以对实时数据进行丰富的可视化，帮助实时监控实验过程。\ntensorboard Tensorboard 是 TensorFlow 的一个附加工具，用于记录训练过程的模型的参数、评价指标与图像等细节内容，并通过 Web 页面提供查看细节与过程的功能，用浏览器可视化的形式展现，帮助我们在实验时观察神经网络的训练过程，把握训练趋势。既然 Tensorboard 工具这么方便，TensorFlow 外的其它深度学习框架自然也想获取 Tensorboard 的便捷功能，于是，TensorboardX 应运而生。 先安装Tensorboard\npip install tensorboard 我这里tensorboard要求的setuptools版本较低，在使用过程中报错\nAttributeError: module 'distutils' has no attribute 'version' 降级版本即可\npip uninstall setuptools\rmicromamba install setuptools==59.5.0\r或者用pip install setuptools==59.5.0 工具使用规范 1、创建SummaryWriter 的实例：\nfrom torch.utils.tensorboard import SummaryWriter\r# 创建一个SummaryWriter的实例\rwriter = SummaryWriter(log_dir=None) 其中的 log_dir 表示保存日志的路径，默认会保存在“runs/ 当前时间 _ 主机名”文件夹中。\nadd_scalar 2、add_scalar方法，这个方法用来记录数字常量（比如损失函数值），它的定义如下：\nadd_scalar(tag, scalar_value, global_step=None, walltime=None) tag：字符串类型，表示对应要监控的数据名称，是任意自定义的，不同名称的数据会使用不同曲线展示； scalar_value：浮点型，表示要监控及保存的数值； global_step：整型，表示训练的 step 数，作为横坐标； walltime：浮点型，表示记录发生的时间，默认为 time.time()。 一般会使用add_scalar方法来记录训练过程的 loss、accuracy、learning rate 等数值的变化，这样就能直观地监控训练过程。每监控一个指标，就需要使用一个add_scalar方法。（如果要看x个指标就使用x次add_scalar方法） 3、add_image方法用来记录单个图像数据（需要 Pillow 库的支持），它的定义如下 add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW') tag、global_step 和 walltime 的含义跟add_scalar方法里一样 img_tensor：PyTorch 的 Tensor 类型或 NumPy 的 array 类型，表示图像数据； dataformats：字符串类型，表示图像数据的格式，默认为“CHW”，即 Channel x Height x Width，还可以是“CHW”、“HWC”或“HW”等。 这里演示一个线性回归的例子 ，演示将epoch次数作为x，损失作为y值的的scalar图 #%%\rimport os\rimport shutil\rdef delete_directory_contents(directory):\rfor filename in os.listdir(directory): # 遍历目录下的所有文件和子目录\rfile_path = os.path.join(directory, filename) # 构建文件或子目录的完整路径\rif os.path.isfile(file_path): # 如果是文件，则直接删除\ros.remove(file_path)\relif os.path.isdir(file_path): # 如果是子目录，则递归调用删除子目录中的内容\rshutil.rmtree(file_path)\r#删除runs目录下的所有文件和目录 delete_directory_contents(\"./runs/\") import torch as t\rimport matplotlib.pyplot as plot\rimport torch.nn as nn\rfrom torch.utils.tensorboard import SummaryWriter\r#########例子演示梯度下降损失（每个epoch的损失）\r#其中的 log_dir 表示保存日志的路径，默认会保存在“runs/ 当前时间 _ 主机名”文件夹中。\rwriter=SummaryWriter(log_dir=None)\rt.manual_seed(42)\r# 使用自动梯度实现线性回归\rx=t.randn(100,1)\rx_test=t.randn(20,1)\ry=3*x+2+t.randn(100,1) #实际值上加上一些随机噪点\ry_test=3*x+2+t.randn(100,1)\rclass LinearModel(nn.Module):\rdef __init__(self):\rnn.Module.__init__(self)\rself.w=nn.Parameter(t.randn(1,1))\rself.b=nn.Parameter(t.randn(1))\rdef forward(self,x):\rreturn t.mm(x,self.w)+self.b\rmodel=LinearModel()\rlossf=nn.MSELoss()\r#定义优化器,第一个参数为模型的参数，参数传入后,自动获取他的梯度并且-梯度*学习率\roptim=t.optim.SGD(model.parameters(),lr=0.01)\r#训练100次，100次梯度下降，计算到最小损失时的w和b\repochCount=100\rfor epoch in range(epochCount):\ry_pre=model(x)\r#注意梯度清零，否则会累加\roptim.zero_grad() loss=lossf(y_pre,y)\rwriter.add_scalar(\"Loss/train\",loss,epoch)\rloss.backward()\r#更新参数w和b\roptim.step()\rplot.plot(x,y,'.')\rplot.plot(x.data.numpy(),y_pre.data.numpy())\rplot.show()\rwriter.close() 运行后在runs目录下生成了日志，切换到当前安装tensorboard的环境执行命令：tensorboard –logdir=runs，\ntensorboard 是热加载的，上面的代码比如调整epoch次数，重新运行，是实时刷新的。\n(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\u003etensorboard --logdir=runs\rTensorFlow installation not found - running with reduced feature set.\rServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\rTensorBoard 2.13.0 at http://localhost:6006/ (Press CTRL+C to quit) 访问：http://localhost:6006/ 可以看到epoch到达80左右基本损失就很小了 我们把代码的epochCount调整到20 可以看到损失梯度下降还没有达到平缓，在看下拟合的图形 再把epochCount调整到10000 可以看到在100左右基本就平缓了，后面的训练是多余的了，所以我们可以观察到epoch到100是最合适的\nadd_histogram 使用 add_histogram 方法来记录一组数据的直方图。\nadd_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None) 参数\ntag (string): 数据名称 values (torch.Tensor, numpy.array, or string/blobname): 用来构建直方图的数据 global_step (int, optional): 训练的 step bins (string, optional): 取值有 ‘tensorflow’、‘auto’、‘fd’ 等, 该参数决定了分桶的方式，详见这里。 walltime (float, optional): 记录发生的时间，默认为 time.time() max_bins (int, optional): 最大分桶数 我们可以通过观察数据、训练参数、特征的直方图，了解到它们大致的分布情况，辅助神经网络的训练过程。 我们来观察下假设10次产生均值是0方差是1的1000条数据，每一次的波动 import numpy as np\rfrom torch.utils.tensorboard import SummaryWriter\rwriter = SummaryWriter()\rflag = 1\rif flag :\rfor x in range(10):\rdata_1 = np.arange(1000)\rdata_2 = np.random.normal(size=1000)\r#直方图的结构是y轴是第多少次，x轴显示value的波动\rwriter.add_histogram(\"data1\",data_1,x)\rwriter.add_histogram('data2',data_2,x)\rwriter.close() 右侧的坐标表示循环的次数，下方的坐标表示这1000个数的分布情况\n运行图 (graph) 使用 add_graph 方法来可视化一个神经网络。\nadd_graph(model, input_to_model=None, verbose=False, **kwargs) 参数\nmodel (torch.nn.Module): 待可视化的网络模型 input_to_model (torch.Tensor or list of torch.Tensor, optional): 待输入神经网络的变量或一组变量 在add_scalar线性回归的代码中我们打印线性模型输入x的计算图 model=LinearModel()\r#加入代码\rwriter.add_graph(model,model.w) 图片add_image 使用 add_image 方法来记录单个图像数据。注意，该方法需要 pillow 库的支持。\nadd_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW') 参数\ntag (string): 数据名称 img_tensor (torch.Tensor / numpy.array): 图像数据 global_step (int, optional): 训练的 step walltime (float, optional): 记录发生的时间，默认为 time.time() dataformats (string, optional): 图像数据的格式，默认为 'CHW'，即 Channel x Height x Width，还可以是 'CHW'、'HWC' 或 'HW' 等 我们一般会使用 add_image 来实时观察生成式模型的生成效果，或者可视化分割、目标检测的结果，帮助调试模型。\nVisdom 后续补",
    "description": "概述 PyTorch是一个基于Python的开源机器学习框架，由Facebook的人工智能研究团队开发并维护。它提供了丰富的工具和接口，用于构建和训练深度神经网络模型。\nPyTorch的主要特点和优势包括：\n动态图：PyTorch使用动态图机制，即在运行时构建计算图。这使得模型的构建和调试更加直观和灵活，能够更好地处理复杂的计算流程和动态控制流。\n简洁明了：PyTorch的API设计简洁明了，易于学习和使用。它提供了一系列高级接口，使得模型的构建、训练和评估变得更加简单和高效。\n强大的GPU加速支持：PyTorch能够利用GPU进行张量运算和模型训练，从而加快计算速度。它提供了简单易用的接口，使得在GPU上进行加速变得更加方便。\n灵活扩展：PyTorch支持自定义操作符和扩展，使得用户可以方便地实现和使用自己的模型组件和功能。\n相比之下，TensorFlow是由Google开发的另一个流行的深度学习框架。与PyTorch相比，TensorFlow的主要特点和优势包括：\n静态图：TensorFlow使用静态图机制，即在编译时构建计算图。这使得TensorFlow在模型运行时能够进行更多的优化和性能提升，适用于大规模的、计算密集型的任务。\n跨平台支持：TensorFlow可以在多种硬件和操作系统上运行，并且具有广泛的部署支持。它提供了TensorFlow Serving、TensorFlow Lite和TensorFlow.js等工具，使得模型的部署和移植更加方便。\n分布式训练支持：TensorFlow提供了分布式训练的功能，可以在多个设备和计算节点上进行模型训练，从而加快训练速度。\n生态系统和社区：TensorFlow具有庞大的生态系统和活跃的社区，提供了丰富的资源和支持，包括模型库、教程和论坛等。\n总的来说，PyTorch和TensorFlow都是优秀的深度学习框架，各有其特点和适用场景。PyTorch适合于快速原型开发、动态计算流程和小规模任务，而TensorFlow适合于大规模、计算密集型的任务和分布式训练。选择哪个框架取决于具体的需求和个人偏好。\n对于初学接触神经网络，建议先学pytorch，它提供的api接近理论概念，有动态图，方便调试，适合做研究使用，，由于最近chargpt的大火，Hugging Face的transforms是使用PyTorch的。Hugging Face是一个提供自然语言处理（NLP）模型和工具的平台，他们的Transformers库主要基于PyTorch实现，他的入门pytorch必须要有基础。这个库提供了一系列用于数据预处理和后处理的函数，可以方便地对文本数据进行转换和处理。\n环境准备 安装cuda和cudnn 一般pc电脑或者服务器都有nvida显卡，可以通过nvidia-smi命令查看。 其中python环境（3.8+版本），cuda和cudnn安装请参考：https://blog.csdn.net/liaomin416100569/article/details/130532993 安装后可以看到我的cuda version是11.2\n安装pytorch 考虑到版本向下兼容，不一定非要下载cuda=11.2对应的那个版本的torch，或许低于这个版本就可以。所以我就选择下载cuda11.1的版本。 以下是pytorch对应的稳定版的网址下载链接，可以根据需要找到对应的torch版本下载。cu版本就是gpu版本，不带cu的是cpu版本，https://download.pytorch.org/whl/torch_stable.html，搜索cu111 直接选择\npip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html 编写测试代码\nimport torch\rprint(torch.__version__)\r#cuda是否可用，如果返回True，表示正常可用gpu\rprint(torch.cuda.is_available())\rprint(torch.cuda.device_count())\rx1=torch.rand(5,3)\r#把x1转换gpu0的tensor\rx1=x1.cuda(0)\rprint(x1) 测试运行",
    "tags": [],
    "title": "深度学习06-pytorch从入门到精通",
    "uri": "/docs/programming/ai/deep_learning/frameworks/dl_06_pytorch/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 生成对抗网络",
    "content": "@[toc]\n概述 GAN（Generative Adversarial Network）是一种生成模型，由两个神经网络组成：生成器（Generator）和判别器（Discriminator）。GAN的基本思想是通过让生成器和判别器相互对抗来学习生成真实样本的能力。\n生成器的作用是将一个随机噪声向量作为输入，通过一系列的神经网络层逐渐将其转化为一个与真实样本相似的输出。生成器的目标是尽量使生成的样本被判别器误认为是真实样本，从而欺骗判别器。生成器的训练目标是最小化生成样本与真实样本之间的差异。\n判别器的作用是将输入的样本区分为真实样本和生成样本。判别器的目标是尽量准确地判断样本的真伪。判别器的训练目标是最大化判别真实样本和生成样本的能力。\nGAN的训练过程可以简述为以下几个步骤：\n初始化生成器和判别器的参数。 从真实样本中随机选择一批样本，作为判别器的训练集。同时，生成一批随机噪声向量，作为生成器的输入。 使用生成器生成一批样本，并将其与真实样本混合，构成判别器的训练集。 使用判别器对训练集中的样本进行判别，并计算生成样本与真实样本的损失。 更新生成器和判别器的参数，使生成样本的质量逐渐提高，同时判别器的判别能力也逐渐增强。 重复步骤2-5，直到生成器能够生成与真实样本相似的样本。 DCGAN（Deep Convolutional GAN）是GAN的一种改进版本，主要通过引入卷积神经网络（CNN）来提高生成器和判别器的性能。DCGAN在生成器和判别器中使用了卷积层和反卷积层，使其能够处理图像数据。相较于传统的GAN，DCGAN在生成图像的细节和纹理上有更好的表现。\n总的来说，GAN是一种通过生成器和判别器相互对抗来学习生成真实样本的生成模型，而DCGAN是在GAN的基础上引入了卷积神经网络，提高了对图像数据的处理能力。\n原理简介 GAN的开山之作是被称为“GAN之父”的Ian Goodfellow发表于2014年的经典论文Generative Adversarial Networks[2]，在这篇论文中他提出了生成对抗网络，并设计了第一个GAN实验——手写数字生成。\nGAN的产生来自于一个灵机一动的想法：\n“What I cannot create，I do not understand.”（那些我所不能创造的，我也没有真正地理解它。） —Richard Feynman\n类似地，如果深度学习不能创造图片，那么它也没有真正地理解图片。当时深度学习已经开始在各类计算机视觉领域中攻城略地，在几乎所有任务中都取得了突破。但是人们一直对神经网络的黑盒模型表示质疑，于是越来越多的人从可视化的角度探索卷积网络所学习的特征和特征间的组合，而GAN则从生成学习角度展示了神经网络的强大能力。GAN解决了非监督学习中的著名问题：给定一批样本，训练一个系统能够生成类似的新样本。\n生成对抗网络的网络结构如图7-2所示，主要包含以下两个子网络。 • 生成器（generator）：输入一个随机噪声，生成一张图片。 • 判别器（discriminator）：判断输入的图片是真图片还是假图片。 训练判别器时，需要利用生成器生成的假图片和来自真实世界的真图片；训练生成器时，只用噪声生成假图片。判别器用来评估生成的假图片的质量，促使生成器相应地调整参数。\n生成器的目标是尽可能地生成以假乱真的图片，让判别器以为这是真的图片；判别器的目标是将生成器生成的图片和真实世界的图片区分开。可以看出这二者的目标相反，在训练过程中互相对抗，这也是它被称为生成对抗网络的原因。 上面的描述可能有点抽象，让我们用收藏齐白石作品（齐白石作品如图7-3所示）的书画收藏家和假画贩子的例子来说明。假画贩子相当于是生成器，他们希望能够模仿大师真迹伪造出以假乱真的假画，骗过收藏家，从而卖出高价；书画收藏家则希望将赝品和真迹区分开，让真迹流传于世，销毁赝品。这里假画贩子和收藏家所交易的画，主要是齐白石画的虾。齐白石画虾可以说是画坛一绝，历来为世人所追捧。 在这个例子中，一开始假画贩子和书画收藏家都是新手，他们对真迹和赝品的概念都很模糊。假画贩子仿造出来的假画几乎都是随机涂鸦，而书画收藏家的鉴定能力很差，有不少赝品被他当成真迹，也有许多真迹被当成赝品。\n首先，书画收藏家收集了一大堆市面上的赝品和齐白石大师的真迹，仔细研究对比，初步学习了画中虾的结构，明白画中的生物形状弯曲，并且有一对类似钳子的“螯足”，对于不符合这个条件的假画全部过滤掉。当收藏家用这个标准到市场上进行鉴定时，假画基本无法骗过收藏家，假画贩子损失惨重。但是假画贩子自己仿造的赝品中，还是有一些蒙骗过关，这些蒙骗过关的赝品中都有弯曲的形状，并且有一对类似钳子的“螯足”。于是假画贩子开始修改仿造的手法，在仿造的作品中加入弯曲的形状和一对类似钳子的“螯足”。除了这些特点，其他地方例如颜色、线条都是随机画的。假画贩子制造出的第一版赝品如所示。 当假画贩子把这些画拿到市面上去卖时，很容易就骗过了收藏家，因为画中有一只弯曲的生物，生物前面有一对类似钳子的东西，符合收藏家认定的真迹的标准，所以收藏家就把它当成真迹买回来。随着时间的推移，收藏家买回越来越多的假画，损失惨重，于是他又闭门研究赝品和真迹之间的区别，经过反复比较对比，他发现齐白石画虾的真迹中除了有弯曲的形状，虾的触须蔓长，通身作半透明状，并且画的虾的细节十分丰富，虾的每一节之间均呈白色状。\n收藏家学成之后，重新出山，而假画贩子的仿造技法没有提升，所制造出来的赝品被收藏家轻松识破。于是假画贩子也开始尝试不同的画虾手法，大多都是徒劳无功，不过在众多尝试之中，还是有一些赝品骗过了收藏家的眼睛。假画贩子发现这些仿制的赝品触须蔓长，通身作半透明状，并且画的虾的细节十分丰富，如图7-5所示。于是假画贩子开始大量仿造这种画，并拿到市面上销售，许多都成功地骗过了收藏家。 收藏家再度损失惨重，被迫关门研究齐白石的真迹和赝品之间的区别，学习齐白石真迹的特点，提升自己的鉴定能力。就这样，通过收藏家和假画贩子之间的博弈，收藏家从零开始慢慢提升了自己对真迹和赝品的鉴别能力，而假画贩子也不断地提高自己仿造齐白石真迹的水平。收藏家利用假画贩子提供的赝品，作为和真迹的对比，对齐白石画虾真迹有了更好的鉴赏能力；而假画贩子也不断尝试，提升仿造水平，提升仿造假画的质量，即使最后制造出来的仍属于赝品，但是和真迹相比也很接近了。收藏家和假画贩子二者之间互相博弈对抗，同时又不断促使着对方学习进步，达到共同提升的目的。\n在这个例子中，假画贩子相当于一个生成器，收藏家相当于一个判别器。一开始生成器和判别器的水平都很差，因为二者都是随机初始化的。训练过程分为两步交替进行，第一步是训练判别器（只修改判别器的参数，固定生成器），目标是把真迹和赝品区分开；第二步是训练生成器（只修改生成器的参数，固定判别器），为的是生成的假画能够被判别器判别为真迹（被收藏家认为是真迹）。这两步交替进行，进而分类器和判别器都达到了一个很高的水平。训练到最后，生成器生成的虾的图片（如图7-6所示）和齐白石的真迹几乎没有差别。 下面我们来思考网络结构的设计。判别器的目标是判断输入的图片是真迹还是赝品，所以可以看成是一个二分类网络，我们可以设计一个简单的卷积网络。生成器的目标是从噪声中生成一张彩色图片，这里我们采用广泛使用的DCGAN（Deep Convolutional Generative Adversarial Networks）结构，即采用全卷积网络，其结构如下图所示。网络的输入是一个100维的噪声，输出是一个3×64×64的图片。这里的输入可以看成是一个100×1×1的图片，通过上卷积慢慢增大为4×4、8×8、16×16、32×32和64×64。上卷积，或称转置卷积，是一种特殊的卷积操作，类似于卷积操作的逆运算。当卷积的stride为2时，输出相比输入会下采样到一半的尺寸；而当上卷积的stride为2时，输出会上采样到输入的两倍尺寸。这种上采样的做法可以理解为图片的信息保存于100个向量之中，神经网络根据这100个向量描述的信息，前几步的上采样先勾勒出轮廓、色调等基础信息，后几步上采样慢慢完善细节。网络越深，细节越详细。 该章节引用自书籍《深度学习框架pytorch，入门到实践》\n专业术语 上采样(Upsample)在应用在计算机视觉的深度学习领域，由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算(e.g.:图像的语义分割)，这个采用扩大图像尺寸，实现图像由小分辨率到大分辨率的映射的操作，叫做上采样(Upsample)。 矩阵零填充（Zero Padding）是指向矩阵的边界添加零值的过程。在计算机视觉和深度学习中，矩阵零填充常用于图像处理和卷积神经网络（CNN）中。 在图像处理中，矩阵零填充可以用于扩展图像的尺寸，以便在进行卷积运算时保持图像的大小不变。通过在图像周围添加零值，可以确保卷积核可以完全覆盖图像的边缘像素。这样做可以避免在卷积操作中丢失图像边缘的信息。 在卷积神经网络（CNN）中，矩阵零填充常用于调整卷积层的输入尺寸和输出尺寸。通过在输入矩阵的边界上添加零值，可以确保卷积操作产生的特征图的尺寸与输入矩阵的尺寸保持一致。这对于构建深度神经网络和处理不同尺寸的输入数据非常重要。 矩阵零填充的大小通常由填充的行数和列数决定。在CNN中，填充的大小往往与卷积核的大小和步幅相关。通过合理选择填充大小，可以在保持输入输出尺寸一致的同时，控制特征图的尺寸和感受野的大小。 反卷积(Transposed Convolution)上采样有3种常见的方法：双线性插值(bilinear)，反卷积(Transposed Convolution)，反池化(Unpooling)，我们这里只讨论反卷积。这里指的反卷积，也叫转置卷积，它并不是正向卷积的完全逆过程，用一句话来解释：反卷积是一种特殊的正向卷积，先按照一定的比例通过补 000 来扩大输入图像的尺寸，接着旋转卷积核，再进行正向卷积。 零填充 对于图像处理中的一些过程，我需要对读取的numpy矩阵进行size的扩充，比如原本是（4，6）的矩阵，现在需要上下左右各扩充3行，且为了不影响数值计算，都用0填充。 比如下图，我有一个4x5大小的全1矩阵，但是现在我要在四周都加上3行的0来扩充大小，最后扩充完还要对原区域进行操作。\n如果原始矩阵的形状为 (m, n)，并且在每个边缘上填充了 p 行和 q 列的值，那么填充后的矩阵的形状将是 (m + 2p, n + 2q) 如果原始矩阵的形状为 (m, n)，并且在每个元素之间插入一个零，那么新的矩阵是（m+m-1,n,n-1）=(2m-1,2n-1) numpy已经封装了一个函数，就是pad\n#%%\rimport numpy as np\roneArry=np.ones((4,5))\rprint(oneArry)\rprint(\"周围\",np.pad(oneArry,3)) #等价于print(np.pad(oneArry,(3,3)))\r#注意元组0是左上角补充3行 ，元组1表示右小角\rprint(\"左上角\",np.pad(oneArry,(3,0)))\rprint(\"右下角\",np.pad(oneArry,(0,3))) 输出\n[[1. 1. 1. 1. 1.]\r[1. 1. 1. 1. 1.]\r[1. 1. 1. 1. 1.]\r[1. 1. 1. 1. 1.]]\r周围 [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\r[0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\r[0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\r[0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\r左上角 [[0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 1. 1. 1. 1. 1.]\r[0. 0. 0. 1. 1. 1. 1. 1.]\r[0. 0. 0. 1. 1. 1. 1. 1.]\r[0. 0. 0. 1. 1. 1. 1. 1.]]\r右下角 [[1. 1. 1. 1. 1. 0. 0. 0.]\r[1. 1. 1. 1. 1. 0. 0. 0.]\r[1. 1. 1. 1. 1. 0. 0. 0.]\r[1. 1. 1. 1. 1. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0.]] 元素内填充指在元素的内部上下左右填充， 比如\n[[1, 1],\r[1, 1]] 填充为：\n[[1, 0, 1], [0, 0, 0],\r[1, 0, 1]] 代码实现\nimport numpy as np\rmatrix = [[1, 1],\r[1, 1]]\rzero_inserted_matrix = np.zeros((2*len(matrix)-1, 2*len(matrix[0])-1))\rfor i in range(len(matrix)):\rfor j in range(len(matrix[0])):\rzero_inserted_matrix[2*i][2*j] = matrix[i][j]\rprint(zero_inserted_matrix) 输出\n[[1. 0. 1.]\r[0. 0. 0.]\r[1. 0. 1.]] 转置卷积 参考：https://www.zhihu.com/question/48279880 ​ 转置卷积或微步幅卷积。但是，需要指出去卷积这个名称并不是很合适，因为转置卷积并非信号/图像处理领域定义的那种真正的去卷积。从技术上讲，信号处理中的去卷积是卷积运算的逆运算。但这里却不是这种运算。后面我们会介绍为什么将这种运算称为转置卷积更自然且更合适。\n​ 我们可以使用常见卷积实现转置卷积。这里我们用一个简单的例子来说明，输入层为2∗2(下面蓝色的部分)，先进行填充值Padding为2∗2单位步长的零填充（下面在蓝色上下左右填充2行2列），再使用步长Stride为1的3∗3卷积核进行卷积操作（卷积一次获得一个值）则实现了上采样，上采样输出的大小为4∗4 也就是（6-3+1，6-3+1）\n值得一提的是，可以通过各种填充和步长，我们可以将同样的2∗2输入映射到不同的图像尺寸。下图，转置卷积被应用在同一张2∗2的输入上（输入之间插入了一个零，并且周围加了2∗2的单位步长的零填充）上应用3∗3的卷积核，得到的结果（即上采样结果）大小为5∗5 通过观察上述例子中的转置卷积能够帮助我们构建起一些直观的认识。但为了进一步应用转置卷积，我们还需要了解计算机的矩阵乘法是如何实现的。从实现过程的角度我们可以理解为何转置卷积才是最合适的名称。\n​ 在卷积中，我们这样定义：用C代表卷积核，input为输入图像，output为输出图像。经过卷积（矩阵乘法）后，我们将input从大图像下采样为小图像output。这种矩阵乘法实现遵循C∗input=output。\n​ 下面的例子展示了这种运算在计算机内的工作方式。它将输入平展（16∗1）矩阵，并将卷积核转换为一个稀疏矩阵、（4∗16）。然后，在稀疏矩阵和平展的输入之间使用矩阵乘法。之后，再将所得到的矩阵（4∗1)转为2∗2输出。 此时，若用卷积核对应稀疏矩阵的转置$C^T$（16∗4）乘以输出的平展（4∗1）所得到的结果（16∗1）的形状和输入的形状（16∗1）相同。\n但值得注意的是，上述两次操作并不是可逆关系，对于同一个卷积核（因非其稀疏矩阵不是正交矩阵，结果转置操作之后并不能恢复到原始的数值，而仅仅保留原始的形状，所以转置卷积的名字由此而来。并回答了上面提到的疑问，相比于逆卷积而言转置卷积更加准确。\n生成动漫图像 使用DCGAN训练一个模型用于生成64*64动漫图像,并通过这个例子规划神经网络的目录结构组织，大部分的开源项目的目录结构相似，以后分析开源model更加容易。\n算力选择 由于gan训练需要的资源较大，时间较长，建议使用gpu服务器 gpt云平台上提供的GPU型号很多。我们按照GPU架构大致分为五类（推荐autodl或者inscode，可以按时计费，用完就释放）：\nNVIDIA Pascal架构的GPU，如TitanXp，GTX 10系列等。 这类GPU缺乏低精度的硬件加速能力，但却具备中等的单精度算力。由于价格便宜，适合用来练习训练小模型(如Cifar10)或调试模型代码。 NVIDIA Volta/Turing架构的GPU，如GTX 20系列, Tesla V100等。 这类GPU搭载专为低精度(int8/float16)计算加速的TensorCore, 但单精度算力相较于上代提升不大。我们建议在实例上启用深度学习框架的混合精度训练来加速模型计算。 相较于单精度训练，混合精度训练通常能够提供2倍以上的训练加速。 NVIDIA Ampere架构的GPU，如GTX 30系列，Tesla A40/A100等。 这类GPU搭载第三代TensorCore。相较于前一代，支持了TensorFloat32格式，可直接加速单精度训练 (PyTorch已默认开启)。但我们仍建议使用超高算力的float16半精度训练模型，可获得比上一代GPU更显著的性能提升。 寒武纪 MLU 200系列加速卡。 暂不支持模型训练。使用该系列加速卡进行模型推理需要量化为int8进行计算。 并且需要安装适配寒武纪MLU的深度学习框架。 华为 Ascend 系列加速卡。 支持模型训练及推理。但需安装MindSpore框架进行计算。 GPU型号的选择并不困难。对于常用的深度学习模型，根据GPU对应精度的算力可大致推算GPU训练模型的性能。AutoDL平台标注并排名了每种型号GPU的算力，方便大家选择适合自己的GPU。\nGPU的数量选择与训练任务有关。一般我们认为模型的一次训练应当在24小时内完成，这样隔天就能训练改进之后的模型。以下是选择多GPU的一些建议：\n1块GPU。适合一些数据集较小的训练任务，如Pascal VOC等。 2块GPU。同单块GPU，但是你可以一次跑两组参数或者把Batchsize扩大。 4块GPU。适合一些中等数据集的训练任务，如MS COCO等。 8块GPU。经典永流传的配置！适合各种训练任务，也非常方便复现论文结果。 我要更多！用于训练大参数模型、大规模调参或超快地完成模型训练。 我常用的gpu按照性能从高到低的顺序，这些机器的GPU算力排名如下，并附上它们的基本配置信息：\nA100： CUDA核心数：6912 Tensor核心数：432 显存容量：40 GB 内存带宽：1555 GB/s 架构：Ampere V100： CUDA核心数：5120 Tensor核心数：640 显存容量：16 GB / 32 GB / 32 GB HBM2 内存带宽：900 GB/s / 1134 GB/s / 1134 GB/s 架构：Volta P100： CUDA核心数：3584 Tensor核心数：0 显存容量：16 GB / 12 GB HBM2 内存带宽：732 GB/s / 549 GB/s 架构：Pascal Tesla T4： CUDA核心数：2560 Tensor核心数：320 显存容量：16 GB 内存带宽：320 GB/s 架构：Turing RTX A4000： CUDA核心数：6144 Tensor核心数：192 显存容量：16 GB 内存带宽：448 GB/s 架构：Ampere 使用P100训练完大概1个小时（3831.2s） 数据集 kagle上：https://www.kaggle.com/code/splcher/starter-anime-face-dataset 邮箱注册个账号即可下载，数据集下有不同的用户基于数据集的训练代码和结果。 参考代码：https://www.kaggle.com/code/splcher/starter-anime-face-dataset\n目录规划 其中各个文件的主要内容和作用如下。\n• checkpoints/：用于保存训练好的模型，可使程序在异常退出后仍能重新载入模型，恢复训练。 • data/：数据相关操作，包括数据预处理、dataset实现等。 • models/：模型定义，可以有多个模型，例如上面的AlexNet和ResNet34，一个模型对应一个文件。 • utils/：可能用到的工具函数，本次实验中主要封装了可视化工具。 • config.py：配置文件，所有可配置的变量都集中在此，并提供默认值。 • main.py：主文件，训练和测试程序的入口，可通过不同的命令来指定不同的操作和参数。 • requirements.txt：程序依赖的第三方库。 • README.md：提供程序的必要说明。\n源代码 数据源加载 将下载好的AnimeFaceDataset添加到data目录，新建dataset.py用于加载数据集\nfrom torch.utils.data import Dataset,DataLoader\rfrom torchvision import datasets, transforms\rclass AtomicDataset(Dataset):\rdef __init__(self,root,image_size):\rDataset.__init__(self)\rself.dataset=datasets.ImageFolder(root,\rtransform=transforms.Compose([\rtransforms.Resize(image_size),\rtransforms.CenterCrop(image_size),\rtransforms.ToTensor(),\rtransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r]))\rdef __getitem__(self,index):\rreturn self.dataset[index]\rdef __len__(self):\rreturn len(self.dataset)\rdef toBatchLoader(self,batch_size):\rreturn DataLoader(self,batch_size=batch_size, shuffle=False) 定义配置类 config.py定义配置类\nclass Config:\r#定义转换后图像的大小\rimg_size=64\r#训练图片所在目录，目录必须是有子目录，子目录名称就是分类名\rimg_root=\"./data/AnimeFaceDataset\"\r#每次加载的批次数\rbatch_size=64\r\"\"\"\r在卷积神经网络中，这些缩写通常表示以下含义：\rnz：表示输入噪声向量的维度。全称为\"noise dimension\"，即噪声维度。\rngf：表示生成器网络中特征图的通道数。全称为\"number of generator features\"，即生成器特征图通道数。\rnc：表示输入图像的通道数。全称为\"number of image channels\"，即图像通道数。\r\"\"\"\r#表示噪声的维度，一般是(100,1,1)\rnz=100\r#表示生成特征图的维度,64*64的图片\rngf=64\r#生成或者传入图片的通道数\rnc=3\r# 表示判别器输入特征图的维度,64*64的图片\rndf = 64\r# 优化器的学习率\rlr = 0.0002\r# Beta1 hyperparam for Adam optimizers\rbeta1 = 0.5\r# epochs的次数\rnum_epochs=50\rdef __init__(self,kv):\rfor key, value in kv.items():\rsetattr(self, key, value) 由于这些配置默认是静态的，可以使用fire将参数定义到命令行，通过\npython main.py 函数名 --参数值1=值1 --参数值2=值2的方式传入到**kwargs main.py定义train方式\ndef train(**kwargs):\rprint(kwargs)\rif __name__ == \"__main__\":\r# 将main.py中所有的函数映射成 python main.py 方法名 --参数1=参数值 --参数2=参数值的形式，这些参数以keyvalue字典的形式传入kwargs\rfire.Fire() 定义模型 在models目录下新建models.py定义G和D模型\nimport torch.nn as nn\r\"\"\"\rnn.ConvTranspose2d的参数包括：\rin_channels：输入通道数\rout_channels：输出通道数\rkernel_size：卷积核大小\rstride：步长\rpadding：填充大小\routput_padding：输出填充大小\rgroups：分组卷积数量，默认为1\rbias：是否使用偏置，默认为True\r生成器的目标是从一个随机噪声向量生成逼真的图像。在生成器中，通道数从大到小可以理解为从抽象的特征逐渐转化为具体的图像细节。通过逐层转置卷积（ConvTranspose2d）操作，\r将低维度的特征逐渐转化为高维度的图像。通道数的减少可以理解为对特征进行提取和压缩，以生成更具细节和逼真度的图像。\r\"\"\"\r#生成网络\rclass Generator(nn.Module):\rdef __init__(self, nz,ngf,nc):\rsuper(Generator, self).__init__()\rself.main = nn.Sequential(\r# nz表示噪声的维度，一般是(100,1,1)\r# ngf表示生成特征图的维度\r# nc表示输入或者输出图像的维度\r#输出尺寸 = (输入尺寸（高度） - 1) * stride - 2 * padding + kernel_size + output_padding\r#如果（卷积核,步长，填充）=(4, 1, 0)表示图像的维度是卷积核的大小（卷积核高,卷积核宽）\r#如果（卷积核,步长，填充）=(4, 2, 1)表示图像的维度是是上一个图像的2被（输入图像高度*2,输入图像宽度*2）\rnn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\rnn.BatchNorm2d(ngf * 8),\rnn.ReLU(True),\r# state size. (ngf*8) x 4 x 4\rnn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\rnn.BatchNorm2d(ngf * 4),\rnn.ReLU(True),\r# state size. (ngf*4) x 8 x 8\rnn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\rnn.BatchNorm2d(ngf * 2),\rnn.ReLU(True),\r# state size. (ngf*2) x 16 x 16\rnn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\rnn.BatchNorm2d(ngf),\rnn.ReLU(True),\r# state size. (ngf) x 32 x 32\rnn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\rnn.Tanh()\r# state size. (nc) x 64 x 64\r)\rdef forward(self, input):\rreturn self.main(input)\r\"\"\"\r和转置卷积相反的是（4,2,1）会让维度2倍降低\r卷积过程是height-kerel+1\r\"\"\"\rclass Discriminator(nn.Module):\rdef __init__(self, nc,ndf):\rsuper(Discriminator, self).__init__()\rself.main = nn.Sequential(\r# input is (nc) x 64 x 64\rnn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\rnn.LeakyReLU(0.2, inplace=True),\r# state size. (ndf) x 32 x 32\rnn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\rnn.BatchNorm2d(ndf * 2),\rnn.LeakyReLU(0.2, inplace=True),\r# state size. (ndf*2) x 16 x 16\rnn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\rnn.BatchNorm2d(ndf * 4),\rnn.LeakyReLU(0.2, inplace=True),\r# state size. (ndf*4) x 8 x 8\rnn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\rnn.BatchNorm2d(ndf * 8),\rnn.LeakyReLU(0.2, inplace=True),\r# state size. (ndf*8) x 4 x 4\rnn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\rnn.Sigmoid()\r# state size （1,1,1）\r)\rdef forward(self, input):\rreturn self.main(input) 训练 训练D模型，让输出的数据和1比较计算损失，让损失最小化，G生成的数据使用D模型预测和0比较（骗不过）计算损失，让损失最小化。 训练G模型，生成的图片，使用D模型预测，和1比较（骗过D模型）损失，让损失最小化\ndef train(**kwargs):\r# 通过传入的参数初始化Config\rdefaultConfig = Config(kwargs)\r# 通过给定的目录和图像大小转换成数据集\rdataset = AtomicDataset(defaultConfig.img_root, defaultConfig.img_size)\r# 转换为可迭代的批次为defaultConfig.batch_size的数据集\rdataloader = dataset.toBatchLoader(defaultConfig.batch_size)\r# 创建生成网络模型\rnetG = Generator(defaultConfig.nz, defaultConfig.ngf, defaultConfig.nc).to(device)\r# 创建分类器模型\rnetD = Discriminator(defaultConfig.nc, defaultConfig.ndf).to(device)\r# 使用criterion = nn.BCELoss()\rcriterion = nn.BCELoss()\r# Setup Adam optimizers for both G and D\roptimizerD = optim.Adam(netD.parameters(), lr=defaultConfig.lr, betas=(defaultConfig.beta1, 0.999))\roptimizerG = optim.Adam(netG.parameters(), lr=defaultConfig.lr, betas=(defaultConfig.beta1, 0.999))\r# 如果是真的图片label=1，伪造的图片为0\rreal_label = 1\rfake_label = 0\r# Lists to keep track of progress\rimg_list = []\rG_losses = []\rD_losses = []\riters = 0\r#生成一个64批次100*1*1的噪声\rfixed_noise = torch.randn(64, defaultConfig.nz, 1, 1, device=device)\rprint(\"Starting Training Loop...\")\r# For each epoch\rfor epoch in range(defaultConfig.num_epochs):\r# For each batch in the dataloader\rfor i, data in enumerate(dataloader, 0):\r############################\r# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\r# 对于真实传入的图片进行判断器训练，label肯定是1\r# 对于噪声传入的图片进行判断器训练，label肯定是0\r###########################\r## 通过真实图片训练D网络\rnetD.zero_grad()\r# 将64批次数据转换为gpu设备\rreal_cpu = data[0].to(device)\r# 获取批次的个数\rb_size = real_cpu.size(0)\r# 生成的是一个一维的张量，其中包含64个元素,每个元素的值为1。\rlabel = torch.full((b_size,), real_label, device=device).float()\r# 分类器捲積后最后产生一个64个批次的1*1，转换成1维数组。\routput = netD(real_cpu).view(-1)\r# 计算和真实数据的损失\rerrD_real = criterion(output, label)\r# 反向传播计算梯度\rerrD_real.backward()\r# D_x的值表示判别器对真实样本的平均预测概率\rD_x = output.mean().item()\r## 通过噪声训练生成器模型\r# 生成噪声的变量 也是64批次，噪声的通道数是100\rnoise = torch.randn(b_size, defaultConfig.nz, 1, 1, device=device)\r# 传入到生成网络中，生成一张64*3*64*64的图片\rfake = netG(noise)\r# 生成器生成的图片对应的真实的label应该是0\rlabel.fill_(fake_label)\r# detach()是PyTorch中的一个函数，它用于从计算图中分离出一个Tensor。当我们调用detach()函数时，它会返回一个新的Tensor，该Tensor与原始Tensor共享相同的底层数据，但不会有梯度信息。\r# 使用判别器网络来判断通过噪声生成的图片，转换为1维\routput = netD(fake.detach()).view(-1)\r# 进行损失函数计算\rerrD_fake = criterion(output, label)\r# 反向传播计算梯度\rerrD_fake.backward()\r# 表示判别器对虚假样本的平均预测概率\rD_G_z1 = output.mean().item()\r# 将真实图片和虚假图片的损失求和获取所有的损失\rerrD = errD_real + errD_fake\r# 更新权重参数\roptimizerD.step()\r############################\r# (2) Update G network: maximize log(D(G(z)))\r# 对于G网络来说，对于虚假传入的图片进行判断器训练，尽量让判别器认为是真1，生成的图片才够真实\r###########################\rnetG.zero_grad()\rlabel.fill_(real_label) # fake labels are real for generator cost\r# 使用之前的G网络生成的图片64*3*64*64,传入D网络\routput = netD(fake).view(-1)\r# 计算G网路的损失\rerrG = criterion(output, label)\r# 反向计算梯度\rerrG.backward()\r#表示判别器对虚假样本判断为真的的平均预测概率\rD_G_z2 = output.mean().item()\r# 更新G的权重\roptimizerG.step()\r# 输出训练统计，每1000批次\rif i % 1000 == 0:\rprint('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\r% (epoch, defaultConfig.num_epochs, i, len(dataloader),\rerrD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\r# Save Losses for plotting later\rG_losses.append(errG.item())\rD_losses.append(errD.item())\r# 即每经过一定数量的迭代（iters % 250 == 0）或者是训练的最后一个epoch的最后一个batch（(epoch == defaultConfig.num_epochs - 1) and (i == len(dataloader) - 1)），\r# 就会使用G网络通过噪声生成64批次3通道64*64的图像，并且加入到img_list去做可视化，看看效果\rif (iters % 250 == 0) or ((epoch == defaultConfig.num_epochs - 1) and (i == len(dataloader) - 1)):\rwith torch.no_grad():\rfake = netG(fixed_noise).detach().cpu()\rimg_list.append(vutils.make_grid(fake, padding=2, normalize=True))\riters += 1\r#保存生成器的网络到checkpoints目录\rtorch.save(netG.state_dict(), \"./checkpoints/optimizerG.pt\") 可视化 绘制损失 #绘制G和D的损失函数图像\rplt.figure(figsize=(10, 5))\rplt.title(\"Generator and Discriminator Loss During Training\")\r#一维数组的索引值是x坐标也就是批次索引\rplt.plot(G_losses, label=\"G\")\rplt.plot(D_losses, label=\"D\")\rplt.xlabel(\"iterations\")\rplt.ylabel(\"Loss\")\rplt.legend()\rplt.show() 绘制生成器图像变化 #创建一个8*8的画布\rfig = plt.figure(figsize=(8, 8))\rplt.axis(\"off\")\rims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in img_list]\rani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\rHTML(ani.to_jshtml()) 每250次迭代就通过G生成64图片，发现越到后面图片就越清晰。 其他项目 CycleGAN 参考地址：https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\nstargan 参考地址：https://github.com/yunjey/stargan",
    "description": "@[toc]\n概述 GAN（Generative Adversarial Network）是一种生成模型，由两个神经网络组成：生成器（Generator）和判别器（Discriminator）。GAN的基本思想是通过让生成器和判别器相互对抗来学习生成真实样本的能力。\n生成器的作用是将一个随机噪声向量作为输入，通过一系列的神经网络层逐渐将其转化为一个与真实样本相似的输出。生成器的目标是尽量使生成的样本被判别器误认为是真实样本，从而欺骗判别器。生成器的训练目标是最小化生成样本与真实样本之间的差异。\n判别器的作用是将输入的样本区分为真实样本和生成样本。判别器的目标是尽量准确地判断样本的真伪。判别器的训练目标是最大化判别真实样本和生成样本的能力。\nGAN的训练过程可以简述为以下几个步骤：\n初始化生成器和判别器的参数。 从真实样本中随机选择一批样本，作为判别器的训练集。同时，生成一批随机噪声向量，作为生成器的输入。 使用生成器生成一批样本，并将其与真实样本混合，构成判别器的训练集。 使用判别器对训练集中的样本进行判别，并计算生成样本与真实样本的损失。 更新生成器和判别器的参数，使生成样本的质量逐渐提高，同时判别器的判别能力也逐渐增强。 重复步骤2-5，直到生成器能够生成与真实样本相似的样本。 DCGAN（Deep Convolutional GAN）是GAN的一种改进版本，主要通过引入卷积神经网络（CNN）来提高生成器和判别器的性能。DCGAN在生成器和判别器中使用了卷积层和反卷积层，使其能够处理图像数据。相较于传统的GAN，DCGAN在生成图像的细节和纹理上有更好的表现。\n总的来说，GAN是一种通过生成器和判别器相互对抗来学习生成真实样本的生成模型，而DCGAN是在GAN的基础上引入了卷积神经网络，提高了对图像数据的处理能力。\n原理简介 GAN的开山之作是被称为“GAN之父”的Ian Goodfellow发表于2014年的经典论文Generative Adversarial Networks[2]，在这篇论文中他提出了生成对抗网络，并设计了第一个GAN实验——手写数字生成。\nGAN的产生来自于一个灵机一动的想法：\n“What I cannot create，I do not understand.”（那些我所不能创造的，我也没有真正地理解它。） —Richard Feynman\n类似地，如果深度学习不能创造图片，那么它也没有真正地理解图片。当时深度学习已经开始在各类计算机视觉领域中攻城略地，在几乎所有任务中都取得了突破。但是人们一直对神经网络的黑盒模型表示质疑，于是越来越多的人从可视化的角度探索卷积网络所学习的特征和特征间的组合，而GAN则从生成学习角度展示了神经网络的强大能力。GAN解决了非监督学习中的著名问题：给定一批样本，训练一个系统能够生成类似的新样本。\n生成对抗网络的网络结构如图7-2所示，主要包含以下两个子网络。 • 生成器（generator）：输入一个随机噪声，生成一张图片。 • 判别器（discriminator）：判断输入的图片是真图片还是假图片。 训练判别器时，需要利用生成器生成的假图片和来自真实世界的真图片；训练生成器时，只用噪声生成假图片。判别器用来评估生成的假图片的质量，促使生成器相应地调整参数。\n生成器的目标是尽可能地生成以假乱真的图片，让判别器以为这是真的图片；判别器的目标是将生成器生成的图片和真实世界的图片区分开。可以看出这二者的目标相反，在训练过程中互相对抗，这也是它被称为生成对抗网络的原因。 上面的描述可能有点抽象，让我们用收藏齐白石作品（齐白石作品如图7-3所示）的书画收藏家和假画贩子的例子来说明。假画贩子相当于是生成器，他们希望能够模仿大师真迹伪造出以假乱真的假画，骗过收藏家，从而卖出高价；书画收藏家则希望将赝品和真迹区分开，让真迹流传于世，销毁赝品。这里假画贩子和收藏家所交易的画，主要是齐白石画的虾。齐白石画虾可以说是画坛一绝，历来为世人所追捧。 在这个例子中，一开始假画贩子和书画收藏家都是新手，他们对真迹和赝品的概念都很模糊。假画贩子仿造出来的假画几乎都是随机涂鸦，而书画收藏家的鉴定能力很差，有不少赝品被他当成真迹，也有许多真迹被当成赝品。\n首先，书画收藏家收集了一大堆市面上的赝品和齐白石大师的真迹，仔细研究对比，初步学习了画中虾的结构，明白画中的生物形状弯曲，并且有一对类似钳子的“螯足”，对于不符合这个条件的假画全部过滤掉。当收藏家用这个标准到市场上进行鉴定时，假画基本无法骗过收藏家，假画贩子损失惨重。但是假画贩子自己仿造的赝品中，还是有一些蒙骗过关，这些蒙骗过关的赝品中都有弯曲的形状，并且有一对类似钳子的“螯足”。于是假画贩子开始修改仿造的手法，在仿造的作品中加入弯曲的形状和一对类似钳子的“螯足”。除了这些特点，其他地方例如颜色、线条都是随机画的。假画贩子制造出的第一版赝品如所示。 当假画贩子把这些画拿到市面上去卖时，很容易就骗过了收藏家，因为画中有一只弯曲的生物，生物前面有一对类似钳子的东西，符合收藏家认定的真迹的标准，所以收藏家就把它当成真迹买回来。随着时间的推移，收藏家买回越来越多的假画，损失惨重，于是他又闭门研究赝品和真迹之间的区别，经过反复比较对比，他发现齐白石画虾的真迹中除了有弯曲的形状，虾的触须蔓长，通身作半透明状，并且画的虾的细节十分丰富，虾的每一节之间均呈白色状。\n收藏家学成之后，重新出山，而假画贩子的仿造技法没有提升，所制造出来的赝品被收藏家轻松识破。于是假画贩子也开始尝试不同的画虾手法，大多都是徒劳无功，不过在众多尝试之中，还是有一些赝品骗过了收藏家的眼睛。假画贩子发现这些仿制的赝品触须蔓长，通身作半透明状，并且画的虾的细节十分丰富，如图7-5所示。于是假画贩子开始大量仿造这种画，并拿到市面上销售，许多都成功地骗过了收藏家。 收藏家再度损失惨重，被迫关门研究齐白石的真迹和赝品之间的区别，学习齐白石真迹的特点，提升自己的鉴定能力。就这样，通过收藏家和假画贩子之间的博弈，收藏家从零开始慢慢提升了自己对真迹和赝品的鉴别能力，而假画贩子也不断地提高自己仿造齐白石真迹的水平。收藏家利用假画贩子提供的赝品，作为和真迹的对比，对齐白石画虾真迹有了更好的鉴赏能力；而假画贩子也不断尝试，提升仿造水平，提升仿造假画的质量，即使最后制造出来的仍属于赝品，但是和真迹相比也很接近了。收藏家和假画贩子二者之间互相博弈对抗，同时又不断促使着对方学习进步，达到共同提升的目的。",
    "tags": [],
    "title": "深度学习07-深度卷积生成对抗网络(DCGAN)",
    "uri": "/docs/programming/ai/deep_learning/gans/dl_07_gans/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/docs/categories/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/docs/tags/index.html"
  }
]
