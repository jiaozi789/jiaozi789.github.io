var relearn_searchindex = [
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库",
    "content": "",
    "description": "",
    "tags": [],
    "title": "transformers",
    "uri": "/docs/programming/ai/tools_libraries/transformers/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers",
    "content": "",
    "description": "",
    "tags": [],
    "title": "transformers模型详解",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 插件开发",
    "content": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "description": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "tags": [],
    "title": "vscode插件",
    "uri": "/docs/programming/plugins/vscode/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能",
    "content": "",
    "description": "",
    "tags": [],
    "title": "工具库",
    "uri": "/docs/programming/ai/tools_libraries/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 编程语言",
    "content": "",
    "description": "",
    "tags": [],
    "title": "汇编语言",
    "uri": "/docs/programming/languages/assembly/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客",
    "content": "",
    "description": "",
    "tags": [],
    "title": "编程开发",
    "uri": "/docs/programming/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发",
    "content": "",
    "description": "",
    "tags": [],
    "title": "编程语言",
    "uri": "/docs/programming/languages/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers",
    "content": "",
    "description": "",
    "tags": [],
    "title": "transformers实战",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发",
    "content": "",
    "description": "",
    "tags": [],
    "title": "人工智能",
    "uri": "/docs/programming/ai/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 插件开发",
    "content": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "description": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "tags": [],
    "title": "chrome插件",
    "uri": "/docs/programming/plugins/chrome/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发",
    "content": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "description": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "tags": [],
    "title": "插件开发",
    "uri": "/docs/programming/plugins/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 编程语言 \u003e 汇编语言",
    "content": "简介 68000 汇编语言是为 Motorola 68000 微处理器设计的低级编程语言。68000 微处理器于 1979 年推出，因其强大的性能和灵活的架构而广泛应用于多种计算机系统和嵌入式设备中。以下是对 68000 汇编语言的背景、应用领域以及学习它的好处的详细介绍。\n产生背景 技术进步：\n68000 微处理器是 16 位架构，具有 32 位的地址总线，能够寻址高达 4GB 的内存。这使得它在当时的微处理器中具有较高的性能和灵活性。 其设计采用了复杂指令集计算（CISC）架构，支持多种寻址模式和丰富的指令集。 市场需求：\n1970 年代末和1980年代初，个人计算机和嵌入式系统的需求迅速增长。68000 微处理器因其性能和成本效益被广泛采用。 许多知名的计算机系统（如 Apple Macintosh、Atari ST 和 Sega Genesis）都使用了 68000 处理器。 应用领域 个人计算机：\n68000 微处理器被用于早期的个人计算机，如 Apple Macintosh 和 Atari ST。这些系统的操作系统和应用程序通常使用 68000 汇编语言进行开发。 嵌入式系统：\n68000 处理器也被广泛应用于嵌入式系统，如工业控制、汽车电子和消费电子产品。 游戏机：\nSega Genesis 和其他游戏机使用 68000 处理器，许多经典游戏都是用 68000 汇编语言编写的，学习后可以做一些hackrom的实战。 实时系统：\n由于其高效的性能，68000 处理器在实时系统中也得到了应用，如医疗设备和航空航天系统。 语言学习 EASy68K EASy68K 是一个 68000 结构化汇编语言集成开发环境（IDE）。EASy68K 允许您在 Windows PC 或 Wine 上编辑、汇编和运行 68000 程序。无需额外的硬件。EASy68K 是一个开源项目，根据 GNU 通用公共使用许可分发。 使用easy68k方便我们学习整套68000的编程和调试，学习这件基础知识，对我们hackrom或者逆向的基础。 下载地址：http://www.easy68k.com/files/SetupEASy68K.exe 安装完成后的目录结构 帮助文档 helm.chm提供了完整的编程和调试工具以及68k语言的学习入门资料，可以直接从该文档入手。 IDE使用 打开EDIT68K.exe，菜单file-\u003enew x68 source file 。 在source里面实现一个功能，打印helloworld，并从空值台输入一个字符串并打印。\n关于指令，标签，寄存器其他相关的内容请移步后续章节。\n源代码\n*-----------------------------------------------------------\r* Title :\r* Written by :\r* Date :\r* Description:\r*-----------------------------------------------------------\rORG $1000 ;告诉编译器代码从1000位置开始，不指定默认从0开始\rSTART: ; first instruction of program\r* 将text字符串地址写给A1\rlea text,A1\r* 将14号task print 给d0,并执行，14号任务自动获取A1地址的数据并打印\rmove #14,D0\rtrap #15\r* 执行2号任务，从输入流获取输入，自动写入到A1\rmove #2,D0\rtrap #15\r* 打印A1地址内容\rmove #14,D0\rtrap #15\r* Put program code here\r*-----------------------------------------------------------\r*HELLO：这是一个标签，标识字符串数据的起始位置。\r*DC.B：这是一个伪指令，表示“定义常量（Define Constant）”，后面的 .B 表示定义的是字节（Byte）数据。\r*'Hello World'：这是一个字符串常量，表示字符数组。每个字符占用一个字节。\r*$D：这是一个十六进制常量，表示一个字节的值。$D 的十进制值是 13，通常表示回车符（Carriage Return）。\r*$A：这是一个十六进制常量，表示一个字节的值。$A 的十进制值是 10，通常表示换行符（Line Feed）。\r*0：这是一个字节的值，表示字符串的结束符（null terminator），在 C 语言中常用来标识字符串的结束。\r*----------------------------------------------------------- text dc.b 'helloworld',0\rSIMHALT ; halt simulator\r* Put variables and constants here\rEND START ; last line of source 点击工具栏运行按钮（如果由错误会有提示，根据情况修正） 会弹出一个确认框 点击execute 绿色圈圈点击变成红色可下断点，F9运行，F8 stepover,F7 stepinto,点击运行可调试。 在view可打开内存窗口，栈窗口等 编程语言 汇编语言程序由以下部分组成：\nlabels 标签 - 用户创建的名称，用于标记程序中的位置。 opcode 操作码 - 微处理器可以执行的特定指令，比如ADD，MOVE等。 operands 操作数 - 某些指令所需的附加数据，比如#1表示10进制立即数1，$1表示16进制的1。 directives 指令 - 发给汇编器的命令，比如ORG $1000，告诉编译器，代码的开始位置，代码段不占用空间，类似于c语言的宏，编译阶段使用。 macros 宏 - 用户创建的源代码集合，可以在编写程序时轻松重用。 comments 注释 - 用户创建的文本字符串，用于记录程序。 寄存器：汇编语言编程需要与微处理器进行直接交互。68000 微处理器包含八个数据寄存器 D0 到 D7。数据寄存器是通用的，可以视为 8 位、16 位或 32 位的整数变量。还有八个地址寄存器 A0 到 A7，地址寄存器的长度为 32 位。它们通常用于引用变量。状态寄存器（SR）包含状态标志，用于指示比较的结果。 以下是一个例子 comments 在 Motorola 68000（68k）汇编语言中，注释用于帮助程序员理解代码的功能和逻辑。68k 汇编语言的注释格式如下：（*或者;开头的为注释）\n* Date TRAP #15 ;将3任务执行，自动打印D1的内容 operands 操作数 #,$,%区别 你可能也注意到了出现在32和0000001E前面的#和$符号，$符号是为了告诉汇编器这个数字是“十六进制”数字(是个地址)，而不是“十进制”数字，例如：\nmove.b #32,$0000001E 汇编器在进行汇编时会将32（十进制）转换为0010 0000（二进制）。0010 0000 是 20 十六进制，因此写 32 和写 $20 是一样的，0010 0000将写入地址0000001E。如果你想要写二进制数，可以使用 % 符号。\nmove.b #%00100000,$0000001E\rmove.b #$20,$0000001E\rmove.b #32,$0000001E 以上所有内容完全相同，顶部是二进制版本（%），中间是十六进制（$），底部是十进制。在本教程中，我们将更多地使用十六进制和二进制，而不是十进制，以帮助你更好地理解和掌握它们。\n另一方面，# 符号告诉汇编器，该数字是一个“立即”值，而不是一个偏移量。那么什么是“立即”值呢？稍安勿躁，让我们先看一个没有 # 符号的例子：\nmove.b $00000010,$0000002D 这将读取偏移量00000010处的字节，并将其复制到偏移量0000002D处，如果偏移量00000010处的字节是49，则0000002D处现在也将是49： 而现在回到“立即数”，在我看来，这只不过是“直接数字”的一个花哨名称，#符号告诉68k这个数字不是偏移量/地址。\n操作数移动 给个例子\nmove.w #$10,$0020 ;将立即数16进制10 写入内存地址$0020\rmove $0020,D0 ;将内存地址$0020的值10赋值给D0\rmove $0020,A0 ;将将内存地址$0020的值10赋值给地址寄存器A0\rmove #$0020,A1 ;将立即数$0020赋值给地址寄存器A1\rmove A1,D1 ;将A1地址#$0020赋予给D1\rmove (A1),D2 ;将A1地址#$0020内存的值10赋予给D2\rmove.w (a0),(a1) ;将a0地址的值赋给a1地址的内存\rmove.w d1,(a0)+ ;将d1的数据，写入a0+word(2个字节)，并且a0寄存器往后移动两位，比如a0=0000,执行完a0=0002\rmove.w d1,$10(a1) ;将d1数据写入a1+10个字节的位置，a1的指向不变，比如a1=0000，写入数据到0010，执行完a1=0000\rmove.b #$98,(a0)+ ;同上上，写入立即数\rmove.l $29(a0),$00120020 ;将a0+29位置的值写入$00120020位置\rmove.b $00120020,(a1)+ 注意：move.w $00000047,d0 这个会导致汇编程序崩溃，因为00000047是一个奇数（奇地址/偏移量），68k在处理时会有问题，并会因“地址错误”而崩溃，字w和双字l必须使用偶数地址，如果要使用奇数地址请使用字节b。\n你只能使用“字节”来访问奇地址上的数据：\nlabels 标签用于通过名称标识程序中的位置或内存位置。需要位置的指令或指令可以使用标签来指示该位置。标签通常在行的第一列开始，必须以空格、制表符或冒号结束。如果使用冒号，它不会成为标签的一部分。如果标签没有在第一列开始，则必须以冒号结束。标签的前 32 个字符是有效的。标签有两种类型：全局标签和局部标签。\n全局标签可以在程序的任何地方被引用。因此，全局标签必须是唯一的名称。全局标签应以字母开头，后面可以跟字母、数字或下划线。局部标签可以在程序中重复使用。局部标签必须以点 ‘.’ 开头，后面可以跟字母、数字或下划线。全局标签定义了局部标签的边界。当定义局部标签时，只有在遇到下一个全局标签之前，才能从局部标签上方或下方的代码中引用它。汇编器通过将局部标签名称附加到前面的全局标签并用冒号 ‘:’ 替换点来创建局部标签的唯一名称。结果名称的前 32 个字符是有效的。\n开始标签 标签可以用来指定程序的起始位置。如果标签 START 指定了程序的起始位置，那么 END 指令的写法如下：\nSTART: Start of program\rcode\rEND START 指令标签 标签常常放在某个指令前用来表示，定义变量，标签指向存储数据的首地址。 DC - DC 指令指示汇编器将后续的值放入当前内存位置。该指令有三种形式：DC.B 用于字节数据，DC.W 用于字（16 位）数据，DC.L 用于长（32 位）数据。定义常量指令不应与 C++ 中声明常量混淆。 例如\nORG $1000 start of the data region depart DC.B 'depart.wav',0 stores as a NULL terminated string in consecutive bytes DC.L $01234567 the value $01234567 is stored as a long word\rDC.W 1,2 two words are stored as $0001 and $0002\rDC.L 1,2 two long words are stored as $00000001 and $00000002 depart 就是一个label是这块内存区域的首地址。\n内存结果\n00001000 64 65 70 61 72 74 2E 77 61 76 00 0000100C 01234567 00001010 0001 0002 00001014 00000001 00000002 其他关于指令标签的用法参考，也可以到指令章节： 位置标签 可以定义一些位置标签，当进行特殊操作时，可以通过控制流opcode跳转到位置标签 实现一个从0，end_index的循环打印\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\r* 实现一个从0，end_index的循环打印\rmove #1,D1\rt:\rmove #3,D0\rTRAP #15 ;将3任务执行，自动打印D1的内容\radd.b #1,d1 ;让d1+1\rCMP #end_index,d1 ;比较d1和end_index的值\rBNE t ;如果不相等继续跳转到t label执行\rSIMHALT ; halt simulator\r* Put variables and constants here\rend_index equ 10\rEND START ; last line of source opcode 操作码 在 68K 汇编语言中，操作码（opcode）是指令的核心部分，定义了要执行的操作。以下是一些常用的 68K 操作码及其功能：\n常用操作码 注意大部分操作码都可以添加结尾.W表示字（2个字节16位）.L表示双字(4个字节32位)，.B（1个字节8位）\n数据传送 - `MOVE`：将数据从一个位置移动到另一个位置。\r- 例：`MOVE.W D0, D1`（将 D0 的值移动到 D1）\r- `MOVEA`：将地址从一个位置移动到另一个位置。\r- 例：`MOVEA.L A0, A1`（将 A0 的地址移动到 A1）\r算术运算 - `ADD`：将两个操作数相加。\r- 例：`ADD.W D0, D1`（将 D0 的值加到 D1）\r- `SUB`：从一个操作数中减去另一个操作数。\r- 例：`SUB.W D1, D0`（从 D0 中减去 D1）\r- `MULS`：有符号乘法。\r- 例：`MULS D0, D1`（将 D0 和 D1 相乘，结果存储在 D1）\r- `DIVS`：有符号除法。\r- 例：`DIVS D0, D1`（将 D1 除以 D0，结果存储在 D1）\r逻辑运算 - `AND`：按位与运算。\r- 例：`AND.W D0, D1`（D1 与 D0 按位与）\r- `OR`：按位或运算。\r- 例：`OR.W D0, D1`（D1 与 D0 按位或）\r- `EOR`：按位异或运算。\r- 例：`EOR.W D0, D1`（D1 与 D0 按位异或）\r- `NOT`：按位取反。\r- 例：`NOT.W D0`（D0 的值取反）\r控制流 常用如下：\r- `BRA`：无条件跳转。\r- 例：`BRA label`（跳转到指定标签）\r- `BEQ`：如果相等则跳转。\r- 例：`BEQ label`（如果零标志位被设置，则跳转）\r- `BNE`：如果不相等则跳转。\r- 例：`BNE label`（如果零标志位未设置，则跳转）\r- `JSR`：跳转到子程序。\r- 例：`JSR subroutine`（跳转到子程序并保存返回地址）\r- `RTS`：从子程序返回。\r- 例：`RTS`（返回到调用子程序的地址）\r分支跳转 该指令将在程序中引发分支，如果某些标志被设置。共有十五种检查标志的方法。每种方法都有一个由两个字母组成的符号，用于替换 “cc” 在 “Bcc” 中。\nBCC：分支如果进位标志清除 - 当 C 标志为 0 时分支。 BCS：分支如果进位标志设置 - 当 C 标志为 1 时分支。 BEQ：分支如果相等 - 当 Z 标志为 1 时分支。 BNE：分支如果不相等 - 当 Z 标志为 0 时分支。 BGE：分支如果大于或等于 - 当 N 和 V 相等时分支。 BGT：分支如果大于 - 当 N 和 V 相等且 Z=0 时分支。 BHI：分支如果高于 - 当 C 和 Z 都为 0 时分支。 BLE：分支如果小于或等于 - 当 Z=1 或 N 和 V 不同时分支。 BLS：分支如果小于或相同 - 当 C=1 或 Z=1 时分支。 BLT：分支如果小于 - 当 N 和 V 不同时分支。 BMI：分支如果负 - 当 N=1 时分支。 BPL：分支如果正 - 当 N=0 时分支。 BVC：分支如果溢出标志清除 - 当 V=0 时分支。 BVS：分支如果溢出标志设置 - 当 V=1 时分支。 BRA：无条件分支 - 始终分支。 上面这些opcode根据标志触发跳转，只能跳转到label，注意进入label后会往下执行，和函数调用不一样，函数调用会返回，继续执行之前代码的下一行，这个不会，是直接跳转过去不回来了。\n例子：\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rinput:\rmove.b #4,d0\rTRAP #15\rCMP #0,d1\rBNE input ;如果不等于0跳转到input标签，继续让输入数字\rBEQ exit ;如果等于0直接退出\rlabel\rexit:\rSIMHALT ; halt simulator\r* Put variables and constants here\rEND START ; last line of source 地址跳转 JMP（跳转）用于将程序控制转移到一个有效地址。它实际上相当于 MOVE.L xxx, PC，因为它将程序计数器更改为一个有效地址（计算得出）。\n注意JMP是无条件跳转，相对于B开头的跳转，他也支持 JMP label的语法，同时他也支持直接JMP 地址的跳转。\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rinput:\rmove.b #4,d0\rTRAP #15\rCMP #0,d1\rBNE input ;如果不等于0跳转到input标签，继续让输入数字\rBEQ exit ;如果等于0直接退出\rlabel\rexit:\rLEA quit,a0 ;=0跳转到这里后，将quit的地址给到a0，JMP直接跳转到地址,相当于：move.l a0,PC（这是伪代码）\rJMP (a0) ;如果想跳转到a0的下一个地址，可以1(a0) 或者n(a0),当然也可以直接JMP quit\rquit:\rSIMHALT ; halt simulator\r* Put variables and constants here\rEND START ; last line of source 子程序跳转 JSR/BSR（跳转到子例程）与 JMP（无条件跳转）类似，但在跳转之前，JSR 会将跳转指令后面的地址压入栈中，这样可以通过 RTS（返回子例程）指令返回，也就相当于调用函数，函数执行完了，执行代码的下一行。\nBSR适合同一代码段里的label直接调用，是相对掉哟个，JSR适合指定一个绝对地址调用(比如JSR $5000) ,但是实际上两个可以互相替换，没啥区别。\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rinput:\rJSR input_notion ;JSR执行完后会自动执行下一行代码，B开头的跳过去就不回来了\rmove.b #4,d0\rTRAP #15\rCMP #0,d1\rBNE input ;如果不等于0跳转到input标签，继续让输入数字\rBEQ exit ;如果等于0直接退出\rinput_notion: ;屏幕上输出提示语\rMOVE #14,D0\rLEA INPUT_STR,A1\rTRAP #15\rRTS ;注意返回了会运行调用这个函数的下一行\rconfirm_exit *屏幕上输出确认提示语\rMOVE #14,D0\rLEA CONFIRM_STR,A1\rTRAP #15\rRTS\rexit:\rJSR confirm_exit\rmove.b #4,d0\rTRAP #15\rCMP #0,d1\rBEQ quit\rBNE input\rquit:\rSIMHALT ; halt simulator\r* Put variables and constants here\rINPUT_STR: dc.b 'please input number(exit=0):',0\rCONFIRM_STR: dc.b 'confirm exit(:exit=0,not=1):',0\rEND START ; last line of source 效果 位操作 - `SHL`：左移。\r- 例：`SHL.W #1, D0`（D0 左移 1 位）\r- `SHR`：右移。\r- 例：`SHR.W #1, D0`（D0 右移 1 位）\r- `ROL`：循环左移。\r- 例：`ROL.W #1, D0`（D0 循环左移 1 位）\r- `ROR`：循环右移。\r- 例：`ROR.W #1, D0`（D0 循环右移 1 位）\r比较 - `CMP`：比较两个操作数。\r- 例：`CMP.W D0, D1`（比较 D0 和 D1 的值）\r堆栈操作 - `PUSH`：将数据压入堆栈。\r- 例：`PUSH.W D0`（将 D0 的值压入堆栈）\r- `POP`：从堆栈弹出数据。\r- 例：`POP.W D0`（从堆栈弹出值到 D0）\rIO操作码 TRAP #15 被用于触发 I/O. 不同的io流任务存储在： D0. 参考chm： 常用的输入输出任务：\n14: 将A1地址对应的字符串输出 以0结尾结束。 13：将A1地址对应的字符串输出 以0结尾结束，加上\\r\\n换行。 2: 从控制台获取一个字符串回车后存储在A1地址中 0结尾。 4：读取一个数字写入D1.L中。 例子\nSTART ORG $1000 Program load address.\rmove #14,D0 ;设置14号任务打印A1地址字符串\rlea text,A1 ;获text地址到A1\rtrap #15 ;激活任务\rSIMHALT text dc.b 'Hello World',0 ;0表示字符串结束\rEND START End of source with start address specified. 其他操作码 关于更加详情的指令参考chm directives 指令 指令是汇编器需要遵循的指令。它们占据源代码行中的第二个字段，与指令操作码占据的位置相同，但指令并不是 68000 操作码。 “DC” 和 “DCB” 是唯一会导致数据被添加到输出文件中的指令。指令还可以用于控制宏的汇编、条件汇编和结构化语法。\n在以下描述中，选项项用方括号 [] 表示。用斜体显示的项应替换为适当的语法。\nUsage:\r[label] directive[.size] [data,data,...]\r^ ^ ^\r\\_________________\\_________\\_____ varies by directive DC指令 全称：Define Constant（定义常量） 用途：用于定义并初始化数据常量。DC 指令可以用于定义一个或多个初始值，这些值会被存储在程序的输出文件中。 内存分配：DC 指令会在程序的内存中分配实际的存储空间，并将指定的值写入该空间。 示例： 使用语法： Usage:\r[label] DC.size data,data,... 例子：\nVALUE1 DC 10 ; 定义常量 VALUE1，值为 10\rVALUE2 DC 20, 30 ; 定义常量 VALUE2，值为 20 和 30 特性： 定义的值在程序运行时是不可更改的。 实际在内存中占用空间。 注意下面的代码修改地址的值是非法的，常量无法修改\nSTART: ; first instruction of program\rlea usercount,A0\rmove.b 20,(A0) ;修改A0地址的常量这是非法的。\r* Put program code here\rSIMHALT ; halt simulator\r* Put variables and constants here\rORG $1200\rusercount dc.b 10,20\rdc.w 23 EQU 指令 全称：Equate（等于）\n用途：用于定义一个符号并将其与一个值关联。EQU 定义的值在整个程序中是不可更改的，通常用于定义常量或符号地址，类似于c语言的#define在预编译将对应引用的地方替换为值。\n内存分配：EQU 不会在内存中分配实际的存储空间。它只是创建一个符号，所有使用该符号的地方都会被替换为其定义的值。\n示例：\nMAX_SIZE EQU 100 ; 定义常量 MAX_SIZE，值为 100\n特性：\n一旦定义，EQU 的值不能被修改。 不占用内存空间，编译时进行替换 ORG $1000 ; 程序起始地址\rSTART: ; 将立即数 10 移动到 D0 寄存器\r; 定义常量\rMAX_COUNT EQU 2 ; 定义 MAX_COUNT 为 100\rSTART_VALUE EQU 1 ; 定义 START_VALUE 为 10\rMOVE.B #10, D0\rADD.B #MAX_COUNT, D0 ; 将 MAX_COUNT (100) 加到 D0\rSUB.B #START_VALUE, D0 ; 将 START_VALUE (10) 从 D0 中减去\rSIMHALT ; 停止模拟器\rORG $1200 ; 数据段起始地址\rEND START SET 指令 用途：用于定义一个符号并赋予一个初始值，但与 DC 不同的是，SET 定义的值是可更改的。SET 通常用于在程序运行时动态地改变值。\n示例：\nCOUNT SET 0 ; 定义符号 COUNT，初始值为 0 COUNT SET COUNT + 1 ; 重新定义 COUNT，值为 COUNT + 1\n内存分配：SET 指令并不分配实际的存储空间来存储值，而是定义一个符号，允许在程序中动态地改变该符号的值。\nDS 指令 全称：Define Space（定义空间）\n用途：用于定义一块未初始化的内存空间。DS 指令只分配内存，但不初始化这些内存的值，随时可改。\n示例：\nBUFFER DS 256 ; 定义一个大小为 256 字节的缓冲区\n内存分配：DS 指令会在输出文件中分配指定大小的内存空间，但这些空间的初始值是未定义的（通常是随机值或零，具体取决于系统）。\n定义一个100字节的空间，可以理解为数组，将MULT_TABLE数字第一个位置设置为：12\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rmove.B #0,D0\rLEA MULT_TABLE, A0\rMOVE.B #12,(A0, D0)\rSIMHALT ; halt simulator\rORG $1200\r* Put variables and constants here\rMULT_TABLE: ; 乘法表的存储位置\rDS.B 10 * 10 ; 预留 10x10 的空间\rEND START ; last line of source 其他指令 参考chm 寄存器 程序计数器（PC） 程序计数器（有时在不同的体系结构中也称为指令指针或指令地址寄存器）保存下一条将要执行的指令的内存地址。每当 CPU 执行一条指令时，PC 的值会自动更新，以指向下一条指令。 更新机制：在大多数情况下，PC 在指令执行后自动加一（或加上指令的长度），以指向下一条指令的地址。 编写一个简单程序 运行，默认会从start:的写一条语句开始，PC寄存器指向初始代码的地址（注意有效的代码时左侧绿色点点的，其他都是指令或者注释） 按下F8执行到下一条 我这里将usercount的地址指向A0 ,同时加了ORG $1200从1200这个地址写入。点击A0的地址可以查看内存： 状态寄存器（SR） 在 68k（Motorola 68000）架构中，状态寄存器（SR，Status Register）是一个重要的寄存器，用于存储处理器的状态信息和控制标志。状态寄存器的内容影响程序的执行流程，特别是在条件跳转和中断处理时。以下是对 68k 状态寄存器的详细介绍：\n状态寄存器的结构 68k 的状态寄存器是一个 16 位的寄存器，包含多个标志位。主要的标志位包括：\nN（Negative）:\n表示最近一次运算的结果是否为负数。 如果结果的最高位（符号位）为 1，则 N 标志被设置。 Z（Zero）:\n表示最近一次运算的结果是否为零。 如果结果为 0，则 Z 标志被设置。 V（Overflow）:\n表示最近一次运算是否发生了溢出。 溢出通常发生在有符号数运算中，当结果超出可表示的范围时，V 标志被设置。 C（Carry）:\n表示最近一次运算是否产生了进位或借位。 在加法运算中，如果产生了进位，C 标志被设置；在减法运算中，如果发生了借位，C 标志也会被设置。 I（Interrupt Mask）:\n这是一个 3 位的中断屏蔽位，控制中断的响应。 I0、I1 和 I2 位用于设置中断优先级，值越大，响应的中断优先级越低。 T（Trace）:\n这是一个单个位，用于启用或禁用跟踪模式。 当 T 位被设置时，处理器将在每个指令执行后产生一个中断，适用于调试。 S（Supervisor）:\n这是一个单个位，指示当前处理器是否处于特权模式（超级用户模式）。 当 S 位被设置时，处理器处于超级用户模式，允许执行特权指令。 状态寄存器的作用 条件跳转: 状态寄存器中的标志位用于条件跳转指令（如 BEQ、BNE 等），根据运算结果的状态决定程序的执行路径。 中断处理: 中断标志位控制中断的响应，允许或禁止特定级别的中断。 运算结果的状态: 通过检查 N、Z、V 和 C 标志，程序可以根据运算结果的状态做出相应的处理。 示例 以下是一个简单的示例，展示如何使用状态寄存器的标志位：\nMOVE.L #5, D0 ; 将 5 加载到 D0\rMOVE.L #3, D1 ; 将 3 加载到 D1\rSUB.L D1, D0 ; D0 = D0 - D1，结果为 2\r; 检查 Z 标志\rBEQ zero_result ; 如果 Z 标志为 1，跳转到 zero_result\r; 检查 N 标志\rBPL positive_result ; 如果 N 标志为 0，跳转到 positive_result\rzero_result:\r; 处理结果为零的情况\r; ...\rpositive_result:\r; 处理结果为正的情况\r; ... 数据寄存器（D) 在 68000（68k）架构中，D 寄存器（数据寄存器）是用于存储数据和操作数的寄存器。68k 处理器有 8 个数据寄存器，分别为 D0 到 D7。\nD 寄存器的特点 数量:\n68k 处理器有 8 个数据寄存器，编号为 D0 到 D7。 大小:\n每个 D 寄存器的大小为 32 位（4 字节），可以存储 32 位的整数或指针。 用途:\nD 寄存器主要用于存储运算的操作数、结果以及临时数据。它们在算术运算、逻辑运算、数据传输等操作中被广泛使用。 寻址模式:\nD 寄存器可以与多种寻址模式结合使用，支持直接寻址、间接寻址等方式，方便数据的访问和操作。 操作:\nD 寄存器可以参与各种指令的操作，如加法、减法、位运算等。指令可以直接对 D 寄存器进行操作，也可以将 D 寄存器的值存储到内存中或从内存中加载数据。 D 寄存器的使用场景 算术运算: D 寄存器用于存储参与运算的数值。 数据传输: 在数据传输指令中，D 寄存器可以作为源或目标。 函数参数: 在调用子程序时，D 寄存器常用于传递参数。 示例 以下是一个简单的汇编代码示例，展示如何使用 D 寄存器进行基本的算术运算：\nMOVE.L #10, D0 ; 将 10 加载到 D0 寄存器\rMOVE.L #5, D1 ; 将 5 加载到 D1 寄存器\rADD.L D1, D0 ; D0 = D0 + D1，D0 现在为 15 地址寄存器（A) 68000（68k）架构中，A 寄存器（地址寄存器）是用于存储内存地址的寄存器。68k 处理器有 8 个地址寄存器，分别为 A0 到 A7。以下是对 A 寄存器的详细描述：\nA 寄存器的特点 数量:\n68k 处理器有 8 个地址寄存器，编号为 A0 到 A7。 大小:\n每个 A 寄存器的大小为 32 位（4 字节），可以存储 32 位的内存地址。 用途:\nA 寄存器主要用于存储内存地址，支持数据的加载和存储操作。它们在指令中用于指向数据或指令的内存位置。 寻址模式:\nA 寄存器可以与多种寻址模式结合使用，包括直接寻址、间接寻址、基址寻址和相对寻址等。这使得程序能够灵活地访问内存中的数据。 堆栈指针:\nA7 寄存器通常用作堆栈指针（SP），指向当前堆栈的顶部。堆栈用于存储函数调用的返回地址、局部变量等。 A 寄存器的使用场景 内存访问: A 寄存器用于指向数据在内存中的位置，支持数据的读取和写入。 函数调用: 在函数调用中，A 寄存器可以用于传递参数和返回地址。 堆栈管理: A7 寄存器作为堆栈指针，管理函数调用的堆栈帧。 示例 以下是一个简单的汇编代码示例，展示如何使用 A 寄存器进行内存操作：\nLEA array, A0 ; 将数组的地址加载到 A0 寄存器\rMOVE.L (A0), D0 ; 从 A0 指向的地址加载数据到 D0 寄存器\rADD.L #1, D0 ; D0 = D0 + 1\rMOVE.L D0, (A0) ; 将 D0 的值存储回 A0 指向的地址 堆栈寄存器（SS) 在68k架构中，堆栈寄存器是用于管理程序运行时的堆栈的关键组件。68k系列处理器使用一个专用的寄存器来指向当前堆栈的顶部，这个寄存器被称为堆栈指针（Stack Pointer）。\n在68k架构中，堆栈指针寄存器通常是 A7（地址寄存器7），它指向当前堆栈的顶部。 堆栈是一个后进先出（LIFO）的数据结构，用于存储临时数据，如函数调用的返回地址、局部变量和中断处理程序的上下文。\n堆栈操作 我们来看下堆栈指针的移动和数据写入逻辑。 在68k汇编语言中，-(A7) 和 (A7)+ 分别用于表示压栈和出栈操作。 执行代码\nmove.l #10,-(a7) 未执行前原始堆栈地址A7指向：01000000，没有任何数据 执行：move.l #10,-(a7) 执行：move.l #20,-(a7) 执行出栈：move.l (a7)+,d0 std函数模拟 我们知道c语言的std约定是：调用函数先压入执行代码的后一个位置，然后参数从右往左压入，在函数内部出栈从左（后入先出）往右获取参数，执行完成获取代码执行的位置，跳转。 我们来模拟这个过程： 假设函数: public int add(int a,int b) 用98k模拟堆栈实现：\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rmove.l #10,-(a7) #第二个参数压栈。\rmove.l #20,-(a7) #第一个参数压栈。\rLEA *+12, A0 *计算下LEA占用4个字节，一直到move.l d0,d2是12个字节，*+12就是从PC当前位置+12个就是下一个执行代码的位置\rmove.l a0,4(a7) *将下一个执行的地址压栈\rJMP add\rmove.l d0,d2\rSIMHALT ; halt simulator\radd:\rmove.l (a7)+,a0 ;地址出栈\rmove.l (a7)+,d0 ;第一个参数出栈\rmove.l (a7)+,d1 ;第二个参数出栈\radd.l d1,d0\rJMP (a0)\r* Put variables and constants here\rEND START ; last line of source 案例(9*9乘法表) *-----------------------------------------------------------\r* Title :\r* Written by :\r* Date :\r* Description:\r*-----------------------------------------------------------\rORG $1000\rSTART: ; first instruction of program\r* Put program code here\rmove.b #start_index,d2 ;行索引\rmove.b #start_index,d3 ;列索引\rrow:\rjsr print_str_line ;到row的部分就添加一个换行，jsr调用子程序，子程序需要RTS返回\radd.b #1,d2 ;每运行一次+1\rmove.b #start_index,d3\rcmp #end_index+1,d2 ;到达最后一行+1直接退出\rBEQ exit\rcol:\radd.b #1,d3\rmove.b d2,d1\rjsr print_num ;打印行的数字\rlea tmp_str,a1\rmove.b #'*',(a1) ;打印一个*\rjsr print_str\rmove.b d3,d1\rjsr print_num ;打印一个列的数字\rmove.b #'=',(a1) jsr print_str ;打印一个=\rmove.b #1,d4\rmuls d2,d4\rmuls d3,d4\rmove.b d4,d1\rjsr print_num ;打印一个列的数字\rmove.b #' ',(a1) jsr print_str ;打印一个空格\rcmp d3,d2\rBEQ row\rBNE col\rprint_num:\rmove.b #3,d0\rTRAP #15 RTS print_str:\rmove.b #0,1(a1) ;打印字符的结尾\rmove.b #14,d0\rTRAP #15 RTS print_str_line:\rmove.b #0,(a1) ;打印字符的结尾\rmove.b #13,d0\rTRAP #15 RTS exit:\rSIMHALT ; halt simulator\r* Put variables and constants here\rtmp_str ds.b 2\rend_index equ 9\rstart_index equ 0\rEND START ; last line of source 效果 其他68k速查的在线文档：\n指令列表：https://github.com/prb28/m68k-instructions-documentation?tab=readme-ov-file 0基础入门：https://mrjester.hapisan.com/04_MC68/ romhack相关所有资源（文档工具）：https://github.com/zengfr/romhack 常用指令备忘录：https://github.com/zengfr/romhack/blob/adf6412c2a969486918bb00c18a2c989abdeaad5/M68000/M68000%E6%8C%87%E4%BB%A4-%E5%A4%87%E5%BF%98%E8%A1%A5%E5%85%85(%E6%95%B4%E7%90%86zengfr).txt",
    "description": "简介 68000 汇编语言是为 Motorola 68000 微处理器设计的低级编程语言。68000 微处理器于 1979 年推出，因其强大的性能和灵活的架构而广泛应用于多种计算机系统和嵌入式设备中。以下是对 68000 汇编语言的背景、应用领域以及学习它的好处的详细介绍。\n产生背景 技术进步：\n68000 微处理器是 16 位架构，具有 32 位的地址总线，能够寻址高达 4GB 的内存。这使得它在当时的微处理器中具有较高的性能和灵活性。 其设计采用了复杂指令集计算（CISC）架构，支持多种寻址模式和丰富的指令集。 市场需求：\n1970 年代末和1980年代初，个人计算机和嵌入式系统的需求迅速增长。68000 微处理器因其性能和成本效益被广泛采用。 许多知名的计算机系统（如 Apple Macintosh、Atari ST 和 Sega Genesis）都使用了 68000 处理器。 应用领域 个人计算机：\n68000 微处理器被用于早期的个人计算机，如 Apple Macintosh 和 Atari ST。这些系统的操作系统和应用程序通常使用 68000 汇编语言进行开发。 嵌入式系统：\n68000 处理器也被广泛应用于嵌入式系统，如工业控制、汽车电子和消费电子产品。 游戏机：\nSega Genesis 和其他游戏机使用 68000 处理器，许多经典游戏都是用 68000 汇编语言编写的，学习后可以做一些hackrom的实战。 实时系统：\n由于其高效的性能，68000 处理器在实时系统中也得到了应用，如医疗设备和航空航天系统。 语言学习 EASy68K EASy68K 是一个 68000 结构化汇编语言集成开发环境（IDE）。EASy68K 允许您在 Windows PC 或 Wine 上编辑、汇编和运行 68000 程序。无需额外的硬件。EASy68K 是一个开源项目，根据 GNU 通用公共使用许可分发。 使用easy68k方便我们学习整套68000的编程和调试，学习这件基础知识，对我们hackrom或者逆向的基础。 下载地址：http://www.easy68k.com/files/SetupEASy68K.exe 安装完成后的目录结构",
    "tags": [],
    "title": "68000汇编实战01-编程基础",
    "uri": "/docs/programming/languages/assembly/68000_01_base/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "liaomin416100569博客",
    "uri": "/docs/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "Hugging Face Hugging Face是一个人工智能（AI）公司，它致力于开发和推广自然语言处理（NLP）相关的技术和工具。该公司以其开源项目和社区而闻名，其最知名的项目之一是Transformers库，它提供了一系列预训练的语言模型，包括BERT、GPT和RoBERTa等。这些模型已经在各种NLP任务中取得了显著的成功，并成为了许多研究和工业应用的基础。\n除了提供预训练的模型之外，Hugging Face还开发了一系列工具和平台，使得使用和部署这些模型变得更加简单。其中包括：\nTransformers库：提供了各种预训练的语言模型的接口和工具，使得开发者可以轻松地使用这些模型进行文本分类、命名实体识别、语言生成等任务。\nDatasets库：包含了各种NLP数据集的接口和工具，使得开发者可以方便地使用这些数据集进行模型训练和评估。\nTrainer库：提供了一个训练和微调模型的框架，使得开发者可以方便地使用自己的数据集对预训练模型进行微调，以适应特定的任务和应用场景。\nModel Hub：一个模型分享和发布平台，开发者可以在这里分享自己训练的模型，也可以找到其他人分享的模型，并且可以直接在自己的项目中使用这些模型。\ndatasets数据集处理，transformers预训练微调等相关教程请参考官网hugging face官方文档。\nTransformers Transformers 是由 Hugging Face 开发的一个 NLP 包，支持加载目前绝大部分的预训练模型。随着 BERT、GPT 等大规模语言模型的兴起，越来越多的公司和研究者采用 Transformers 库来构建 NLP 应用，官网地址。 它提供了各种预训练的 Transformer 模型，包括 BERT、GPT、RoBERTa、DistilBERT 等。这些模型在多个 NLP 任务上取得了 state-of-the-art 的性能，并且 Transformers 库提供了简单易用的接口，使得使用这些预训练模型变得非常便捷。\n安装 官网安装教程参考：https://huggingface.co/docs/transformers/installation\n您可以通过 pip 安装 Transformers 库。在终端或命令行界面中执行以下命令（我这里使用pytorch，如果需要tensorflow的版本参考官网）：\npip install 'transformers[torch]'\n这将会自动从 PyPI（Python Package Index）下载并安装最新版本的 Transformers 库及其依赖项。\n如果您使用的是 Anaconda 环境，您也可以通过 conda 安装：\nconda install -c huggingface transformers\n这将会从 Anaconda 仓库中下载并安装 Transformers 库及其依赖项。\n安装完成后，您就可以在 Python 环境中使用 Transformers 库了。您可以编写代码来加载预训练模型、执行各种 NLP 任务，或者使用 Transformers 提供的高级 API，如 pipelines 来快速完成任务。\n模型 自然语言处理（NLP）模型是指用于处理和理解自然语言文本的计算机模型。这些模型的设计和训练旨在使计算机能够自动处理和分析语言数据，以执行各种语言相关的任务。以下是几种常见的NLP模型类型及其功能：\n基于规则的模型：这些模型使用手工制定的规则和规则集来处理文本，例如语法分析或关键词提取。这种方法的局限性在于需要大量的人工工作和难以处理的复杂性。\n基于统计的模型：这些模型利用统计学习技术从大量文本数据中学习语言模式和规律。例如，n-gram语言模型可以预测给定单词序列的下一个单词，而隐马尔可夫模型则用于词性标注和语音识别。\n神经网络模型：随着深度学习的发展，神经网络在NLP中的应用越来越广泛。这些模型使用多层神经网络结构来学习复杂的语言特征和模式。例如，递归神经网络（RNN）、长短时记忆网络（LSTM）和变压器（Transformer）等模型已经在机器翻译、文本生成、情感分析等任务中取得了显著的成就。\n预训练语言模型：这些模型通过在大规模文本数据上进行自监督学习来预先训练，例如BERT（Bidirectional Encoder Representations from Transformers）、GPT（Generative Pre-trained Transformer）等。预训练模型在各种NLP任务中表现出色，并通过微调适应特定的下游任务。\nNLP模型的选择取决于任务的性质和复杂度，以及可用的训练数据和计算资源。随着技术的进步和研究的深入，NLP模型不断演进和改进，为语言处理领域带来了许多创新和新的应用可能性。\n文件结构 训练模型通常具有以下常见的目录文件结构和文件：\nvocab.json: 这是一个包含词汇表的文件，它将模型训练时使用的词汇映射到整数索引。这对于将文本转换为模型可以理解的输入格式（如整数索引或者词嵌入）非常重要。\ntokenizer_config.json: 这个文件包含有关模型使用的分词器（tokenizer）的配置信息，例如分词器的类型、参数设置等。分词器用于将文本划分为词语或子词的序列，并将其转换为模型可以处理的输入格式。\ntokenizer.json: 如果模型使用了特定的分词器，此文件可能包含分词器的具体实现和配置信息。这对于加载和使用模型的时候确保分词器能正确地工作非常重要。\nconfig.json: 这个文件包含了模型本身的配置信息，例如模型的类型（如BERT、GPT）、层数、隐藏单元数等超参数设置。这些信息对于构建和初始化模型极为关键。\npytorch_model.bin 或 tensorflow_model.h5：这是包含预训练模型权重的二进制文件，其格式取决于所使用的深度学习框架。这些权重是模型学习到的参数，用于实际的预测和推理任务。\nspecial_tokens_map.json: 如果模型包含了特殊标记（如填充标记、起始标记等），此文件将包含这些特殊标记的定义及其在模型中的使用方式。\nmerges.txt（对于BERT等子词级别的模型）：这个文件包含将词汇划分为子词或者字符的规则或者合并操作，这对于分词器的工作非常关键。\nREADME.md 或者 model_card.md：这些文件通常包含了关于模型的详细信息，如作者、许可证、训练数据集、性能评估等，对于了解和使用模型非常有帮助。\n每个模型的具体结构和文件可能会有所不同，但上述文件是构成大多数预训练模型的基本要素。通过理解和操作这些文件，可以更好地理解和使用预训练模型进行自然语言处理任务。\npipelines 在 Hugging Face Transformers 中，pipelines 是一种方便的高级 API，用于执行各种自然语言处理（NLP）任务，如文本分类、命名实体识别、问答等。使用 pipelines，您无需编写大量的代码来加载模型、预处理输入数据、执行推理等操作，而是可以通过简单的函数调用来完成这些任务。\nransformers 库将目前的 NLP 任务归纳为几下几类：\n文本分类：例如情感分析、句子对关系判断等； 对文本中的词语进行分类：例如词性标注 (POS)、命名实体识别 (NER) 等； 文本生成：例如填充预设的模板 (prompt)、预测文本中被遮掩掉 (masked) 的词语； 从文本中抽取答案：例如根据给定的问题从一段文本中抽取出对应的答案； 根据输入文本生成新的句子：例如文本翻译、自动摘要等。 Transformers 库最基础的对象就是 pipeline() 函数，它封装了预训练模型和对应的前处理和后处理环节。我们只需输入文本，就能得到预期的答案。目前常用的 pipelines 有：\naudio-classification（音频分类）：用于对音频进行分类，识别音频中的类别或属性。 automatic-speech-recognition（自动语音识别）：用于将音频转换为文本，实现语音识别的功能。 conversational（会话式处理）：用于构建和处理对话系统，实现对话式交互的功能。 depth-estimation（深度估计）：用于从单张图片或视频中估计场景的深度信息。 document-question-answering（文档问答）：用于从文档中回答问题，帮助用户获取文档内容中的相关信息。 feature-extraction（特征提取）：用于从文本、图片等数据中提取特征，用于后续的任务或分析。 fill-mask（填空）：用于给定带有空白的句子，预测并填补空白处的单词或短语。 image-classification（图片分类）：用于对图片进行分类，识别图片中的类别或属性。 image-feature-extraction（图片特征提取）：用于从图片中提取特征，用于后续的任务或分析。 image-segmentation（图片分割）：用于将图片分割成不同的区域或对象，进行图像分割任务。 image-to-image（图片到图片）：用于执行图片到图片的转换，如图像风格转换、图像去噪等。 image-to-text（图片到文本）：用于从图片中提取文本信息，实现图片中的文字识别功能。 mask-generation（遮罩生成）：用于生成图片中的遮罩或掩码，用于图像处理或分割任务。 object-detection（目标检测）：用于从图片或视频中检测和识别出图像中的目标对象。 question-answering（问答）：用于回答给定问题的模型，从文本中找出包含答案的部分。 summarization（摘要生成）：用于生成文本的摘要或总结，将文本内容压缩为简短的形式。 table-question-answering（表格问答）：用于从表格数据中回答问题，帮助用户从表格中获取信息。 text2text-generation（文本到文本生成）：用于生成文本的模型，可以执行文本到文本的转换或生成任务。 text-classification（文本分类）：(别名\"sentiment-analysis\" 可用，情感分析)用于将文本进行分类，识别文本中的类别或属性。 text-generation（文本生成）：用于生成文本的模型，可以生成连续的文本序列。 text-to-audio（文本到音频）：用于将文本转换为语音，实现文本到语音的功能。 token-classification（标记分类）：别名\"ner\" 可用，命名实体识别，用于将文本中的每个标记或单词进行分类，识别每个标记的类别或属性。 translation（翻译）：用于执行文本的翻译任务，将文本从一种语言翻译成另一种语言。 video-classification（视频分类）：用于对视频进行分类，识别视频中的类别或属性。 visual-question-answering（视觉问答）：用于从图片或视频中回答问题，结合视觉和文本信息进行问答。 zero-shot-classification（零样本分类）：用于执行零样本分类任务，即在没有见过该类别的情况下对新样本进行分类。 zero-shot-image-classification（零样本图片分类）：用于执行零样本分类任务，即在没有见过该类别的情况下对新图片进行分类。 zero-shot-audio-classification（零样本音频分类）：用于执行零样本分类任务，即在没有见过该类别的情况下对新音频进行分类。 zero-shot-object-detection（零样本目标检测）：用于执行零样本目标检测任务，即在没有见过该类别的情况下对新图片中的目标对象进行检测。 如果需要了解更多的task类型更新，参考官网pipeline： 下面我们以常见的几个 NLP 任务为例，展示如何调用这些 pipeline 模型。\n图片转文本 教程参考自官网：https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/pipelines#transformers.ImageToTextPipeline from transformers import pipeline\ritt=pipeline(\"image-to-text\",model=\"ydshieh/vit-gpt2-coco-en\") #model不指定会使用默认模型。\rrtn=itt(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\rprint(rtn) 可以看到输出是需要先下载模型（下载一次，自动缓存），下载在C:\\Users\\admin.cache\\huggingface\\hub目录下。\n:\\python\\evn311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--ydshieh--vit-gpt2-coco-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\rTo support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\rwarnings.warn(message) 最后输出：[{‘generated_text’: ’two birds are standing next to each other ‘}]\n如果希望使用其他的image-to-text模型可以在官网搜索 https://huggingface.co/models?pipeline_tag=image-to-text\u0026sort=trending 比如选择image-to-text右侧文本框输入chinese，看下是不是有中文描述的 使用这个模型来测试下\nimage_path=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\rfrom transformers import pipeline\ritt=pipeline(\"image-to-text\",model=\"IDEA-CCNL/Taiyi-BLIP-750M-Chinese\")\rrtn=itt(image_path)\rprint(rtn) 输出（效果没有英文的模型好，明明是两只鹦鹉啊，不过识别出了鹦鹉，英文的只是两只鸟） [{‘generated_text’: ‘一 只 鹦 鹉 的 黑 白 照 片 。’}]\n文本生成 我们首先根据任务需要构建一个模板 (prompt)，然后将其送入到模型中来生成后续文本。注意，由于文本生成具有随机性，因此每次运行都会得到不同的结果。\n#%%\rfrom transformers import pipeline\rgenerator = pipeline(\"text-generation\",model=\"openai-community/gpt2\")\rprint(generator(\"I can't believe you did such a \")) 输出：\n[{'generated_text': 'I can\\'t believe you did such a _____t!\"\\n\\n\"You know I\\'m kind of an asshole to you, I mean?\"\\n\\n\"Just because I had one thing to do doesn\\'t mean I hate you. I know you'}] 在huggerface上搜索一个古诗词生成的模型， 左侧选择tag Text Generation ，搜索poem，选择最多人喜欢。 from transformers import pipeline\rgenerator = pipeline(\"text-generation\",model=\"uer/gpt2-chinese-poem\")\rprint(generator(\"[CLS] 离 离 原 上 草 ，\")) 输出\n[{'generated_text': '[CLS] 离 离 原 上 草 ， 濯 濯 原 上 桑 。 春 风 吹 罗 衣 ， 行 人 泪 成 行 。 离 人 不 可 留 ， 况 乃 隔 河 梁 。 当 和 露 餐 ， 勿 复 怨 秋 凉 。 愿 言 崇 令 德 ， 以 配 君 子 光 。 毋 怀 远 心 ， 皓 月 鉴 我 伤 。 莫 怨 东 风 ， 飘 然 入 西 楼 。 举 手 倚 阑 干 ， 举 酒 相 劝 酬 。 良 时 焉 可 再 ， 逝 水 何 悠 悠 。 我 金 石 交 ， 沉 邈 焉 能 求'}] 情感分析 借助情感分析 pipeline，我们只需要输入文本，就可以得到其情感标签（积极/消极）以及对应的概率：\nfrom transformers import pipeline\rclassifier = pipeline(\"sentiment-analysis\")\rresult = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\rprint(result)\rresults = classifier(\r[\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\r)\rprint(results) No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\r[{'label': 'POSITIVE', 'score': 0.9598048329353333}]\r[{'label': 'POSITIVE', 'score': 0.9598048329353333}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}] pipeline 模型会自动完成以下三个步骤：\n将文本预处理为模型可以理解的格式； 将预处理好的文本送入模型； 对模型的预测值进行后处理，输出人类可以理解的格式。 pipeline 会自动选择合适的预训练模型来完成任务。例如对于情感分析，默认就会选择微调好的英文情感模型 distilbert-base-uncased-finetuned-sst-2-english。\nTransformers 库会在创建对象时下载并且缓存模型，只有在首次加载模型时才会下载，后续会直接调用缓存好的模型。\n零训练样本分类 零训练样本分类 pipeline 允许我们在不提供任何标注数据的情况下自定义分类标签。\nfrom transformers import pipeline\rclassifier = pipeline(\"zero-shot-classification\")\rresult = classifier(\r\"This is a course about the Transformers library\",\rcandidate_labels=[\"education\", \"politics\", \"business\"],\r)\rprint(result) No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\r{'sequence': 'This is a course about the Transformers library', 'labels': ['education', 'business', 'politics'], 'scores': [0.8445973992347717, 0.11197526752948761, 0.043427325785160065]} 可以看到，pipeline 自动选择了预训练好的 facebook/bart-large-mnli 模型来完成任务。\n遮盖词填充 给定一段部分词语被遮盖掉 (masked) 的文本，使用预训练模型来预测能够填充这些位置的词语。\nfrom transformers import pipeline\runmasker = pipeline(\"fill-mask\")\rresults = unmasker(\"This course will teach you all about \u003cmask\u003e models.\", top_k=2)\rprint(results) No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\r[{'sequence': 'This course will teach you all about mathematical models.', 'score': 0.19619858264923096, 'token': 30412, 'token_str': ' mathematical'}, {'sequence': 'This course will teach you all about computational models.', 'score': 0.04052719101309776, 'token': 38163, 'token_str': ' computational'}] 可以看到，pipeline 自动选择了预训练好的 distilroberta-base 模型来完成任务。\n命名实体识别 命名实体识别 (NER) pipeline 负责从文本中抽取出指定类型的实体，例如人物、地点、组织等等。\nfrom transformers import pipeline\rner = pipeline(\"ner\", grouped_entities=True)\rresults = ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\rprint(results) No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\r[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.97960186, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}] 可以看到，模型正确地识别出了 Sylvain 是一个人物，Hugging Face 是一个组织，Brooklyn 是一个地名。\n这里通过设置参数 grouped_entities=True，使得 pipeline 自动合并属于同一个实体的多个子词 (token)，例如这里将“Hugging”和“Face”合并为一个组织实体，实际上 Sylvain 也进行了子词合并，因为分词器会将 Sylvain 切分为 S、##yl 、##va 和 ##in 四个 token。\n自动问答 自动问答 pipeline 可以根据给定的上下文回答问题，例如：\nfrom transformers import pipeline\rquestion_answerer = pipeline(\"question-answering\")\ranswer = question_answerer(\rquestion=\"Where do I work?\",\rcontext=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\r)\rprint(answer) No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\r{'score': 0.6949771046638489, 'start': 33, 'end': 45, 'answer': 'Hugging Face'} 可以看到，pipeline 自动选择了在 SQuAD 数据集上训练好的 distilbert-base 模型来完成任务。这里的自动问答 pipeline 实际上是一个抽取式问答模型，即从给定的上下文中抽取答案，而不是生成答案。\n根据形式的不同，自动问答 (QA) 系统可以分为三种：\n**抽取式 QA (extractive QA)：**假设答案就包含在文档中，因此直接从文档中抽取答案； **多选 QA (multiple-choice QA)：**从多个给定的选项中选择答案，相当于做阅读理解题； **无约束 QA (free-form QA)：**直接生成答案文本，并且对答案文本格式没有任何限制。 自动摘要 自动摘要 pipeline 旨在将长文本压缩成短文本，并且还要尽可能保留原文的主要信息，例如：\nfrom transformers import pipeline\rsummarizer = pipeline(\"summarization\")\rresults = summarizer(\r\"\"\"\rAmerica has changed dramatically during recent years. Not only has the number of graduates in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering declined, but in most of the premier American universities engineering curricula now concentrate on and encourage largely the study of engineering science. As a result, there are declining offerings in engineering subjects dealing with infrastructure, the environment, and related issues, and greater concentration on high technology subjects, largely supporting increasingly complex scientific developments. While the latter is important, it should not be at the expense of more traditional engineering.\rRapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance the teaching of engineering. Both China and India, respectively, graduate six and eight times as many traditional engineers as does the United States. Other industrial countries at minimum maintain their output, while America suffers an increasingly serious decline in the number of engineering graduates and a lack of well-educated engineers.\r\"\"\"\r)\rprint(results) No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\r[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance engineering .'}] 可以看到，pipeline 自动选择了预训练好的 distilbart-cnn-12-6 模型来完成任务。与文本生成类似，我们也可以通过 max_length 或 min_length 参数来控制返回摘要的长度。\npipeline 背后做了什么？ 这些简单易用的 pipeline 模型实际上封装了许多操作，下面我们就来了解一下它们背后究竟做了啥。以第一个情感分析 pipeline 为例，我们运行下面的代码\nfrom transformers import pipeline\rclassifier = pipeline(\"sentiment-analysis\")\rresult = classifier(\"This course is amazing!\")\rprint(result) 就会得到结果：\n[{'label': 'POSITIVE', 'score': 0.9998824596405029}] 实际上它的背后经过了三个步骤：\n预处理 (preprocessing)，将原始文本转换为模型可以接受的输入格式； 将处理好的输入送入模型； 对模型的输出进行后处理 (postprocessing)，将其转换为人类方便阅读的格式。 使用分词器进行预处理 因为神经网络模型无法直接处理文本，因此首先需要通过预处理环节将文本转换为模型可以理解的数字。具体地，我们会使用每个模型对应的分词器 (tokenizer) 来进行：\n将输入切分为词语、子词或者符号（例如标点符号），统称为 tokens； 根据模型的词表将每个 token 映射到对应的 token 编号（就是一个数字）； 根据模型的需要，添加一些额外的输入。 我们对输入文本的预处理需要与模型自身预训练时的操作完全一致，只有这样模型才可以正常地工作。注意，每个模型都有特定的预处理操作，如果对要使用的模型不熟悉，可以通过 Model Hub 查询。这里我们使用 AutoTokenizer 类和它的 from_pretrained() 函数，它可以自动根据模型 checkpoint 名称来获取对应的分词器。\n情感分析 pipeline 的默认 checkpoint 是 distilbert-base-uncased-finetuned-sst-2-english，下面我们手工下载并调用其分词器：\nfrom transformers import AutoTokenizer\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\rprint(inputs) {\r'input_ids': tensor([\r[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102],\r[ 101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0,\r0, 0, 0, 0, 0, 0]\r]), 'attention_mask': tensor([\r[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\r])\r} 可以看到，输出中包含两个键 input_ids 和 attention_mask，其中 input_ids 对应分词之后的 tokens 映射到的数字编号列表，而 attention_mask 则是用来标记哪些 tokens 是被填充的（这里“1”表示是原文，“0”表示是填充字符）。\n先不要关注 padding、truncation 这些参数，以及 attention_mask 项，后面我们会详细介绍:)。\n将预处理好的输入送入模型 预训练模型的下载方式和分词器 (tokenizer) 类似，Transformers 包提供了一个 AutoModel 类和对应的 from_pretrained() 函数。下面我们手工下载这个 distilbert-base 模型：\nfrom transformers import AutoModel\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rmodel = AutoModel.from_pretrained(checkpoint) 预训练模型的本体只包含基础的 Transformer 模块，对于给定的输入，它会输出一些神经元的值，称为 hidden states 或者特征 (features)。对于 NLP 模型来说，可以理解为是文本的高维语义表示。这些 hidden states 通常会被输入到其他的模型部分（称为 head），以完成特定的任务，例如送入到分类头中完成文本分类任务。\n其实前面我们举例的所有 pipelines 都具有类似的模型结构，只是模型的最后一部分会使用不同的 head 以完成对应的任务。 Transformers库结构 Transformers 库封装了很多不同的结构，常见的有：\n*Model （返回 hidden states） *ForCausalLM （用于条件语言模型），是一种用于因果语言模型的Transformer接口类型。在Transformer架构中,ForCausalLM是一个特殊的模型头,它被设计用于生成文本,即从前一个词预测下一个词,这类模型主要用于以下任务：1.根据已有文本生成后续文本，例如自动写作、对话生成等,2.语言建模：预测给定文本序列中下一个词的概率。 *ForSeq2SeqLM代表用于序列到序列（Sequence-to-Sequence）任务的语言模型。这类模型通常用于以下任务：1.机器翻译：将一种语言的句子翻译成另一种语言,2.文本摘要：将长文本生成简短的摘要,3.问答系统：从上下文中生成回答。。 *ForMaskedLM （用于遮盖语言模型），在MLM任务中,输入序列中的某些词会被随机mask掉,模型的目标是预测这些被mask的词。 *ForMultipleChoice （用于多选任务） *ForQuestionAnswering （用于自动问答任务） *ForSequenceClassification （用于文本分类任务），类似api如：图像分类：*ForImageClassification，输出维度对应于整个序列的类别预测,即整个输入序列只有一个类别输出。 *ForTokenClassification （用于 token 分类任务，例如 NER），输出维度对应于每个token的类别预测,即每个输入token都有一个类别输出。 模块输出 Transformer 模块的输出是一个维度为 (Batch size, Sequence length, Hidden size) 的三维张量，其中 Batch size 表示每次输入的样本（文本序列）数量，即每次输入多少个句子，上例中为 2；Sequence length 表示文本序列的长度，即每个句子被分为多少个 token，上例中为 16；Hidden size 表示每一个 token 经过模型编码后的输出向量（语义表示）的维度。\n预训练模型编码后的输出向量的维度通常都很大，例如 Bert 模型 base 版本的输出为 768 维，一些大模型的输出维度为 3072 甚至更高。\n我们可以打印出这里使用的 distilbert-base 模型的输出维度：\nfrom transformers import AutoTokenizer, AutoModel\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rmodel = AutoModel.from_pretrained(checkpoint)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\rprint(outputs.last_hidden_state.shape) torch.Size([2, 16, 768]) Transformers 模型的输出格式类似 namedtuple 或字典，可以像上面那样通过属性访问，也可以通过键（outputs[\"last_hidden_state\"]），甚至索引访问（outputs[0]）。\n对于情感分析任务，很明显我们最后需要使用的是一个文本分类 head。因此，实际上我们不会使用 AutoModel 类，而是使用 AutoModelForSequenceClassification：\nfrom transformers import AutoTokenizer\rfrom transformers import AutoModelForSequenceClassification\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\rprint(outputs.logits.shape) torch.Size([2, 2]) 可以看到，对于 batch 中的每一个样本，模型都会输出一个两维的向量（每一维对应一个标签，positive 或 negative）。\n对模型输出进行后处理 由于模型的输出只是一些数值，因此并不适合人类阅读。例如我们打印出上面例子的输出：\nfrom transformers import AutoTokenizer\rfrom transformers import AutoModelForSequenceClassification\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\rprint(outputs.logits) tensor([[-1.5607, 1.6123],\r[ 4.1692, -3.3464]], grad_fn=\u003cAddmmBackward0\u003e) 模型对第一个句子输出 [−1.5607,1.6123]，对第二个句子输出 [4.1692,−3.3464]，它们并不是概率值，而是模型最后一层输出的 logits 值。要将他们转换为概率值，还需要让它们经过一个 SoftMax 层，例如：\nimport torch\rpredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\rprint(predictions) tensor([[4.0195e-02, 9.5980e-01],\r[9.9946e-01, 5.4418e-04]], grad_fn=\u003cSoftmaxBackward0\u003e) 所有 Transformers 模型都会输出 logits 值，因为训练时的损失函数通常会自动结合激活函数（例如 SoftMax）与实际的损失函数（例如交叉熵 cross entropy）。\n这样模型的预测结果就是容易理解的概率值：第一个句子 [0.0402,0.9598]，第二个句子 [0.9995,0.0005]。最后，为了得到对应的标签，可以读取模型 config 中提供的 id2label 属性：\nprint(model.config.id2label) {0: 'NEGATIVE', 1: 'POSITIVE'} 于是我们可以得到最终的预测结果：\n第一个句子: NEGATIVE: 0.0402, POSITIVE: 0.9598 第二个句子: NEGATIVE: 0.9995, POSITIVE: 0.0005 本文部分文本引用自：https://transformers.run/",
    "description": "Hugging Face Hugging Face是一个人工智能（AI）公司，它致力于开发和推广自然语言处理（NLP）相关的技术和工具。该公司以其开源项目和社区而闻名，其最知名的项目之一是Transformers库，它提供了一系列预训练的语言模型，包括BERT、GPT和RoBERTa等。这些模型已经在各种NLP任务中取得了显著的成功，并成为了许多研究和工业应用的基础。\n除了提供预训练的模型之外，Hugging Face还开发了一系列工具和平台，使得使用和部署这些模型变得更加简单。其中包括：\nTransformers库：提供了各种预训练的语言模型的接口和工具，使得开发者可以轻松地使用这些模型进行文本分类、命名实体识别、语言生成等任务。\nDatasets库：包含了各种NLP数据集的接口和工具，使得开发者可以方便地使用这些数据集进行模型训练和评估。\nTrainer库：提供了一个训练和微调模型的框架，使得开发者可以方便地使用自己的数据集对预训练模型进行微调，以适应特定的任务和应用场景。\nModel Hub：一个模型分享和发布平台，开发者可以在这里分享自己训练的模型，也可以找到其他人分享的模型，并且可以直接在自己的项目中使用这些模型。\ndatasets数据集处理，transformers预训练微调等相关教程请参考官网hugging face官方文档。\nTransformers Transformers 是由 Hugging Face 开发的一个 NLP 包，支持加载目前绝大部分的预训练模型。随着 BERT、GPT 等大规模语言模型的兴起，越来越多的公司和研究者采用 Transformers 库来构建 NLP 应用，官网地址。 它提供了各种预训练的 Transformer 模型，包括 BERT、GPT、RoBERTa、DistilBERT 等。这些模型在多个 NLP 任务上取得了 state-of-the-art 的性能，并且 Transformers 库提供了简单易用的接口，使得使用这些预训练模型变得非常便捷。\n安装 官网安装教程参考：https://huggingface.co/docs/transformers/installation\n您可以通过 pip 安装 Transformers 库。在终端或命令行界面中执行以下命令（我这里使用pytorch，如果需要tensorflow的版本参考官网）：\npip install 'transformers[torch]'\n这将会自动从 PyPI（Python Package Index）下载并安装最新版本的 Transformers 库及其依赖项。\n如果您使用的是 Anaconda 环境，您也可以通过 conda 安装：\nconda install -c huggingface transformers\n这将会从 Anaconda 仓库中下载并安装 Transformers 库及其依赖项。\n安装完成后，您就可以在 Python 环境中使用 Transformers 库了。您可以编写代码来加载预训练模型、执行各种 NLP 任务，或者使用 Transformers 提供的高级 API，如 pipelines 来快速完成任务。",
    "tags": [],
    "title": "Transformers实战01-开箱即用的 pipelines",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_01/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "BERT（Bidirectional Encoder Representations from Transformers）是一种基于 Transformer 模型的预训练语言表示方法，由Google研究团队于2018年提出。BERT 通过在大规模文本语料上进行无监督的预训练，学习了通用的语言表示，并且在各种自然语言处理任务中取得了显著的性能提升。\nBERT仅使用了Transformer架构的Encoder部分。BERT自2018年由谷歌发布后，在多种NLP任务中（例如QA、文本生成、情感分析等等）都实现了更好的结果。\n“Word2vec与GloVe都有一个特点，就是它们是上下文无关（context-free）的词嵌入。所以它们没有解决：一个单词在不同上下文中代表不同的含义的问题。例如，对于单词bank，它在不同的上下文中，有银行、河畔这种差别非常大的含义。BERT的出现，解决了这个问题。\nBERT 的主要特点包括：\n双向性：BERT 使用双向 Transformer 模型来处理输入序列，从而能够同时考虑上下文的信息，而不仅仅是单向的上下文信息。这种双向性使得 BERT 能够更好地理解句子中的语义和语境。\n预训练-微调框架：BERT 使用了预训练-微调的方法。首先，在大规模文本语料上进行无监督的预训练，通过 Masked Language Model（MLM）和 Next Sentence Prediction（NSP）任务学习语言表示；然后，在特定的下游任务上微调模型参数，使其适应于特定的任务，如文本分类、命名实体识别等。\nTransformer 模型：BERT 基于 Transformer 模型结构，其中包括多层的编码器，每个编码器由自注意力机制和前馈神经网络组成。这种结构能够有效地捕获输入序列中的长距离依赖关系，有助于提高模型在各种自然语言处理任务中的性能。\n多层表示：BERT 提供了多层的语言表示，使得用户可以根据具体任务选择不同层的表示进行应用。较底层的表示通常更加接近原始输入，而较高层的表示则更加抽象，包含了更多的语义信息。\n开放源代码：BERT 的源代码和预训练模型已经在 GitHub 上开放，使得研究人员和开发者可以基于 BERT 进行进一步的研究和应用开发。\nBERT 通过预训练大规模文本语料上的通用语言表示，以及在各种下游任务上的微调，有效地提高了自然语言处理任务的性能，并且成为了当前领域内最具影响力的预训练模型之一。\ntransformer提供了不同领域中常见的机器学习模型类型：\nTEXT MODELS（文本模型）：用于处理和分析文本数据的模型，如自然语言处理（NLP）中的BERT、GPT等。\nVISION MODELS（视觉模型）：用于处理和分析图像数据的模型，如卷积神经网络（CNN）中的ResNet、VGG,Vision Transformer (ViT)等。\nAUDIO MODELS（音频模型）：用于处理和分析音频数据的模型，如声学模型、语音识别模型等。\nVIDEO MODELS（视频模型）：用于处理和分析视频数据的模型，如视频分类、目标检测、行为识别等。\nMULTIMODAL MODELS（多模态模型）：结合多种数据类型（如文本、图像、音频等）进行分析和预测的模型，如OpenAI的CLIP。\nREINFORCEMENT LEARNING MODELS（强化学习模型）：用于解决强化学习问题的模型，如Deep Q-Networks（DQN）、Actor-Critic等。\nTIME SERIES MODELS（时间序列模型）：用于分析和预测时间序列数据的模型，如循环神经网络（RNN）、长短期记忆网络（LSTM）等。\nGRAPH MODELS（图模型）：用于处理和分析图数据的模型，如图神经网络（GNN）、图卷积网络（GCN）等。\nBERT的基本原理 BERT基于的是Transformer模型，并且仅使用Transformer模型的Encoder部分。在Transformer模型中，Encoder的输入是一串序列，输出的是对序列中每个字符的表示。同样，在BERT中，输入的是一串序列，输出的是也是对应序列中每个单词的编码。 以“He got bit by Python”为例，BERT的输入输出如下图所示： 其中输入为序列“He got bit by Python”，输出的是对每个单词的编码$R_{word}$。这样在经过了BERT处理后，即得到了对每个单词包含的上下文表示$R_{word}$。\n分词 from transformers import AutoModel, BertTokenizer\rmodel_name=\"bert-base-chinese\" #bert-base-uncased\rmodel=AutoModel.from_pretrained(model_name)\rtokenizer=BertTokenizer.from_pretrained(model_name)\rprint(type(model),type(tokenizer))\rsequence = [\"我出生在湖南A阳,我得家在深圳.\",\"我得儿子是廖X谦\"]\r#输出中包含两个键 input_ids 和 attention_mask，其中 input_ids 对应分词之后的 tokens 映射到的数字编号列表，而 attention_mask 则是用来标记哪些 tokens #是被填充的（这里“1”表示是原文，“0”表示是填充字符）。\rprint(tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\",pair=True))\r#将输入切分为词语、子词或者符号（例如标点符号），统称为 tokens；\rprint(tokenizer.tokenize(sequence[0]),len(tokenizer.tokenize(sequence[0])))\r#我们通过 convert_tokens_to_ids() 将切分出的 tokens 转换为对应的 token IDs：\rprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence[0])))\r#可以通过 encode() 函数将这两个步骤合并，并且 encode() 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加[CLS] 和 [SEP]\rprint(tokenizer.encode(sequence[0]))\r#解码还原文字，可以看到encode前后加了[CLS] 和 [SEP]\rprint(tokenizer.decode(tokenizer.encode(sequence[1]))) 输出\n\u003cclass 'transformers.models.bert.modeling_bert.BertModel'\u003e \u003cclass 'transformers.models.bert.tokenization_bert.BertTokenizer'\u003e\r{'input_ids': tensor([[ 101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 2533,\r2157, 1762, 3918, 1766, 119, 102],\r[ 101, 2769, 2533, 1036, 2094, 3221, 2445, 3813, 6472, 102, 0, 0,\r0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\r['我', '出', '生', '在', '湖', '南', 'A', '阳', ',', '我', '得', '家', '在', '深', '圳', '.'] 16\r[2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 2533, 2157, 1762, 3918, 1766, 119]\r[101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 2533, 2157, 1762, 3918, 1766, 119, 102]\r[CLS] 我 得 儿 子 是 廖 X 谦 [SEP] 模型输出 #这里演示最终输出隐藏状态得输出\rfrom transformers import AutoModel,AutoTokenizer\rmodel_name=\"bert-base-chinese\" #bert-base-uncased\rmodel=AutoModel.from_pretrained(model_name)\rtokenizer=BertTokenizer.from_pretrained(model_name)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\rprint(\"词个数\",len(tokenizer.encode(raw_inputs[0])))\r\"\"\"\r在BERT模型中，last_hidden_state 的形状是 [batch_size, sequence_length, hidden_size]，其中：\rbatch_size 表示批量大小，即输入的样本数量。在你的例子中，batch_size 是 2，表示你有两个句子。\rsequence_length 表示序列长度，即输入文本中词元的数量。在你的例子中，sequence_length 是 19，表示每个句子包含 19 个词元,我爱中国，我就是一个词元，爱也是一个词元。\rhidden_size 表示隐藏状态的维度，通常是模型的隐藏层的大小。在BERT-base模型中，hidden_size 是 768，表示每个词元的隐藏状态是一个包含 768 个值的向量。\r\"\"\"\rprint(outputs.last_hidden_state.shape) 输出：torch.Size([2, 19, 768])\nBERT预训练的方法 BERT的预训练语料库使用的是Toronto BookCorpus和Wikipedia数据集。在准备训练数据时，首先从语料库中采样2条句子，例如Sentence-A与Sentence-B。这里需要注意的是：2条句子的单词之和不能超过512个。对于采集的这些句子，50%为两个句子是相邻句子，另50%为两个句子毫无关系。\n假设采集了以下2条句子：\nBeijing is a beautiful city\rI love Beijing 对这2条句子先做分词：\nTokens = [ [CLS], Beijing, is, a, beautiful, city, [SEP], I, love, Beijing, [SEP] ] 然后，以15%的概率遮挡单词，并遵循80%-10%-10%的规则。假设遮挡的单词为city，则：\nTokens = [ [CLS], Beijing, is, a, beautiful, [MASK], [SEP], I, love, Beijing, [SEP] ]\n接下来将Tokens送入到BERT中，并训练BERT预测被遮挡的单词，同时也要预测这2条句子是否为相邻（句子2是句子1的下一条句子）。也就是说，BERT是同时训练Masked Language Modeling和NSP任务。\nBERT的训练参数是：1000000个step，每个batch包含256条序列（256 * 512个单词 = 128000单词/batch）。使用的是Adam，learning rate为1e-4、β1 = 0.9、β2 = 0.999。L2正则权重的衰减参数为0.01。对于learning rete，前10000个steps使用了rate warmup，之后开始线性衰减learning rate（简单地说，就是前期训练使用一个较大的learning rate，后期开始线性减少）。对所有layer使用0.1概率的dropout。使用的激活函数为gelu，而非relu。 验证使用两条句子。\ncheckpoint = \"bert-base-chinese\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rraw_inputs = [\r\"拼多多得货物真是差劲.\",\r\"我喜欢天猫，天猫货物都很好\",\r]\rraw_inputs1 = [\r\"拼多多买了一件掉色衣服.\",\r\"我在天猫买的衣服颜色还行\",\r]\r#允许传入两个数组，相同索引会自动通过[SEP]拼接。\rinputs = tokenizer(raw_inputs,raw_inputs1, padding=True, truncation=True, return_tensors=\"pt\")\rprint(tokenizer.decode(inputs.input_ids[0]))\rprint(tokenizer.decode(inputs.input_ids[1])) 输出\n[CLS] 拼 多 多 得 货 物 真 是 差 劲. [SEP] 拼 多 多 买 了 一 件 掉 色 衣 服. [SEP] [PAD] [PAD]\r[CLS] 我 喜 欢 天 猫 ， 天 猫 货 物 都 很 好 [SEP] 我 在 天 猫 买 的 衣 服 颜 色 还 行 [SEP] 预测的整个过程\n#演示预测的整个过程。\rimport torch\rfrom transformers import AutoModelForSequenceClassification\r#情感分析任务\rcheckpoint = \"bert-base-chinese\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\rprint(type(model))\rraw_inputs = [\r\"拼多多得货物真是差劲.\",\r\"我喜欢天猫，天猫货物都很好\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\r#将分词的词反编码出来\rprint(tokenizer.decode(inputs.input_ids[0]),tokenizer.decode(inputs.input_ids[1]))\r#\"Logits\" 是指模型在分类问题中输出的未经过 softmax 或 sigmoid 函数处理的原始预测值。\rprint(\"分类输出形状:\",outputs.logits.shape)\rprint(\"分类输出:\",outputs.logits)\r#经过softmax就是预测的结果了\rpredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\r#预测的每一行是一个句子，第一列表示积极的概率，第二列表示不积极的概率\rprint(\"预测结果:\",predictions)\r#有两种分类0表示积极，1表示不积极\rprint(\"label和索引:\",print(model.config.id2label)) 输出\n[CLS] 拼 多 多 得 货 物 真 是 差 劲. [SEP] [PAD] [PAD] [CLS] 我 喜 欢 天 猫 ， 天 猫 货 物 都 很 好 [SEP]\r分类输出形状: torch.Size([2, 2])\r分类输出: tensor([[0.4789, 1.0043],\r[0.2907, 0.7432]], grad_fn=\u003cAddmmBackward0\u003e)\r预测结果: tensor([[0.3716, 0.6284],\r[0.3888, 0.6112]], grad_fn=\u003cSoftmaxBackward0\u003e)\r{0: 'LABEL_0', 1: 'LABEL_1'} BERT模型微调 加载数据集 我们以同义句判断任务为例（每次输入两个句子，判断它们是否为同义句），带大家构建我们的第一个 Transformers 模型。我们选择蚂蚁金融语义相似度数据集 AFQMC 作为语料，它提供了官方的数据划分，训练集（train.json） / 验证集（dev.json） / 测试集(test.json)分别包含 34334 / 4316 / 3861 个句子对，标签 0 表示非同义句，1 表示同义句：\n{\"sentence1\": \"还款还清了，为什么花呗账单显示还要还款\", \"sentence2\": \"花呗全额还清怎么显示没有还款\", \"label\": \"1\"} 训练集用于训练模型，验证集用于每次epoch后训练集的正确率，测试集用于验证最后生成模型的准确率。\nDataset Pytorch 通过 Dataset 类和 DataLoader 类处理数据集和加载样本。同样地，这里我们首先继承 Dataset 类构造自定义数据集，以组织样本和标签。AFQMC 样本以 json 格式存储，因此我们使用 json 库按行读取样本，并且以行号作为索引构建数据集。\nclass MyDataSet(Dataset):\rdef __init__(self,filePath):\rself.data={}\rcurrent_directory = os.getcwd()\rwith open(current_directory+\"/dataset/\"+filePath,\"rt\", encoding=\"utf-8\") as f:\rfor idx,line in enumerate(f):\rself.data[idx]=json.loads(line.strip())\rdef __getitem__(self, item):\rreturn self.data[item]\rdef __len__(self):\rreturn len(self.data)\rtrain_data=MyDataSet(\"train.json\")\rdev_data=MyDataSet(\"dev.json\")\rprint(dev_data[1]) 输出:\n{'id': 1, 'sentence1': '网商贷怎么转变成借呗', 'sentence2': '如何将网商贷切换为借呗'} 可以看到，我们编写的 AFQMC 类成功读取了数据集，每一个样本都以字典形式保存，分别以 sentence1、sentence2 和 label 为键存储句子对和标签。\n如果数据集非常巨大，难以一次性加载到内存中，我们也可以继承 IterableDataset 类构建迭代型数据集：\nclass MyDataSetIter(IterableDataset):\rdef __init__(self,filePath):\rself.filePath=filePath\rdef __iter__(self):\rcurrent_directory = os.getcwd()\rwith open(current_directory+\"/dataset/\"+self.filePath,\"rt\", encoding=\"utf-8\") as f:\rfor _,line in enumerate(f):\rdata=json.loads(line.strip())\ryield data\rprint(next(iter(MyDataSetIter(\"dev.json\")))) 输出：\n{'sentence1': '双十一花呗提额在哪', 'sentence2': '里可以提花呗额度', 'label': '0'} DataLoader 接下来就需要通过 DataLoader 库按批 (batch) 加载数据，并且将样本转换成模型可以接受的输入格式。对于 NLP 任务，这个环节就是将每个 batch 中的文本按照预训练模型的格式进行编码（包括 Padding、截断等操作）。\n我们通过手工编写 DataLoader 的批处理函数 collate_fn 来实现。首先加载分词器，然后对每个 batch 中的所有句子对进行编码，同时把标签转换为张量格式：\n#DataLoader处理数据为seq1 [SEP] seq2\rfrom transformers import AutoTokenizer\rimport torch\rfrom torch.utils.data import DataLoader\rcheckpoint = \"bert-base-chinese\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rdef collote_fn(batch_samples):\rbatch_sentence_1, batch_sentence_2 = [], []\rbatch_label = []\rfor sample in batch_samples:\rbatch_sentence_1.append(sample['sentence1'])\rbatch_sentence_2.append(sample['sentence2'])\rbatch_label.append(int(sample['label']))\rX = tokenizer(\rbatch_sentence_1,\rbatch_sentence_2,\rpadding=True,\rtruncation=True,\rreturn_tensors=\"pt\"\r)\ry = torch.tensor(batch_label)\rreturn X, y\rtrain_loader=DataLoader(train_data,batch_size=4,shuffle=False,collate_fn=collote_fn)\rX,y=next(iter(train_loader))\rprint(\"label的维度\",y.shape)\rprint(\"s1,s2合并的维度\",X.input_ids.shape)\rfor idx,d in enumerate(X.input_ids):\rprint(\"第一批次4个元素中的第{}个：{},label={}\".format(idx,tokenizer.decode(X.input_ids[idx]),y[idx])) 输出\nlabel的维度 torch.Size([4])\rs1,s2合并的维度 torch.Size([4, 30])\r第一批次4个元素中的第0个：[CLS] 蚂 蚁 借 呗 等 额 还 款 可 以 换 成 先 息 后 本 吗 [SEP] 借 呗 有 先 息 到 期 还 本 吗 [SEP],label=0\r第一批次4个元素中的第1个：[CLS] 蚂 蚁 花 呗 说 我 违 约 一 次 [SEP] 蚂 蚁 花 呗 违 约 行 为 是 什 么 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD],label=0\r第一批次4个元素中的第2个：[CLS] 帮 我 看 一 下 本 月 花 呗 账 单 有 没 有 结 清 [SEP] 下 月 花 呗 账 单 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD],label=0\r第一批次4个元素中的第3个：[CLS] 蚂 蚁 借 呗 多 长 时 间 综 合 评 估 一 次 [SEP] 借 呗 得 评 估 多 久 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD],label=0 可以看到，DataLoader 按照我们设置的 batch size 每次对 4 个样本进行编码，并且通过设置 padding=True 和 truncation=True 来自动对每个 batch 中的样本进行补全和截断。这里我们选择 BERT 模型作为 checkpoint，所以每个样本都被处理成了“了“[CLS] sen1 [SEP] sen2 [SEP]”的形式。\n这种只在一个 batch 内进行补全的操作被称为动态补全 (Dynamic padding)，Hugging Face 也提供了 DataCollatorWithPadding 类来进行，如果感兴趣可以自行了解。\n训练模型 构建模型 常见的写法是继承 Transformers 库中的预训练模型来创建自己的模型。例如这里我们可以继承 BERT 模型（BertPreTrainedModel 类）来创建一个与上面模型结构完全相同的分类器：\n#构建模型\rfrom transformers import BertPreTrainedModel,BertModel,AutoConfig\rfrom torch import nn\rclass BertForPartwiseCLs(BertPreTrainedModel):\r\"\"\"\r定义模型继承自BertPreTrainedModel\r\"\"\"\rdef __init__(self,config):\r\"\"\"\r传入config，原始镜像的config\r\"\"\"\rsuper().__init__(config)\r#定义BertModel\rself.model=BertModel(config, add_pooling_layer=False)\r#丢弃10%\rself.dropout=nn.Dropout(config.hidden_dropout_prob)\r#全连接为2分类\rself.classifier=nn.Linear(768,2)\r#初始化权重参数\rself.post_init()\rdef forward(self,input):\r#执行模型产生一个(批次，词元,隐藏神经元)的输出\rbert_output=self.model(**input)\r#输出的数据有多个词元，取第一个[CLS]词元，因为每个词元通过注意力机制都包含了和其他词的语义信息，所以只需要一个即可\r#这里句子的维度编程了[批次,1,768]\rvector_data=bert_output.last_hidden_state[:,0,:]\rvector_data=self.dropout(vector_data)\rlogits=self.classifier(vector_data)\rreturn logits\rcheckpoint = \"bert-base-chinese\"\rconfig=AutoConfig.from_pretrained(checkpoint)\rmodel=BertForPartwiseCLs.from_pretrained(checkpoint,config=config)\rprint(model)\rX,y=next(iter(train_loader))\rprint(model(X).shape) 输出\nD:\\python\\evn311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\rwarnings.warn(\rSome weights of BertForPartwiseCLs were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['bert.classifier.bias', 'bert.classifier.weight', 'bert.model.embeddings.LayerNorm.bias', 'bert.model.embeddings.LayerNorm.weight', 'bert.model.embeddings.position_embeddings.weight', 'bert.model.embeddings.token_type_embeddings.weight', 'bert.model.embeddings.word_embeddings.weight', 'bert.model.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.0.attention.output.dense.bias', 'bert.model.encoder.layer.0.attention.output.dense.weight', 'bert.model.encoder.layer.0.attention.self.key.bias', 'bert.model.encoder.layer.0.attention.self.key.weight', 'bert.model.encoder.layer.0.attention.self.query.bias', 'bert.model.encoder.layer.0.attention.self.query.weight', 'bert.model.encoder.layer.0.attention.self.value.bias', 'bert.model.encoder.layer.0.attention.self.value.weight', 'bert.model.encoder.layer.0.intermediate.dense.bias', 'bert.model.encoder.layer.0.intermediate.dense.weight', 'bert.model.encoder.layer.0.output.LayerNorm.bias', 'bert.model.encoder.layer.0.output.LayerNorm.weight', 'bert.model.encoder.layer.0.output.dense.bias', 'bert.model.encoder.layer.0.output.dense.weight', 'bert.model.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.1.attention.output.dense.bias', 'bert.model.encoder.layer.1.attention.output.dense.weight', 'bert.model.encoder.layer.1.attention.self.key.bias', 'bert.model.encoder.layer.1.attention.self.key.weight', 'bert.model.encoder.layer.1.attention.self.query.bias', 'bert.model.encoder.layer.1.attention.self.query.weight', 'bert.model.encoder.layer.1.attention.self.value.bias', 'bert.model.encoder.layer.1.attention.self.value.weight', 'bert.model.encoder.layer.1.intermediate.dense.bias', 'bert.model.encoder.layer.1.intermediate.dense.weight', 'bert.model.encoder.layer.1.output.LayerNorm.bias', 'bert.model.encoder.layer.1.output.LayerNorm.weight', 'bert.model.encoder.layer.1.output.dense.bias', 'bert.model.encoder.layer.1.output.dense.weight', 'bert.model.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.10.attention.output.dense.bias', 'bert.model.encoder.layer.10.attention.output.dense.weight', 'bert.model.encoder.layer.10.attention.self.key.bias', 'bert.model.encoder.layer.10.attention.self.key.weight', 'bert.model.encoder.layer.10.attention.self.query.bias', 'bert.model.encoder.layer.10.attention.self.query.weight', 'bert.model.encoder.layer.10.attention.self.value.bias', 'bert.model.encoder.layer.10.attention.self.value.weight', 'bert.model.encoder.layer.10.intermediate.dense.bias', 'bert.model.encoder.layer.10.intermediate.dense.weight', 'bert.model.encoder.layer.10.output.LayerNorm.bias', 'bert.model.encoder.layer.10.output.LayerNorm.weight', 'bert.model.encoder.layer.10.output.dense.bias', 'bert.model.encoder.layer.10.output.dense.weight', 'bert.model.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.11.attention.output.dense.bias', 'bert.model.encoder.layer.11.attention.output.dense.weight', 'bert.model.encoder.layer.11.attention.self.key.bias', 'bert.model.encoder.layer.11.attention.self.key.weight', 'bert.model.encoder.layer.11.attention.self.query.bias', 'bert.model.encoder.layer.11.attention.self.query.weight', 'bert.model.encoder.layer.11.attention.self.value.bias', 'bert.model.encoder.layer.11.attention.self.value.weight', 'bert.model.encoder.layer.11.intermediate.dense.bias', 'bert.model.encoder.layer.11.intermediate.dense.weight', 'bert.model.encoder.layer.11.output.LayerNorm.bias', 'bert.model.encoder.layer.11.output.LayerNorm.weight', 'bert.model.encoder.layer.11.output.dense.bias', 'bert.model.encoder.layer.11.output.dense.weight', 'bert.model.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.2.attention.output.dense.bias', 'bert.model.encoder.layer.2.attention.output.dense.weight', 'bert.model.encoder.layer.2.attention.self.key.bias', 'bert.model.encoder.layer.2.attention.self.key.weight', 'bert.model.encoder.layer.2.attention.self.query.bias', 'bert.model.encoder.layer.2.attention.self.query.weight', 'bert.model.encoder.layer.2.attention.self.value.bias', 'bert.model.encoder.layer.2.attention.self.value.weight', 'bert.model.encoder.layer.2.intermediate.dense.bias', 'bert.model.encoder.layer.2.intermediate.dense.weight', 'bert.model.encoder.layer.2.output.LayerNorm.bias', 'bert.model.encoder.layer.2.output.LayerNorm.weight', 'bert.model.encoder.layer.2.output.dense.bias', 'bert.model.encoder.layer.2.output.dense.weight', 'bert.model.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.3.attention.output.dense.bias', 'bert.model.encoder.layer.3.attention.output.dense.weight', 'bert.model.encoder.layer.3.attention.self.key.bias', 'bert.model.encoder.layer.3.attention.self.key.weight', 'bert.model.encoder.layer.3.attention.self.query.bias', 'bert.model.encoder.layer.3.attention.self.query.weight', 'bert.model.encoder.layer.3.attention.self.value.bias', 'bert.model.encoder.layer.3.attention.self.value.weight', 'bert.model.encoder.layer.3.intermediate.dense.bias', 'bert.model.encoder.layer.3.intermediate.dense.weight', 'bert.model.encoder.layer.3.output.LayerNorm.bias', 'bert.model.encoder.layer.3.output.LayerNorm.weight', 'bert.model.encoder.layer.3.output.dense.bias', 'bert.model.encoder.layer.3.output.dense.weight', 'bert.model.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.4.attention.output.dense.bias', 'bert.model.encoder.layer.4.attention.output.dense.weight', 'bert.model.encoder.layer.4.attention.self.key.bias', 'bert.model.encoder.layer.4.attention.self.key.weight', 'bert.model.encoder.layer.4.attention.self.query.bias', 'bert.model.encoder.layer.4.attention.self.query.weight', 'bert.model.encoder.layer.4.attention.self.value.bias', 'bert.model.encoder.layer.4.attention.self.value.weight', 'bert.model.encoder.layer.4.intermediate.dense.bias', 'bert.model.encoder.layer.4.intermediate.dense.weight', 'bert.model.encoder.layer.4.output.LayerNorm.bias', 'bert.model.encoder.layer.4.output.LayerNorm.weight', 'bert.model.encoder.layer.4.output.dense.bias', 'bert.model.encoder.layer.4.output.dense.weight', 'bert.model.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.5.attention.output.dense.bias', 'bert.model.encoder.layer.5.attention.output.dense.weight', 'bert.model.encoder.layer.5.attention.self.key.bias', 'bert.model.encoder.layer.5.attention.self.key.weight', 'bert.model.encoder.layer.5.attention.self.query.bias', 'bert.model.encoder.layer.5.attention.self.query.weight', 'bert.model.encoder.layer.5.attention.self.value.bias', 'bert.model.encoder.layer.5.attention.self.value.weight', 'bert.model.encoder.layer.5.intermediate.dense.bias', 'bert.model.encoder.layer.5.intermediate.dense.weight', 'bert.model.encoder.layer.5.output.LayerNorm.bias', 'bert.model.encoder.layer.5.output.LayerNorm.weight', 'bert.model.encoder.layer.5.output.dense.bias', 'bert.model.encoder.layer.5.output.dense.weight', 'bert.model.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.6.attention.output.dense.bias', 'bert.model.encoder.layer.6.attention.output.dense.weight', 'bert.model.encoder.layer.6.attention.self.key.bias', 'bert.model.encoder.layer.6.attention.self.key.weight', 'bert.model.encoder.layer.6.attention.self.query.bias', 'bert.model.encoder.layer.6.attention.self.query.weight', 'bert.model.encoder.layer.6.attention.self.value.bias', 'bert.model.encoder.layer.6.attention.self.value.weight', 'bert.model.encoder.layer.6.intermediate.dense.bias', 'bert.model.encoder.layer.6.intermediate.dense.weight', 'bert.model.encoder.layer.6.output.LayerNorm.bias', 'bert.model.encoder.layer.6.output.LayerNorm.weight', 'bert.model.encoder.layer.6.output.dense.bias', 'bert.model.encoder.layer.6.output.dense.weight', 'bert.model.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.7.attention.output.dense.bias', 'bert.model.encoder.layer.7.attention.output.dense.weight', 'bert.model.encoder.layer.7.attention.self.key.bias', 'bert.model.encoder.layer.7.attention.self.key.weight', 'bert.model.encoder.layer.7.attention.self.query.bias', 'bert.model.encoder.layer.7.attention.self.query.weight', 'bert.model.encoder.layer.7.attention.self.value.bias', 'bert.model.encoder.layer.7.attention.self.value.weight', 'bert.model.encoder.layer.7.intermediate.dense.bias', 'bert.model.encoder.layer.7.intermediate.dense.weight', 'bert.model.encoder.layer.7.output.LayerNorm.bias', 'bert.model.encoder.layer.7.output.LayerNorm.weight', 'bert.model.encoder.layer.7.output.dense.bias', 'bert.model.encoder.layer.7.output.dense.weight', 'bert.model.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.8.attention.output.dense.bias', 'bert.model.encoder.layer.8.attention.output.dense.weight', 'bert.model.encoder.layer.8.attention.self.key.bias', 'bert.model.encoder.layer.8.attention.self.key.weight', 'bert.model.encoder.layer.8.attention.self.query.bias', 'bert.model.encoder.layer.8.attention.self.query.weight', 'bert.model.encoder.layer.8.attention.self.value.bias', 'bert.model.encoder.layer.8.attention.self.value.weight', 'bert.model.encoder.layer.8.intermediate.dense.bias', 'bert.model.encoder.layer.8.intermediate.dense.weight', 'bert.model.encoder.layer.8.output.LayerNorm.bias', 'bert.model.encoder.layer.8.output.LayerNorm.weight', 'bert.model.encoder.layer.8.output.dense.bias', 'bert.model.encoder.layer.8.output.dense.weight', 'bert.model.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.9.attention.output.dense.bias', 'bert.model.encoder.layer.9.attention.output.dense.weight', 'bert.model.encoder.layer.9.attention.self.key.bias', 'bert.model.encoder.layer.9.attention.self.key.weight', 'bert.model.encoder.layer.9.attention.self.query.bias', 'bert.model.encoder.layer.9.attention.self.query.weight', 'bert.model.encoder.layer.9.attention.self.value.bias', 'bert.model.encoder.layer.9.attention.self.value.weight', 'bert.model.encoder.layer.9.intermediate.dense.bias', 'bert.model.encoder.layer.9.intermediate.dense.weight', 'bert.model.encoder.layer.9.output.LayerNorm.bias', 'bert.model.encoder.layer.9.output.LayerNorm.weight', 'bert.model.encoder.layer.9.output.dense.bias', 'bert.model.encoder.layer.9.output.dense.weight']\rYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\rBertForPartwiseCLs(\r(model): BertModel(\r(embeddings): BertEmbeddings(\r(word_embeddings): Embedding(21128, 768, padding_idx=0)\r(position_embeddings): Embedding(512, 768)\r(token_type_embeddings): Embedding(2, 768)\r(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(dropout): Dropout(p=0.1, inplace=False)\r)\r(encoder): BertEncoder(\r(layer): ModuleList(\r(0-11): 12 x BertLayer(\r(attention): BertAttention(\r(self): BertSelfAttention(\r(query): Linear(in_features=768, out_features=768, bias=True)\r(key): Linear(in_features=768, out_features=768, bias=True)\r(value): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.1, inplace=False)\r)\r(output): BertSelfOutput(\r(dense): Linear(in_features=768, out_features=768, bias=True)\r(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(dropout): Dropout(p=0.1, inplace=False)\r)\r)\r(intermediate): BertIntermediate(\r(dense): Linear(in_features=768, out_features=3072, bias=True)\r(intermediate_act_fn): GELUActivation()\r)\r(output): BertOutput(\r(dense): Linear(in_features=3072, out_features=768, bias=True)\r(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(dropout): Dropout(p=0.1, inplace=False)\r)\r)\r)\r)\r)\r(dropout): Dropout(p=0.1, inplace=False)\r(classifier): Linear(in_features=768, out_features=2, bias=True)\r)\rtorch.Size([4, 2]) 可以看到模型输出了一个 4×2 的张量，符合我们的预期（每个样本输出 2 维的 logits 值分别表示两个类别的预测分数，batch 内共 4 个样本）。\ntqdm使用 tqdm是一个Python库,用于在终端中显示进度条。它广泛应用于各种数据处理任务中,如循环、迭代器、pandas数据帧等。以下是对tqdm的简要介绍:\n简单易用: tqdm提供了简单直观的API,可以快速集成到代码中,只需要几行代码即可实现进度条显示。 丰富的功能: tqdm不仅可以显示进度条,还可以显示预估的剩余时间、完成百分比、已处理的数据量等信息。 自动检测环境: tqdm可以自动检测运行环境,在支持ANSI转义码的终端中使用动态进度条,在不支持的环境中使用静态进度条。 支持各种迭代器: tqdm支持各种Python内置迭代器,如list、range、enumerate等,也支持自定义迭代器。 可定制性强: tqdm提供了丰富的参数供用户自定义进度条的样式和行为,如颜色、宽度、刷新间隔等。 代码\n#tqdm进度条使用\rfrom tqdm.auto import tqdm\rimport time\r# 创建一个迭代对象，比如一个列表\ritems = range(10)\r# 使用tqdm来迭代这个对象，并显示进度条\rfor item in tqdm(items, desc='Processing'):\r# 在这里执行你的任务\rtime.sleep(0.1) # 模拟一些长时间运行的任务\r# range(10) 其实就是0-9\rprint([i for i in range(10)])\r#创建一个tqdm对象，传入得必须是range对象，range(10) 其实就是0-9\rprint(range(10),len(range(10)))\rtdm=tqdm(range(10), desc='Processing')\rfor item in range(10):\rtime.sleep(1) # 模拟一些长时间运行的任务\r#更新一次,其实就是进度条加上： 1/len(range(10))\rtdm.update(1) 效果 训练模型 在训练模型时，我们将每一轮 Epoch 分为训练循环和验证/测试循环。在训练循环中计算损失、优化模型的参数，在验证/测试循环中评估模型的性能，与 Pytorch 类似，Transformers 库同样实现了很多的优化器，并且相比 Pytorch 固定学习率，Transformers 库的优化器会随着训练过程逐步减小学习率（通常会产生更好的效果）。例如我们前面使用过的 AdamW 优化器 完整的训练过程，可与使用colab来进行训练。\n#训练模型和验证测试\r#定义损失函数\rfrom torch.nn import CrossEntropyLoss\rfrom transformers import get_scheduler\r#定义优化函数,from torch.optim import AdamW\rfrom transformers import AdamW\rfrom tqdm.auto import tqdm\r#定义epoch训练次数\repochs = 3\r#默认学习率\rlearning_rate = 1e-5\r# batchsize\rbatch_size=4\r#AdamW是Adam优化器的一种变体，它在Adam的基础上进行了一些改进，旨在解决Adam优化器可能引入的权重衰减问题。\roptimizer=AdamW(model.parameters(),lr=1e-5)\r#定义交叉熵损失函数\rloss_fn=CrossEntropyLoss()\r#重新初始化数据集\rtrain_loader=DataLoader(train_data,batch_size=batch_size,shuffle=False,collate_fn=collote_fn)\rdev_loader=DataLoader(MyDataSet(\"dev.json\"),batch_size=batch_size,shuffle=False,collate_fn=collote_fn)\r#总步数=epoch*批次数(总记录数train_data/一批次多少条数据batch_size)\rnum_training_steps = epochs * len(train_loader)\r#默认情况下，优化器会线性衰减学习率，对于上面的例子，学习率会线性地从le-5 降到0\r#。为了正确地定义学习率调度器，我们需要知道总的训练步数 (step)，它等于训练轮数 (Epoch number) 乘以每一轮中的步数（也就是训练 dataloader 的大小）\rlr_scheduler = get_scheduler(\r\"linear\",\roptimizer=optimizer,\rnum_warmup_steps=0,\rnum_training_steps=num_training_steps,\r)\r#初始化模型\rdevice = 'cuda' if torch.cuda.is_available() else 'cpu'\rcheckpoint = \"bert-base-chinese\"\rconfig=AutoConfig.from_pretrained(checkpoint)\rmodel=BertForPartwiseCLs.from_pretrained(checkpoint,config=config).to(device)\r#定义总损失\rtotal_loss=0\r#完成总batch\rcomplete_batch_count=0\r#最好的正确率\rbest_acc = 0.\rcurrent_directory = os.getcwd()\rfor step in range(epochs):\r#进入训练模式\rmodel.train()\rprint(f\"Epoch {step+1}/{epochs}\\n-------------------------------\")\rprogress_bar=tqdm(range(len(train_loader)))\rfor batch,(X,y) in enumerate(train_loader):\rX,y=X.to(device),y.to(device)\r#获取预测结果\rpred=model(X)\r#计算损失函数\rloss=loss_fn(pred,y)\r#清空梯度\roptimizer.zero_grad()\r#前向传播\rloss.backward();\r#更新模型参数\roptimizer.step();\r#学习率线性下降,必须是更新模型参数之后，函数根据设定的规则来调整学习率。这个调整需要基于当前的模型状态,包括参数、损失函数值等,所以要放在optimizer.step()之后。\rlr_scheduler.step()\rtotal_loss+=loss.item()\rcomplete_batch_count+=1\ravg_loss=total_loss/complete_batch_count\rprogress_bar.set_description(\"loss:{}\".format(avg_loss))\rprogress_bar.update(1)\r#使用验证集验证模型正确性。\r#进入预测模式,当前这一次epoch训练数据的正确率\rmodel.eval()\rcorrect=0\r#加载验证集的数据\rfor batch,(X,y) in enumerate(dev_loader):\r#获取预测结果\rpred=model(X)\r#因为是[[0.9,0.1],[0.3,0.4]]所以取dim=1维度上最大值的索引，概率大的索引就是预测的类别，如果和label值y相等就加起来，算个数\rcorrect += (pred.argmax(dim=1) == y).type(torch.float).sum().item()\r#正确/总数就是争取率\rvalid_acc=correct/len(dev_loader.dataset)\rprint(f\"{step+1} Accuracy: {(100*valid_acc):\u003e0.1f}%\\n\")\rif valid_acc \u003e best_acc:\rbest_acc = valid_acc\rprint('saving new weights...\\n')\rtorch.save(model.state_dict(), current_directory+f'/epoch_{step+1}_valid_acc_{(100*valid_acc):0.1f}_model_weights.bin')\rprint(\"Done!\") 模型预测 最后，我们加载验证集上最优的模型权重，汇报其在测试集上的性能。由于 AFQMC 公布的测试集上并没有标签，无法评估性能，这里我们暂且用验证集代替进行演示：\ncurrent_directory = os.getcwd()\rmodel.load_state_dict(torch.load(current_directory+'/model_weights.bin'))\rmodel.eval()\rtest_loader=DataLoader(test_data,batch_size=4,shuffle=False,collate_fn=collote_fn)\rX,y=next(iter(test_loader))\rX, y = X.to(device), y.to(device)\rpred = model(X)\rprint(pred.argmax(1) == y) 文章部分文字引用：https://transformers.run/c2/2021-12-17-transformers-note-4/",
    "description": "BERT（Bidirectional Encoder Representations from Transformers）是一种基于 Transformer 模型的预训练语言表示方法，由Google研究团队于2018年提出。BERT 通过在大规模文本语料上进行无监督的预训练，学习了通用的语言表示，并且在各种自然语言处理任务中取得了显著的性能提升。\nBERT仅使用了Transformer架构的Encoder部分。BERT自2018年由谷歌发布后，在多种NLP任务中（例如QA、文本生成、情感分析等等）都实现了更好的结果。\n“Word2vec与GloVe都有一个特点，就是它们是上下文无关（context-free）的词嵌入。所以它们没有解决：一个单词在不同上下文中代表不同的含义的问题。例如，对于单词bank，它在不同的上下文中，有银行、河畔这种差别非常大的含义。BERT的出现，解决了这个问题。\nBERT 的主要特点包括：\n双向性：BERT 使用双向 Transformer 模型来处理输入序列，从而能够同时考虑上下文的信息，而不仅仅是单向的上下文信息。这种双向性使得 BERT 能够更好地理解句子中的语义和语境。\n预训练-微调框架：BERT 使用了预训练-微调的方法。首先，在大规模文本语料上进行无监督的预训练，通过 Masked Language Model（MLM）和 Next Sentence Prediction（NSP）任务学习语言表示；然后，在特定的下游任务上微调模型参数，使其适应于特定的任务，如文本分类、命名实体识别等。\nTransformer 模型：BERT 基于 Transformer 模型结构，其中包括多层的编码器，每个编码器由自注意力机制和前馈神经网络组成。这种结构能够有效地捕获输入序列中的长距离依赖关系，有助于提高模型在各种自然语言处理任务中的性能。\n多层表示：BERT 提供了多层的语言表示，使得用户可以根据具体任务选择不同层的表示进行应用。较底层的表示通常更加接近原始输入，而较高层的表示则更加抽象，包含了更多的语义信息。\n开放源代码：BERT 的源代码和预训练模型已经在 GitHub 上开放，使得研究人员和开发者可以基于 BERT 进行进一步的研究和应用开发。\nBERT 通过预训练大规模文本语料上的通用语言表示，以及在各种下游任务上的微调，有效地提高了自然语言处理任务的性能，并且成为了当前领域内最具影响力的预训练模型之一。\ntransformer提供了不同领域中常见的机器学习模型类型：\nTEXT MODELS（文本模型）：用于处理和分析文本数据的模型，如自然语言处理（NLP）中的BERT、GPT等。\nVISION MODELS（视觉模型）：用于处理和分析图像数据的模型，如卷积神经网络（CNN）中的ResNet、VGG,Vision Transformer (ViT)等。\nAUDIO MODELS（音频模型）：用于处理和分析音频数据的模型，如声学模型、语音识别模型等。\nVIDEO MODELS（视频模型）：用于处理和分析视频数据的模型，如视频分类、目标检测、行为识别等。\nMULTIMODAL MODELS（多模态模型）：结合多种数据类型（如文本、图像、音频等）进行分析和预测的模型，如OpenAI的CLIP。\nREINFORCEMENT LEARNING MODELS（强化学习模型）：用于解决强化学习问题的模型，如Deep Q-Networks（DQN）、Actor-Critic等。\nTIME SERIES MODELS（时间序列模型）：用于分析和预测时间序列数据的模型，如循环神经网络（RNN）、长短期记忆网络（LSTM）等。\nGRAPH MODELS（图模型）：用于处理和分析图数据的模型，如图神经网络（GNN）、图卷积网络（GCN）等。\nBERT的基本原理 BERT基于的是Transformer模型，并且仅使用Transformer模型的Encoder部分。在Transformer模型中，Encoder的输入是一串序列，输出的是对序列中每个字符的表示。同样，在BERT中，输入的是一串序列，输出的是也是对应序列中每个单词的编码。 以“He got bit by Python”为例，BERT的输入输出如下图所示： 其中输入为序列“He got bit by Python”，输出的是对每个单词的编码$R_{word}$。这样在经过了BERT处理后，即得到了对每个单词包含的上下文表示$R_{word}$。",
    "tags": [],
    "title": "Transformers实战02-BERT预训练模型微调",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_02/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "简介 PEFT PEFT（Parameter-Efficient Fine-Tuning）是一个用于高效地将大型预训练模型适配到各种下游应用的库，而无需对模型的所有参数进行微调，因为这在计算上是非常昂贵的。PEFT 方法只微调少量的（额外的）模型参数——显著降低了计算和存储成本——同时其性能与完全微调的模型相当。这使得在消费者硬件上训练和存储大型语言模型（LLMs）变得更加可行。\nPEFT 集成了 Transformers、Diffusers 和 Accelerate 库，以提供更快、更简单的方法来加载、训练和使用大型模型进行推理。\nLORA方法 一种高效训练大型模型的流行方法是在注意力块中插入较小的可训练矩阵，这些矩阵是微调期间要学习的增量权重矩阵的低秩分解。预训练模型的原始权重矩阵被冻结，仅更新较小的矩阵。这减少了可训练参数的数量，降低了内存使用和训练时间，而这些在大型模型中可能非常昂贵。\n有几种不同的方法可以将权重矩阵表示为低秩分解，但最常见的方法是低秩适应（LoRA原理）。PEFT 库支持几种其他 LoRA 变体，例如低秩Hadamard积（LoHa）、低秩Kronecker积（LoKr）和自适应低秩适应（AdaLoRA）。你可以在适配器指南中了解这些方法的概念工作原理。如果你有兴趣将这些方法应用于其他任务和用例，比如语义分割、标记分类，可以看看我们的笔记本集合！\nVision Transformer (ViT) Vision Transformer（ViT）模型是由Alexey Dosovitskiy，Lucas Beyer，Alexander Kolesnikov，Dirk Weissenborn，Xiaohua Zhai，Thomas Unterthiner，Mostafa Dehghani，Matthias Minderer，Georg Heigold，Sylvain Gelly，Jakob Uszkoreit，Neil Houlsby在《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》中提出的。这是第一篇成功在ImageNet上训练Transformer编码器并获得非常好结果的论文。\n这篇论文的摘要是：\n虽然Transformer架构已经成为自然语言处理任务的事实标准，但它在计算机视觉中的应用仍然有限。在视觉领域，注意力要么与卷积网络一起应用，要么用来替换卷积网络的某些组件，同时保持其总体结构不变。我们展示了在这种对CNN的依赖不是必要的，纯Transformer直接应用于图像块序列可以在图像分类任务上表现得非常好。当在大量数据上进行预训练并转移到多个中等规模或小型图像识别基准数据集（ImageNet，CIFAR-100，VTAB等）时，Vision Transformer（ViT）与最先进的卷积网络相比取得了出色的结果，同时训练所需的计算资源大大减少。\n具体关于该模型得结构参考：https://huggingface.co/docs/transformers/model_doc/vit\nlora方法实战 图像分类微调 本指南将向你展示如何快速训练一个图像分类模型——使用低秩分解方法——来识别图像中显示的食物类别。 案例来自官网：https://huggingface.co/docs/peft/task_guides/lora_based_methods\n模型选择 google/vit-base-patch16-224-in21k 是一个基于Transformer编码器的模型（类似于BERT），在监督方式下，即ImageNet-21k上以224x224像素的分辨率预训练了大量图像。\n图像被呈现给模型作为固定大小的补丁序列（分辨率为16x16），这些补丁被线性嵌入。在序列的开头还添加了一个[CLS]标记，用于分类任务。在将序列馈送到Transformer编码器的层之前，还会添加绝对位置嵌入。\n需要注意的是，这个模型不提供任何经过微调的头部，因为这些头部被Google研究人员清零了。但是，模型包括预训练的汇聚层，可以用于下游任务（如图像分类）。\n通过预训练模型，它学习了图像的内部表示，然后可以用于提取对下游任务有用的特征：例如，如果您有一个带标签的图像数据集，可以通过在预训练编码器顶部放置一个线性层来训练标准分类器。通常将线性层放置在[CLS]标记的顶部，因为该标记的最后隐藏状态可以被视为整个图像的表示。\nfrom transformers import ViTImageProcessor, FlaxViTModel\rfrom PIL import Image\rimport requests\rurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\rimage = Image.open(requests.get(url, stream=True).raw)\rprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\rmodel = FlaxViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\rinputs = processor(images=image, return_tensors=\"np\")\routputs = model(**inputs)\rlast_hidden_states = outputs.last_hidden_state\rprint(last_hidden_states.shape) 不包含分类信息，不包含label信息\n模块 一个模型通常有很多个模块和层，模块是nn.Module是一个更高级别的组织单元，可以包含多个层、子模块或其他操作，层（Layer） 是模块的基本组成部分，用于执行特定的数学运算或神经网络中的某一步骤。\n# 自定义一个简单的模块，包含两个线性层\rclass MyModule(nn.Module):\rdef __init__(self):\rsuper(MyModule, self).__init__()\rself.layer1 = nn.Linear(10, 20)\rself.layer2 = nn.Linear(20, 5)\rdef forward(self, x):\rx = self.layer1(x)\rx = self.layer2(x)\rreturn x 其中MyModule是模块，self.layer1就是层，都可以直接运行，反向传播。 我们可以看看’google/vit-base-patch16-224-in21k’有哪些模块和层\nfrom transformers import AutoModelForImageClassification, TrainingArguments, Trainer\rmodel = AutoModelForImageClassification.from_pretrained(\r\"google/vit-base-patch16-224-in21k\",\rignore_mismatched_sizes=True,\r)\r# 打印模型的结构，查看有哪些模块\rfor name, module in model.named_modules():\rprint(name,\"=\", module) 输出，其中ViTForImageClassification就是用于图形分类的模块，如果通过AutoModelForImageClassification，加载的必然是这个模块，后面我们通过lora优化的是其中的模块的和层，输出的是最后的classifier层。\n= ViTForImageClassification(\r(vit): ViTModel(\r(embeddings): ViTEmbeddings(\r(patch_embeddings): ViTPatchEmbeddings(\r(projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(encoder): ViTEncoder(\r(layer): ModuleList(\r(0-11): 12 x ViTLayer(\r(attention): ViTSdpaAttention(\r(attention): ViTSdpaSelfAttention(\r(query): Linear(in_features=768, out_features=768, bias=True)\r(key): Linear(in_features=768, out_features=768, bias=True)\r(value): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(output): ViTSelfOutput(\r(dense): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r)\r(intermediate): ViTIntermediate(\r(dense): Linear(in_features=768, out_features=3072, bias=True)\r(intermediate_act_fn): GELUActivation()\r)\r(output): ViTOutput(\r(dense): Linear(in_features=3072, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r)\r)\r)\r(layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r)\r(classifier): Linear(in_features=768, out_features=2, bias=True)\r)\rvit = ViTModel(\r(embeddings): ViTEmbeddings(\r(patch_embeddings): ViTPatchEmbeddings(\r(projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(encoder): ViTEncoder(\r(layer): ModuleList(\r(0-11): 12 x ViTLayer(\r(attention): ViTSdpaAttention(\r(attention): ViTSdpaSelfAttention(\r(query): Linear(in_features=768, out_features=768, bias=True)\r(key): Linear(in_features=768, out_features=768, bias=True)\r(value): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(output): ViTSelfOutput(\r(dense): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r)\r(intermediate): ViTIntermediate(\r(dense): Linear(in_features=768, out_features=3072, bias=True)\r(intermediate_act_fn): GELUActivation()\r)\r(output): ViTOutput(\r(dense): Linear(in_features=3072, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r)\r)\r)\r(layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r)\rvit.embeddings = ViTEmbeddings(\r(patch_embeddings): ViTPatchEmbeddings(\r(projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r)\r(dropout): Dropout(p=0.0, inplace=False)\r)\rvit.embeddings.patch_embeddings = ViTPatchEmbeddings(\r(projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r)\r。。。\rclassifier = Linear(in_features=768, out_features=2, bias=True) google/vit-base-patch16-224 是一个在大规模图像数据集上进行监督预训练的转换器编码器模型（类似于BERT），即ImageNet-21k，分辨率为224x224像素。接下来，该模型在ImageNet上进行微调（也称为ILSVRC2012），这是一个包含100万张图像和1000个类别的数据集，分辨率也为224x224。\n图像被呈现给模型作为固定大小补丁（分辨率为16x16）的序列，这些补丁被线性嵌入。还在序列开始处添加了一个[CLS]标记，以用于分类任务。在将序列馈送到Transformer编码器的层之前，还会添加绝对位置嵌入。\n通过对模型进行预训练，它学习了图像的内部表示，然后可以用于提取对下游任务有用的特征：例如，如果您有一个带标签的图像数据集，您可以在预训练编码器之上放置一个标准分类器的线性层。通常将线性层放置在[CLS]标记的顶部，因为该标记的最后隐藏状态可以被视为整个图像的表示。\nfrom transformers import AutoImageProcessor, ViTForImageClassification\rfrom PIL import Image\rimport requests,torch\rurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\rimage = Image.open(requests.get(url, stream=True).raw)\rprocessor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')\rmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\rinputs = processor(images=image, return_tensors=\"pt\")\rprint(inputs)\rprint(inputs[\"pixel_values\"].shape)\routputs = model(**inputs)\rwith torch.no_grad():\rlogits = model(**inputs).logits\rpredicted_label = logits.argmax(-1).item()\rprint(model.config.id2label[predicted_label]) 输出：\n{'pixel_values': tensor([[[[ 0.1137, 0.1686, 0.1843, ..., -0.1922, -0.1843, -0.1843],\r[ 0.1373, 0.1686, 0.1843, ..., -0.1922, -0.1922, -0.2078],\r[ 0.1137, 0.1529, 0.1608, ..., -0.2314, -0.2235, -0.2157],\r...,\r[ 0.8353, 0.7882, 0.7333, ..., 0.7020, 0.6471, 0.6157],\r[ 0.8275, 0.7961, 0.7725, ..., 0.5843, 0.4667, 0.3961],\r[ 0.8196, 0.7569, 0.7569, ..., 0.0745, -0.0510, -0.1922]],\r[[-0.8039, -0.8118, -0.8118, ..., -0.8902, -0.8902, -0.8980],\r[-0.7882, -0.7882, -0.7882, ..., -0.8745, -0.8745, -0.8824],\r[-0.8118, -0.8039, -0.7882, ..., -0.8902, -0.8902, -0.8902],\r...,\r[-0.2706, -0.3176, -0.3647, ..., -0.4275, -0.4588, -0.4824],\r[-0.2706, -0.2941, -0.3412, ..., -0.4824, -0.5451, -0.5765],\r[-0.2784, -0.3412, -0.3490, ..., -0.7333, -0.7804, -0.8353]],\r[[-0.5451, -0.4667, -0.4824, ..., -0.7412, -0.6941, -0.7176],\r[-0.5529, -0.5137, -0.4902, ..., -0.7412, -0.7098, -0.7412],\r[-0.5216, -0.4824, -0.4667, ..., -0.7490, -0.7490, -0.7647],\r...,\r[ 0.5686, 0.5529, 0.4510, ..., 0.4431, 0.3882, 0.3255],\r[ 0.5451, 0.4902, 0.5137, ..., 0.3020, 0.2078, 0.1294],\r[ 0.5686, 0.5608, 0.5137, ..., -0.2000, -0.4275, -0.5294]]]])}\rtorch.Size([1, 3, 224, 224])\r{0: 'tench, Tinca tinca', 1: 'goldfish, Carassius auratus', 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias', 3: 'tiger shark, Galeocerdo cuvieri', 4: 'hammerhead, hammerhead shark', 5: 'electric ray, crampfish, numbfish, torpedo', 6: 'stingray', 7: 'cock', 8: 'hen', 9: 'ostrich, Struthio camelus', 10: 'brambling, Fringilla montifringilla', 11: 'goldfinch, Carduelis carduelis', 12: 'house finch, linnet, Carpodacus mexicanus', 13: 'junco, snowbird', 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea', 15: 'robin, American robin, Turdus migratorius', 16: 'bulbul', 17: 'jay', 18: 'magpie', 19: 'chickadee', 20: 'water ouzel, dipper', 21: 'kite', 22: 'bald eagle, American eagle, Haliaeetus leucocephalus', 23: 'vulture', 24: 'great grey owl, great gray owl, Strix nebulosa', 25: 'European fire salamander, Salamandra salamandra', 26: 'common newt, Triturus vulgaris', 27: 'eft', 28: 'spotted salamander, Ambystoma maculatum', 29: 'axolotl, mud puppy, Ambystoma mexicanum', 30: 'bullfrog, Rana catesbeiana', 31: 'tree frog, tree-frog', 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui', 33: 'loggerhead, loggerhead turtle, Caretta caretta', 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea', 35: 'mud turtle', 36: 'terrapin', 37: 'box turtle, box tortoise', 38: 'banded gecko', 39: 'common iguana, iguana, Iguana iguana', 40: 'American chameleon, anole, Anolis carolinensis', 41: 'whiptail, whiptail lizard', 42: 'agama', 43: 'frilled lizard, Chlamydosaurus kingi', 44: 'alligator lizard', 45: 'Gila monster, Heloderma suspectum', 46: 'green lizard, Lacerta viridis', 47: 'African chameleon, Chamaeleo chamaeleon', 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis', 49: 'African crocodile, Nile crocodile, Crocodylus niloticus', 50: 'American alligator, Alligator mississipiensis', 51: 'triceratops', 52: 'thunder snake, worm snake, Carphophis amoenus', 53: 'ringneck snake, ring-necked snake, ring snake', 54: 'hognose snake, puff adder, sand viper', 55: 'green snake, grass snake', 56: 'king snake, kingsnake', 57: 'garter snake, grass snake', 58: 'water snake', 59: 'vine snake', 60: 'night snake, Hypsiglena torquata', 61: 'boa constrictor, Constrictor constrictor', 62: 'rock python, rock snake, Python sebae', 63: 'Indian cobra, Naja naja', 64: 'green mamba', 65: 'sea snake', 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus', 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus', 68: 'sidewinder, horned rattlesnake, Crotalus cerastes', 69: 'trilobite', 70: 'harvestman, daddy longlegs, Phalangium opilio', 71: 'scorpion', 72: 'black and gold garden spider, Argiope aurantia', 73: 'barn spider, Araneus cavaticus', 74: 'garden spider, Aranea diademata', 75: 'black widow, Latrodectus mactans', 76: 'tarantula', 77: 'wolf spider, hunting spider', 78: 'tick', 79: 'centipede', 80: 'black grouse', 81: 'ptarmigan', 82: 'ruffed grouse, partridge, Bonasa umbellus', 83: 'prairie chicken, prairie grouse, prairie fowl', 84: 'peacock', 85: 'quail', 86: 'partridge', 87: 'African grey, African gray, Psittacus erithacus', 88: 'macaw', 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita', 90: 'lorikeet', 91: 'coucal', 92: 'bee eater', 93: 'hornbill', 94: 'hummingbird', 95: 'jacamar', 96: 'toucan', 97: 'drake', 98: 'red-breasted merganser, Mergus serrator', 99: 'goose', 100: 'black swan, Cygnus atratus', 101: 'tusker', 102: 'echidna, spiny anteater, anteater', 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus', 104: 'wallaby, brush kangaroo', 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus', 106: 'wombat', 107: 'jellyfish', 108: 'sea anemone, anemone', 109: 'brain coral', 110: 'flatworm, platyhelminth', 111: 'nematode, nematode worm, roundworm', 112: 'conch', 113: 'snail', 114: 'slug', 115: 'sea slug, nudibranch', 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore', 117: 'chambered nautilus, pearly nautilus, nautilus', 118: 'Dungeness crab, Cancer magister', 119: 'rock crab, Cancer irroratus', 120: 'fiddler crab', 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica', 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus', 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish', 124: 'crayfish, crawfish, crawdad, crawdaddy', 125: 'hermit crab', 126: 'isopod', 127: 'white stork, Ciconia ciconia', 128: 'black stork, Ciconia nigra', 129: 'spoonbill', 130: 'flamingo', 131: 'little blue heron, Egretta caerulea', 132: 'American egret, great white heron, Egretta albus', 133: 'bittern', 134: 'crane', 135: 'limpkin, Aramus pictus', 136: 'European gallinule, Porphyrio porphyrio', 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana', 138: 'bustard', 139: 'ruddy turnstone, Arenaria interpres', 140: 'red-backed sandpiper, dunlin, Erolia alpina', 141: 'redshank, Tringa totanus', 142: 'dowitcher', 143: 'oystercatcher, oyster catcher', 144: 'pelican', 145: 'king penguin, Aptenodytes patagonica', 146: 'albatross, mollymawk', 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus', 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca', 149: 'dugong, Dugong dugon', 150: 'sea lion', 151: 'Chihuahua', 152: 'Japanese spaniel', 153: 'Maltese dog, Maltese terrier, Maltese', 154: 'Pekinese, Pekingese, Peke', 155: 'Shih-Tzu', 156: 'Blenheim spaniel', 157: 'papillon', 158: 'toy terrier', 159: 'Rhodesian ridgeback', 160: 'Afghan hound, Afghan', 161: 'basset, basset hound', 162: 'beagle', 163: 'bloodhound, sleuthhound', 164: 'bluetick', 165: 'black-and-tan coonhound', 166: 'Walker hound, Walker foxhound', 167: 'English foxhound', 168: 'redbone', 169: 'borzoi, Russian wolfhound', 170: 'Irish wolfhound', 171: 'Italian greyhound', 172: 'whippet', 173: 'Ibizan hound, Ibizan Podenco', 174: 'Norwegian elkhound, elkhound', 175: 'otterhound, otter hound', 176: 'Saluki, gazelle hound', 177: 'Scottish deerhound, deerhound', 178: 'Weimaraner', 179: 'Staffordshire bullterrier, Staffordshire bull terrier', 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier', 181: 'Bedlington terrier', 182: 'Border terrier', 183: 'Kerry blue terrier', 184: 'Irish terrier', 185: 'Norfolk terrier', 186: 'Norwich terrier', 187: 'Yorkshire terrier', 188: 'wire-haired fox terrier', 189: 'Lakeland terrier', 190: 'Sealyham terrier, Sealyham', 191: 'Airedale, Airedale terrier', 192: 'cairn, cairn terrier', 193: 'Australian terrier', 194: 'Dandie Dinmont, Dandie Dinmont terrier', 195: 'Boston bull, Boston terrier', 196: 'miniature schnauzer', 197: 'giant schnauzer', 198: 'standard schnauzer', 199: 'Scotch terrier, Scottish terrier, Scottie', 200: 'Tibetan terrier, chrysanthemum dog', 201: 'silky terrier, Sydney silky', 202: 'soft-coated wheaten terrier', 203: 'West Highland white terrier', 204: 'Lhasa, Lhasa apso', 205: 'flat-coated retriever', 206: 'curly-coated retriever', 207: 'golden retriever', 208: 'Labrador retriever', 209: 'Chesapeake Bay retriever', 210: 'German short-haired pointer', 211: 'vizsla, Hungarian pointer', 212: 'English setter', 213: 'Irish setter, red setter', 214: 'Gordon setter', 215: 'Brittany spaniel', 216: 'clumber, clumber spaniel', 217: 'English springer, English springer spaniel', 218: 'Welsh springer spaniel', 219: 'cocker spaniel, English cocker spaniel, cocker', 220: 'Sussex spaniel', 221: 'Irish water spaniel', 222: 'kuvasz', 223: 'schipperke', 224: 'groenendael', 225: 'malinois', 226: 'briard', 227: 'kelpie', 228: 'komondor', 229: 'Old English sheepdog, bobtail', 230: 'Shetland sheepdog, Shetland sheep dog, Shetland', 231: 'collie', 232: 'Border collie', 233: 'Bouvier des Flandres, Bouviers des Flandres', 234: 'Rottweiler', 235: 'German shepherd, German shepherd dog, German police dog, alsatian', 236: 'Doberman, Doberman pinscher', 237: 'miniature pinscher', 238: 'Greater Swiss Mountain dog', 239: 'Bernese mountain dog', 240: 'Appenzeller', 241: 'EntleBucher', 242: 'boxer', 243: 'bull mastiff', 244: 'Tibetan mastiff', 245: 'French bulldog', 246: 'Great Dane', 247: 'Saint Bernard, St Bernard', 248: 'Eskimo dog, husky', 249: 'malamute, malemute, Alaskan malamute', 250: 'Siberian husky', 251: 'dalmatian, coach dog, carriage dog', 252: 'affenpinscher, monkey pinscher, monkey dog', 253: 'basenji', 254: 'pug, pug-dog', 255: 'Leonberg', 256: 'Newfoundland, Newfoundland dog', 257: 'Great Pyrenees', 258: 'Samoyed, Samoyede', 259: 'Pomeranian', 260: 'chow, chow chow', 261: 'keeshond', 262: 'Brabancon griffon', 263: 'Pembroke, Pembroke Welsh corgi', 264: 'Cardigan, Cardigan Welsh corgi', 265: 'toy poodle', 266: 'miniature poodle', 267: 'standard poodle', 268: 'Mexican hairless', 269: 'timber wolf, grey wolf, gray wolf, Canis lupus', 270: 'white wolf, Arctic wolf, Canis lupus tundrarum', 271: 'red wolf, maned wolf, Canis rufus, Canis niger', 272: 'coyote, prairie wolf, brush wolf, Canis latrans', 273: 'dingo, warrigal, warragal, Canis dingo', 274: 'dhole, Cuon alpinus', 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus', 276: 'hyena, hyaena', 277: 'red fox, Vulpes vulpes', 278: 'kit fox, Vulpes macrotis', 279: 'Arctic fox, white fox, Alopex lagopus', 280: 'grey fox, gray fox, Urocyon cinereoargenteus', 281: 'tabby, tabby cat', 282: 'tiger cat', 283: 'Persian cat', 284: 'Siamese cat, Siamese', 285: 'Egyptian cat', 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 287: 'lynx, catamount', 288: 'leopard, Panthera pardus', 289: 'snow leopard, ounce, Panthera uncia', 290: 'jaguar, panther, Panthera onca, Felis onca', 291: 'lion, king of beasts, Panthera leo', 292: 'tiger, Panthera tigris', 293: 'cheetah, chetah, Acinonyx jubatus', 294: 'brown bear, bruin, Ursus arctos', 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus', 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus', 297: 'sloth bear, Melursus ursinus, Ursus ursinus', 298: 'mongoose', 299: 'meerkat, mierkat', 300: 'tiger beetle', 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle', 302: 'ground beetle, carabid beetle', 303: 'long-horned beetle, longicorn, longicorn beetle', 304: 'leaf beetle, chrysomelid', 305: 'dung beetle', 306: 'rhinoceros beetle', 307: 'weevil', 308: 'fly', 309: 'bee', 310: 'ant, emmet, pismire', 311: 'grasshopper, hopper', 312: 'cricket', 313: 'walking stick, walkingstick, stick insect', 314: 'cockroach, roach', 315: 'mantis, mantid', 316: 'cicada, cicala', 317: 'leafhopper', 318: 'lacewing, lacewing fly', 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\", 320: 'damselfly', 321: 'admiral', 322: 'ringlet, ringlet butterfly', 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus', 324: 'cabbage butterfly', 325: 'sulphur butterfly, sulfur butterfly', 326: 'lycaenid, lycaenid butterfly', 327: 'starfish, sea star', 328: 'sea urchin', 329: 'sea cucumber, holothurian', 330: 'wood rabbit, cottontail, cottontail rabbit', 331: 'hare', 332: 'Angora, Angora rabbit', 333: 'hamster', 334: 'porcupine, hedgehog', 335: 'fox squirrel, eastern fox squirrel, Sciurus niger', 336: 'marmot', 337: 'beaver', 338: 'guinea pig, Cavia cobaya', 339: 'sorrel', 340: 'zebra', 341: 'hog, pig, grunter, squealer, Sus scrofa', 342: 'wild boar, boar, Sus scrofa', 343: 'warthog', 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius', 345: 'ox', 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis', 347: 'bison', 348: 'ram, tup', 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis', 350: 'ibex, Capra ibex', 351: 'hartebeest', 352: 'impala, Aepyceros melampus', 353: 'gazelle', 354: 'Arabian camel, dromedary, Camelus dromedarius', 355: 'llama', 356: 'weasel', 357: 'mink', 358: 'polecat, fitch, foulmart, foumart, Mustela putorius', 359: 'black-footed ferret, ferret, Mustela nigripes', 360: 'otter', 361: 'skunk, polecat, wood pussy', 362: 'badger', 363: 'armadillo', 364: 'three-toed sloth, ai, Bradypus tridactylus', 365: 'orangutan, orang, orangutang, Pongo pygmaeus', 366: 'gorilla, Gorilla gorilla', 367: 'chimpanzee, chimp, Pan troglodytes', 368: 'gibbon, Hylobates lar', 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus', 370: 'guenon, guenon monkey', 371: 'patas, hussar monkey, Erythrocebus patas', 372: 'baboon', 373: 'macaque', 374: 'langur', 375: 'colobus, colobus monkey', 376: 'proboscis monkey, Nasalis larvatus', 377: 'marmoset', 378: 'capuchin, ringtail, Cebus capucinus', 379: 'howler monkey, howler', 380: 'titi, titi monkey', 381: 'spider monkey, Ateles geoffroyi', 382: 'squirrel monkey, Saimiri sciureus', 383: 'Madagascar cat, ring-tailed lemur, Lemur catta', 384: 'indri, indris, Indri indri, Indri brevicaudatus', 385: 'Indian elephant, Elephas maximus', 386: 'African elephant, Loxodonta africana', 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens', 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca', 389: 'barracouta, snoek', 390: 'eel', 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch', 392: 'rock beauty, Holocanthus tricolor', 393: 'anemone fish', 394: 'sturgeon', 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus', 396: 'lionfish', 397: 'puffer, pufferfish, blowfish, globefish', 398: 'abacus', 399: 'abaya', 400: \"academic gown, academic robe, judge's robe\", 401: 'accordion, piano accordion, squeeze box', 402: 'acoustic guitar', 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier', 404: 'airliner', 405: 'airship, dirigible', 406: 'altar', 407: 'ambulance', 408: 'amphibian, amphibious vehicle', 409: 'analog clock', 410: 'apiary, bee house', 411: 'apron', 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin', 413: 'assault rifle, assault gun', 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack', 415: 'bakery, bakeshop, bakehouse', 416: 'balance beam, beam', 417: 'balloon', 418: 'ballpoint, ballpoint pen, ballpen, Biro', 419: 'Band Aid', 420: 'banjo', 421: 'bannister, banister, balustrade, balusters, handrail', 422: 'barbell', 423: 'barber chair', 424: 'barbershop', 425: 'barn', 426: 'barometer', 427: 'barrel, cask', 428: 'barrow, garden cart, lawn cart, wheelbarrow', 429: 'baseball', 430: 'basketball', 431: 'bassinet', 432: 'bassoon', 433: 'bathing cap, swimming cap', 434: 'bath towel', 435: 'bathtub, bathing tub, bath, tub', 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon', 437: 'beacon, lighthouse, beacon light, pharos', 438: 'beaker', 439: 'bearskin, busby, shako', 440: 'beer bottle', 441: 'beer glass', 442: 'bell cote, bell cot', 443: 'bib', 444: 'bicycle-built-for-two, tandem bicycle, tandem', 445: 'bikini, two-piece', 446: 'binder, ring-binder', 447: 'binoculars, field glasses, opera glasses', 448: 'birdhouse', 449: 'boathouse', 450: 'bobsled, bobsleigh, bob', 451: 'bolo tie, bolo, bola tie, bola', 452: 'bonnet, poke bonnet', 453: 'bookcase', 454: 'bookshop, bookstore, bookstall', 455: 'bottlecap', 456: 'bow', 457: 'bow tie, bow-tie, bowtie', 458: 'brass, memorial tablet, plaque', 459: 'brassiere, bra, bandeau', 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty', 461: 'breastplate, aegis, egis', 462: 'broom', 463: 'bucket, pail', 464: 'buckle', 465: 'bulletproof vest', 466: 'bullet train, bullet', 467: 'butcher shop, meat market', 468: 'cab, hack, taxi, taxicab', 469: 'caldron, cauldron', 470: 'candle, taper, wax light', 471: 'cannon', 472: 'canoe', 473: 'can opener, tin opener', 474: 'cardigan', 475: 'car mirror', 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig', 477: \"carpenter's kit, tool kit\", 478: 'carton', 479: 'car wheel', 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM', 481: 'cassette', 482: 'cassette player', 483: 'castle', 484: 'catamaran', 485: 'CD player', 486: 'cello, violoncello', 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone', 488: 'chain', 489: 'chainlink fence', 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour', 491: 'chain saw, chainsaw', 492: 'chest', 493: 'chiffonier, commode', 494: 'chime, bell, gong', 495: 'china cabinet, china closet', 496: 'Christmas stocking', 497: 'church, church building', 498: 'cinema, movie theater, movie theatre, movie house, picture palace', 499: 'cleaver, meat cleaver, chopper', 500: 'cliff dwelling', 501: 'cloak', 502: 'clog, geta, patten, sabot', 503: 'cocktail shaker', 504: 'coffee mug', 505: 'coffeepot', 506: 'coil, spiral, volute, whorl, helix', 507: 'combination lock', 508: 'computer keyboard, keypad', 509: 'confectionery, confectionary, candy store', 510: 'container ship, containership, container vessel', 511: 'convertible', 512: 'corkscrew, bottle screw', 513: 'cornet, horn, trumpet, trump', 514: 'cowboy boot', 515: 'cowboy hat, ten-gallon hat', 516: 'cradle', 517: 'crane', 518: 'crash helmet', 519: 'crate', 520: 'crib, cot', 521: 'Crock Pot', 522: 'croquet ball', 523: 'crutch', 524: 'cuirass', 525: 'dam, dike, dyke', 526: 'desk', 527: 'desktop computer', 528: 'dial telephone, dial phone', 529: 'diaper, nappy, napkin', 530: 'digital clock', 531: 'digital watch', 532: 'dining table, board', 533: 'dishrag, dishcloth', 534: 'dishwasher, dish washer, dishwashing machine', 535: 'disk brake, disc brake', 536: 'dock, dockage, docking facility', 537: 'dogsled, dog sled, dog sleigh', 538: 'dome', 539: 'doormat, welcome mat', 540: 'drilling platform, offshore rig', 541: 'drum, membranophone, tympan', 542: 'drumstick', 543: 'dumbbell', 544: 'Dutch oven', 545: 'electric fan, blower', 546: 'electric guitar', 547: 'electric locomotive', 548: 'entertainment center', 549: 'envelope', 550: 'espresso maker', 551: 'face powder', 552: 'feather boa, boa', 553: 'file, file cabinet, filing cabinet', 554: 'fireboat', 555: 'fire engine, fire truck', 556: 'fire screen, fireguard', 557: 'flagpole, flagstaff', 558: 'flute, transverse flute', 559: 'folding chair', 560: 'football helmet', 561: 'forklift', 562: 'fountain', 563: 'fountain pen', 564: 'four-poster', 565: 'freight car', 566: 'French horn, horn', 567: 'frying pan, frypan, skillet', 568: 'fur coat', 569: 'garbage truck, dustcart', 570: 'gasmask, respirator, gas helmet', 571: 'gas pump, gasoline pump, petrol pump, island dispenser', 572: 'goblet', 573: 'go-kart', 574: 'golf ball', 575: 'golfcart, golf cart', 576: 'gondola', 577: 'gong, tam-tam', 578: 'gown', 579: 'grand piano, grand', 580: 'greenhouse, nursery, glasshouse', 581: 'grille, radiator grille', 582: 'grocery store, grocery, food market, market', 583: 'guillotine', 584: 'hair slide', 585: 'hair spray', 586: 'half track', 587: 'hammer', 588: 'hamper', 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier', 590: 'hand-held computer, hand-held microcomputer', 591: 'handkerchief, hankie, hanky, hankey', 592: 'hard disc, hard disk, fixed disk', 593: 'harmonica, mouth organ, harp, mouth harp', 594: 'harp', 595: 'harvester, reaper', 596: 'hatchet', 597: 'holster', 598: 'home theater, home theatre', 599: 'honeycomb', 600: 'hook, claw', 601: 'hoopskirt, crinoline', 602: 'horizontal bar, high bar', 603: 'horse cart, horse-cart', 604: 'hourglass', 605: 'iPod', 606: 'iron, smoothing iron', 607: \"jack-o'-lantern\", 608: 'jean, blue jean, denim', 609: 'jeep, landrover', 610: 'jersey, T-shirt, tee shirt', 611: 'jigsaw puzzle', 612: 'jinrikisha, ricksha, rickshaw', 613: 'joystick', 614: 'kimono', 615: 'knee pad', 616: 'knot', 617: 'lab coat, laboratory coat', 618: 'ladle', 619: 'lampshade, lamp shade', 620: 'laptop, laptop computer', 621: 'lawn mower, mower', 622: 'lens cap, lens cover', 623: 'letter opener, paper knife, paperknife', 624: 'library', 625: 'lifeboat', 626: 'lighter, light, igniter, ignitor', 627: 'limousine, limo', 628: 'liner, ocean liner', 629: 'lipstick, lip rouge', 630: 'Loafer', 631: 'lotion', 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system', 633: \"loupe, jeweler's loupe\", 634: 'lumbermill, sawmill', 635: 'magnetic compass', 636: 'mailbag, postbag', 637: 'mailbox, letter box', 638: 'maillot', 639: 'maillot, tank suit', 640: 'manhole cover', 641: 'maraca', 642: 'marimba, xylophone', 643: 'mask', 644: 'matchstick', 645: 'maypole', 646: 'maze, labyrinth', 647: 'measuring cup', 648: 'medicine chest, medicine cabinet', 649: 'megalith, megalithic structure', 650: 'microphone, mike', 651: 'microwave, microwave oven', 652: 'military uniform', 653: 'milk can', 654: 'minibus', 655: 'miniskirt, mini', 656: 'minivan', 657: 'missile', 658: 'mitten', 659: 'mixing bowl', 660: 'mobile home, manufactured home', 661: 'Model T', 662: 'modem', 663: 'monastery', 664: 'monitor', 665: 'moped', 666: 'mortar', 667: 'mortarboard', 668: 'mosque', 669: 'mosquito net', 670: 'motor scooter, scooter', 671: 'mountain bike, all-terrain bike, off-roader', 672: 'mountain tent', 673: 'mouse, computer mouse', 674: 'mousetrap', 675: 'moving van', 676: 'muzzle', 677: 'nail', 678: 'neck brace', 679: 'necklace', 680: 'nipple', 681: 'notebook, notebook computer', 682: 'obelisk', 683: 'oboe, hautboy, hautbois', 684: 'ocarina, sweet potato', 685: 'odometer, hodometer, mileometer, milometer', 686: 'oil filter', 687: 'organ, pipe organ', 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO', 689: 'overskirt', 690: 'oxcart', 691: 'oxygen mask', 692: 'packet', 693: 'paddle, boat paddle', 694: 'paddlewheel, paddle wheel', 695: 'padlock', 696: 'paintbrush', 697: \"pajama, pyjama, pj's, jammies\", 698: 'palace', 699: 'panpipe, pandean pipe, syrinx', 700: 'paper towel', 701: 'parachute, chute', 702: 'parallel bars, bars', 703: 'park bench', 704: 'parking meter', 705: 'passenger car, coach, carriage', 706: 'patio, terrace', 707: 'pay-phone, pay-station', 708: 'pedestal, plinth, footstall', 709: 'pencil box, pencil case', 710: 'pencil sharpener', 711: 'perfume, essence', 712: 'Petri dish', 713: 'photocopier', 714: 'pick, plectrum, plectron', 715: 'pickelhaube', 716: 'picket fence, paling', 717: 'pickup, pickup truck', 718: 'pier', 719: 'piggy bank, penny bank', 720: 'pill bottle', 721: 'pillow', 722: 'ping-pong ball', 723: 'pinwheel', 724: 'pirate, pirate ship', 725: 'pitcher, ewer', 726: \"plane, carpenter's plane, woodworking plane\", 727: 'planetarium', 728: 'plastic bag', 729: 'plate rack', 730: 'plow, plough', 731: \"plunger, plumber's helper\", 732: 'Polaroid camera, Polaroid Land camera', 733: 'pole', 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria', 735: 'poncho', 736: 'pool table, billiard table, snooker table', 737: 'pop bottle, soda bottle', 738: 'pot, flowerpot', 739: \"potter's wheel\", 740: 'power drill', 741: 'prayer rug, prayer mat', 742: 'printer', 743: 'prison, prison house', 744: 'projectile, missile', 745: 'projector', 746: 'puck, hockey puck', 747: 'punching bag, punch bag, punching ball, punchball', 748: 'purse', 749: 'quill, quill pen', 750: 'quilt, comforter, comfort, puff', 751: 'racer, race car, racing car', 752: 'racket, racquet', 753: 'radiator', 754: 'radio, wireless', 755: 'radio telescope, radio reflector', 756: 'rain barrel', 757: 'recreational vehicle, RV, R.V.', 758: 'reel', 759: 'reflex camera', 760: 'refrigerator, icebox', 761: 'remote control, remote', 762: 'restaurant, eating house, eating place, eatery', 763: 'revolver, six-gun, six-shooter', 764: 'rifle', 765: 'rocking chair, rocker', 766: 'rotisserie', 767: 'rubber eraser, rubber, pencil eraser', 768: 'rugby ball', 769: 'rule, ruler', 770: 'running shoe', 771: 'safe', 772: 'safety pin', 773: 'saltshaker, salt shaker', 774: 'sandal', 775: 'sarong', 776: 'sax, saxophone', 777: 'scabbard', 778: 'scale, weighing machine', 779: 'school bus', 780: 'schooner', 781: 'scoreboard', 782: 'screen, CRT screen', 783: 'screw', 784: 'screwdriver', 785: 'seat belt, seatbelt', 786: 'sewing machine', 787: 'shield, buckler', 788: 'shoe shop, shoe-shop, shoe store', 789: 'shoji', 790: 'shopping basket', 791: 'shopping cart', 792: 'shovel', 793: 'shower cap', 794: 'shower curtain', 795: 'ski', 796: 'ski mask', 797: 'sleeping bag', 798: 'slide rule, slipstick', 799: 'sliding door', 800: 'slot, one-armed bandit', 801: 'snorkel', 802: 'snowmobile', 803: 'snowplow, snowplough', 804: 'soap dispenser', 805: 'soccer ball', 806: 'sock', 807: 'solar dish, solar collector, solar furnace', 808: 'sombrero', 809: 'soup bowl', 810: 'space bar', 811: 'space heater', 812: 'space shuttle', 813: 'spatula', 814: 'speedboat', 815: \"spider web, spider's web\", 816: 'spindle', 817: 'sports car, sport car', 818: 'spotlight, spot', 819: 'stage', 820: 'steam locomotive', 821: 'steel arch bridge', 822: 'steel drum', 823: 'stethoscope', 824: 'stole', 825: 'stone wall', 826: 'stopwatch, stop watch', 827: 'stove', 828: 'strainer', 829: 'streetcar, tram, tramcar, trolley, trolley car', 830: 'stretcher', 831: 'studio couch, day bed', 832: 'stupa, tope', 833: 'submarine, pigboat, sub, U-boat', 834: 'suit, suit of clothes', 835: 'sundial', 836: 'sunglass', 837: 'sunglasses, dark glasses, shades', 838: 'sunscreen, sunblock, sun blocker', 839: 'suspension bridge', 840: 'swab, swob, mop', 841: 'sweatshirt', 842: 'swimming trunks, bathing trunks', 843: 'swing', 844: 'switch, electric switch, electrical switch', 845: 'syringe', 846: 'table lamp', 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle', 848: 'tape player', 849: 'teapot', 850: 'teddy, teddy bear', 851: 'television, television system', 852: 'tennis ball', 853: 'thatch, thatched roof', 854: 'theater curtain, theatre curtain', 855: 'thimble', 856: 'thresher, thrasher, threshing machine', 857: 'throne', 858: 'tile roof', 859: 'toaster', 860: 'tobacco shop, tobacconist shop, tobacconist', 861: 'toilet seat', 862: 'torch', 863: 'totem pole', 864: 'tow truck, tow car, wrecker', 865: 'toyshop', 866: 'tractor', 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi', 868: 'tray', 869: 'trench coat', 870: 'tricycle, trike, velocipede', 871: 'trimaran', 872: 'tripod', 873: 'triumphal arch', 874: 'trolleybus, trolley coach, trackless trolley', 875: 'trombone', 876: 'tub, vat', 877: 'turnstile', 878: 'typewriter keyboard', 879: 'umbrella', 880: 'unicycle, monocycle', 881: 'upright, upright piano', 882: 'vacuum, vacuum cleaner', 883: 'vase', 884: 'vault', 885: 'velvet', 886: 'vending machine', 887: 'vestment', 888: 'viaduct', 889: 'violin, fiddle', 890: 'volleyball', 891: 'waffle iron', 892: 'wall clock', 893: 'wallet, billfold, notecase, pocketbook', 894: 'wardrobe, closet, press', 895: 'warplane, military plane', 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin', 897: 'washer, automatic washer, washing machine', 898: 'water bottle', 899: 'water jug', 900: 'water tower', 901: 'whiskey jug', 902: 'whistle', 903: 'wig', 904: 'window screen', 905: 'window shade', 906: 'Windsor tie', 907: 'wine bottle', 908: 'wing', 909: 'wok', 910: 'wooden spoon', 911: 'wool, woolen, woollen', 912: 'worm fence, snake fence, snake-rail fence, Virginia fence', 913: 'wreck', 914: 'yawl', 915: 'yurt', 916: 'web site, website, internet site, site', 917: 'comic book', 918: 'crossword puzzle, crossword', 919: 'street sign', 920: 'traffic light, traffic signal, stoplight', 921: 'book jacket, dust cover, dust jacket, dust wrapper', 922: 'menu', 923: 'plate', 924: 'guacamole', 925: 'consomme', 926: 'hot pot, hotpot', 927: 'trifle', 928: 'ice cream, icecream', 929: 'ice lolly, lolly, lollipop, popsicle', 930: 'French loaf', 931: 'bagel, beigel', 932: 'pretzel', 933: 'cheeseburger', 934: 'hotdog, hot dog, red hot', 935: 'mashed potato', 936: 'head cabbage', 937: 'broccoli', 938: 'cauliflower', 939: 'zucchini, courgette', 940: 'spaghetti squash', 941: 'acorn squash', 942: 'butternut squash', 943: 'cucumber, cuke', 944: 'artichoke, globe artichoke', 945: 'bell pepper', 946: 'cardoon', 947: 'mushroom', 948: 'Granny Smith', 949: 'strawberry', 950: 'orange', 951: 'lemon', 952: 'fig', 953: 'pineapple, ananas', 954: 'banana', 955: 'jackfruit, jak, jack', 956: 'custard apple', 957: 'pomegranate', 958: 'hay', 959: 'carbonara', 960: 'chocolate sauce, chocolate syrup', 961: 'dough', 962: 'meat loaf, meatloaf', 963: 'pizza, pizza pie', 964: 'potpie', 965: 'burrito', 966: 'red wine', 967: 'espresso', 968: 'cup', 969: 'eggnog', 970: 'alp', 971: 'bubble', 972: 'cliff, drop, drop-off', 973: 'coral reef', 974: 'geyser', 975: 'lakeside, lakeshore', 976: 'promontory, headland, head, foreland', 977: 'sandbar, sand bar', 978: 'seashore, coast, seacoast, sea-coast', 979: 'valley, vale', 980: 'volcano', 981: 'ballplayer, baseball player', 982: 'groom, bridegroom', 983: 'scuba diver', 984: 'rapeseed', 985: 'daisy', 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\", 987: 'corn', 988: 'acorn', 989: 'hip, rose hip, rosehip', 990: 'buckeye, horse chestnut, conker', 991: 'coral fungus', 992: 'agaric', 993: 'gyromitra', 994: 'stinkhorn, carrion fungus', 995: 'earthstar', 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa', 997: 'bolete', 998: 'ear, spike, capitulum', 999: 'toilet tissue, toilet paper, bathroom tissue'}\rEgyptian cat 经过转码后得json结构是一个key=pixel_values的像素数组，维度是：[批次，通道数，宽度，高度]。 通过model.config.id2label可以看到总共1000个label。\n数据集 food101包含多种食物类别，数据集地址：https://huggingface.co/datasets/ethz/food101。\nfrom datasets import load_dataset\rds = load_dataset(\"food101\")\rprint(\"数据集\",ds)\r#获取训练集数据\rds = load_dataset(\"food101\",split=\"train\")\rprint(\"训练集\",ds)\rprint(\"第一个数据集\",ds[0])\r#获取所有label\rlabels = ds.features[\"label\"].names\rprint(labels)\rprint(len(labels)) 输出\n数据集 DatasetDict({\rtrain: Dataset({\rfeatures: ['image', 'label'],\rnum_rows: 75750\r})\rvalidation: Dataset({\rfeatures: ['image', 'label'],\rnum_rows: 25250\r})\r})\r训练集 Dataset({\rfeatures: ['image', 'label'],\rnum_rows: 75750\r})\r第一个数据集 {'image': \u003cPIL.Image.Image image mode=RGB size=384x512 at 0x7A1FCE415750\u003e, 'label': 6}\r['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\r101 总共101个food总类。 显示第二张图片和label\nimport matplotlib.pyplot as plt\rplt.imshow(ds[1][\"image\"])\rplt.axis('off') # 关闭坐标轴\rplt.show()\rprint(labels[ds[1][\"label\"]]) 显示 这里我们看到第二个数据集的label=6,也就是beignets。\n我们需要生成label和id关系的字典。\nlabels = ds[\"train\"].features[\"label\"].names\rlabel2id, id2label = dict(), dict()\rfor i, label in enumerate(labels):\rlabel2id[label] = i\rid2label[i] = label\rprint(id2label)\rprint(label2id) 输出：\n{0: 'apple_pie', 1: 'baby_back_ribs', 2: 'baklava', 3: 'beef_carpaccio', 4: 'beef_tartare', 5: 'beet_salad', 6: 'beignets', 7: 'bibimbap', 8: 'bread_pudding', 9: 'breakfast_burrito', 10: 'bruschetta', 11: 'caesar_salad', 12: 'cannoli', 13: 'caprese_salad', 14: 'carrot_cake', 15: 'ceviche', 16: 'cheesecake', 17: 'cheese_plate', 18: 'chicken_curry', 19: 'chicken_quesadilla', 20: 'chicken_wings', 21: 'chocolate_cake', 22: 'chocolate_mousse', 23: 'churros', 24: 'clam_chowder', 25: 'club_sandwich', 26: 'crab_cakes', 27: 'creme_brulee', 28: 'croque_madame', 29: 'cup_cakes', 30: 'deviled_eggs', 31: 'donuts', 32: 'dumplings', 33: 'edamame', 34: 'eggs_benedict', 35: 'escargots', 36: 'falafel', 37: 'filet_mignon', 38: 'fish_and_chips', 39: 'foie_gras', 40: 'french_fries', 41: 'french_onion_soup', 42: 'french_toast', 43: 'fried_calamari', 44: 'fried_rice', 45: 'frozen_yogurt', 46: 'garlic_bread', 47: 'gnocchi', 48: 'greek_salad', 49: 'grilled_cheese_sandwich', 50: 'grilled_salmon', 51: 'guacamole', 52: 'gyoza', 53: 'hamburger', 54: 'hot_and_sour_soup', 55: 'hot_dog', 56: 'huevos_rancheros', 57: 'hummus', 58: 'ice_cream', 59: 'lasagna', 60: 'lobster_bisque', 61: 'lobster_roll_sandwich', 62: 'macaroni_and_cheese', 63: 'macarons', 64: 'miso_soup', 65: 'mussels', 66: 'nachos', 67: 'omelette', 68: 'onion_rings', 69: 'oysters', 70: 'pad_thai', 71: 'paella', 72: 'pancakes', 73: 'panna_cotta', 74: 'peking_duck', 75: 'pho', 76: 'pizza', 77: 'pork_chop', 78: 'poutine', 79: 'prime_rib', 80: 'pulled_pork_sandwich', 81: 'ramen', 82: 'ravioli', 83: 'red_velvet_cake', 84: 'risotto', 85: 'samosa', 86: 'sashimi', 87: 'scallops', 88: 'seaweed_salad', 89: 'shrimp_and_grits', 90: 'spaghetti_bolognese', 91: 'spaghetti_carbonara', 92: 'spring_rolls', 93: 'steak', 94: 'strawberry_shortcake', 95: 'sushi', 96: 'tacos', 97: 'takoyaki', 98: 'tiramisu', 99: 'tuna_tartare', 100: 'waffles'}\r{'apple_pie': 0, 'baby_back_ribs': 1, 'baklava': 2, 'beef_carpaccio': 3, 'beef_tartare': 4, 'beet_salad': 5, 'beignets': 6, 'bibimbap': 7, 'bread_pudding': 8, 'breakfast_burrito': 9, 'bruschetta': 10, 'caesar_salad': 11, 'cannoli': 12, 'caprese_salad': 13, 'carrot_cake': 14, 'ceviche': 15, 'cheesecake': 16, 'cheese_plate': 17, 'chicken_curry': 18, 'chicken_quesadilla': 19, 'chicken_wings': 20, 'chocolate_cake': 21, 'chocolate_mousse': 22, 'churros': 23, 'clam_chowder': 24, 'club_sandwich': 25, 'crab_cakes': 26, 'creme_brulee': 27, 'croque_madame': 28, 'cup_cakes': 29, 'deviled_eggs': 30, 'donuts': 31, 'dumplings': 32, 'edamame': 33, 'eggs_benedict': 34, 'escargots': 35, 'falafel': 36, 'filet_mignon': 37, 'fish_and_chips': 38, 'foie_gras': 39, 'french_fries': 40, 'french_onion_soup': 41, 'french_toast': 42, 'fried_calamari': 43, 'fried_rice': 44, 'frozen_yogurt': 45, 'garlic_bread': 46, 'gnocchi': 47, 'greek_salad': 48, 'grilled_cheese_sandwich': 49, 'grilled_salmon': 50, 'guacamole': 51, 'gyoza': 52, 'hamburger': 53, 'hot_and_sour_soup': 54, 'hot_dog': 55, 'huevos_rancheros': 56, 'hummus': 57, 'ice_cream': 58, 'lasagna': 59, 'lobster_bisque': 60, 'lobster_roll_sandwich': 61, 'macaroni_and_cheese': 62, 'macarons': 63, 'miso_soup': 64, 'mussels': 65, 'nachos': 66, 'omelette': 67, 'onion_rings': 68, 'oysters': 69, 'pad_thai': 70, 'paella': 71, 'pancakes': 72, 'panna_cotta': 73, 'peking_duck': 74, 'pho': 75, 'pizza': 76, 'pork_chop': 77, 'poutine': 78, 'prime_rib': 79, 'pulled_pork_sandwich': 80, 'ramen': 81, 'ravioli': 82, 'red_velvet_cake': 83, 'risotto': 84, 'samosa': 85, 'sashimi': 86, 'scallops': 87, 'seaweed_salad': 88, 'shrimp_and_grits': 89, 'spaghetti_bolognese': 90, 'spaghetti_carbonara': 91, 'spring_rolls': 92, 'steak': 93, 'strawberry_shortcake': 94, 'sushi': 95, 'tacos': 96, 'takoyaki': 97, 'tiramisu': 98, 'tuna_tartare': 99, 'waffles': 100} 加载一个图像处理器，以正确调整大小并对训练和评估图像的像素值进行归一化。\nfrom transformers import AutoImageProcessor\rimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\") 您还可以使用图像处理器来准备一些转换函数，用于数据增强和像素缩放。\nfrom torchvision.transforms import (\rCenterCrop,\rCompose,\rNormalize,\rRandomHorizontalFlip,\rRandomResizedCrop,\rResize,\rToTensor,\r)\rnormalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\rtrain_transforms = Compose(\r[\rRandomResizedCrop(image_processor.size[\"height\"]),\rRandomHorizontalFlip(),\rToTensor(),\rnormalize,\r]\r)\rval_transforms = Compose(\r[\rResize(image_processor.size[\"height\"]),\rCenterCrop(image_processor.size[\"height\"]),\rToTensor(),\rnormalize,\r]\r)\rdef preprocess_train(example_batch):\rexample_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\rreturn example_batch\rdef preprocess_val(example_batch):\rexample_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\rreturn example_batch 定义训练和验证数据集，并使用set_transform函数在运行时应用转换。\ntrain_ds = ds[\"train\"]\rval_ds = ds[\"validation\"]\rtrain_ds.set_transform(preprocess_train)\rval_ds.set_transform(preprocess_val) 最后，您需要一个数据整理器来创建训练和评估数据的批次，并将标签转换为torch.tensor对象。\nimport torch\rdef collate_fn(examples):\rpixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\rlabels = torch.tensor([example[\"label\"] for example in examples])\rreturn {\"pixel_values\": pixel_values, \"labels\": labels} 模型 现在让我们加载一个预训练模型作为基础模型。本指南使用了google/vit-base-patch16-224-in21k模型，但您可以使用任何您想要的图像分类模型。将label2id和id2label字典传递给模型，以便它知道如何将整数标签映射到它们的类标签，并且如果您正在微调已经微调过的检查点，可以选择传递ignore_mismatched_sizes=True参数。\nfrom transformers import AutoModelForImageClassification, TrainingArguments, Trainer\rmodel = AutoModelForImageClassification.from_pretrained(\r\"google/vit-base-patch16-224-in21k\",\rlabel2id=label2id,\rid2label=id2label,\rignore_mismatched_sizes=True,\r) PEFT configuration and model 每个 PEFT 方法都需要一个配置，其中包含了指定 PEFT 方法应该如何应用的所有参数。一旦配置设置好了，就将其传递给 get_peft_model() 函数，同时还要传递基础模型，以创建一个可训练的 PeftModel。\n调用 print_trainable_parameters() 方法来比较 PeftModel 的参数数量与基础模型的参数数量！\nLoRA将权重更新矩阵分解为两个较小的矩阵。这些低秩矩阵的大小由其秩或r确定。更高的秩意味着模型有更多的参数需要训练，但也意味着模型有更大的学习能力。您还需要指定 target_modules，确定较小矩阵插入的位置。对于本指南，您将针对注意力块的查询和值矩阵进行目标指定。设置的其他重要参数包括 lora_alpha（缩放因子）、bias（是否应该训练none、all或只有 LoRA 偏置参数）、modules_to_save（除了 LoRA 层之外需要训练和保存的模块）。所有这些参数 - 以及更多 - 都可以在 LoraConfig 中找到。\nfrom peft import LoraConfig, get_peft_model\rconfig = LoraConfig(\rr=16,\rlora_alpha=16,\rtarget_modules=[\"query\", \"value\"],\rlora_dropout=0.1,\rbias=\"none\",\rmodules_to_save=[\"classifier\"],\r)\rmodel = get_peft_model(model, config)\rmodel.print_trainable_parameters() 输出：“trainable params: 667,493 || all params: 86,543,818 || trainable%: 0.7712775047664294”\n在LoRA中，为了简化和精简，可能只针对查询和值矩阵进行权重分解，而不对键矩阵进行处理。这样可以在一定程度上减少计算量和参数数量，同时仍然提高模型的学习能力和灵活性。\n参数说明：\ntask_type：指定任务类型。如：条件生成任务（SEQ_2_SEQ_LM），因果语言建模（CAUSAL_LM）等。 inference_mode：是否在推理模式下使用Peft模型。 r： LoRA低秩矩阵的维数。关于秩的选择，通常，使用4，8，16即可。 lora_alpha： LoRA低秩矩阵的缩放系数，为一个常数超参，调整alpha与调整学习率类似。 lora_dropout：LoRA 层的丢弃（dropout）率，取值范围为[0, 1)。 target_modules：要替换为 LoRA 的模块名称列表或模块名称的正则表达式。针对不同类型的模型，模块名称不一样，因此，我们需要根据具体的模型进行设置，比如，LLaMa的默认模块名为[q_proj, v_proj]，我们也可以自行指定为：[q_proj,k_proj,v_proj,o_proj]。 在 PEFT 中支持的模型默认的模块名如下所示： TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\r\"t5\": [\"q\", \"v\"],\r\"mt5\": [\"q\", \"v\"],\r\"bart\": [\"q_proj\", \"v_proj\"],\r\"gpt2\": [\"c_attn\"],\r\"bloom\": [\"query_key_value\"],\r\"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"],\r\"opt\": [\"q_proj\", \"v_proj\"],\r\"gptj\": [\"q_proj\", \"v_proj\"],\r\"gpt_neox\": [\"query_key_value\"],\r\"gpt_neo\": [\"q_proj\", \"v_proj\"],\r\"bert\": [\"query\", \"value\"],\r\"roberta\": [\"query\", \"value\"],\r\"xlm-roberta\": [\"query\", \"value\"],\r\"electra\": [\"query\", \"value\"],\r\"deberta-v2\": [\"query_proj\", \"value_proj\"],\r\"deberta\": [\"in_proj\"],\r\"layoutlm\": [\"query\", \"value\"],\r\"llama\": [\"q_proj\", \"v_proj\"],\r\"chatglm\": [\"query_key_value\"],\r\"gpt_bigcode\": [\"c_attn\"],\r\"mpt\": [\"Wqkv\"],\r} 训练 对于训练，让我们使用Transformers中的Trainer类。Trainer类包含一个PyTorch训练循环，当您准备好时，调用train开始训练。要自定义训练运行，请在TrainingArguments类中配置训练超参数。对于类似LoRA的方法，您可以承受更高的批量大小和学习率。\nbatch_size = 128\rargs = TrainingArguments(\r#peft_model_id,\routput_dir=\"/kaggle/working\",\rremove_unused_columns=False,\revaluation_strategy=\"epoch\",\rsave_strategy=\"epoch\",\rlearning_rate=5e-3,\rreport_to=\"none\",\rper_device_train_batch_size=batch_size,\rgradient_accumulation_steps=4,\rper_device_eval_batch_size=batch_size,\rfp16=True,\rnum_train_epochs=5,\rlogging_steps=10,\rload_best_model_at_end=True,\rlabel_names=[\"labels\"],\r) 这里是对TrainingArguments中参数的解释：\noutput_dir: 指定训练过程中输出模型和日志的目录。 remove_unused_columns: 控制是否在训练过程中删除未使用的列。 evaluation_strategy: 指定评估策略，这里设置为“epoch”表示在每个epoch结束时进行评估。 save_strategy: 指定模型保存策略，这里设置为“epoch”表示在每个epoch结束时保存模型。 learning_rate: 学习率设置为5e-3，即0.005。 report_to: 控制训练过程中的报告输出，这里设置为“none”表示不输出任何报告。 per_device_train_batch_size: 每个设备上的训练批量大小。 gradient_accumulation_steps: 梯度累积步数。 per_device_eval_batch_size: 每个设备上的评估批量大小。 fp16: 控制是否使用混合精度训练。 num_train_epochs: 训练的总epoch数。 logging_steps: 控制日志输出的步数。 load_best_model_at_end: 在训练结束时是否加载最佳模型。 label_names: 标签的名称列表。 这些参数是用来配置训练过程的，例如指定训练和评估的批量大小、学习率、训练时长等等。 开始训练\ntrainer = Trainer(\rmodel,\rargs,\rtrain_dataset=train_ds,\reval_dataset=val_ds,\rtokenizer=image_processor,\rdata_collator=collate_fn,\r)\rtrainer.train() 使用kaggle的免费gpu T4*2(双倍时间消耗累计)，gpu基本100%，gpu是一周30hhrs免费时间的,我为了节省时间，用2epoch，batch_size=128,因为kaggle的session有效期在12hours内，越快越好，否则session断开就白训练了，简单看下效果，大概1个小时左右，也可以save session让他在后台跑。 看下速度 第一次epoch完成 查看输出 点击后面的三个点下载所有的文件，然后将模型下载下来，点击输入的上传-new model 输入model名称，选择私有，点击create model 输入平台：transformer，点击addnewvariation 定义附件名称，选择协议，点击右下侧的create 关闭后点击return to notebook，就可以看到输入的模型了，点击右侧的复制路径即可。 这里input的是持久的不会丢失，output数据再页面关闭session关闭后就丢失，所以尽快保存下来，或者上传到huggingface。\n预测 切换到p100(按分钟算，省钱)验证 代码\nmodel_name=\"/kaggle/input/image-classifity/transformers/version1/1/checkpoint-74\" #复制输入的路径\rfrom peft import PeftConfig, PeftModel\rfrom transformers import AutoImageProcessor, AutoModelForImageClassification\rfrom PIL import Image\rimport requests,torch\rfrom datasets import load_dataset\rds = load_dataset(\"food101\")\rlabels = ds[\"train\"].features[\"label\"].names\rlabel2id, id2label = dict(), dict()\rfor i, label in enumerate(labels):\rlabel2id[label] = i\rid2label[i] = label\rconfig = PeftConfig.from_pretrained(model_name)\rmodel = AutoModelForImageClassification.from_pretrained(\rconfig.base_model_name_or_path,#google/vit-base-patch16-224-in21k\rlabel2id=label2id,\rid2label=id2label,\rignore_mismatched_sizes=True,\r)\rmodel = PeftModel.from_pretrained(model, model_name)\rurl = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg\"\rimage = Image.open(requests.get(url, stream=True).raw)\rprint(image)\rimage_processor = AutoImageProcessor.from_pretrained(config.base_model_name_or_path)\rencoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")\rwith torch.no_grad():\routputs = model(**encoding)\rlogits = outputs.logits\rpredicted_class_idx = logits.argmax(-1).item()\rprint(\"Predicted class:\", model.config.id2label[predicted_class_idx]) 输出：beignets",
    "description": "简介 PEFT PEFT（Parameter-Efficient Fine-Tuning）是一个用于高效地将大型预训练模型适配到各种下游应用的库，而无需对模型的所有参数进行微调，因为这在计算上是非常昂贵的。PEFT 方法只微调少量的（额外的）模型参数——显著降低了计算和存储成本——同时其性能与完全微调的模型相当。这使得在消费者硬件上训练和存储大型语言模型（LLMs）变得更加可行。\nPEFT 集成了 Transformers、Diffusers 和 Accelerate 库，以提供更快、更简单的方法来加载、训练和使用大型模型进行推理。\nLORA方法 一种高效训练大型模型的流行方法是在注意力块中插入较小的可训练矩阵，这些矩阵是微调期间要学习的增量权重矩阵的低秩分解。预训练模型的原始权重矩阵被冻结，仅更新较小的矩阵。这减少了可训练参数的数量，降低了内存使用和训练时间，而这些在大型模型中可能非常昂贵。\n有几种不同的方法可以将权重矩阵表示为低秩分解，但最常见的方法是低秩适应（LoRA原理）。PEFT 库支持几种其他 LoRA 变体，例如低秩Hadamard积（LoHa）、低秩Kronecker积（LoKr）和自适应低秩适应（AdaLoRA）。你可以在适配器指南中了解这些方法的概念工作原理。如果你有兴趣将这些方法应用于其他任务和用例，比如语义分割、标记分类，可以看看我们的笔记本集合！\nVision Transformer (ViT) Vision Transformer（ViT）模型是由Alexey Dosovitskiy，Lucas Beyer，Alexander Kolesnikov，Dirk Weissenborn，Xiaohua Zhai，Thomas Unterthiner，Mostafa Dehghani，Matthias Minderer，Georg Heigold，Sylvain Gelly，Jakob Uszkoreit，Neil Houlsby在《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》中提出的。这是第一篇成功在ImageNet上训练Transformer编码器并获得非常好结果的论文。\n这篇论文的摘要是：\n虽然Transformer架构已经成为自然语言处理任务的事实标准，但它在计算机视觉中的应用仍然有限。在视觉领域，注意力要么与卷积网络一起应用，要么用来替换卷积网络的某些组件，同时保持其总体结构不变。我们展示了在这种对CNN的依赖不是必要的，纯Transformer直接应用于图像块序列可以在图像分类任务上表现得非常好。当在大量数据上进行预训练并转移到多个中等规模或小型图像识别基准数据集（ImageNet，CIFAR-100，VTAB等）时，Vision Transformer（ViT）与最先进的卷积网络相比取得了出色的结果，同时训练所需的计算资源大大减少。\n具体关于该模型得结构参考：https://huggingface.co/docs/transformers/model_doc/vit\nlora方法实战 图像分类微调 本指南将向你展示如何快速训练一个图像分类模型——使用低秩分解方法——来识别图像中显示的食物类别。 案例来自官网：https://huggingface.co/docs/peft/task_guides/lora_based_methods\n模型选择 google/vit-base-patch16-224-in21k 是一个基于Transformer编码器的模型（类似于BERT），在监督方式下，即ImageNet-21k上以224x224像素的分辨率预训练了大量图像。\n图像被呈现给模型作为固定大小的补丁序列（分辨率为16x16），这些补丁被线性嵌入。在序列的开头还添加了一个[CLS]标记，用于分类任务。在将序列馈送到Transformer编码器的层之前，还会添加绝对位置嵌入。\n需要注意的是，这个模型不提供任何经过微调的头部，因为这些头部被Google研究人员清零了。但是，模型包括预训练的汇聚层，可以用于下游任务（如图像分类）。\n通过预训练模型，它学习了图像的内部表示，然后可以用于提取对下游任务有用的特征：例如，如果您有一个带标签的图像数据集，可以通过在预训练编码器顶部放置一个线性层来训练标准分类器。通常将线性层放置在[CLS]标记的顶部，因为该标记的最后隐藏状态可以被视为整个图像的表示。\nfrom transformers import ViTImageProcessor, FlaxViTModel\rfrom PIL import Image\rimport requests\rurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\rimage = Image.open(requests.get(url, stream=True).raw)\rprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\rmodel = FlaxViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\rinputs = processor(images=image, return_tensors=\"np\")\routputs = model(**inputs)\rlast_hidden_states = outputs.last_hidden_state\rprint(last_hidden_states.shape) 不包含分类信息，不包含label信息",
    "tags": [],
    "title": "Transformers实战03-PEFT库使用LORA方法微调VIT图像分类。",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_03/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "简介 GPT-2（Generative Pre-trained Transformer 2）是由OpenAI开发的一种基于Transformer架构的自然语言处理模型。以下是关于GPT-2的一些关键特点和信息：\nTransformer架构：GPT-2基于Transformer模型架构，这是一种使用自注意力机制来捕捉输入序列中词语之间依赖关系的深度学习模型。\n预训练：GPT-2是一个预训练的语言模型，意味着它在大规模的文本数据上进行了预训练，以学习文本数据的统计特性和语言模式。\n无监督学习：在预训练过程中，GPT-2采用了无监督学习的方式，即模型仅仅通过文本数据本身来学习，而没有使用人工标注的标签或监督信号。\n生成式任务：GPT-2被设计用于生成式任务，如文本生成、对话生成和摘要生成等。它可以根据给定的上下文生成连贯的文本，并且在语言理解和生成方面表现出色。\n多层次架构：GPT-2具有多层的Transformer编码器，其中包含数百万个参数，使得模型能够捕获复杂的语言结构和语义关系。\n大小变种：GPT-2有多个大小的变种，从117M到1.5B个参数不等，每个变种都具有不同的性能和资源要求。更大的模型往往在生成更加准确和流畅的文本方面表现更好，但同时也需要更多的计算资源。\n开放许可：GPT-2是在OpenAI的研究下开发的，其模型和相关资源以开放许可的形式发布，使得研究人员和开发者可以自由地使用和构建基于GPT-2的应用。\n总的来说，GPT-2是一种强大的语言模型，具有广泛的应用潜力，可用于自然语言生成、理解、翻译等各种NLP任务。\n案例 该案例来源huggingface学习中心nlp-course，Training a causal language model from scratch 文章\n描述 我们将构建一个缩减版的代码生成模型：我们将专注于一行补全，而不是完整的函数或类，使用Python代码的一个子集。在Python中处理数据时，您会频繁接触到Python数据科学栈，包括matplotlib、seaborn、pandas和scikit-learn库。在使用这些框架时，经常需要查找特定的命令，因此如果我们可以使用一个模型来为我们完成这些调用，那将是很好的。\n收集数据 我们使用huggingface收集得content包含：“pandas”, “sklearn”, “matplotlib”, “seaborn” 这些关键字python代码 这个数据集是从github公共仓库爬取，比如 from datasets import load_dataset, DatasetDict\rds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\rds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\rraw_datasets = DatasetDict(\r{\r\"train\": ds_train, # .shuffle().select(range(50000)),\r\"valid\": ds_valid, # .shuffle().select(range(500))\r}\r) 让我们看一个数据集中的例子。我们只需显示每个字段的前200个字符：\nfor key in raw_datasets[\"train\"][0]:\rprint(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\") 输出\n'REPO_NAME: kmike/scikit-learn'\r'PATH: sklearn/utils/__init__.py'\r'COPIES: 3'\r'SIZE: 10094'\r'''CONTENT: \"\"\"\rThe :mod:`sklearn.utils` module includes various utilites.\r\"\"\"\rfrom collections import Sequence\rimport numpy as np\rfrom scipy.sparse import issparse\rimport warnings\rfrom .murmurhash import murm\rLICENSE: bsd-3-clause''' 数据集处理 首先要对数据进行标记化，这样我们才能用它进行训练。由于我们的目标主要是自动补全短函数调用，所以我们可以保持上下文大小相对较小。这样做的好处是我们可以更快地训练模型，并且需要的内存量明显较少。如果你的应用程序需要更多的上下文（例如，如果你希望模型能够基于包含函数定义的文件编写单元测试），请确保增加该数字，但也要记住这会增加GPU的内存占用。目前，让我们将上下文大小固定为128个标记，而不是 GPT-2 或 GPT-3 中分别使用的 1,024 或 2,048。\n回顾预处理 input_ids和attention_mask： input_ids是tokenizer处理后得到的输入特征，它将文本转换为模型能够处理的数字序列。每个单词或者标记（token）都会被映射成对应的唯一整数。这些整数序列就是模型的实际输入。 示例：假设原始文本经过tokenizer处理后，生成的input_ids可能是一个整数序列，如[101, 2023, 2003, 1037, 2814, 2242, 102]，每个整数对应一个token。\nattention_mask用于告诉模型哪些部分是真实的输入，哪些部分是填充（padding）的，以便模型在计算时能够正确处理。 对于输入中的真实token，对应位置的attention_mask值为1；对于填充的位置，attention_mask值为0。 示例：如果input_ids是[101, 2023, 2003, 1037, 2814, 2242, 102]，那么对应的attention_mask可能是[1, 1, 1, 1, 1, 1, 1]，表示所有位置都是真实的输入，如果某个句子词元比他小，可能就需要填充。\n#这里演示分词器\rfrom transformers import AutoModel, BertTokenizer\rmodel_name=\"bert-base-chinese\" #bert-base-uncased\rmodel=AutoModel.from_pretrained(model_name)\rtokenizer=BertTokenizer.from_pretrained(model_name)\rprint(type(model),type(tokenizer))\rsequence = [\"我出生在湖南岳阳,我的家在深圳.\",\"我得儿子是小谦谦\"]\r#输出中包含两个键 input_ids 和 attention_mask，其中 input_ids 对应分词之后的 tokens 映射到的数字编号列表，而 attention_mask 则是用来标记哪些 tokens #是被填充的（这里“1”表示是原文，“0”表示是填充字符）。\rprint(tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\",pair=True))\r# 获取填充token的id\rpad_token_id = tokenizer.pad_token_id\r# 获取填充token的字符串表示\rpad_token = tokenizer.convert_ids_to_tokens(pad_token_id)\rprint(f\"实际填充是id,padid={pad_token_id},padtoken={pad_token}\")\r#获取词汇表大小\rvocab = tokenizer.get_vocab()\rvocab_size = len(vocab)\rprint(\"词汇表大小:\", vocab_size,len(tokenizer))\r# 打印词汇表内容（可选）\rprint(\"词汇表内容:\", vocab)\r#将输入切分为词语、子词或者符号（例如标点符号），统称为 tokens；\rprint(tokenizer.tokenize(sequence[0]),len(tokenizer.tokenize(sequence[0])))\r#我们通过 convert_tokens_to_ids() 将切分出的 tokens 转换为对应的 token IDs：\rprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence[0])))\r#可以通过 encode() 函数将这两个步骤合并，并且 encode() 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加[CLS] 和 [SEP]\rprint(tokenizer.encode(sequence[0]))\r#解码还原文字，可以看到encode前后加了[CLS] 和 [SEP]\rprint(tokenizer.decode(tokenizer.encode(sequence[1]))) 输出\n\u003cclass 'transformers.models.bert.modeling_bert.BertModel'\u003e \u003cclass 'transformers.models.bert.tokenization_bert.BertTokenizer'\u003e\r{'input_ids': tensor([[ 101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638,\r2157, 1762, 3918, 1766, 119, 102],\r[ 101, 2769, 2533, 1036, 2094, 3221, 2207, 6472, 6472, 102, 0, 0,\r0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\r实际填充是id,padid=0,padtoken=[PAD]\r词汇表大小: 21128 21128\r词汇表内容: {'[PAD]': 0, '[unused1]': 1, '[unused2]': 2, '[unused3]': 3, '[unused4]': 4, '[unused5]': 5, '[unused6]': 6, '[unused7]': 7, '[unused8]': 8, '[unused9]': 9, '[unused10]': 10, '[unused11]': 11,。。。。。。。。。。。\r['我', '出', '生', '在', '湖', '南', '岳', '阳', ',', '我', '的', '家', '在', '深', '圳', '.'] 16\r[2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638, 2157, 1762, 3918, 1766, 119]\r[101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638, 2157, 1762, 3918, 1766, 119, 102]\r[CLS] 我 得 儿 子 是 小 谦 谦 [SEP] special token Tokenizer 的特殊标记（special tokens）是在处理文本数据时经常用到的一些特殊符号或者字符串，它们在自然语言处理中起着重要的作用。这些特殊标记通常包括以下几类：\nPadding token ([PAD]) pad_token：\n在进行批量处理时，序列长度不一致是很常见的情况。为了保证输入数据的统一性，我们通常会使用 [PAD] 标记来填充较短的序列，使其与其他序列的长度相同。\nStart of sequence token ([CLS]) bos_token：\n在许多自然语言处理任务（如文本分类）中，需要在输入序列的开头添加一个特殊标记，例如 [CLS]，用于模型理解这是一个序列的起始点，gpt2的开始token是：\u003c|endoftext|\u003e。\nEnd of sequence token ([SEP]) eos_token：\n类似地，[SEP] 标记通常用于表示序列的结束，特别是在处理多个句子或文本对时，可以用 [SEP] 分隔它们。\nMask token ([MASK]) mask_token：\n在预训练语言模型中，为了进行语言模型的掩码语言建模（Masked Language Modeling），我们需要将一些单词或子词随机地用 [MASK] 标记替换掉，让模型预测被掩码的部分。\nunk_token 是 tokenizer 中的一个特殊标记，通常用来表示未登录词（Unknown Token）。在自然语言处理中，未登录词指的是在训练数据中没有出现过的词汇或者子词。当模型在处理输入文本时遇到未登录词，它会用 unk_token 来替代这些词，以便继续进行处理或预测。\nsep_token 是 tokenizer 中的另一个特殊标记，通常用来表示序列的分隔符。在自然语言处理（NLP）任务中，sep_token 主要用于以下几个方面： 某些预训练语言模型（如 BERT）要求输入数据按照特定格式组织，包括使用 sep_token 来分隔输入的各个部分。例如，在文本对分类任务中，可以用 [SEP] 标记分隔两个句子： [CLS] Sentence A [SEP] Sentence B [SEP]\ncls_token 是 tokenizer 中的另一个特殊标记，通常用来表示序列的开头或者分类任务中的特殊标记。\n这些特殊标记在不同的任务和模型中具有不同的用途，但它们的共同作用是帮助模型更好地处理文本数据，处理输入序列的长度变化，以及在特定任务中引导模型学习和预测。通过适当使用特殊标记，可以有效地增强模型对语言数据的理解和处理能力。\n#特殊token\rfrom transformers import GPT2Tokenizer,AutoTokenizer\r# 初始化 GPT-2 分词器\rtokenizer = GPT2Tokenizer.from_pretrained('gpt2')\rtokenizer1 = AutoTokenizer.from_pretrained('bert-base-chinese')\r# 打印所有特殊标记\rprint(\"gpt2特殊标记:\")\rfor token_name, token_value in tokenizer.special_tokens_map.items():\rprint(f\"{token_name}: {token_value}\")\rprint(\"bert-base-chinese特殊标记:\")\rfor token_name, token_value in tokenizer1.special_tokens_map.items():\rprint(f\"{token_name}: {token_value}\") 输出\ngpt2特殊标记:\rbos_token: \u003c|endoftext|\u003e\reos_token: \u003c|endoftext|\u003e\runk_token: \u003c|endoftext|\u003e\r--------------------------\rbert-base-chinese特殊标记:\runk_token: [UNK]\rsep_token: [SEP]\rpad_token: [PAD]\rcls_token: [CLS]\rmask_token: [MASK] chunk 当你有多个句子或文本段落需要处理时，你可以将它们划分成固定长度的小块（chunks），以便输入到模型中进行处理。这个过程通常用于处理较长的文本，以确保模型可以有效地处理输入数据，特别是在使用Transformer等模型时，其输入长度通常是有限制的。\nchunk的逻辑是，输入数据的每一行句子，超过max_length 都会被截断，当前句子被拆分成的chuck的个数为：len(句子)%max_length +1，当前有些模型会添加一些开始和分割字符 比如[CLS][SEQ]等也要算入长度。\n注意tokenizer拆分小块的开启由 truncation=True,决定，如果是False max_length等就无效了。\n#truck的问题。\rcontent = [\"This is the first sentence. This is the second sentence.\",\"i am a stupid man\"]\rfrom transformers import AutoTokenizer\r# 选择一个预训练模型和对应的tokenizer\rmodel_name = \"bert-base-uncased\"\rtokenizer = AutoTokenizer.from_pretrained(model_name)\r# 最大的字符长度，因为字符的最前面会加一个[CLS],最后会补一个[SEP]，每一个句子都会被拆分一次，也就是一个truck行只能10个字符，content【0】因为超过10个字符，所以被切割成2个truck。\r# 输出的trucklength是[10,6]，第二个句子不满10个只有7个，最后length=[10, 6, 7]\rmax_length = 10\r# 进行tokenization，并返回结果\routputs = tokenizer(\rcontent,\rtruncation=True,\rmax_length=max_length,\rreturn_overflowing_tokens=True,\rreturn_length=True,\r)\r# 输出结果\rprint(outputs)\rprint(tokenizer.decode(outputs['input_ids'][0]))\rprint(tokenizer.decode(outputs['input_ids'][1])) 输出\n{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 2023, 2003, 102], [101, 1996, 2117, 6251, 1012, 102], [101, 1045, 2572, 1037, 5236, 2158, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]], 'length': [10, 6, 7], 'overflow_to_sample_mapping': [0, 0, 1]}\r[CLS] this is the first sentence. this is [SEP]\r[CLS] the second sentence. [SEP] 注意overflow_to_sample_mapping中是标识每个小chuck属于之前哪个句子索引，第1-2个chuck是属于第0个索引也就是第一个句子，3个第二个句子。\n如果加了 padding=True,所有的子句都会自动补上padding_id，最终length都会是10，结果就变成\n{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 2023, 2003, 102], [101, 1996, 2117, 6251, 1012, 102, 0, 0, 0, 0], [101, 1045, 2572, 1037, 5236, 2158, 102, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], 'length': [10, 10, 10], 'overflow_to_sample_mapping': [0, 0, 1]}\r[CLS] this is the first sentence. this is [SEP]\r[CLS] the second sentence. [SEP] [PAD] [PAD] [PAD] [PAD] 其他更详细的预处理参考：https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb\ndatacollator DataCollatorForLanguageModeling 的主要功能是为掩码语言模型（Masked Language Modeling，MLM）任务准备数据。它的主要作用是随机地掩盖输入中的一些标记，并生成相应的标签，以便模型在训练时能够预测这些被掩盖的标记。 DataCollatorWithPadding：对输入进行填充，使得输入张量具有相同的长度。 更多相关类的实现，请参考官方api\n以下是一个例子\nimport torch\rfrom transformers import BertTokenizer, DataCollatorForLanguageModeling\r# 初始化BERT分词器\rtokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r# 定义示例文本\rtexts = [\"Hello, how are you?\", \"I am fine, thank you.\"]\r# 对文本进行编码\rinputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\r# 打印编码后的输入\rprint(\"Encoded inputs:\", inputs)\r# 将输入转换为列表，以适应DataCollatorForLanguageModeling的输入格式,他的格式要求有多少个句子就多少行[{'input_ids':,'attention_mask':},{'input_ids':,'attention_mask':}]\r# tokenizer encode的格式是字典 {'input_ids': [[],[]]是在二维数组体现，所以强制转一下\rbatch = [{key: val[i] for key, val in inputs.items()} for i in range(len(texts))]\rprint(\"collator需要格式\",batch)\r# 初始化数据整理器，指定进行掩码语言模型任务\rdata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\r# 对输入数据进行整理\rcollated_inputs = data_collator(batch)\r# 打印整理后的输入,这里因为mlm=True是自动掩盖，有15%的数据被掩盖，被掩盖的数据在input_ids被替换成103，然后在生成的labels上，没有被掩盖的数据都变成-100，被掩盖的数据替换为之前的数据\r# labels是最后的标签，通过训练反向就能很好的优化模型，这就是masked模型数据处理\rprint(\"Collated inputs:\", collated_inputs)\rdata_collator1 = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\rcollated_inputs = data_collator1(batch)\r#mlm=False，不会产生遮盖，所有的输入生成的是输出相同的labels，如果是padding字符，labels是-100\rprint(\"Collated inputs:\", collated_inputs) 输出\nEncoded inputs: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0],\r[ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\r[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\rcollator需要格式 [{'input_ids': tensor([ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0])}, {'input_ids': tensor([ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012, 102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])}]\rCollated inputs: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0],\r[ 101, 1045, 2572, 2986, 103, 4067, 103, 1012, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\r[1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100],\r[-100, -100, -100, -100, 1010, -100, 2017, -100, -100]])}\rCollated inputs: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0],\r[ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\r[1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, -100],\r[ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012, 102]])} map 在使用 transformers 库时，datasets 中的 map 方法是一个非常有用的工具，用于对数据集进行预处理、特征提取、数据增强等操作。下面是一个示例，展示如何使用 map 方法对数据集进行预处理，以便于将其用于训练一个文本分类模型。 详细处理参考：https://huggingface.co/docs/datasets/use_dataset map 函数是 datasets 库中一个非常强大的工具，它允许你对数据集的每个样本或批次进行操作和变换。以下是 map 函数的几个关键参数及其解释：\nfunction 这是一个用户定义的函数，它将应用于数据集的每个样本或批次。函数可以接受一个样本或一组样本作为输入，并返回一个或多个新的字段。\ndef preprocess_function(examples): # 你的预处理逻辑 return examples\nbatched 类型：bool 默认值：False 解释：如果设置为 True，function 将会批量应用到数据集中。这意味着 function 将接收一个包含多个样本的字典作为输入。 dataset.map(preprocess_function, batched=True)\nbatch_size 类型：int 默认值：1000 解释：指定批量处理时的批次大小。仅当 batched=True 时有效。 dataset.map(preprocess_function, batched=True, batch_size=32)\nremove_columns 类型：list or str 默认值：None 解释：指定要从数据集中移除的列。这对于清理不需要的字段非常有用。 dataset.map(preprocess_function, remove_columns=[\"column_name\"])\n# 导入必要的库\rfrom datasets import Dataset\r# 创建一个简单的数据集\rdata = {\r'text': [\r\"This is the first sentence.\",\r\"Here's the second sentence.\",\r\"And this is the third one.\"\r],\r'label': [1, 0, 1]\r}\r# 转换为 Dataset 对象\rdataset = Dataset.from_dict(data)\r# 打印原始数据集\rprint(\"原始数据集：\")\rprint(dataset)\r# 导入必要的库\rfrom transformers import AutoTokenizer\r# 加载预训练的分词器\rtokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\r# 定义预处理函数\rdef preprocess_function(examples):\rprint(\"传入数据集\",examples)\r# 使用分词器对文本进行编码\rencoded_tokenizer = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=8)\rprint(\"分词数据集\",encoded_tokenizer)\r#返回的字典数据会被累加到原始数据集上。\rreturn encoded_tokenizer\r# 使用 map 方法应用预处理函数\rencoded_dataset = dataset.map(preprocess_function, batched=True,batch_size=2)\r# 打印预处理后的数据集\rprint(\"\\n预处理后的数据集结构：\",encoded_dataset)\rprint(\"\\n预处理后的数据集：\",encoded_dataset[0:3])\r# 使用 map 方法应用预处理函数,remove_columns表示删除某些列是个数组。\rencoded_dataset = dataset.map(preprocess_function, batched=True,batch_size=2,remove_columns=dataset.features)\r# 打印预处理后的数据集\rprint(\"\\n预处理后的数据集：\",encoded_dataset[0:3]) 输出：\n原始数据集：\rDataset({\rfeatures: ['text', 'label'],\rnum_rows: 3\r})\rMap: 100%\r3/3 [00:00\u003c00:00, 138.43 examples/s]\r传入数据集 {'text': ['This is the first sentence.', \"Here's the second sentence.\", 'And this is the third one.'], 'label': [1, 0, 1]}\r分词数据集 {'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\r预处理后的数据集结构： Dataset({\rfeatures: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\rnum_rows: 3\r})\r预处理后的数据集： {'text': ['This is the first sentence.', \"Here's the second sentence.\", 'And this is the third one.'], 'label': [1, 0, 1], 'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\r预处理后的数据集： {'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]} 预处理 大多数文档的标记数远超过 128 个，因此简单地将输入截断到最大长度会消除我们数据集的很大一部分。相反，我们将使用 return_overflowing_tokens 选项来对整个输入进行标记，并将其拆分为几个块。我们还将使用 return_length 选项自动返回每个创建块的长度。通常，最后一个块会小于上下文大小，我们将去掉这些部分以避免填充问题；实际上我们不需要它们，因为我们有很多数据。 让我们通过查看前两个例子来看看这到底是如何工作的：\nfrom transformers import AutoTokenizer\rcontext_length = 128\r#这个分词器专门为代码搜索和理解任务设计。它主要用于处理编程语言（如 Python、JavaScript、Java 等）的源代码。\rtokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\routputs = tokenizer(\r#获取0，1这两个数据集的脚本内容\rraw_datasets[\"train\"][:2][\"content\"],\rtruncation=True,\rmax_length=context_length,\rreturn_overflowing_tokens=True,\rreturn_length=True,\r)\rprint(f\"Input IDs length: {len(outputs['input_ids'])}\")\rprint(f\"Input chunk lengths: {(outputs['length'])}\")\rprint(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\") huggingface-course/code-search-net-tokenizer\n设计目标：这个分词器专门为代码搜索和理解任务设计。它主要用于处理编程语言（如 \u003ePython、JavaScript、Java 等）的源代码。 训练数据：该分词器使用 CodeSearchNet 数据集进行训练，数据集中包含了大量的代码示例\u003e和注释。 应用领域：适用于代码搜索、代码补全、代码生成和其他与代码相关的任务。 词汇表：词汇表中包含了大量的编程语言特定的标记（如关键字、操作符、变量名等），以及\u003e常见的编程语言语法和结构。 注意：分词器模型的作用是将单词转换为一个个的数字，训练时使用的数字计算数字之间的上下文关系，最后推算对应的数字后，反向通过词典解析成文字，所以如果需要训练中文，你只需要有一个中文分词模型即可，训练只和数字相关。 输出：\nInput IDs length: 34\rInput chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\rChunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 我们可以看到从这两个例子中总共得到了 34 个片段。查看片段长度，我们可以看到两个文档末尾的片段都少于 128 个标记（分别为 117 和 41）。这些仅占我们拥有的总片段的一小部分，因此我们可以安全地丢弃它们。使用 overflow_to_sample_mapping 字段，我们还可以重建哪些片段属于哪些输入样本。\n通过这个操作，我们利用了 🤗 Datasets 中 Dataset.map() 函数的一个便利功能，即它不需要一一对应的映射，我们可以创建比输入批次多或少的元素批次。当进行数据增强或数据过滤等会改变元素数量的操作时，这非常有用。在我们的例子中，当将每个元素标记为指定上下文大小的块时，我们从每个文档中创建了许多样本。我们只需要确保删除现有列，因为它们的大小不一致。如果我们想保留它们，可以适当重复并在 Dataset.map() 调用中返回它们：\ndef tokenize(element):\routputs = tokenizer(\relement[\"content\"],\rtruncation=True,\rmax_length=context_length,\rreturn_overflowing_tokens=True,\rreturn_length=True,\r)\rinput_batch = []\r#获取当前input_ids和长度，末尾chuck不等于context_length，就不需要加入了\rfor length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\rif length == context_length:\rinput_batch.append(input_ids)\rreturn {\"input_ids\": input_batch}\rtokenized_datasets = raw_datasets.map(\rtokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\r)\rtokenized_datasets 输出：\nDatasetDict({\rtrain: Dataset({\rfeatures: ['input_ids'],\rnum_rows: 16702061\r})\rvalid: Dataset({\rfeatures: ['input_ids'],\rnum_rows: 93164\r})\r}) 我们现在有 1670 万个例子，每个例子有 128 个标记，总共对应大约 21 亿个标记。供参考，OpenAI 的 GPT-3 和 Codex 模型分别在 300 和 1000 亿个标记上训练，其中 Codex 模型是从 GPT-3 检查点初始化的。我们在这一部分的目标不是与这些模型竞争，这些模型可以生成长而连贯的文本，而是创建一个缩减版本，为数据科学家提供快速自动补全功能。 现在我们已经准备好数据集，接下来让我们设置模型！\n初始化模型 回顾模型 参数计算 在 PyTorch 中，t.numel() 是一个张量方法，用于返回张量中所有元素的数量。它等价于计算张量的大小（shape）的所有维度的乘积。例如，一个形状为 (3, 4, 5) 的张量有 3 * 4 * 5 = 60 个元素。\n在你提供的代码中：\nmodel_size = sum(t.numel() for t in model.parameters())\n这里 model.parameters() 返回模型中所有参数的一个生成器。通过 t.numel() 计算每个参数张量中的元素数量，然后使用 sum() 函数将所有这些数量加起来，得到整个模型中所有参数的总元素数量，即模型的总大小。 示例 假设有一个简单的神经网络模型：\nimport torch\rimport torch.nn as nn\rclass SimpleModel(nn.Module):\rdef __init__(self):\rsuper(SimpleModel, self).__init__()\rself.fc1 = nn.Linear(10, 20)\rself.fc2 = nn.Linear(20, 30)\rdef forward(self, x):\rx = self.fc1(x)\rx = self.fc2(x)\rreturn x\rmodel = SimpleModel() 计算模型大小的代码如下：\nmodel_size = sum(t.numel() for t in model.parameters())\rprint(model_size) 在这个例子中，model.parameters() 会返回 fc1 和 fc2 的参数张量。\nfc1 的权重张量形状为 (20, 10)，有 20 * 10 = 200 个元素。 fc1 的偏置张量形状为 (20,)，有 20 个元素。 fc2 的权重张量形状为 (30, 20)，有 30 * 20 = 600 个元素。 fc2 的偏置张量形状为 (30,)，有 30 个元素。 总计模型中有 200 + 20 + 600 + 30 = 850 个参数元素。因此，model_size 的值将是 850。\n初始化 我们的第一步是初始化一个GPT-2模型。我们将为我们的模型使用与小型GPT-2模型相同的配置，因此我们加载预训练的配置，确保标记器大小与模型词汇大小匹配，并传递bos和eos（序列开始和结束）令牌ID：\nfrom transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\rconfig = AutoConfig.from_pretrained(\r\"gpt2\",\rvocab_size=len(tokenizer), #获取词汇表大小\rn_ctx=context_length,\rbos_token_id=tokenizer.bos_token_id,\reos_token_id=tokenizer.eos_token_id,\r) 因为使用了不同的分词器，所以重新加载配置\n通过该配置，我们可以加载一个新模型。请注意，这是我们第一次不使用 from_pretrained() 函数，因为我们实际上是在自己初始化一个模型：\nmodel = GPT2LMHeadModel(config)\rmodel_size = sum(t.numel() for t in model.parameters())\rprint(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\") 输出\nGPT-2 size: 124.2M parameters 我们的模型有 124M 个参数需要调优。在开始训练之前，我们需要设置一个数据整理器，来处理创建批次的工作。我们可以使用 DataCollatorForLanguageModeling 整理器，它是专门为语言建模设计的（正如其名称微妙地暗示的那样）。除了堆叠和填充批次外，它还负责创建语言模型标签——在因果语言建模中，输入也作为标签（仅偏移一个元素），这个数据整理器在训练过程中实时创建它们，因此我们不需要重复 input_ids。 请注意，DataCollatorForLanguageModeling 支持掩码语言建模 (MLM) 和因果语言建模 (CLM)。默认情况下，它为 MLM 准备数据，但我们可以通过设置参数 mlm=False 切换到 CLM：\nfrom transformers import DataCollatorForLanguageModeling\rtokenizer.pad_token = tokenizer.eos_token\rdata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False) 让我们来看一个例子：\nout = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\rfor key in out:\rprint(f\"{key} shape: {out[key].shape}\") 输出\ninput_ids shape: torch.Size([5, 128])\rattention_mask shape: torch.Size([5, 128])\rlabels shape: torch.Size([5, 128]) 我们可以看到示例已经被堆叠，所有张量形状相同。\n剩下的就是配置训练参数并启动训练器。我们将使用余弦学习率调度，并进行一些预热，实际批量大小为256（per_device_train_batch_size * gradient_accumulation_steps）。当单个批次无法适应内存时，会使用梯度累积，它通过多次前向/反向传递逐步累积梯度。当我们使用🤗 Accelerate 创建训练循环时，我们将看到这一点的实际应用。\nfrom transformers import Trainer, TrainingArguments\rargs = TrainingArguments(\routput_dir=\"codeparrot-ds\",\rper_device_train_batch_size=32,\rper_device_eval_batch_size=32,\revaluation_strategy=\"steps\",\reval_steps=5_000,\rlogging_steps=5_000,\rgradient_accumulation_steps=8,\rnum_train_epochs=1,\rweight_decay=0.1,\rwarmup_steps=1_000,\rlr_scheduler_type=\"cosine\",\rlearning_rate=5e-4,\rsave_steps=5_000,\rfp16=True,\rpush_to_hub=True,\r)\rtrainer = Trainer(\rmodel=model,\rtokenizer=tokenizer,\rargs=args,\rdata_collator=data_collator,\rtrain_dataset=tokenized_datasets[\"train\"],\reval_dataset=tokenized_datasets[\"valid\"],\r) 现在我们可以启动训练器并等待训练完成。根据您是在完整的训练集上运行还是在子集上运行，这将分别需要 20 小时或 2 小时，所以准备几杯咖啡和一本好书来阅读吧！\ntrainer.train() 完整代码 from datasets import load_dataset, DatasetDict\rds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\rds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\rraw_datasets = DatasetDict(\r{\r\"train\": ds_train, # .shuffle().select(range(50000)),\r\"valid\": ds_valid, # .shuffle().select(range(500))\r}\r)\rfrom transformers import AutoTokenizer\rcontext_length = 128\r#这个分词器模型专门为代码搜索和理解任务设计。它主要用于处理编程语言（如 Python、JavaScript、Java 等）的源代码，分词器的目的是将对应词元转换为数字，让模型通过计算来理解数字和数字之间的关系，选择模型的分词器非常重要。\rtokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\rdef tokenize(element):\routputs = tokenizer(\relement[\"content\"],\rtruncation=True,\rmax_length=context_length,\rreturn_overflowing_tokens=True,\rreturn_length=True,\r)\rinput_batch = []\r#获取当前input_ids和长度，末尾chuck不等于context_length，就不需要加入了\rfor length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\rif length == context_length:\rinput_batch.append(input_ids)\rreturn {\"input_ids\": input_batch}\rtokenized_datasets = raw_datasets.map(\rtokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\r)\rfrom transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\rconfig = AutoConfig.from_pretrained(\r\"gpt2\",\rvocab_size=len(tokenizer), #获取词汇表大小\rn_ctx=context_length,\rbos_token_id=tokenizer.bos_token_id,\reos_token_id=tokenizer.eos_token_id,\r)\rmodel = GPT2LMHeadModel(config)\rfrom transformers import DataCollatorForLanguageModeling\rtokenizer.pad_token = tokenizer.eos_token\rdata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\rfrom transformers import Trainer, TrainingArguments\rargs = TrainingArguments(\routput_dir=\"/kaggle/working\",\rper_device_train_batch_size=32,\rper_device_eval_batch_size=32,\revaluation_strategy=\"steps\",\reval_steps=5_000,\rlogging_steps=5_000,\rgradient_accumulation_steps=8,\rnum_train_epochs=1,\rweight_decay=0.1,\rwarmup_steps=1_000,\rlr_scheduler_type=\"cosine\",\rlearning_rate=5e-4,\rsave_steps=5_000,\rfp16=True,\rreport_to=\"none\",\rpush_to_hub=False,\r)\rtrainer = Trainer(\rmodel=model,\rtokenizer=tokenizer,\rargs=args,\rdata_collator=data_collator,\rtrain_dataset=tokenized_datasets[\"train\"],\reval_dataset=tokenized_datasets[\"valid\"],\r)\rtrainer.train() 测试 由于使用kaggle的gpu无法在12小时训练完成，所以这里只能用官方已经训练好的镜像测试了。\n现在是见证结果的时刻：让我们看看训练好的模型实际表现如何！我们可以在日志中看到损失值一直在稳定下降，但为了真正测试模型的效果，我们来看看它在一些提示信息上的表现。为此，我们将模型封装到一个文本生成管道中，并如果条件允许的话，将其部署到 GPU 上以实现快速生成：\nimport torch\rfrom transformers import pipeline\rdevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\rpipe = pipeline(\r\"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\r) 让我们从创建散点图的简单任务开始：\ntxt = \"\"\"\\\r# create some data\rx = np.random.randn(100)\ry = np.random.randn(100)\r# create scatter plot with x, y\r\"\"\"\rprint(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) 输出：\n# create some data\rx = np.random.randn(100)\ry = np.random.randn(100)\r# create scatter plot with x, y\rplt.scatter(x, y)\r# create scatter 结果看起来是正确的。对于 pandas 的操作是否也适用呢？我们来看看能否从两个数组创建一个 DataFrame：\ntxt = \"\"\"\\\r# create some data\rx = np.random.randn(100)\ry = np.random.randn(100)\r# create dataframe from x and y\r\"\"\"\rprint(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) 输出\n# create some data\rx = np.random.randn(100)\ry = np.random.randn(100)\r# create dataframe from x and y\rdf = pd.DataFrame({'x': x, 'y': y})\rdf.insert(0,'x', x)\rfor 好的，这是正确的答案——尽管随后又插入了列 x。由于生成的令牌数量有限，下面的 for 循环被截断了。我们来看看能否做一些更复杂的事情，并让模型帮助我们使用 groupby 操作：\ntxt = \"\"\"\\\r# dataframe with profession, income and name\rdf = pd.DataFrame({'profession': x, 'income':y, 'name': z})\r# calculate the mean income per profession\r\"\"\"\rprint(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) 输出\n# dataframe with profession, income and name\rdf = pd.DataFrame({'profession': x, 'income':y, 'name': z})\r# calculate the mean income per profession\rprofession = df.groupby(['profession']).mean()\r# compute the 还不错；这样做是对的。最后，让我们看看是否也能用它来为 scikit-learn 设置一个随机森林模型：\ntxt = \"\"\"\r# import random forest regressor from scikit-learn\rfrom sklearn.ensemble import RandomForestRegressor\r# fit random forest model with 300 estimators on X, y:\r\"\"\"\rprint(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) 输出\n# import random forest regressor from scikit-learn\rfrom sklearn.ensemble import RandomForestRegressor\r# fit random forest model with 300 estimators on X, y:\rrf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)\rrf.fit(X, y)\rrf 查看这几个例子，模型似乎学到了一些 Python 数据科学套件的语法。",
    "description": "简介 GPT-2（Generative Pre-trained Transformer 2）是由OpenAI开发的一种基于Transformer架构的自然语言处理模型。以下是关于GPT-2的一些关键特点和信息：\nTransformer架构：GPT-2基于Transformer模型架构，这是一种使用自注意力机制来捕捉输入序列中词语之间依赖关系的深度学习模型。\n预训练：GPT-2是一个预训练的语言模型，意味着它在大规模的文本数据上进行了预训练，以学习文本数据的统计特性和语言模式。\n无监督学习：在预训练过程中，GPT-2采用了无监督学习的方式，即模型仅仅通过文本数据本身来学习，而没有使用人工标注的标签或监督信号。\n生成式任务：GPT-2被设计用于生成式任务，如文本生成、对话生成和摘要生成等。它可以根据给定的上下文生成连贯的文本，并且在语言理解和生成方面表现出色。\n多层次架构：GPT-2具有多层的Transformer编码器，其中包含数百万个参数，使得模型能够捕获复杂的语言结构和语义关系。\n大小变种：GPT-2有多个大小的变种，从117M到1.5B个参数不等，每个变种都具有不同的性能和资源要求。更大的模型往往在生成更加准确和流畅的文本方面表现更好，但同时也需要更多的计算资源。\n开放许可：GPT-2是在OpenAI的研究下开发的，其模型和相关资源以开放许可的形式发布，使得研究人员和开发者可以自由地使用和构建基于GPT-2的应用。\n总的来说，GPT-2是一种强大的语言模型，具有广泛的应用潜力，可用于自然语言生成、理解、翻译等各种NLP任务。\n案例 该案例来源huggingface学习中心nlp-course，Training a causal language model from scratch 文章\n描述 我们将构建一个缩减版的代码生成模型：我们将专注于一行补全，而不是完整的函数或类，使用Python代码的一个子集。在Python中处理数据时，您会频繁接触到Python数据科学栈，包括matplotlib、seaborn、pandas和scikit-learn库。在使用这些框架时，经常需要查找特定的命令，因此如果我们可以使用一个模型来为我们完成这些调用，那将是很好的。\n收集数据 我们使用huggingface收集得content包含：“pandas”, “sklearn”, “matplotlib”, “seaborn” 这些关键字python代码 这个数据集是从github公共仓库爬取，比如 from datasets import load_dataset, DatasetDict\rds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\rds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\rraw_datasets = DatasetDict(\r{\r\"train\": ds_train, # .shuffle().select(range(50000)),\r\"valid\": ds_valid, # .shuffle().select(range(500))\r}\r) 让我们看一个数据集中的例子。我们只需显示每个字段的前200个字符：",
    "tags": [],
    "title": "Transformers实战04-微调gpt-2生成python代码。",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_04/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "简介 模型量化（Model Quantization）是一种优化技术，旨在减少机器学习模型的计算资源需求和存储空间，同时在精度损失最小化的前提下提高推理效率。量化通过将模型权重和激活函数的数值从高精度（如 32 位浮点数）转换为低精度（如 8 位整数），显著减少了模型大小和计算复杂度。\n主要类型 静态量化（Post-Training Quantization, PTQ）\n在模型训练完成后进行量化。 通过分析训练数据的分布，将权重和激活函数映射到低精度表示。 不需要重新训练模型。 适用于对性能影响较小的场景。 动态量化（Dynamic Quantization）\n在推理时动态地将浮点数转换为低精度整数。 在运行过程中对激活函数进行量化。 比静态量化更简单，因为不需要分析训练数据。 对推理速度有显著提升，尤其是对模型输入依赖较少的层（如全连接层）。 量化感知训练（Quantization-Aware Training, QAT）\n在训练过程中模拟量化影响。 模型在训练过程中考虑量化误差，以便在量化后保持更高的精度。 比静态量化和动态量化需要更多的计算资源，但精度损失最小。 适用于对精度要求较高的应用。 这里例子就演示下动态量化，bitsandbytes本身以上三种都支持。\n量化的优点 减小模型大小：通过将权重和激活函数表示从 32 位浮点数转换为 8 位整数，模型大小可以显著减少。 加快推理速度：低精度运算速度更快，可以显著提高推理效率。 降低内存带宽需求：低精度表示占用更少的内存，减少了内存带宽的需求。 量化的缺点 精度损失：由于数值表示的精度降低，模型可能会经历一定程度的精度损失，具体程度取决于模型结构和数据分布。 复杂性增加：在某些情况下，量化过程可能会增加模型部署的复杂性，尤其是需要进行量化感知训练时。 量化过程 以下过程只是一种最简单的思路，方便理解，实际要比这更复杂。\n量化过程 确定值域： 首先，确定要量化的数据的值域范围。例如，假设我们有一组数据的值域为 $[min,max]$。\n确定量化级别： 确定量化的级别或分辨率，这决定了将值域划分成多少个区间。在4位整数的情况下，共有 $2^4=16$ 个可能的值。\n线性映射： 将原始数据映射到4位整数的范围内。通常使用线性映射来实现，计算公式如下： $$\\text{quantized_value} = \\frac{\\text{original_value} - \\text{min}}{\\text{max} - \\text{min}} \\times (\\text{number of levels} - 1)$$\n这里的 number of levels 是16（对应4位整数的值域范围）。\n反量化过程 解码反量化： 在使用量化数据进行计算之前，需要将其解码回原始的数据表示形式（如32位浮点数或其他高精度表示）。解码公式通常为： $$\\text{original_value} = \\text{quantized_value} \\times \\frac{\\text{max} - \\text{min}}{\\text{number of levels} - 1} + \\text{min}$$ 这里的 quantized_value是是量化后的4位整数值,min和max是原始数据的最小值和最大值。\n两个不同的原始值在量化后可能相同，被还原为同一个值。这种情况表明精度损失是不可避免的。为了减少这种精度损失带来的影响，通常采取以下策略：\n增加量化级别： 增加量化级别（如使用8位、16位量化）以减少不同原始值被量化为同一个值的概率。\n量化感知训练（Quantization-aware training）： 在训练过程中模拟量化误差，以提高模型在量化后的精度表现。\n非线性量化： 使用对数量化或其他非线性量化方法，使得量化更适应数据的分布特性，从而减少精度损失。\n精细调节量化参数： 通过精细调整量化的最小值、最大值和比例因子，尽量减少量化误差对关键值的影响。\n精度和参数 模型中每个参数常见的存储类型包括：\nFP32（32-bit Floating Point）: 每个参数占用 4 字节（32 位），单精度浮点数（32位浮点数），范围大约：$[-3.4 \\times 10^{38}, 3.4 \\times 10^{38}]$。 FP16（16-bit Floating Point）: 每个参数占用 2 字节（16 位），半精度浮点数使用16位（1位符号、5位指数、10位尾数），FP16的数值范围大约是 [−65504,65504]，大约 3 位有效数字。 INT8（8-bit Integer）: 每个参数占用 1 字节（8 位），将模型的权重和激活值量化为8位整数（范围通常是0到255），相对于32位浮点数，精度的损失较小。8-bit量化比4-bit提供更好的精度，并且通常可以更接近原始模型的性能。 INT4（4-bit Integer）: 每个参数占用4位，将模型的权重和激活值量化为4位整数（范围通常是-8到7或者0到15），因此相对于32位浮点数，它的精度显著降低。这种量化可以显著减小模型的大小和计算需求，但可能会损失一定的模型精度。 如何获取某个模型的精度了\nimport torch\rfrom transformers import AutoModel, BertTokenizer\rmodel_name=\"bert-base-chinese\" #bert-base-uncased\rmodel=AutoModel.from_pretrained(model_name)\r#获取模型参数的精度\r\"\"\"\rFP32（32-bit Floating Point）: 每个参数占用 4 字节（32 位）。\rFP16（16-bit Floating Point）: 每个参数占用 2 字节（16 位）。\rINT8（8-bit Integer）: 每个参数占用 1 字节（8 位）。\r\"\"\"\rdtype=list(model.parameters())[0].dtype\rprint(\"精度:\",dtype)\rtotal_params = sum(p.numel() for p in model.parameters())\rdtype_to_bytes = {\rtorch.float32: 4, # FP32: 4字节\rtorch.float16: 2, # FP16: 2字节\rtorch.int8: 1, # INT8: 1字节\rtorch.int32: 4, # INT32: 4字节\rtorch.int64: 8, # INT64: 8字节\rtorch.float64: 8, # FP64 (double): 8字节\r}\rmodel_size = total_params * dtype_to_bytes[dtype]\rprint(f'Model size: {model_size / (1024**2):.2f} MB') 输出\n精度: torch.float32\rModel size: 390.12 MB 量化实例 bitsandbytes bitsandbytes 通过 PyTorch 的 k 位量化技术使大型语言模型的访问变得可行。bitsandbytes 提供了三个主要功能以显著降低推理和训练时的内存消耗：\n8 位优化器采用区块式量化技术，在极小的内存成本下维持 32 位的表现。 LLM.Int() 或 8 位量化使大型语言模型推理只需一半的内存需求，并且不会有任何性能下降。该方法基于向量式的量化技术将大部分特性量化到 8 位，并且用 16 位矩阵乘法单独处理异常值。 QLoRA 或 4 位量化使大型语言模型训练成为可能，它结合了几种节省内存的技术，同时又不牺牲性能。该方法将模型量化至 4 位，并插入一组可训练的低秩适应（LoRA）权重来允许训练。 安装bitsandbytes bitsandbytes 仅支持 CUDA 版本 11.0 - 12.5 的 CUDA GPU。\n!pip install -U bitsandbytes\r!pip install transformers\r!pip install accelerate 4bit量化(加载) 加载并量化一个模型到4位，并使用bfloat16数据类型进行计算：\n您使用 bnb_4bit_compute_dtype=torch.bfloat16，这意味着计算过程中会反量化使用 bfloat16 数据类型，而存储时则可能使用4位表示。这解释了为什么您看到的 dtype 仍然是 fp16 或者 bfloat16。\nBigScience 是一个全球性的开源AI研究合作项目，旨在推动大型语言模型（LLM）的发展。bloom-1b7 是 BigScience 项目下的一部分，具体来说，是一个包含约17亿参数的语言模型。\nimport torch\rfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\rmodel_name=\"bigscience/bloom-1b7\" quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\rmodel = AutoModelForCausalLM.from_pretrained(\rmodel_name,\rdevice_map=\"auto\",\r)\rmodel_4bit = AutoModelForCausalLM.from_pretrained(\rmodel_name,\rdevice_map=\"auto\",\rquantization_config=quantization_config,\r)\rdtype=list(model.parameters())[0].dtype\rprint(\"原始精度:\",dtype)\rdest_dtype=list(model_4bit.parameters())[0].dtype\rprint(\"量化精度:\",dest_dtype)\r# 检查模型的量化配置\rprint(\"量化配置:\", model_4bit.config.quantization_config)\rdef print_model_info(model):\rtotal_params = 0\rfor name, param in model.named_parameters():\rtotal_params += param.numel()\r#print(f\"Total parameters: {total_params / 1e6}M\")\rreturn total_params\rtotal_model_size=print_model_info(model)\rtotal_model_4bit_size=print_model_info(model_4bit)\rprint(\"模型参数个数：\",total_model_size)\rprint(\"量化后的模型参数个数：\",total_model_4bit_size)\rdtype_to_bytes = {\rtorch.float32: 4, # FP32: 4字节\rtorch.float16: 2, # FP16: 2字节\rtorch.int8: 1, # INT8: 1字节\rtorch.int32: 4, # INT32: 4字节\rtorch.int64: 8, # INT64: 8字节\rtorch.float64: 8, # FP64 (double): 8字节\r}\rmodel_size = total_model_size * dtype_to_bytes[dtype]\rmodel_size = total_model_size * dtype_to_bytes[dtype]\rprint(f'origin Model size: {model_size / (1024**2):.2f} MB')\rmodel_size = total_model_4bit_size * dtype_to_bytes[dest_dtype]\rprint(f'quan Model size: {model_size / (1024**2):.2f} MB')\rmodel_4bit.save_pretrained(\"/tmp/p\")\rmodel.save_pretrained(\"/tmp/o\") 输出：\n原始精度: torch.float32\r量化精度: torch.float16\r量化配置: BitsAndBytesConfig {\r\"_load_in_4bit\": true,\r\"_load_in_8bit\": false,\r\"bnb_4bit_compute_dtype\": \"bfloat16\",\r\"bnb_4bit_quant_storage\": \"uint8\",\r\"bnb_4bit_quant_type\": \"fp4\",\r\"bnb_4bit_use_double_quant\": false,\r\"llm_int8_enable_fp32_cpu_offload\": false,\r\"llm_int8_has_fp16_weight\": false,\r\"llm_int8_skip_modules\": null,\r\"llm_int8_threshold\": 6.0,\r\"load_in_4bit\": true,\r\"load_in_8bit\": false,\r\"quant_method\": \"bitsandbytes\"\r}\r模型参数信息： 1722408960\r量化后的模型参数信息： 1118429184\rorigin Model size: 6570.47 MB\rquan Model size: 2133.23 MB 总的参数个数减少。这通常是由于量化过程中进行了优化或者参数压缩的操作。 量化在深度学习中通常是指将模型中的浮点数参数转换为更低精度的整数或定点数表示，以节省内存和提高计算效率。\n为啥量化模型的dtype是fp16了而不是int4，以下是对量化模型加载过程中 dtype 问题的一些解释：\n参数存储与计算类型的区别：\n存储时，模型参数可能被压缩或量化为较低位宽的整数类型（如4位整数）。 加载时，为了方便后续计算，这些参数可能会被解码为较高精度的浮点类型（如 fp16 或 bfloat16）。 量化过程的具体实现：\n许多量化库在加载模型时，会将低位宽的量化参数解码为浮点类型，以便在计算时可以直接使用这些参数。 这就是为什么即使您使用了 load_in_4bit=True，在加载后检查参数的 dtype 时仍然看到的是 fp16。 通过查看模型保存的就可以确定了 查看量化的模型：\n!ls /tmp/p -al --block-size=M | grep model 输出:\n-rw-r--r-- 1 root root 1630M Aug 6 08:04 model.safetensors 可以看到我们之前在内存中打印的是2133.23（内存中计算还是会被反量化到bnb_4bit_compute_dtype指定类型，但是参数都是压缩后去掉了一些参数） ，存储后变成了1630M，比之前计算的少一些，说明存储使用了4bit。 在看下没有量化的模型：\n!ls /tmp/o -al --block-size=M | grep model 输出了：\n-rw-r--r-- 1 root root 4714M Aug 6 08:05 model-00001-of-00002.safetensors\r-rw-r--r-- 1 root root 1857M Aug 6 08:05 model-00002-of-00002.safetensors\r-rw-r--r-- 1 root root 1M Aug 6 08:05 model.safetensors.index.json 可以看到我们之前在内存中打印的是6570.47 MB ，存储后没变，分文件存储了4714M+1857M 。\n8bit量化(加载) 代码和4bit相似，调整下配置即可\nquantization_config = BitsAndBytesConfig(load_in_8bit=True) 同4bit代码，输出\n原始精度: torch.float32\r量化精度: torch.float16\r量化配置: BitsAndBytesConfig {\r\"_load_in_4bit\": false,\r\"_load_in_8bit\": true,\r\"bnb_4bit_compute_dtype\": \"float32\",\r\"bnb_4bit_quant_storage\": \"uint8\",\r\"bnb_4bit_quant_type\": \"fp4\",\r\"bnb_4bit_use_double_quant\": false,\r\"llm_int8_enable_fp32_cpu_offload\": false,\r\"llm_int8_has_fp16_weight\": false,\r\"llm_int8_skip_modules\": null,\r\"llm_int8_threshold\": 6.0,\r\"load_in_4bit\": false,\r\"load_in_8bit\": true,\r\"quant_method\": \"bitsandbytes\"\r}\r模型参数信息： 1722408960\r量化后的模型参数信息： 1722408960\rorigin Model size: 6570.47 MB\rquan Model size: 3285.23 MB 可以看到8bit不需要指定内存计算的类型，量化内存计算精度默认就是fp16。 查看模型保存大小\n!ls /tmp/p -al --block-size=M | grep model\r#----------------------------------------------------------------------------------------------------\r!ls /tmp/o -al --block-size=M | grep model 输出\n-rw-r--r-- 1 root root 2135M Aug 6 08:30 model.safetensors\r#----------------------------------------------------------------------------------------------------\r-rw-r--r-- 1 root root 4714M Aug 6 08:30 model-00001-of-00002.safetensors\r-rw-r--r-- 1 root root 1857M Aug 6 08:31 model-00002-of-00002.safetensors\r-rw-r--r-- 1 root root 1M Aug 6 08:31 model.safetensors.index.json 验证效果 这里用之前的4bit模型来和原始模型比较\nimport time\rdef benchmark_model(model, input_text, tokenizer):\rinputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\rstart_time = time.time()\rwith torch.no_grad():\routputs = model.generate(**inputs)\r# 解码并打印生成的文本\rgenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\rprint(\"Generated text:\", generated_text)\rend_time = time.time()\rinference_time = end_time - start_time\rprint(f\"Inference time: {inference_time:.2f} seconds\")\rfrom transformers import AutoTokenizer\rtokenizer = AutoTokenizer.from_pretrained(model_name)\rinput_text = \"Hello, how are you?\"\rprint(\"未量化模型性能测试：\")\rbenchmark_model(model, input_text, tokenizer)\rprint(\"量化模型性能测试：\")\rbenchmark_model(model_4bit, input_text, tokenizer) 输出\n未量化模型性能测试：\rGenerated text: Hello, how are you? I hope you are doing well. I am a newbie in this\rInference time: 0.31 seconds\r量化模型性能测试：\rGenerated text: Hello, how are you?\"\r\"I'm fine,\" I said.\r\"I'm just a\rInference time: 0.62 seconds 这里看到量化的模型反而推理需要更多的时间，量化模型在理论上应该提高推理速度和减少内存占用,这里使用float16gpu显存占用肯定少了一半以上，但是推理速度比较慢，在实际应用中，可能会因为多个因素导致性能下降。",
    "description": "简介 模型量化（Model Quantization）是一种优化技术，旨在减少机器学习模型的计算资源需求和存储空间，同时在精度损失最小化的前提下提高推理效率。量化通过将模型权重和激活函数的数值从高精度（如 32 位浮点数）转换为低精度（如 8 位整数），显著减少了模型大小和计算复杂度。\n主要类型 静态量化（Post-Training Quantization, PTQ）\n在模型训练完成后进行量化。 通过分析训练数据的分布，将权重和激活函数映射到低精度表示。 不需要重新训练模型。 适用于对性能影响较小的场景。 动态量化（Dynamic Quantization）\n在推理时动态地将浮点数转换为低精度整数。 在运行过程中对激活函数进行量化。 比静态量化更简单，因为不需要分析训练数据。 对推理速度有显著提升，尤其是对模型输入依赖较少的层（如全连接层）。 量化感知训练（Quantization-Aware Training, QAT）\n在训练过程中模拟量化影响。 模型在训练过程中考虑量化误差，以便在量化后保持更高的精度。 比静态量化和动态量化需要更多的计算资源，但精度损失最小。 适用于对精度要求较高的应用。 这里例子就演示下动态量化，bitsandbytes本身以上三种都支持。\n量化的优点 减小模型大小：通过将权重和激活函数表示从 32 位浮点数转换为 8 位整数，模型大小可以显著减少。 加快推理速度：低精度运算速度更快，可以显著提高推理效率。 降低内存带宽需求：低精度表示占用更少的内存，减少了内存带宽的需求。 量化的缺点 精度损失：由于数值表示的精度降低，模型可能会经历一定程度的精度损失，具体程度取决于模型结构和数据分布。 复杂性增加：在某些情况下，量化过程可能会增加模型部署的复杂性，尤其是需要进行量化感知训练时。 量化过程 以下过程只是一种最简单的思路，方便理解，实际要比这更复杂。\n量化过程 确定值域： 首先，确定要量化的数据的值域范围。例如，假设我们有一组数据的值域为 $[min,max]$。\n确定量化级别： 确定量化的级别或分辨率，这决定了将值域划分成多少个区间。在4位整数的情况下，共有 $2^4=16$ 个可能的值。\n线性映射： 将原始数据映射到4位整数的范围内。通常使用线性映射来实现，计算公式如下： $$\\text{quantized_value} = \\frac{\\text{original_value} - \\text{min}}{\\text{max} - \\text{min}} \\times (\\text{number of levels} - 1)$$\n这里的 number of levels 是16（对应4位整数的值域范围）。",
    "tags": [],
    "title": "Transformers实战05-模型量化",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_05/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "前言 Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。\n在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。\nAttention is All You Need：Attention Is All You Need\nTransformer 整体结构 首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构： 可以看到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：\n第一步：获取输入句子的每一个单词的表示向量 X，X由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。 第二步：将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 x) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 C，如下图。单词向量矩阵用 $X_{n\\times d}$ 表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。 第三步：将 Encoder 输出的编码信息矩阵 C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。 上图 Decoder 接收了 Encoder 的编码矩阵 C，然后首先输入一个翻译开始符 “\"，预测第一个单词 “I”；然后输入翻译开始符 “” 和单词 “I”，预测单词 “have”，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。\nTransformer 的输入 Transformer 中单词的输入表示 x由单词 Embedding 和位置 Embedding （Positional Encoding）相加得到。 单词 Embedding 单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。\n原理 什么是Word Embedding（词嵌入）？\n词嵌入是自然语言处理中语言模型与表征技术技术的统称。讲人话就是： 就是把词语（字符串类型）这一文本数据转换成 计算机能认识 的数字表征的数据（一般为浮点型数据）。因为我们的机器学习模型或者深度学习模型，需要的数据都是数字类型的，无法处理文本类型的数据，所以我们需要把单词转换成数字类型。 词嵌入为 文本AI系统的上游任务，只有通过词嵌入模型才能得到文本AI系统才能得到数字类型的输入数据。 现有的词嵌入模型有：word2vec，GloVe，ELMo，BERT等 以下使用word2vec的原理来解释下词embedding实现逻辑\nword2vec是词向量化技术的一种，通过神经网络来实现。其在表面上看起来是一种无监督学习技术，但本质上仍然是有监督学习。 利用文本的上下文信息构造有监督数据集，通过这一数据集来训练神经网络，最后取得训练好的神经网络两个网络层之间的权重 矩阵作为的词向量表（每个单词对应其中一行数据）。\nword2vec 有两个模型：\nSkip-gram模型：其特点为，根据当前单词预测上下文单词，使用中心词来预测上下文词。 CBOW模型：全称为 Continuous Bag-of-Word，连续词袋模型，该模型的特点是，输入已知的上下文，输出对当前单词的预测，其实就是利用中心两侧的词来预测中心的词。 以下两幅图展现了CBOW模型和Skip-gram模型。 CBOW 模型 如果对以下神经网络连接不太清楚的，可以先去看看：https://blog.csdn.net/liaomin416100569/article/details/130572559?spm=1001.2014.3001.5501\none-hot 参考：rnn中关于one-hot和nn.embedding章节\nOne-hot编码又称一位有效编码，是将文字数字化的过程。假如我们有一个语料库：”I drink coffee everyday“。我们对其以” “（空格）进行分词，则我们会得到4个单词，假设这4个单词是我们所有的单词种类（也就是说，我们的字典中只有这四个单词），这样我们对其进行one-hot编码后，可以得到如下编码结果： 表1\n单词 One-hot编码 I [1, 0, 0, 0] drink [0, 1, 0, 0] coffee [0, 0, 1, 0] everyday [0, 0, 0, 1] 这里使用one-hot的原因是列的个数就是单词的格式，最后使用隐藏层的w作为嵌入结果，刚好是(列数，隐藏层神经元个数)\n构建 CBOW 训练数据集 cbow是使用两侧的词语，预测中心的词语，预测窗口大小为 2，输入就是左侧和右侧的两个单词，预测的单词就是中心的单词。 我们语料库仍然为：”I drink coffee everyday“，假设我们的预测窗口大小为 2，通过语料库我们可以构建以下训练集，表2\n输入词 预测词 [drink, coffee] I [I, coffee, everyday] drink [I, drink, everyday] coffee [drink, coffee] everyday 构建 CBOW 神经网络 从上可知，我们的输入层有4个输入单元（one-hot的4列，因为one-hot所以就是原始单词个数），输出层神经元的个数应该跟输入层保持一致，输出层也是4个神经元，加入我们想要每个单词为一个五维的向量表示，那么我们的隐藏层则为五个神经元。由此，我们可以构建一个输入层为4，隐藏层为5，输出层为4的全连接神经网络，如下图所示，训练好的模型的权重矩阵w1可以作为我们的词向量化表。 训练 CBOW 神经网络 这时我们可以根据构建的CBOW数据集对模型进行训练了，假设我们要预测的词是coffee，那么由表2可知，我们输入词为[I, drink, everyday]，我们可以得到如下训练过程。 首先，我们将输入词[I, drink, everyday]转换为对应的one-hot编码向量。假设我们的词汇表中有四个词（I, drink, coffee, everyday），则输入词的one-hot编码分别为：\nI: [1, 0, 0, 0]\rdrink: [0, 1, 0, 0]\reveryday: [0, 0, 0, 1] 接下来，我们将每个one-hot编码向量乘以词嵌入矩阵，以获取词嵌入向量。假设我们已经有了每个词的词嵌入矩阵（这些矩阵在实际应用中是通过训练得到的）,这也是我们经过多次训练之后，最终得到的嵌入矩阵，因为初始化肯定是一个初始值，经过训练反向传播得到一个最佳值，这里假设它们分别为： $$ W = \\begin{bmatrix} 0.1 \u0026 0.2 \u0026 0.3 \u0026 0.4 \u0026 0.5 \\ 0.2 \u0026 0.3 \u0026 0.4 \u0026 0.5 \u0026 0.6 \\ 0.3 \u0026 0.4 \u0026 0.5 \u0026 0.6 \u0026 0.7 \\ 0.4 \u0026 0.5 \u0026 0.6 \u0026 0.7 \u0026 0.8 \\ \\end{bmatrix} $$ 接下来，我们将每个one-hot编码向量乘以词嵌入矩阵，以获取词嵌入向量。例如：\n输入词I的词嵌入向量：$$[1, 0, 0, 0] \\times W = [0.1, 0.2, 0.3, 0.4, 0.5] $$ 输入词drink的词嵌入向量：$$[0, 1, 0, 0] \\times W = [0.2, 0.3, 0.4, 0.5, 0.6] $$ 输入词everyday的词嵌入向量：$$ [0, 0, 0, 1] \\times W = [0.4, 0.5, 0.6, 0.7, 0.8] $$ 接下来，我们将上下文单词的词嵌入向量加起来或求平均以获取一个特征向量。在这个例子中，我们将对它们求平均。\n平均特征向量 = $$\\text{平均特征向量} = \\frac{( \\text{词嵌入向量(I)} + \\text{词嵌入向量(drink)} + \\text{词嵌入向量(everyday)} )}{3}$$ $$= \\frac{( [0.1, 0.2, 0.3, 0.4, 0.5] + [0.2, 0.3, 0.4, 0.5, 0.6] + [0.4, 0.5, 0.6, 0.7, 0.8] )}{3}$$ $$= \\left[ \\frac{(0.1 + 0.2 + 0.4)}{3}, \\frac{(0.2 + 0.3 + 0.5)}{3}, \\frac{(0.3 + 0.4 + 0.6)}{3}, \\frac{(0.4 + 0.5 + 0.7)}{3}, \\frac{(0.5 + 0.6 + 0.8)}{3} \\right]$$ $$= [0.233, 0.333, 0.433, 0.533, 0.633]$$ 现在，我们得到了一个特征向量$$ [0.233, 0.333, 0.433, 0.533, 0.633]$$它表示了上下文单词[I, drink, everyday]的语义信息。\n理解CBOW模型中将上下文单词的词嵌入向量加起来或求平均的原因需要考虑两个方面： 1.上下文信息的整合：CBOW模型的目标是通过上下文单词来预测目标词。因此，对于一个给定的目标词，在预测时需要综合考虑其周围的上下文信息。将上下文单词的词嵌入向量加起来或求平均，可以将这些单词的语义信息整合到一个特征向量中，使得该特征向量更全面地表示了整个句子的语境信息，而不仅仅是单个词的信息。这样可以帮助模型更准确地捕捉句子的语义信息，从而提高模型在目标词预测任务上的性能。 2.语义信息的提取：虽然CBOW模型是用来预测目标词的，但实际上，在训练过程中，模型会学习到每个词的词嵌入向量，这些词嵌入向量包含了每个单词的语义信息。当将上下文单词的词嵌入向量加起来或求平均时，实际上是在利用这些已经学习到的词嵌入向量来提取整个句子的语义信息。由于词嵌入向量是通过大规模语料库训练得到的，其中包含了丰富的语义信息，因此将它们加起来或求平均可以帮助提取句子的语义特征，而不仅仅是单个词的语义特征。\n接下来，我们将特征向量输入到一个全连接层（也称为投影层），并应用softmax函数以获取预测概率。假设全连接层的权重矩阵为： $$W_{proj} = \\begin{bmatrix} 0.1 \u0026 0.2 \u0026 0.3 \u0026 0.4 \u0026 0.5 \\ 0.2 \u0026 0.3 \u0026 0.4 \u0026 0.5 \u0026 0.6 \\ 0.3 \u0026 0.4 \u0026 0.5 \u0026 0.6 \u0026 0.7 \\ 0.4 \u0026 0.5 \u0026 0.6 \u0026 0.7 \u0026 0.8 \\ 0.5 \u0026 0.6 \u0026 0.7 \u0026 0.8 \u0026 0.9 \\ \\end{bmatrix}$$ 我们将特征向量乘以权重矩阵，并应用softmax函数，以获取每个词作为预测目标的概率。 $$z = [0.233, 0.333, 0.433, 0.533, 0.633] \\times W_{proj}$$\n经过训练之后，我们拿 W1( 4*5 权重矩阵) 作为我们的词向量化表，我们可以得到**如下词向量化表（假设）。\n单词索引 向量 I [0.11, 0.22, 0.23, 0.25, 0.31] drink [0.32, 0.22, 0.33, 0.11, 0.32] coffee [0.23, 0.03, 0.62, 0.12, 0.17] everyday [0.05, 0.25, 0.55, 0.17, 0.47 ] 假如我们要词向量化”I drink coffee“这句话，我们便可以直接查询上表，拿到我们的词向量矩阵，即为$$[ [0.11, 0.22, 0.23, 0.25, 0.31],\\ [0.32, 0.22, 0.33, 0.11, 0.32], \\ [0.23, 0.03, 0.62, 0.12, 0.17] ]$$\nSkip-gram 模型 one-hot 参考：rnn中关于one-hot和nn.embedding章节\nOne-hot编码又称一位有效编码，是将文字数字化的过程。假如我们有一个语料库：”I drink coffee everyday“。我们对其以” “（空格）进行分词，则我们会得到4个单词，假设这4个单词是我们所有的单词种类（也就是说，我们的字典中只有这四个单词），这样我们对其进行one-hot编码后，可以得到如下编码结果： 表1\n单词 One-hot编码 I [1, 0, 0, 0] drink [0, 1, 0, 0] coffee [0, 0, 1, 0] everyday [0, 0, 0, 1] 这里使用one-hot的原因是列的个数就是单词的格式，最后使用隐藏层的w作为嵌入结果，刚好是(列数，隐藏层神经元个数)\n构建 Skip-gram训练数据集 cbow是使用两侧的词语，预测中心的词语，预测窗口大小为 2，输入就是左侧和右侧的两个单词，预测的单词就是中心的单词。 skip-gram是使用中心的词语，预测两侧的词语，预测窗口大小为 2，输入就是中心词语，预测的单词就是左侧和右侧的两个单词。 我们语料库仍然为：”I drink coffee everyday“，假设我们的预测窗口大小为 2，通过语料库我们可以构建以下训练集，表2\n预测词 输入词 I drink I coffee drink I drink coffee drink everyday coffee I coffee drink coffee everyday everyday drink everyday coffee 注意输入是一个词，输出是一个词\n训练 Skip-gram神经网络 这时我们可以根据构建的Skip-gram数据集对模型进行训练了，假设我们要预测的词是coffee，那么由表2可知，我们输入词为[I, drink, everyday]中的任何一个，由表2可知，对其进行one-hot编码后的结果为 [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1]], **我们选择其中一个就可以得到一个 1*4 的输入向量，那么我们可以得到如下训练过程。 经过训练之后，我们拿 W1( 4*5 权重矩阵) 作为我们的词向量化表。 训练过程不表,类似于CBOW 。\nWord2Vec实例 数据训练 导入必要的库： #安装 pip install gensim jieba from gensim.models import Word2Vec\rimport logging # 用来设置日志输出\rimport jieba 准备文本数据： context = [\"word2vec是监督学习算法，其会通过句子中词的前后顺序构建有标签数据集，通过数据集 训练神经网络模型 得到这一数据集的 词向量 表（可以理解成我们的新华字典）。\"\r,\"word2vec是用来进行 对词语进行向量化 的模型，也就是对文本类型的数据进行 特征提取\"\r,\"word2vec一般为一个3层（输入层、隐藏层、输出层） 的 全连接神经网络。\"\r,\"本文主要从原理、代码实现 理论结合实战两个角度来剖析word2vec算法\"\r,\"理论部分主要是关于 什么是 word2vec，其两种常见的模型\"\r,\"实战部分主要是通过Gensim库中的word2vec模型，实现文本特征提取\"] 中文分词：\n使用jieba库对文本进行中文分词，并将分词结果保存在context列表中。 for i in range(len(context)):\rsplit_s = context[i]\rcontext[i] = \" \".join(jieba.cut(split_s, HMM=True))\rcontext = [e.split(\" \") for e in context] 配置日志：\n配置日志输出格式和级别。\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) 训练Word2Vec模型：\n使用Word2Vec类来训练模型，传入分词后的文本数据以及一些参数：\nsentences: 分词后的文本数据。 workers: 训练时使用的线程数。 window: 上下文窗口大小，表示一个词周围的上下文词数量。 vector_size: 词向量的维度大小。 epochs: 训练轮数。 min_count: 忽略词频低于此值的词语。 model = Word2Vec(sentences=context, workers=8, window=4, vector_size=10, epochs=30, min_count=3) 查看词汇表和词向量： print(model.wv.key_to_index) # 打印词汇表\rprint(model.wv[\"word2vec\"]) model.wv.key_to_index用于查看词汇表，而model.wv[\"word2vec\"]则用于查看特定词的词向量，这里是查询单词word2vec的词向量。 输出结果\n{'': 0, '的': 1, 'word2vec': 2, '，': 3, '是': 4, '层': 5, '模型': 6, '数据': 7, '主要': 8, '、': 9, '进行': 10, '集': 11, '通过': 12}\r[ 0.07315318 0.05167933 0.06995787 0.00852275 0.0644208 -0.03653978\r-0.00503093 0.06105096 -0.081814 -0.04047652] 可以使用Gensim提供的save()方法将训练好的Word2Vec模型保存到文件。这样可以在之后加载模型并重用它。以下是保存模型的示例代码：\n注意：词汇表里单词都是词频次数超过min_count的词。\n保存和加载 保存模型\nmodel.save(\"word2vec_model.bin\") 这将把训练好的模型保存到名为\"word2vec_model.bin\"的文件中。然后，您可以使用以下代码加载保存的模型：\nfrom gensim.models import Word2Vec\r# 加载模型\rloaded_model = Word2Vec.load(\"word2vec_model.bin\")",
    "description": "前言 Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。\n在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。\nAttention is All You Need：Attention Is All You Need\nTransformer 整体结构 首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构： 可以看到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：\n第一步：获取输入句子的每一个单词的表示向量 X，X由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。 第二步：将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 x) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 C，如下图。单词向量矩阵用 $X_{n\\times d}$ 表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。 第三步：将 Encoder 输出的编码信息矩阵 C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。 上图 Decoder 接收了 Encoder 的编码矩阵 C，然后首先输入一个翻译开始符 “\"，预测第一个单词 “I”；然后输入翻译开始符 “” 和单词 “I”，预测单词 “have”，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。",
    "tags": [],
    "title": "Transformer模型详解01-Word Embedding",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_01/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "什么是位置编码 在transformer的encoder和decoder的输入层中，使用了Positional Encoding，使得最终的输入满足： $$input = input_embedding + positional_encoding $$\nword embedding：理解参考\n这里，input_embedding是通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model，由于是相加关系，自然而然地，这里的positional_encoding也是一个d_model维度的向量。（在原论文里，d_model = 512） 注意：在Transformer模型中，“token”（标记）是指输入序列中的每个元素，它通常是一个单词、一个子词或一个字符，假设我们有一个句子：“The cat sat on the mat.\"，单词级别的标记： [“The”, “cat”, “sat”, “on”, “the”, “mat”, “.\"]。然后被转换成词嵌入（word embeddings）和位置嵌入（position embeddings），然后这两种嵌入会被相加起来形成输入嵌入（input embeddings）。这个输入嵌入会作为模型的输入，并传递到Transformer的神经网络中进行处理,token本身不会再作为数据传递到模型中。\nInput Embedding为什么解决的是语义问题，没有解决位置问题？？，语义不是有顺序才有吗？？\nInput Embedding (输入嵌入):\ninput_embedding 主要解决的是词汇语义的表示问题。通过将单词映射为连续的低维向量空间，词嵌入技术（如Word2Vec、GloVe等）可以捕获单词之间的语义关系，比如单词的近义词、反义词等。这使得神经网络在处理文本时能够更好地理解单词的含义，从而提高了对语义的建模能力。 但是，词嵌入并没有直接解决词序的问题。即使单词被嵌入到向量空间中，神经网络在处理这些向量时仍然不知道它们在句子中的位置。这就是为什么我们需要进一步引入位置编码的原因。 Positional Encoding (位置编码):\npositional_encoding 解决的是序列数据的位置信息丢失问题。在自然语言处理中，文本是由单词或字符组成的序列，这些单词的排列顺序对句子的含义至关重要。通过引入位置编码，我们可以向神经网络提供关于单词在序列中位置的信息，从而使网络能够区分不同位置的单词并更好地处理序列数据。 位置编码通常是与词嵌入相加的方式来融合位置信息和语义信息。这样，神经网络在处理输入数据时既能考虑单词的语义关系，又能考虑单词在句子中的位置关系，从而更全面地理解文本数据。 因此，input_embedding 和 positional_encoding 两者都是为了帮助神经网络更好地理解文本数据，但它们解决的是不同层面的问题：input_embedding 解决的是语义表示问题，而 positional_encoding 解决的是位置信息丢失问题。这两者结合起来能够提高神经网络对文本数据的建模能力。\n在transformer的self-attention模块中，序列的输入输出如下（不了解self-attention没关系，这里只要关注它的输入输出就行）： 在self-attention模型中，输入是一整排的tokens，对于人来说，我们很容易知道tokens的位置信息，比如：\n（1）绝对位置信息。a1是第一个token，a2是第二个token……\n（2）相对位置信息。a2在a1的后面一位，a4在a2的后面两位……\n（3）不同位置间的距离。a1和a3差两个位置，a1和a4差三个位置….\n但是这些对于self-attention来说，是无法分辩的信息，因为self-attention的运算是无向的。因为，我们要想办法，把tokens的位置信息，喂给模型。\n连续有界 有界又连续的概念是数学中对函数或者集合的性质进行描述的。一个函数或者集合被称为有界的意思是它在某个范围内有限，即它的值不能无限增长或减小；而连续则表示函数或者集合中的元素在某个区间内没有断裂或跳跃。\n举个例子，考虑函数 (f(x) = \\sin(x))。这个函数是有界的，因为正弦函数的值范围在 ([-1, 1]) 之间，不会超出这个范围。而且，正弦函数在定义域内是连续的，没有断点或跳跃。因此，正弦函数 (f(x) = \\sin(x)) 是一个有界又连续的函数。\n另一个例子是闭区间 ([0, 1]) 上的实数集合。这个集合是有界的，因为它的元素都在区间 ([0, 1]) 内；同时，这个集合是连续的，因为在闭区间内没有任何间隔或断裂。\n总的来说，有界又连续的概念在数学中非常常见，许多函数、集合以及数学对象都可以被描述为有界又连续的。\n为什么要有界 在Transformer等模型中，位置编码用于为序列中的不同位置提供唯一的标识，以便模型能够区分不同位置的词语。通常情况下，位置编码是与词嵌入向量相加的，因此需要确保位置编码与词嵌入向量的范围相匹配，以避免结果的数值过大或过小。\n此外，由于模型的输入通常是通过词嵌入向量表示的，而词嵌入向量通常是有限范围的，因此位置编码的范围也被限制在一个合理的范围内，以保持整个输入的稳定性和可训练性。\n因此，尽管位置编码并不一定必须是有界的，但在实践中，为了保持模型的稳定性和可训练性，通常会设计位置编码为有界的。\n为什么要连续 位置编码必须是连续的，因为它们用于表示序列中的位置信息，而序列中的位置是连续的。在自然语言处理任务中，如语言模型或机器翻译，序列中的每个词或标记都对应着一个连续的位置。\n如果位置编码不是连续的，那么模型将无法正确地理解序列中各个位置之间的关系。例如，如果某个位置的位置编码与其相邻位置的位置编码之间存在不连续性，模型可能会误解序列中的顺序关系，从而影响其性能。\n另外，连续的位置编码有助于模型更好地捕捉序列中的局部和全局关系，因为它们可以在连续的空间中表示位置信息，使模型能够更准确地理解序列中不同位置之间的距离和关联。\n综上所述，位置编码必须是连续的，以确保模型能够有效地理解序列中的位置信息，并正确地捕捉序列中的关系和结构。\n位置编码的演变 用整型值标记位置 一种自然而然的想法是，给第一个token标记0，给第二个token标记1…，以此类推。 这种方法产生了以下几个主要问题：\n模型可能遇见比训练时所用的序列更长的序列。不利于模型的泛化。 模型的位置表示是无界的。随着序列长度的增加，位置值会越来越大。 用[0,1]范围标记位置 为了解决整型值带来的问题，可以考虑将位置值的范围限制在[0, 1]之内，其中，0表示第一个token，1表示最后一个token。比如有3个token，那么位置信息就表示成[0, 0.5, 1]；若有四个token，位置信息就表示成[0, 0.33, 0.69, 1]。 但这样产生的问题是，当序列长度不同时，token间的相对距离是不一样的。例如在序列长度为3时，token间的相对距离为0.5；在序列长度为4时，token间的相对距离就变为0.33。 因此，我们需要这样一种位置表示方式，满足于：\n它能用来表示一个token在序列中的绝对位置 在序列长度不同的情况下，不同序列中token的相对位置/距离也要保持一致 可以用来表示模型在训练过程中从来没有看到过的句子长度。 用二进制向量标记位置 考虑到位置信息作用在input embedding上，因此比起用单一的值，更好的方案是用一个和input embedding维度一样的向量来表示位置。这时我们就很容易想到二进制编码。如下图，假设d_model = 3，那么我们的位置向量可以表示成： 这下所有的值都是有界的（位于0，1之间），且transformer中的d_model本来就足够大，基本可以把我们要的每一个位置都编码出来了。 但是这种编码方式也存在问题：这样编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。假设d_model = 2，我们有4个位置需要编码，这四个位置向量可以表示成[0,0],[0,1],[1,0],[1,1]。我们把它的位置向量空间做出来： 用周期函数（sin）来表示位置 sin函数 先回顾下sin函数的几个概念，因为下面要用到sin函数：\n周期 周期是从一个最高点到下一个最高点（或任何一点到下一个相对点）： 振幅，相移，垂直位移 振幅是从中（平）线到最高点的高度（或到最低点），也是从最高点到最低点的距离除以2。 相移是函数比通常的位置水平向右移了多远。 垂直位移是函数比通常的位置垂直向上移了多远。 我们可以全部放进一个方程里：\ny = A sin(Bx + C) + D\n振幅是：A 周期是：2π/B 相移是：−C/B 垂直移位是：D 例子：sin(x) 这是正弦的基本公式。A = 1, B = 1, C = 0 and D = 0\n所以振幅是1，周期是2π，没有相移或垂直移位： 振幅 1，周期 2pi，没有相移或垂直移位 频率 频率是在一个时间单位里发生多少次（每 “1”）。 例子：这个正弦函数在0到1之间重复了4次： 所以频率是 4 周期是 $\\frac{1}{4}$ 其实周期和频率是相连的,周期越大，频率越小： 频率 = $\\frac{1}{周期}$ 周期 = $\\frac{1}{频率}$\n波长 波长λ=vT，其中v是波速，T是周长。波长是一个周期内波前进的距离，而这段周期内波都是匀速直线前进的，所以直接使用匀速直线运动的位移公式即可。\nsin表示位置 回想一下，现在我们需要一个有界又连续的函数，最简单的，正弦函数sin就可以满足这一点。我们可以考虑把位置向量当中的每一个元素都用一个sin函数来表示，则第t个token的位置向量($d_{model}$表示嵌入向量维度)可以表示为： $$ PE_t = [sin(\\frac{1}{2^0}t),sin(\\frac{1}{2^1}t)…,sin(\\frac{1}{2^{i-1}}t), …,sin(\\frac{1}{2^{d_{model}-1}}t)]\\ $$\nPE:位置编码：Positional Encoding，t表示第t个token，i表示位置编码是第几个列。 列sin函数，越往右波长（入*(2π/B)）越长，频率越低。\n说个题外话，说说音量调节，后面会有用： 假设你在调节音量。如果你向右旋转音量旋钮，音量（精度）可能会从低到高逐渐增加。一开始，当音量较低时，每次向右旋转可能只会增加一点音量，这时候你可能希望更细微地调整音量。但是，当音量已经相对较高时，每次向右旋转可能会增加更多的音量，这时候你可能不希望调整得太大，因此需要更小的步进来精确地调整音量。 因此，可以概括，向右旋转旋钮会增加调整参数的精度，也就是每次移动的步幅会变小，以便更精细地调整参数的值。\n言归正传： 结合下图，来理解一下这样设计的含义。图中每一行表示一个$PE_t$，每一列表示$PE_t$中的第i个元素。旋钮用于调整精度，越往右边的旋钮，需要调整的精度越大，因此指针移动的步伐越小。每一排的旋钮都在上一排的基础上进行调整（函数中t的作用）。通过频率sin(12i−1t)sin(\\frac{1}{2^{i-1}}t)sin(2i−11​t)来控制sin函数的波长，频率不断减小，则波长不断变大，此时sin函数对t的变动越不敏感，以此来达到越向右的旋钮，指针移动步伐越小的目的。 由于sin是周期函数，因此从纵向来看，如果当函数的频率增大并导致波长缩短时，意味着波形在相同时间内完成了更多的周期，则不同t下的位置向量可能出现重合的情况。比如在下图中(d_model = 3），图中的点表示每个token的位置向量，颜色越深，token的位置越往后，在频率偏大的情况下，位置相连点连成了一个闭环，靠前位置（黄色）和靠后位置（棕黑色）竟然靠得非常近： 为了避免这种情况，我们尽量将函数的波长拉长。一种简单的解决办法是同一把所有的频率都设成一个非常小的值。因此在transformer的论文中，采用了$\\frac{1}{10000^{i/(d_{model}-1)}}$这个频率（这里i其实不是表示第i个位置，但是大致意思差不多，下面会细说） 总结一下，到这里我们把位置向量表示为： $$ PE_t = [sin(w_0t),sin(w_1t)…,sin(w_{i-1}t), …,sin(w_{d_{model}-1}t)]\\ $$ 其中，$w_{i} = \\frac{1}{10000^{i/(d_{model}-1)}}$\n用sin和cos交替来表示位置 先来回顾下线性变化旋转的相关概念，后续用到。\n线形变换——旋转 在二维坐标系中，一个位置向量的旋转公式可以由三角函数的几何意义推出。 如上图假设：\n已知：假设向量$R_{A}$=(x0，y0) 角度为：A，向右旋转了角度B，新向量$R_{A+B}$角度为：A+B，模：$|\\mathbf{R}|=|\\mathbf{R_{A}}|= |\\mathbf{R_{A+B}}| = \\sqrt{x_0^2 + y_0^2}$。 未知：旋转后向量为:(x1,y1) 上面的命题就是向量$R_{A}$旋转了角度B，求新向量$R_{A+B}$（模大小相同）,$R_{A}$和$R_{A+B}$之间有绝对关系也有相对关系，相对一个角度B，我们需要通过公式来获得一个$R_{A+B}$和$R_{A}$的关系 $R_{A+B}=T_B*R_A$,$T_B$表示一个线性变换矩阵,我们可以通过公式推算出来。 在左图中，我们有关系： $x0 = |R| * cosA =\u003e cosA = x0 / |R|$ $y0 = |R| * sinA =\u003e sinA = y0 / |R|$ 在右图中，我们有关系： $x1 = |R| * cos（A+B）$ $y1 = |R| * sin（A+B）$ 其中（x1， y1）就是（x0， y0）旋转角B后得到的点。我们展开cos（A+B）和sin（A+B），得到： $x1 = |R| * （cosAcosB - sinAsinB）$ $y1 = |R| * （sinAcosB + cosAsinB）$ 现在把 $cosA = x0 / |R|$ 和 $sinA = y0 / |R|$ 代入上面的式子，得到： $x1 = |R| * （x0 * cosB / |R| - y0 * sinB / |R|） =\u003e x1 = x0 * cosB - y0 * sinB$ $y1 = |R| * （y0 * cosB / |R| + x0 * sinB / |R|） =\u003e y1 = x0 * sinB + y0 * cosB$ 这样我们就得到了二维坐标下向量围绕圆点的逆时针旋转公式。顺时针旋转就把角度变为负： $x1 = x0 * cos（-B） - y0 * sin（-B） =\u003e x1 = x0 * cosB + y0 * sinB$ $y1 = x0 * sin（-B） + y0 * cos（-B）=\u003e y1 = -x0 * sinB + y0 * cosB$ 现在我要把这个旋转公式写成矩阵的形式，有一个概念我简单提一下，平面或空间里的每个线性变换（这里就是旋转变换）都对应一个矩阵，叫做变换矩阵。对一个点实施线性变换就是通过乘上该线性变换的矩阵完成的。好了，打住，不然就跑题了。\n现在来将下面公式转换成矩阵 逆时针： $x1 = x0 * cosB - y0 * sinB$ $y1 = x0 * sinB + y0 * cosB$ 所以二维旋转变换矩阵就是： $$ [x, y] * \\left[\\begin{matrix}cosB \u0026 sinB \\ -sinB \u0026 cosB\\end{matrix}\\right] = [xcosB-ysinB ,xsinB+ycosB] $$ 变换矩阵为$\\left[\\begin{matrix}cosB \u0026 sinB \\ -sinB \u0026 cosB\\end{matrix}\\right]$ 顺时针： $x1 = x0 * cosB + y0 * sinB$ $y1 = -x0 * sinB + y0 * cosB$ 同理变换矩阵为：$\\left[\\begin{matrix}cosB \u0026 -sinB \\ sinB \u0026 cosB\\end{matrix}\\right]$\nsin和cos交替表示位置 目前为止，我们的位置向量实现了如下功能：\n每个token的向量唯一（每个sin函数的频率足够小） 位置向量的值是有界的，且位于连续空间中。模型在处理位置向量时更容易泛化，即更好处理长度和训练数据分布不一致的序列（sin函数本身的性质） 那现在我们对位置向量再提出一个要求，不同的位置向量是可以通过线性转换得到的。这样，我们不仅能表示一个token的绝对位置，还可以表示一个token的相对位置(也就是两个token之间得线性关系)，即我们想要： $$ PE_{t+\\bigtriangleup t} = T_{\\bigtriangleup t} * PE_{t} $$ 这里，T表示一个线性变换矩阵。观察下面这个目标式子，联想到在向量空间中一种常用的线形变换——旋转。在这里，我们将t想象为一个角度，那么 $\\bigtriangleup t$就是其旋转的角度，则上面的式子可以进一步写成： $$\\begin{pmatrix} \\sin(t + \\bigtriangleup t)\\ \\cos((t + \\bigtriangleup t) \\end{pmatrix}=\\begin{pmatrix} \\cos\\bigtriangleup t\u0026\\sin\\bigtriangleup t \\ -\\sin\\bigtriangleup t\u0026\\cos\\bigtriangleup t \\end{pmatrix}\\begin{pmatrix} \\sin t\\ \\cos t \\end{pmatrix} $$ 有了这个构想，我们就可以把原来元素全都是sin函数的 $PE_{t}$ 做一个替换，我们让位置两两一组，分别用sin和cos的函数对来表示它们，则现在我们有： $$ PE_t = [sin(w_0t),cos(w_0t), sin(w_1t),cos(w_1t),…,sin(w_{\\frac{d_{model}}{2}-1}t), cos(w_{\\frac{d_{model}}{2}-1}t)]\\ $$ 在这样的表示下，我们可以很容易用一个线性变换，把 $PE_{t}$ 转变为 $PE_{t+\\bigtriangleup t}$ $$PE_{t+\\bigtriangleup t} = T_{\\bigtriangleup t} * PE_{t} =\\begin{pmatrix} \\begin{bmatrix} cos(w_0\\bigtriangleup t)\u0026 sin(w_0\\bigtriangleup t)\\ -sin(w_0\\bigtriangleup t)\u0026 cos(w_0\\bigtriangleup t) \\end{bmatrix}\u0026…\u00260 \\ …\u0026 …\u0026 …\\ 0\u0026 …\u0026 \\begin{bmatrix} cos(w_{\\frac{d_{model}}{2}-1 }\\bigtriangleup t)\u0026 sin(w_{\\frac{d_{model}}{2}-1}\\bigtriangleup t)\\ -sin(w_{\\frac{d_{model}}{2}-1}\\bigtriangleup t)\u0026 cos(w_{\\frac{d_{model}}{2}-1}\\bigtriangleup t) \\end{bmatrix} \\end{pmatrix}\\begin{pmatrix} sin(w_0t)\\ cos(w_0t)\\ …\\ sin(w_{\\frac{d_{model}}{2}-1}t)\\ cos(w_{\\frac{d_{model}}{2}-1}t) \\end{pmatrix} = \\begin{pmatrix} sin(w_0(t+\\bigtriangleup t))\\ cos(w_0(t+\\bigtriangleup t))\\ …\\ sin(w_{\\frac{d_{model}}{2}-1}(t+\\bigtriangleup t))\\ cos(w_{\\frac{d_{model}}{2}-1}(t+\\bigtriangleup t)) \\end{pmatrix}$$ 变换矩阵，也是两个一组和$PE_{t}$进行点乘，变换数组一行就有多组，最后也是个由转换角度+参数(常量)的线性变换。\nTransformer中位置编码方法 Transformer 位置编码定义 有了上面的演变过程后，现在我们就可以正式来看transformer中的位置编码方法了。\n定义：\nt是这个token在序列中的实际位置（例如第一个token为1，第二个token为2…）\n-$PE_t\\in\\mathbb{R}^d$是这个token的位置向量， $PE_{t}^{(i)}$表示这个位置向量里的第i个元素 $d_{model}$是这个token的维度（在论文中，是512) 则 $PE_{t}^{(i)}$ 可以表示为： $$PE_{t}^{(i)} = \\left{\\begin{matrix} \\sin(w_kt),\u0026if\\ i=2k ( 偶数行 ) \\ \\cos(w_kt),\u0026if\\ i = 2k+1(奇数行) \\end{matrix}\\right.$$ 这里： $w_k = \\frac{1}{10000^{2k/d_{model}}}$ $i = 0,1,2,3,…,\\frac{d_{model}}{2} -1$\n注意：当使用 $w_k = \\frac{1}{10000^{2k/d_{\\text{model}}}}$作为位置编码的调节因子时，当 k 增大时，分母中的指数项会变得非常大，可能导致数值溢出或者数值精度问题。为了避免这种情况，可以使用其对数形式$-\\frac{\\log(10000.0)}{d_{\\text{model}}}$这样做有以下几个优点：\n数值稳定性： 对数形式避免了指数项过大导致的数值溢出或者数值精度问题。 计算效率： 对数形式的计算更加高效，避免了重复计算指数项。 一致性： 使用对数形式可以保持代码中的一致性，因为在其他部分可能也会涉及到对数形式的处理。 把512维的向量两两一组，每组都是一个sin和一个cos，这两个函数共享同一个频率$w_i$ ，一共有256组，由于我们从0开始编号，所以最后一组编号是255。sin/cos函数的波长（由 $w_i$ 决定）则从 $2\\pi$增长到 $2\\pi*10000$,下面是代码实现\nclass PositionalEncoding(nn.Module): def __init__(self, d_model, dropout, max_len=5000): \"\"\" 位置编码器类的初始化函数 共有三个参数，分别是 d_model：词嵌入维度 dropout: dropout触发比率 max_len：每个句子的最大长度 \"\"\" super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings # 注意下面代码的计算方式与公式中给出的是不同的，但是是等价的，你可以尝试简单推导证明一下。 # 这样计算是为了避免中间的数值计算结果超出float的范围， pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer('pe', pe) def forward(self, x): x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) return self.dropout(x) 确认是否维度越往后，是否波长越长\nplt.figure(figsize=(15, 5))\rpe = PositionalEncoding(20, 0)\ry = pe.forward(torch.zeros(1, 100, 20))\rplt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\rplt.legend([\"dim %d\"%p for p in [4,5,6,7]]) Transformer位置编码可视化 下图是一串序列长度为100，位置编码维度为512的位置编码可视化结果： 途中y轴表示单词的位置，从0开始到100，横坐标表示每个单词的512维度，颜色表示值，sin，cos函数的值在【-1，1】之间\n可以发现，由于sin/cos函数的性质，位置向量的每一个值都位于[-1, 1]之间。同时，纵向来看，图的右半边几乎都是黄色的，这是因为越往后的位置，频率越小，波长越长，所以不同的t对最终的结果影响不大。而越往左边走，颜色交替的频率越频繁。 代码：\nimport matplotlib.pyplot as plt\rimport numpy as np\r# 设置序列长度和模型维度\rsequence_length = 100 # 序列长度\rd_model = 512 # 模型维度\r# 初始化位置编码矩阵\rpositional_encoding = np.zeros((sequence_length, d_model))\r# 计算位置编码\rfor pos in range(sequence_length):\rfor i in range(d_model):\rif i % 2 == 0:\r# 偶数索引使用正弦函数\rpositional_encoding[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\relse:\r# 奇数索引使用余弦函数\rpositional_encoding[pos, i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\r# 绘制位置编码的图像\rplt.figure(figsize=(10, 8))\rplt.imshow(positional_encoding, cmap='hot', interpolation='nearest')\rplt.title('Positional Encoding')\rplt.xlabel('Depth')\rplt.ylabel('Position')\rplt.colorbar()\rplt.show()",
    "description": "什么是位置编码 在transformer的encoder和decoder的输入层中，使用了Positional Encoding，使得最终的输入满足： $$input = input_embedding + positional_encoding $$\nword embedding：理解参考\n这里，input_embedding是通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model，由于是相加关系，自然而然地，这里的positional_encoding也是一个d_model维度的向量。（在原论文里，d_model = 512） 注意：在Transformer模型中，“token”（标记）是指输入序列中的每个元素，它通常是一个单词、一个子词或一个字符，假设我们有一个句子：“The cat sat on the mat.\"，单词级别的标记： [“The”, “cat”, “sat”, “on”, “the”, “mat”, “.\"]。然后被转换成词嵌入（word embeddings）和位置嵌入（position embeddings），然后这两种嵌入会被相加起来形成输入嵌入（input embeddings）。这个输入嵌入会作为模型的输入，并传递到Transformer的神经网络中进行处理,token本身不会再作为数据传递到模型中。\nInput Embedding为什么解决的是语义问题，没有解决位置问题？？，语义不是有顺序才有吗？？\nInput Embedding (输入嵌入):\ninput_embedding 主要解决的是词汇语义的表示问题。通过将单词映射为连续的低维向量空间，词嵌入技术（如Word2Vec、GloVe等）可以捕获单词之间的语义关系，比如单词的近义词、反义词等。这使得神经网络在处理文本时能够更好地理解单词的含义，从而提高了对语义的建模能力。 但是，词嵌入并没有直接解决词序的问题。即使单词被嵌入到向量空间中，神经网络在处理这些向量时仍然不知道它们在句子中的位置。这就是为什么我们需要进一步引入位置编码的原因。 Positional Encoding (位置编码):\npositional_encoding 解决的是序列数据的位置信息丢失问题。在自然语言处理中，文本是由单词或字符组成的序列，这些单词的排列顺序对句子的含义至关重要。通过引入位置编码，我们可以向神经网络提供关于单词在序列中位置的信息，从而使网络能够区分不同位置的单词并更好地处理序列数据。 位置编码通常是与词嵌入相加的方式来融合位置信息和语义信息。这样，神经网络在处理输入数据时既能考虑单词的语义关系，又能考虑单词在句子中的位置关系，从而更全面地理解文本数据。 因此，input_embedding 和 positional_encoding 两者都是为了帮助神经网络更好地理解文本数据，但它们解决的是不同层面的问题：input_embedding 解决的是语义表示问题，而 positional_encoding 解决的是位置信息丢失问题。这两者结合起来能够提高神经网络对文本数据的建模能力。",
    "tags": [],
    "title": "Transformer模型详解02-Positional Encoding（位置编码）",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_02/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "简介 下图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 Multi-Head Attention，是由多个 Self-Attention组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add \u0026 Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。\n因为 Self-Attention是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。 基础知识 向量的内积 向量的内积是什么，如何计算，最重要的，其几何意义是什么？\n内积的计算方法是将两个向量对应分量相乘，然后将结果相加。 内积的几何意义是非常重要的。在二维空间中，两个向量的内积等于两个向量的模（长度）之积乘以它们之间的夹角的余弦值。具体来说，如果 θ 是两个向量之间的夹角，则它们的内积为： $$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| |\\mathbf{b}| \\cos(\\theta)$$ 这个公式表明，内积可以用来衡量两个向量的相似程度。当两个向量的夹角为 0时(cos0=1)，它们的内积取得最大值，表示它们的方向相同；当夹角为 90时(cos90=0)，内积为 0，表示它们的方向垂直；当夹角为180(cos180=-1) 时，内积取得最小值，表示它们的方向相反。\n矩阵与转置相乘 一个矩阵 与其自身的转置相乘，得到的结果有什么意义？ 矩阵的对称性指的是矩阵在某种变换下保持不变的性质。对称矩阵是一种特殊的矩阵，它满足以下性质：矩阵的转置等于它自身。 具体来说，对称矩阵 A 满足以下条件： $$\\mathbf{A} = \\mathbf{A}^\\intercal$$ 这意味着矩阵的主对角线上的元素保持不变，而其他元素关于主对角线对称。\n例如，如果一个矩阵 A 的元素为： $$\\mathbf{A} = \\begin{pmatrix} a \u0026 b \u0026 c \\ b \u0026 d \u0026 e \\ c \u0026 e \u0026 f \\end{pmatrix}$$ 矩阵中的元素对称于主对角线。 对称矩阵在数学和工程领域中非常重要，因为它们具有许多有用的性质，比如特征值都是实数、可以通过正交变换对角化等。在应用中，对称矩阵广泛用于描述对称系统、表示物理现象等。\n当一个矩阵与其自身的转置相乘时，得到的结果矩阵具有重要的性质，其中最显著的是结果矩阵是一个对称矩阵。这个性质在许多领域中都有重要的应用，比如在统计学中用于协方差矩阵的计算，以及在机器学习中用于特征提取和数据降维。\n让我们用一个具体的矩阵示例来演示这个性质。考虑一个 $3 \\times 2$的矩阵 的矩阵$\\mathbf{A}$： $$\\mathbf{A} = \\begin{pmatrix} 1 \u0026 2 \\ 3 \u0026 4 \\ 5 \u0026 6 \\end{pmatrix}$$ 首先，我们计算 A 的转置 $\\mathbf{A}^\\intercal$ $$\\mathbf{A}^\\intercal = \\begin{pmatrix} 1 \u0026 3 \u0026 5 \\ 2 \u0026 4 \u0026 6 \\end{pmatrix}$$ 然后，我们将 𝐴 相乘$\\mathbf{A}^\\intercal$，得到结果矩阵 $\\mathbf{A} \\mathbf{A}^\\intercal$ $$\\mathbf{A} \\mathbf{A}^\\intercal = \\begin{pmatrix} 1 \u0026 2 \\ 3 \u0026 4 \\ 5 \u0026 6 \\end{pmatrix} \\begin{pmatrix} 1 \u0026 3 \u0026 5 \\ 2 \u0026 4 \u0026 6 \\end{pmatrix} = \\begin{pmatrix} 5 \u0026 11 \u0026 17 \\ 11 \u0026 25 \u0026 39 \\ 17 \u0026 39 \u0026 61 \\end{pmatrix}$$\n可以观察到，结果矩阵 $\\mathbf{A} \\mathbf{A}^\\intercal$是一个对称矩阵。\nQ,K,V 在Transformer模型中，Q（Query）、K（Key）和V（Value）在注意力机制中起着关键作用。让我们通过一个简单的例子来理解它们的物理意义：\n假设我们要翻译一段文本，比如将英文句子 “The cat sat on the mat” 翻译成法文。在这个例子中，Q、K 和 V 可以被解释为：\nQuery（查询）：在翻译时，Query表示当前正在翻译的单词或者短语。例如，当我们尝试翻译 “sat” 这个词时，“sat” 就是当前的 Query。\nKey（键）：Key表示源语言（英文）中其他位置的信息，用于与当前 Query 进行比较。在翻译任务中，Key可以是源语言句子中的其他单词或者短语。比如，在翻译 “sat” 时，Key 可能是源语言句子中的 “The”、“cat”、“on” 等单词。\nValue（值）：Value包含了与 Key 相关的实际数值信息。在翻译任务中，Value 可以是源语言句子中与 Key 对应的词语的嵌入向量或者表示。比如，与 Key “The” 相关的 Value 可能是 “Le”，与 Key “cat” 相关的 Value 可能是 “chat”，等等。\n在注意力机制中，系统会计算当前 Query（如 “sat”）与所有 Key（如 “The”、“cat”、“on”）之间的相关性得分，然后使用这些得分对 Value（如 “Le”、“chat”）进行加权求和，以产生最终的翻译输出。这样，模型可以根据输入的 Query（即要翻译的单词或短语）选择性地关注源语言句子中与之相关的信息，并生成相应的翻译结果。\n通过上面的例子稍微理解Q,K,V概念后，对后续理解公式有帮助。\n什么是Attention 所谓Attention，顾名思义：注意力，意思是处理一个问题的时候把\"注意力\"放到重要的地方上。Attention思想其实是从人类的习惯中提取出来的。人们在第一次看一张照片的时候，第一眼一定落到这张照片的某个位置上，可能是个显著的建筑物，或者是一个有特点的人等等，总之，人们通常并没有看清图片的全部内容，而是将注意力集中在了图片的焦点上。\n2017年的某一天,Google 机器翻译团队发表了《Attention is All You Need》这篇论文，犹如一道惊雷，Attention横空出世了！（有一说一，这标题也太他喵嚣张了，不过人家有这个资本(oﾟ▽ﾟ)o ）\nAttention 机制最早是在计算机视觉里应用的，随后在NLP领域也开始应用了，真正发扬光大是在NLP领域，由于2018年GPT模型的效果显著，Transformer和Attention这些核心才开始被大家重点关注。\n下面举个例子上说明一下注意力和自注意力，可能不够严谨，但足以说明注意力和自注意力是什么了。 首先我们不去考虑得到注意力分数的细节，而是把这个操作认为是一个封装好的函数。比如定义为attention_score(a,b)，表示词a和b的注意力分数。现在有两个句子A=“you are beautiful”和B=“你很漂亮”，我们想让B句子中的词“你”更加关注A句子中的词“you”，该怎么做呢？答案是对于每一个A句子中的词，计算一下它与“you”的注意力分数。也就是把\nattention_score(“you”,“你”)\rattention_score(“are”,“你”)\rattention_score(“beautiful”,“你”) 都计算一遍，在实现attention_score这个函数的时候，底层的运算会让相似度比较大的两个词分数更高，因此attention_score(“you”,“你”)的分数最高，也相当于告诉了计算机，在对B句子中“你”进行某些操作的时候，你应该更加关注A句子中的“you”，而不是“are”或者“beautiful”。 以上这种方式就是注意力机制，两个不同的句子去进行注意力的计算。而当句子只有一个的时候，只能去计算自己与自己的注意力，这种方式就是自注意力机制。比如只看A句子，去计算\nattention_score(“you”,“you”)\rattention_score(“are”,“you”)\rattention_score(“beautiful”,“you”) 这种方式可以把注意力放在句子内部各个单词之间的联系，非常适合寻找一个句子内部的语义关系。\n再举个例子比如这句话“这只蝴蝶真漂亮，停在花朵上，我很喜欢它”，我们怎么知道这个“它”指的是“蝴蝶”还是“花朵”呢？答案是用自注意力机制计算出这个“它”和其他所有输入词的“分数”，这个“分数”一定程度上决定了其他单词与这个联系。可以理解成越相似的，分就越高（通过权重来控制）。通过计算，发现对于“它”这个字，“蝴蝶”比“花朵”打的分高。所以对于“它”来说，“蝴蝶”更重要，我们可以认为这个“它”指的就是蝴蝶。\nSelf Attention 原理 通俗易懂理解 在人类的理解中，对待问题是有明显的侧重。具体举个例子来说：“我喜欢踢足球，更喜欢打篮球。”，对于人类来说，显然知道这个人更喜欢打篮球。但对于深度学习来说，在不知道”更“这个字的含义前，是没办法知道这个结果的。所以在训练模型的时候，我们会加大“更”字的权重，让它在句子中的重要性获得更大的占比。比如： $$C(seq) = F(0.1d(我)，0.1d(喜)，…，0.8d(更)，0.2d(喜)，…) $$ 在知道了attention在机器学习中的含义之后（下文都称之为注意力机制）。人为设计的注意力机制，是非常主观的，而且没有一个准则来评定，这个权重设置为多少才好。所以，如何让模型自己对变量的权重进行自赋值成了一个问题，这个权重自赋值的过程也就是self-attention。\n定义：假设有四个输入变量$a^1$，$a^2$，$a^3$，$a^4$，希望它们经过一个self-attention layer之后变为$b^1$，$b^2$，$b^3$，$b^4$ 拿$a^1$和$b^1$做例子,$b^1$这个结果是综合了$a^1$，$a^2$，$a^3$，$a^4$而得出来的一个结果。既然得到一个b 是要综合所有的a才行，那么最直接的做法就是$a^1$与$a^2$，$a^3$，$a^4$ 都做一次运算，得到的结果就代表了这个变量的注意力系数。直接做乘法太暴力了，所以选择一个更柔和的方法：引入三个变量$W^q$,$W^k$,$W^v$这三个变量与$a^1$相乘得到$q^1$,$k^1$,$v^1$ 同样的方法对$a^2$，$a^3$，$a^4$都做一次,至于这里的q , k , v具体代表什么，下面就慢慢展开讲解。 然后拿自己的q与别人的k相乘就可以得到一个系数$\\alpha$。这里$q^1$在和其他的k做内积时，可近似的看成是在做相似度计算(前面基础向量的内积)。比如： $$ \\alpha_{1,1} =q^1\\cdot k^1\\ \\alpha_{1,2} =q^1\\cdot k^2\\ \\alpha_{1,3} =q^1\\cdot k^3\\ \\alpha_{1,4} =q^1\\cdot k^4\\ $$\n在实际的神经网络计算过程中，还得除于一个缩放系数$\\sqrt{d}$这个d是指q和k的维度,因为q和k会做内积，所以维度是一样的。之所以要除$\\sqrt{d}$​，是因为做完内积之后，，$\\alpha$会随着它们的维度增大而增大，除$\\sqrt{d}$相当于标准化。 得到了四个$\\alpha$之后，我们分别对其进行softmax，得到四个 $\\hat{\\alpha}_1$，增加模型的非线性。 四个$\\alpha$分别是 $\\hat{\\alpha}_1$,$\\hat{\\alpha}2$,$\\hat{\\alpha}3$,$\\hat{\\alpha}4$,别忘了还有我们一开始计算出来的 ${v}^1$,${v}^2$,${v}^3$,${v}^4$，直接把各个$\\hat{\\alpha}1$直接与各个a 相乘不就得出了最后的结果了吗？虽然这么说也没错，但为了增加网络深度，将a变成v也可以减少原始的a对最终注意力计算的影响。 那么距离最后计算出$b^1$只剩最后一步，我们将所有的$\\hat{\\alpha}1$ 与所有的v分别相乘，然后求和，就得出$b^1$啦！具体计算如下： $$b^1=\\hat{\\alpha}{1,1}*v^1+\\hat{\\alpha}{1,2}*v^2+\\hat{\\alpha}{1,3}*v^3+\\hat{\\alpha}{1,4}*v^4 $$ 公式简化为： $$b^1=\\sum_i\\hat{\\alpha}{1,i}*v^i $$ 同样的计算过程，我们对剩下的a都进行一次，就可以得到$b^2$,$b^3$,$b^4$,每个b都是综合了每个a之间的相关性计算出来的，这个相关性就是我们所说的注意力机制,。那么我们将这样的计算层称为self-attention layer。 我们把一个句子中的每个字代入上图的 $x^1$，$x^2$ ,$x^3$， $x^4$\n矩阵计算 通过注意力计算出来的结果是每个位置单词的上下文表示，每个位置的上下文表示是指在自注意力机制中，通过将每个位置的词嵌入向量与注意力权重进行加权求和，得到的每个位置的语义表示。这个语义表示包含了输入序列中每个位置的语义信息，经过加权后更加全局和丰富。\n举个例子来说明：\n假设我们有一个输入序列：“The cat sat on the mat.\"，并且使用 Transformer 模型进行编码，其中每个单词对应一个位置。在自注意力机制中，模型会计算每个位置对其他位置的注意力权重，然后将这些权重与对应位置的词嵌入向量进行加权求和，得到每个位置的上下文表示。\n考虑位置 3，对应单词 “sat”。在计算注意力权重时，模型会考虑 “sat” 与其他单词之间的关联程度。假设在这个例子中，“sat” 与 “cat”、“mat” 之间有较高的注意力权重，而与 “on” 的关联较低。因此，经过加权求和后，位置 3 的上下文表示将会强调 “sat” 与 “cat”、“mat” 之间的语义关系。\n通过这种方式，每个位置的上下文表示会受到整个输入序列中所有位置的影响，从而更好地捕捉输入序列的语义结构和信息。\n上图是 Self-Attention 的结构，在计算的时候需要用到矩阵Q(查询),K(键值),V(值)。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而Q,K,V正是通过 Self-Attention 的输入进行线性变换得到的。\nQ，K，V计算 输入矩阵X=position encoding+word embedding,其中维度d_model，行为句子中单词的个数。 需要知道x的格式参考：Word2Vec实例\nSelf-Attention 的输入用矩阵X进行表示，则可以使用线性变阵矩阵WQ,WK,WV计算得到Q,K,V。计算 如下图所示，注意 X, Q, K, V 的每一行都表示一个单词，WQ，WK，WV是一个d_model(输入矩阵的列)行的线性变阵参数，X的每一行都会都会有自己的QKV，比如$X_1$对应$Q_1,K_1,V_1$,即$X_n$对应$Q_n,K_n,V_n$，所以 X, Q, K, V 的每一行都表示一个单词。 这里通过线性变换的Q,K,V的物理意义是包含了原始数据的信息，可能关注的特征点不一样。\nSelf-Attention 的输出 得到矩阵 Q, K, V之后就可以计算出 Self-Attention 的输出了，计算的公式如下： 公式中计算矩阵Q和K每一行向量的内积，为了防止内积过大，因此除以$d_k$的平方根，缩放注意力，以使得注意力分布的方差在不同维度上保持一致，从而更好地控制梯度的稳定性。。Q乘以K的转置后，得到的矩阵行列数都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为Q乘以$K^T$, 1234 表示的是句子中的单词。 $QK^T$其实终算出来的是，每一个单词与其他所有的单词的注意力系数，因为Q代表单词本身假如 我有一只猫，分词后1=我，2=有，3=一只，4=猫。 因为K代表其他单词，转至相乘最终$QK^T$矩阵的(1,1)这个各自就是标识我和我的注意力权重，(1,2)就是我和有的注意力权重，(2,4)就是有和猫的注意力权重\n得到$QK^T$之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1. 得到 Softmax 矩阵之后可以和V相乘，得到最终的输出Z。 上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出$Z_1$等于所有单词 i 的值$V_i$根据 attention 系数的比例加在一起得到，如下图所示： 这里算出来的$Z_1$第一行就是第一个单词和其他单词的注意力系数权重，\n优势 从self-attention的原理中可以看出，这一层需要学习的参数只有$W^q$ ,$W^k$, $W^v$，大部分变量来自于内部计算得出来的，所以它的参数量少但每个参数所涵盖的信息多，这是它的第一个优点。 每个b的计算都是独立的，这一点相比之前的RNN来说很不一样，RNN是需要等前面的$a^1$算完了才能算$a^2$，是串行的。所以RNN无论是训练还是推理，都会因为不能计算并行而变慢，这是它的的第二个优点。 RNN的一个最大的问题是：前面的变量在经过多次RNN计算后，已经失去了原有的特征。越到后面，最前面的变量占比就越小，这是一个很反人类的设计。而self-attention在每次计算中都能保证每个输入变量 a的初始占比是一样的，这样才能保证经过self-attention layer计算后他的注意力系数是可信的。 所以总结下来，它的三个优点分别是：\n需要学习的参数量少 可以并行计算 能够保证每个变量初始占比是一样的 代码实现 import torch\rimport torch.nn as nn\rimport torch.nn.functional as F\rclass SelfAttention(nn.Module):\rdef __init__(self, embed_size, num_heads):\r\"\"\"\r初始化 SelfAttention 层\r参数:\rembed_size (int): 输入特征的维度\rnum_heads (int): 注意力头的数量\r\"\"\"\rsuper(SelfAttention, self).__init__()\rself.embed_size = embed_size\rself.num_heads = num_heads\rself.head_dim = embed_size // num_heads\r# 确保 embed_size 能被 num_heads 整除\rassert (\rself.head_dim * num_heads == embed_size\r), \"Embedding size needs to be divisible by heads\"\r# 初始化线性层\rself.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\rself.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\rself.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\rself.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\rdef forward(self, values, keys, query):\r\"\"\"\r前向传播函数\r参数:\rvalues (Tensor): 值的张量，形状为 (batch_size, value_len, embed_size)\rkeys (Tensor): 键的张量，形状为 (batch_size, key_len, embed_size)\rquery (Tensor): 查询的张量，形状为 (batch_size, query_len, embed_size)\r返回:\rout (Tensor): 输出张量，形状为 (batch_size, query_len, embed_size)\r\"\"\"\r# 获取张量的大小\rN = query.shape[0]\rvalue_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\r# 将输入张量按头数和头维度进行切分\rvalues = values.reshape(N, value_len, self.num_heads, self.head_dim)\rkeys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\rqueries = query.reshape(N, query_len, self.num_heads, self.head_dim)\r# 通过线性层进行变换\rvalues = self.values(values)\rkeys = self.keys(keys)\rqueries = self.queries(queries)\r# 计算点积注意力\renergy = torch.einsum(\"nqhd,nkhd-\u003enhqk\", [queries, keys]) # batch_size, num_heads, query_len, key_len\r# 计算注意力权重\rattention = torch.nn.functional.softmax(energy / (self.embed_size ** (1/2)), dim=3)\r# 将注意力权重应用到值上\rout = torch.einsum(\"nhql,nlhd-\u003enqhd\", [attention, values]).reshape(\rN, query_len, self.num_heads * self.head_dim\r)\r# 合并多个头并通过线性层进行变换\rout = self.fc_out(out)\rreturn out Multi-head self-attention 为什么要多头 在多头注意力机制中，每个注意力头学习不同的特征表示，这是为了提高模型的表征能力和泛化能力。这种设计允许模型在不同抽象级别上关注输入的不同部分，从而更好地捕获输入之间的关系。\n具体来说，每个注意力头都有自己的权重矩阵（通常是通过学习得到的），这些权重矩阵决定了每个头对输入的不同部分的关注程度。通过允许多个头并且每个头学习不同的特征表示，模型可以同时关注输入的不同方面，从而更好地捕获输入之间的复杂关系。\n举例来说，考虑一个用于自然语言处理的 Transformer 模型。在这种情况下，每个注意力头可以学习关注句子中的不同单词或短语，其中一些头可能更关注主语-谓语关系，另一些头可能更关注宾语-谓语关系，而其他头可能关注句子中的修饰词或者语法结构等。通过允许每个头学习不同的特征表示，模型可以更好地捕获句子中不同部分之间的语义关系，从而提高了模型的性能。\n总的来说，多头注意力机制允许模型以多个不同的视角来观察输入数据，从而提高了模型对输入数据的表征能力和泛化能力。\n原理 通俗易懂理解 multi-head self-attention，所谓head也就是指一个a 衍生出几个q , k , v 。上述所讲解的self-attention是基于single-head的。以2 head为例： 首先，$a^i$先生成$q^1$,$k^1$,$v^1$,然后，接下来就和single-head不一样了，$q^i$生成$q^{i,1},q^{i,2}$生成的方式有两种：\n$q^i$乘上一个$W^{q,1}$得到$q^{i,2}$，这个和single-head的生成是差不多的； $q^i$直接从通道维，平均拆分成两个，得到$q^{i,1},q^{i,2}$ 这两种方式，在最后结果上都差不多。至于为啥，后面会讲一下原因。 那么这里的图解使用第1个方式，先得到$q^{i,1}$,$k^{i,1}$,$v^{i,1}$。对$a^j$做同样的操作得到 ,对$a^j$做同样的操作得到$q^{j,1}$,$k^{j,1}$,$v^{j,1}$。这边需要注意的一点，$q^{i,1}$是要和$k^{j,1}$做矩阵乘法，而非$k^{j,2}$，一一对应。后面计算就和single-head一样了，最后得到$b^{i,1}$ 第二步，对$q^{i,2}$,$k^{i,2}$,$v^{i,2}$做一样的操作，得到$b^{i,2}$ 这里我们算出的$b^{i,1}$,$b^{i,2}$是同维度的，我们可以将其concat在一起，再通过一个$W^0$把他转成想要的维度。这也就不难理解，为什么说multi-head的两种生成方式是一样的，因为最终决定是输出维度的是$W^o$。我们可以将multi-head的过程看成是cnn中的隐藏层，multi-head的数量也就对应着Conv2D的filter数量，每一个head各司其职，提取不同的特征。 矩阵计算 我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 Z，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。 从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入X分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵Z。下图是 h=8 时候的情况，此时会得到 8 个输出矩阵Z。 得到 8 个输出矩阵$Z_1$到$Z_8$之后，Multi-Head Attention 将它们拼接在一起 (Concat)，然后传入一个Linear层，得到 Multi-Head Attention 最终的输出Z。 可以看到 Multi-Head Attention 输出的矩阵Z与其输入的矩阵X的维度是一样的。\n代码实现 import torch\rimport torch.nn.functional as F\rclass MultiHeadSelfAttention(torch.nn.Module):\rdef __init__(self, d_model, num_heads):\rsuper(MultiHeadSelfAttention, self).__init__()\rself.num_heads = num_heads\rself.d_model = d_model\rassert d_model % num_heads == 0 # 确保 d_model 可以被 num_heads 整除\r# 初始化 Q、K、V 矩阵和输出矩阵\rself.W_q = torch.nn.Linear(d_model, d_model)\rself.W_k = torch.nn.Linear(d_model, d_model)\rself.W_v = torch.nn.Linear(d_model, d_model)\rself.W_o = torch.nn.Linear(d_model, d_model)\rdef forward(self, x):\rbatch_size, seq_len, d_model = x.size()\r# 将输入 x 拆分成 num_heads 个头\rQ = self.W_q(x).view(batch_size, seq_len, self.num_heads, d_model // self.num_heads)\rK = self.W_k(x).view(batch_size, seq_len, self.num_heads, d_model // self.num_heads)\rV = self.W_v(x).view(batch_size, seq_len, self.num_heads, d_model // self.num_heads)\r# 对每个头进行 scaled dot-product attention\rattention_scores = torch.matmul(Q, K.transpose(1, 2)) / (d_model ** 0.5)\rattention_probs = F.softmax(attention_scores, dim=-1)\rattention_output = torch.matmul(attention_probs, V)\r# 将每个头的输出拼接起来\rattention_output = attention_output.view(batch_size, seq_len, d_model)\r# 经过线性变换得到最终的输出\routput = self.W_o(attention_output)\rreturn output\r# 为了测试\rd_model = 512 # 模型的维度\rnum_heads = 8 # 头的数量\rseq_len = 10 # 序列长度\rbatch_size = 4 # 批次大小\r# 创建一个随机输入张量\rx = torch.rand(batch_size, seq_len, d_model)\r# 创建 Multi-Head Self-Attention 模块并进行前向传播\rmultihead_attention = MultiHeadSelfAttention(d_model, num_heads)\routput = multihead_attention(x)\r# 打印输出张量的形状\rprint(\"Output shape:\", output.shape)",
    "description": "简介 下图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 Multi-Head Attention，是由多个 Self-Attention组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add \u0026 Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。\n因为 Self-Attention是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。 基础知识 向量的内积 向量的内积是什么，如何计算，最重要的，其几何意义是什么？\n内积的计算方法是将两个向量对应分量相乘，然后将结果相加。 内积的几何意义是非常重要的。在二维空间中，两个向量的内积等于两个向量的模（长度）之积乘以它们之间的夹角的余弦值。具体来说，如果 θ 是两个向量之间的夹角，则它们的内积为： $$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| |\\mathbf{b}| \\cos(\\theta)$$ 这个公式表明，内积可以用来衡量两个向量的相似程度。当两个向量的夹角为 0时(cos0=1)，它们的内积取得最大值，表示它们的方向相同；当夹角为 90时(cos90=0)，内积为 0，表示它们的方向垂直；当夹角为180(cos180=-1) 时，内积取得最小值，表示它们的方向相反。",
    "tags": [],
    "title": "Transformer模型详解03-Self-Attention（自注意力机制）",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_03/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "简介 Transformer 模型中的 Encoder 层主要负责将输入序列进行编码，将输入序列中的每个词或标记转换为其对应的向量表示，并且捕获输入序列中的语义和关系。\n具体来说，Transformer Encoder 层的作用包括：\n词嵌入（Word Embedding）：将输入序列中的每个词或标记映射为其对应的词嵌入向量。这些词嵌入向量包含了词语的语义信息，并且可以在模型中进行学习。\n位置编码（Positional Encoding）：因为 Transformer 模型不包含任何关于序列顺序的信息，为了将位置信息引入模型，需要添加位置编码。位置编码是一种特殊的向量，用于表示输入序列中每个词的位置信息，以便模型能够区分不同位置的词。\n多头自注意力机制（Multi-Head Self-Attention）：自注意力机制允许模型在处理每个词时，同时考虑到输入序列中所有其他词之间的关系。多头自注意力机制通过将输入进行多次线性变换并计算多组注意力分数，从而允许模型在不同的表示子空间中学习到不同的语义信息。\n残差连接（Residual Connection）：为了减轻梯度消失和加速训练，Transformer Encoder 层使用了残差连接。残差连接允许模型直接学习到输入序列的增量变换，而不是完全替代原始输入。\n层归一化（Layer Normalization）：在残差连接后应用层归一化，有助于提高模型的训练稳定性，加快训练速度。\nTransformer Encoder 层的主要作用是将输入序列转换为其对应的向量表示，并且捕获输入序列中的语义和位置信息，以便后续的模型能够更好地理解和处理输入序列。\n前面我们已经详解了三个点的计算过程，现在了解一下 Add \u0026 Norm 和 Feed Forward 部分。\n基础知识 归一化 归一化是将数据转换为具有统一尺度的过程，常用于机器学习、数据挖掘和统计分析中。归一化可以确保不同特征或变量之间具有相似的数值范围，有助于提高模型的性能和收敛速度。\n作用 让我用一个简单的例子来说明归一化的作用。\n假设你有一个数据集，其中包含两个特征：年龄和收入。年龄的范围是 0 到 100 岁，而收入的范围是 1000 到 100000 美元。这两个特征的范围差异很大。\n现在，你想要使用这些特征来训练一个机器学习模型，比如线性回归模型，来预测一个人是否会购买某种产品。由于特征的范围差异较大，这可能会导致某些问题：\n收入的范围比年龄大得多，这可能会使得模型过度关注收入而忽略年龄，因为收入的变化可能会对预测产生更大的影响。 模型可能会受到数值范围的影响，而不是特征本身的重要性。 这时候，归一化就可以派上用场了。通过归一化，你可以将不同特征的值缩放到相似的范围内，从而消除数值范围差异带来的影响。比如，你可以将年龄和收入都缩放到 0 到 1 之间的范围内，或者使用其他归一化方法，如标准化 (standardization)。\n通过归一化，你可以确保模型不会因为特征值的范围差异而偏向某个特定的特征，而是可以更平衡地利用所有的特征信息来进行预测。\n常用归一化 下面是几种常用的归一化方式及其公式：\nMin-Max 归一化： Min-Max 归一化将数据线性映射到一个指定的范围内，通常是 [0, 1] 或 [-1, 1]。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X - X_{\\text{min}}}}{{X_{\\text{max}} - X_{\\text{min}}}}]$$\n其中，$(X_{\\text{norm}})$ 是归一化后的数据，(X) 是原始数据，$(X_{\\text{min}})$ 和 $(X_{\\text{max}})$分别是数据的最小值和最大值。\nZ-Score 标准化： Z-Score 标准化将数据转换为均值为 0，标准差为 1 的正态分布。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X - \\mu}}{{\\sigma}}]$$\n其中，$(X_{\\text{norm}})$是归一化后的数据，$(X)$ 是原始数据，$\\mu$是数据的均值，$(\\sigma)$是数据的标准差。\nDecimal Scaling 归一化： Decimal Scaling 归一化将数据缩放到[-1,1]或者[0,1]的范围内，通过除以数据中的最大绝对值来实现。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X}}{{\\max(|X|)}}]$$\n其中，$(X_{\\text{norm}})$ 是归一化后的数据，$(X)$ 是原始数据，$(\\max(|X|))$ 是数据中的最大绝对值。\nRobust Scaling： Robust Scaling 是一种针对离群值鲁棒的归一化方法，通过除以数据的四分位距（IQR）来缩放数据。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X - Q_1}}{{Q_3 - Q_1}}]$$\n其中，$(X_{\\text{norm}})$ 是归一化后的数据，$(X)$是原始数据，$(Q_1)$ 是数据的第一四分位数（25th percentile），$(Q_3)$ 是数据的第三四分位数（75th percentile）。\n这些是常用的归一化方式，选择适合你的数据和模型的归一化方法可以提高模型的性能和稳定性。\n残差连接 残差连接（Residual Connection）是一种在深度神经网络中用于解决梯度消失和梯度爆炸问题的技术。它通过将输入直接添加到神经网络的某些层的输出中，从而允许梯度直接通过残差路径传播，减轻了梯度消失的问题，加速了训练过程。\n具体来说，假设我们有一个包含多个层的神经网络，每个层都由输入 $x$ 经过一些变换 $F(x)$得到输出 $H(x)$。传统的神经网络会直接将 $H(x)$ 作为下一层的输入，而残差连接则是将 $x$ 与 $H(x)$ 相加，即 $H(x)+x$，然后再输入到下一层。这样做可以使得网络学习到的变换是相对于输入的增量，而不是完全替代原始输入。\n残差连接的作用包括：\n缓解梯度消失：通过保留原始输入的信息，使得梯度可以更容易地传播到较浅层，从而减轻了梯度消失问题。 加速训练：残差连接可以使得神经网络更快地收敛，因为它减少了训练过程中的信息丢失。 提高模型性能：残差连接使得神经网络可以更深，更复杂，从而能够更好地捕捉输入数据的特征和模式。 举个例子，考虑一个包含残差连接的深度残差网络（Residual Network，ResNet）。在这个网络中，每个残差块都由两个或多个卷积层组成，其中第一个卷积层产生特征图 $H(x)$，而第二个卷积层则对 $H(x)$ 进行进一步变换。然后，原始输入 $x$ 被添加到 $H(x)$ 上，得到 $F(x)=H(x)+x$。这样，输出 $F(x)$ 就包含了相对于输入 $x$ 的增量，网络可以更轻松地学习到残差部分，从而更有效地优化模型。\nAdd \u0026 Norm Add \u0026 Norm 层由 Add 和 Norm 两部分组成，其计算公式如下： 第一个Add\u0026Norm中Add指 X+MultiHeadAttention(X)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 ResNet 中经常用到： Norm指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。\nFeed Forward Feed Forward 层比较简单，是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下。 $$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$ 也就是： 在这个公式中：\n$(X)$ 是输入的隐藏表示，维度为 $(d_{\\text{model}})$，是Add\u0026Norm输出； $(W_1)$ 和 $(W_2)$ 是权重矩阵，分别用于第一层和第二层的线性变换，维度分别为 $(d_{\\text{model}} \\times d_{\\text{ff}})$ 和 $(d_{\\text{ff}} \\times d_{\\text{model}})$； $(b_1)$ 和 $(b_2)$ 是偏置项； $(\\text{ReLU})$ 表示修正线性单元，是一种非线性激活函数，用于引入模型的非线性性。 Feed Forward 最终得到的输出矩阵的维度与X一致。\nFeed Forward 层在深度学习模型中具有重要意义，它主要有以下几个方面的作用：\n特征变换与组合： Feed Forward 层通过线性变换和非线性激活函数将输入数据进行特征变换和组合，使得模型能够学习到更高级、更复杂的特征表示。这有助于模型更好地理解数据的内在结构和规律。\n引入非线性： 非线性激活函数（如 ReLU、sigmoid、tanh 等）可以引入非线性变换，从而使得模型能够学习到非线性关系，提高模型的表达能力。如果没有非线性变换，多个线性变换的组合仍然只会得到线性变换，模型的表达能力将受到限制。\n增加模型的深度： Feed Forward 层通常是深度神经网络中的一个组成部分，通过堆叠多个 Feed Forward 层可以构建深度模型。深度模型能够学习到更多层次、更抽象的特征表示，从而提高模型的性能和泛化能力。\n提高模型的泛化能力： Feed Forward 层通过特征变换和非线性变换有助于模型学习到数据的高级抽象表示，这有助于提高模型对新样本的泛化能力，使得模型更好地适应未见过的数据。\n组成 Encoder 通过上面描述的 Multi-Head Attention, Feed Forward, Add \u0026 Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵$X_(nd)$, 并输出一个矩阵$O_(nd)$,通过多个 Encoder block 叠加就可以组成 Encoder。 第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是编码信息矩阵 C，这一矩阵后续会用到 Decoder 中。 代码实现 import torch\rimport torch.nn as nn\rimport torch.nn.functional as F\rclass TransformerEncoderLayer(nn.Module):\rdef __init__(self, d_model, n_heads, d_ff, dropout=0.1):\rsuper(TransformerEncoderLayer, self).__init__()\rself.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\rself.linear1 = nn.Linear(d_model, d_ff)\rself.linear2 = nn.Linear(d_ff, d_model)\rself.dropout = nn.Dropout(dropout)\rself.norm1 = nn.LayerNorm(d_model)\rself.norm2 = nn.LayerNorm(d_model)\rdef forward(self, src, src_mask=None):\r# Multi-head self-attention\rsrc2 = self.self_attn(src, src, src, attn_mask=src_mask)[0]\rsrc = src + self.dropout(src2)\rsrc = self.norm1(src)\r# Feed Forward Layer\rsrc2 = self.linear2(F.relu(self.linear1(src)))\rsrc = src + self.dropout(src2)\rsrc = self.norm2(src)\rreturn src\rclass TransformerEncoder(nn.Module):\rdef __init__(self, num_layers, d_model, n_heads, d_ff, dropout=0.1):\rsuper(TransformerEncoder, self).__init__()\rself.layers = nn.ModuleList([TransformerEncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\rdef forward(self, src, src_mask=None):\rfor layer in self.layers:\rsrc = layer(src, src_mask)\rreturn src",
    "description": "简介 Transformer 模型中的 Encoder 层主要负责将输入序列进行编码，将输入序列中的每个词或标记转换为其对应的向量表示，并且捕获输入序列中的语义和关系。\n具体来说，Transformer Encoder 层的作用包括：\n词嵌入（Word Embedding）：将输入序列中的每个词或标记映射为其对应的词嵌入向量。这些词嵌入向量包含了词语的语义信息，并且可以在模型中进行学习。\n位置编码（Positional Encoding）：因为 Transformer 模型不包含任何关于序列顺序的信息，为了将位置信息引入模型，需要添加位置编码。位置编码是一种特殊的向量，用于表示输入序列中每个词的位置信息，以便模型能够区分不同位置的词。\n多头自注意力机制（Multi-Head Self-Attention）：自注意力机制允许模型在处理每个词时，同时考虑到输入序列中所有其他词之间的关系。多头自注意力机制通过将输入进行多次线性变换并计算多组注意力分数，从而允许模型在不同的表示子空间中学习到不同的语义信息。\n残差连接（Residual Connection）：为了减轻梯度消失和加速训练，Transformer Encoder 层使用了残差连接。残差连接允许模型直接学习到输入序列的增量变换，而不是完全替代原始输入。\n层归一化（Layer Normalization）：在残差连接后应用层归一化，有助于提高模型的训练稳定性，加快训练速度。\nTransformer Encoder 层的主要作用是将输入序列转换为其对应的向量表示，并且捕获输入序列中的语义和位置信息，以便后续的模型能够更好地理解和处理输入序列。\n前面我们已经详解了三个点的计算过程，现在了解一下 Add \u0026 Norm 和 Feed Forward 部分。\n基础知识 归一化 归一化是将数据转换为具有统一尺度的过程，常用于机器学习、数据挖掘和统计分析中。归一化可以确保不同特征或变量之间具有相似的数值范围，有助于提高模型的性能和收敛速度。\n作用 让我用一个简单的例子来说明归一化的作用。\n假设你有一个数据集，其中包含两个特征：年龄和收入。年龄的范围是 0 到 100 岁，而收入的范围是 1000 到 100000 美元。这两个特征的范围差异很大。\n现在，你想要使用这些特征来训练一个机器学习模型，比如线性回归模型，来预测一个人是否会购买某种产品。由于特征的范围差异较大，这可能会导致某些问题：\n收入的范围比年龄大得多，这可能会使得模型过度关注收入而忽略年龄，因为收入的变化可能会对预测产生更大的影响。 模型可能会受到数值范围的影响，而不是特征本身的重要性。 这时候，归一化就可以派上用场了。通过归一化，你可以将不同特征的值缩放到相似的范围内，从而消除数值范围差异带来的影响。比如，你可以将年龄和收入都缩放到 0 到 1 之间的范围内，或者使用其他归一化方法，如标准化 (standardization)。\n通过归一化，你可以确保模型不会因为特征值的范围差异而偏向某个特定的特征，而是可以更平衡地利用所有的特征信息来进行预测。\n常用归一化 下面是几种常用的归一化方式及其公式：\nMin-Max 归一化： Min-Max 归一化将数据线性映射到一个指定的范围内，通常是 [0, 1] 或 [-1, 1]。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X - X_{\\text{min}}}}{{X_{\\text{max}} - X_{\\text{min}}}}]$$",
    "tags": [],
    "title": "Transformer模型详解04-Encoder 结构",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_04/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "@[toc]\n简介 Transformer 模型由编码器（Encoder）和解码器（Decoder）两部分组成。这里我会着重描述解码器的结构以及在预训练、输入输出和预测时的输入输出。\n解码器结构：\n自注意力层（Self-Attention Layers）：与编码器类似，解码器也包含多个自注意力层，用于在解码器端对输出序列的不同位置进行关注，解码器中的自注意力层被修改为接受一个遮盖（masking）向量，以便在计算注意力权重时将未来的信息屏蔽掉，只关注当前位置之前的信息。。\n编码器-解码器注意力层（Encoder-Decoder Attention Layers）：除了自注意力层外，解码器还包含编码器-解码器注意力层，用于将编码器端的信息与解码器端的信息进行交互，帮助解码器更好地理解输入序列。\n前馈神经网络（Feed-Forward Neural Networks）：与编码器一样，解码器也包含前馈神经网络层，用于对特征进行映射和转换。\n位置编码（Positional Encoding）：解码器也需要位置编码来将位置信息融入模型中，以便模型能够理解输入序列的顺序信息。\nDecoder在预训练、输入输出和预测时的输入输出：\n预训练：\n输入：在预训练期间，解码器的输入通常是由目标序列（target sequence）以及可选的编码器端输出的上下文信息组成。这些输入经过嵌入（embedding）和位置编码后，被送入解码器中。 输出：解码器预训练的目标是生成目标序列的下一个词的概率分布。因此，在每个时间步，解码器会生成一个预测概率分布，以便训练模型。 输入输出：\n输入：在进行输入输出（Inference）时，解码器的输入通常是由上一个时间步生成的词以及编码器端的上下文信息组成。这些输入通过嵌入和位置编码后，传递给解码器。 输出：解码器在每个时间步生成的输出通常是一个概率分布，用于预测下一个词的概率。根据应用场景，可以使用不同的策略（如贪婪搜索、束搜索等）来选择最终的输出序列。 预测：\n输入：在预测阶段，解码器的输入通常是由起始符号（如）以及编码器端的上下文信息组成。这些输入经过嵌入和位置编码后，传递给解码器。 输出：解码器生成的输出是一个概率分布，用于预测下一个词的概率。根据应用需求，可以根据生成的概率分布采样得到最终的预测结果。 结构 上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：\n包含两个 Multi-Head Attention 层。 第一个 Multi-Head Attention 层采用了 Masked 操作。 第二个 Multi-Head Attention 层的K, V矩阵使用 Encoder 的编码信息矩阵C进行计算，而Q使用上一个 Decoder block 的输出计算。 最后有一个 Softmax 层计算下一个翻译单词的概率。 先理解:自注意力的计算过程\n原理 第一个 Multi-Head Attention Decoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 “我有一只猫” 翻译成 “I have a cat” 为例，了解一下 Masked 操作。\n下面的描述中使用了类似 Teacher Forcing 的概念，不熟悉 Teacher Forcing 的童鞋可以参考以下上一篇文章Seq2Seq 模型详解。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 “” 预测出第一个单词为 “I”，然后根据输入 “ I” 预测下一个单词 “have”。 Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 ( I have a cat) 和对应输出 (I have a cat ) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 “ I have a cat \"。\n第一步：是 Decoder 的输入矩阵和 Mask 矩阵，输入矩阵包含 “ I have a cat” (0, 1, 2, 3, 4) 五个单词的表示向量，Mask 是一个 5×5 的矩阵。在 Mask 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。 第二步：接下来的操作和之前的 Self-Attention 一样，通过输入矩阵X计算得到Q,K,V矩阵。然后计算Q和$K^T$的乘积$QK^T$ 第三步：在得到 $QK^T$之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用Mask矩阵遮挡住每一个单词之后的信息，遮挡操作如下： 得到 Mask $QK^T$之后在 Mask$QK^T$上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。 第四步：使用 Mask $QK^T$与矩阵 V相乘，得到输出 Z，则单词 1 的输出向量 $Z_1$是只包含单词 1 信息的。 第五步：通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 $Z_i$，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出$Z_i$然后计算得到第一个 Multi-Head Attention 的输出Z，Z与输入X维度一样。\n第二个 Multi-Head Attention Decoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 K, V矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 Encoder 的编码信息矩阵 C 计算的。\n根据 Encoder 的输出 C计算得到 K, V，根据上一个 Decoder block 的输出 Z 计算 Q (如果是第一个 Decoder block 则使用输入矩阵 X 进行计算)，后续的计算方法与之前描述的一致。\n这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 Mask)。\n什么使用Encoder计算k,v decoder计算Q\n在 Transformer 模型的解码器中，使用了编码器的键（key）和值（value），而使用解码器的查询（query）。这种结构是为了充分利用编码器端对输入序列的理解，同时使得解码器端能够更好地根据自身生成的部分序列来做出决策。这种设计的物理意义可以从以下几个方面来理解：\n利用编码器的上下文信息：编码器对输入序列进行编码，生成了对输入序列全局理解的表示。因此，使用编码器的键和值可以提供丰富的上下文信息，帮助解码器更好地理解输入序列。\n解码器的自注意力：解码器的自注意力机制中，查询用于计算注意力权重，而键和值则用于构建注意力分布。使用解码器的查询意味着模型在计算注意力时更关注当前正在生成的部分序列，这有助于确保生成的序列在语法和语义上的连贯性。\n解耦编码器和解码器：使用不同的键、值和查询将编码器和解码器的功能分开，使得模型更具灵活性和泛化能力。解码器可以独立地根据当前正在生成的序列来调整自己的注意力，而不受编码器端信息的限制。\n总之，通过在解码器中使用编码器的键和值，以及使用解码器的查询，Transformer 模型能够更好地利用编码器端对输入序列的理解，并在解码器端根据当前正在生成的序列来做出决策，从而提高了生成序列的质量和连贯性。\nSoftmax 预测输出单词 Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下： Softmax 根据输出矩阵的每一行预测下一个单词： 这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。",
    "description": "@[toc]\n简介 Transformer 模型由编码器（Encoder）和解码器（Decoder）两部分组成。这里我会着重描述解码器的结构以及在预训练、输入输出和预测时的输入输出。\n解码器结构：\n自注意力层（Self-Attention Layers）：与编码器类似，解码器也包含多个自注意力层，用于在解码器端对输出序列的不同位置进行关注，解码器中的自注意力层被修改为接受一个遮盖（masking）向量，以便在计算注意力权重时将未来的信息屏蔽掉，只关注当前位置之前的信息。。\n编码器-解码器注意力层（Encoder-Decoder Attention Layers）：除了自注意力层外，解码器还包含编码器-解码器注意力层，用于将编码器端的信息与解码器端的信息进行交互，帮助解码器更好地理解输入序列。\n前馈神经网络（Feed-Forward Neural Networks）：与编码器一样，解码器也包含前馈神经网络层，用于对特征进行映射和转换。\n位置编码（Positional Encoding）：解码器也需要位置编码来将位置信息融入模型中，以便模型能够理解输入序列的顺序信息。\nDecoder在预训练、输入输出和预测时的输入输出：\n预训练：\n输入：在预训练期间，解码器的输入通常是由目标序列（target sequence）以及可选的编码器端输出的上下文信息组成。这些输入经过嵌入（embedding）和位置编码后，被送入解码器中。 输出：解码器预训练的目标是生成目标序列的下一个词的概率分布。因此，在每个时间步，解码器会生成一个预测概率分布，以便训练模型。 输入输出：\n输入：在进行输入输出（Inference）时，解码器的输入通常是由上一个时间步生成的词以及编码器端的上下文信息组成。这些输入通过嵌入和位置编码后，传递给解码器。 输出：解码器在每个时间步生成的输出通常是一个概率分布，用于预测下一个词的概率。根据应用场景，可以使用不同的策略（如贪婪搜索、束搜索等）来选择最终的输出序列。 预测：\n输入：在预测阶段，解码器的输入通常是由起始符号（如）以及编码器端的上下文信息组成。这些输入经过嵌入和位置编码后，传递给解码器。 输出：解码器生成的输出是一个概率分布，用于预测下一个词的概率。根据应用需求，可以根据生成的概率分布采样得到最终的预测结果。 结构 上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：\n包含两个 Multi-Head Attention 层。 第一个 Multi-Head Attention 层采用了 Masked 操作。 第二个 Multi-Head Attention 层的K, V矩阵使用 Encoder 的编码信息矩阵C进行计算，而Q使用上一个 Decoder block 的输出计算。 最后有一个 Softmax 层计算下一个翻译单词的概率。 先理解:自注意力的计算过程",
    "tags": [],
    "title": "Transformer模型详解05-Decoder 结构",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_05/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 插件开发 \u003e vscode插件",
    "content": "概述 Visual Studio Code（简称 VS Code）是一款由 Microsoft 开发的开源轻量级编辑器，支持跨平台（Windows、macOS、Linux）。\n其最大的优势之一是强大的插件系统，开发者可以通过编写扩展（Extension）来增强 VS Code 的功能，比如支持新的编程语言、代码提示、调试器、界面主题等。\nVS Code 插件的主要原理是：\n插件运行在独立的进程（Extension Host）中，不会阻塞编辑器主线程。 插件通过 VS Code 提供的 API 与编辑器进行交互，比如注册命令、添加菜单、修改编辑器行为等。 插件开发语言主要是 TypeScript 或 JavaScript，并基于 Node.js 生态。 安装 VS Code 安装 打开 VS Code 官方下载页面。 选择对应操作系统（Windows、macOS 或 Linux）。 按提示进行安装，安装完成后可以通过 code 命令（需要在安装时勾选“添加到 PATH”）在命令行中启动 VS Code。 插件开发环境安装 插件开发需要以下工具：\nyo（Yeoman 脚手架工具） generator-code（VS Code 插件项目生成器） vsce（VS Code Extension CLI，用于打包和发布插件） 安装步骤：\n# 安装 yo 和 generator-code npm install -g yo generator-code # 安装 vsce npm install -g @vscode/vsce 开发 生成代码 使用 Yeoman 脚手架生成插件项目：\nyo code 执行后会有交互式提示，例如：\n选择插件类型（TypeScript / JavaScript） 插件名称 描述 初始化 Git 仓库等 生成完成后，项目目录大致结构如下：\nmy-extension/\r├── .vscode/ # VS Code 调试配置\r├── src/ # 插件源码\r│ └── extension.ts # 插件入口文件\r├── package.json # 插件描述文件，配置命令、激活事件、依赖等\r├── tsconfig.json # TypeScript 配置（如果是 TS 项目）\r└── README.md # 插件说明文档 package.json：插件的核心配置文件，用来描述插件元信息和扩展点。 extension.ts：插件入口文件，负责注册命令和功能。 package.json 核心配置 package.json 是插件的描述文件，控制插件如何被 VS Code 加载。主要字段：\n{\r\"name\": \"my-extension\",\r\"displayName\": \"My Extension\",\r\"description\": \"一个简单的 VS Code 插件示例\",\r\"version\": \"0.0.1\",\r\"publisher\": \"your-name\",\r\"engines\": {\r\"vscode\": \"^1.80.0\"\r},\r\"activationEvents\": [\r\"onCommand:extension.helloWorld\"\r],\r\"main\": \"./out/extension.js\",\r\"contributes\": {\r\"commands\": [\r{\r\"command\": \"extension.helloWorld\",\r\"title\": \"Hello World\"\r}\r]\r},\r\"scripts\": {\r\"vscode:prepublish\": \"npm run compile\",\r\"compile\": \"tsc -p ./\",\r\"watch\": \"tsc -watch -p ./\",\r\"test\": \"npm run compile \u0026\u0026 node ./out/test/runTest.js\"\r},\r\"devDependencies\": {\r\"typescript\": \"^5.0.0\",\r\"vscode\": \"^1.1.37\"\r}\r} 核心字段说明：\nname：插件的唯一 ID（发布后不可更改）。 displayName：VS Code Marketplace 上显示的名称。 version：插件版本。 publisher：发布者名称（需与 Marketplace 发布者一致）。 engines.vscode：兼容的 VS Code 版本范围。 activationEvents：触发插件激活的事件（如 onCommand、onLanguage、*）。 main：插件的入口文件（一般是编译后的 extension.js）。 contributes：插件扩展点，例如命令、菜单、快捷键、配置等。 extension.ts 核心函数 extension.ts 是插件的入口文件，负责插件的生命周期和功能实现。\nimport * as vscode from 'vscode';\r/**\r* 插件被激活时调用\r* @param context 插件上下文对象，包含订阅、全局存储等\r*/\rexport function activate(context: vscode.ExtensionContext) {\rconsole.log('插件已激活！');\r// 注册命令\rlet disposable = vscode.commands.registerCommand('extension.helloWorld', () =\u003e {\rvscode.window.showInformationMessage('Hello World from My Extension!');\r});\r// 将命令注册到插件上下文，确保插件卸载时清理资源\rcontext.subscriptions.push(disposable);\r}\r/**\r* 插件被停用时调用\r* 通常用于清理资源、保存数据\r*/\rexport function deactivate() {} 核心点解释：\nactivate：插件激活时执行（如首次运行命令、打开特定文件类型）。 deactivate：插件停用时执行，用于清理资源。 vscode.commands.registerCommand：注册一个命令（命令 ID 必须和 package.json 中一致）。 vscode.window.showInformationMessage：在 VS Code 界面右下角弹出提示消息。 context.subscriptions：插件上下文，保存所有注册的资源，确保在插件停用时能正确释放。 Hello World 示例 编辑 src/extension.ts，添加一个最简单的命令： import * as vscode from 'vscode';\rexport function activate(context: vscode.ExtensionContext) {\rconsole.log('插件已激活！');\rlet disposable = vscode.commands.registerCommand('extension.helloWorld', () =\u003e {\rvscode.window.showInformationMessage('Hello World from My Extension!');\r});\rcontext.subscriptions.push(disposable);\r}\rexport function deactivate() {} 在 package.json 中配置命令： {\r\"contributes\": {\r\"commands\": [\r{\r\"command\": \"extension.helloWorld\",\r\"title\": \"Hello World\"\r}\r]\r}\r} 运行调试： 按 F5 启动调试，会打开一个新的 VS Code 窗口（Extension Development Host）。 打开命令面板（Ctrl+Shift+P / Cmd+Shift+P），输入并运行 Hello World。 会弹出消息 “Hello World from My Extension!\"。 拓展介绍 VS Code 插件 API 非常丰富，常见扩展能力包括：\n编辑器扩展：代码高亮、自动补全、格式化器。\nUI 扩展：状态栏、活动栏、侧边栏视图。\n调试扩展：调试适配器，用于支持新的调试语言。\n文件系统扩展：实现虚拟文件系统。\n常见配置示例（在 package.json 中添加）：\n1. 命令（Commands） 命令是最常见的扩展方式，用户可以在命令面板（Ctrl+Shift+P）或绑定快捷键来触发。\n配置（package.json）：\n{ \"contributes\": { \"commands\": [ { \"command\": \"extension.helloWorld\", \"title\": \"Hello World\" } ] } } 实现（extension.ts）：\nvscode.commands.registerCommand('extension.helloWorld', () =\u003e {\rvscode.window.showInformationMessage('Hello World!');\r}); 2. 菜单（Menus） 可以把命令挂载到编辑器右键菜单、资源管理器右键菜单等位置。\n配置（package.json）：\n{\r\"contributes\": {\r\"commands\": [\r{\r\"command\": \"extension.helloWorld\",\r\"title\": \"hello\"\r}，\r\"menus\": {\r\"editor/context\": [\r{\r\"command\": \"extension.helloWorld\",\r\"when\": \"editorLangId == javascript\",\r\"group\": \"navigation\"\r}\r]\r}\r}\r} 说明：\neditor/context 表示编辑器内右键菜单。 when 条件限制了命令只在 JavaScript 文件中出现。 group 决定菜单项分组（navigation = 导航相关）。 菜单本身没有名字，只能通过命令 title 来显示，菜单本省command会关联到commands的命令通过command的title显示菜单名称。 菜单位置由 menus 的 key 决定，比如：\n菜单位置 key:\r`editor/context` 编辑器右键菜单\r`editor/title` 编辑器标题栏按钮\r`editor/title/context` 编辑器标题栏右键菜单\r`explorer/context` 资源管理器右键菜单\r`commandPalette` 命令面板（Ctrl+Shift+P）\r`view/title` 视图面板标题栏按钮\r`scm/title` 版本控制标题栏按钮 3. 快捷键（Keybindings） 可以为命令绑定快捷键。\n配置（package.json）：\n{\r\"contributes\": {\r\"keybindings\": [\r{\r\"command\": \"extension.helloWorld\",\r\"key\": \"ctrl+alt+h\",\r\"when\": \"editorTextFocus\"\r}\r]\r}\r} 说明：\nkey：快捷键组合。 when：触发条件，这里是“编辑器有焦点时”。 4. 状态栏（Status Bar Items） 可以在底部状态栏添加一个按钮。\n实现（extension.ts）：\nlet statusBar = vscode.window.createStatusBarItem(vscode.StatusBarAlignment.Right, 100);\rstatusBar.text = \"$(smiley) Hello\";\rstatusBar.command = \"extension.helloWorld\";\rstatusBar.show();\rcontext.subscriptions.push(statusBar); 说明：\ncreateStatusBarItem 用于创建状态栏元素。 text 可以包含图标（如 $(smiley)）。 command 绑定点击事件。 5. 侧边栏视图（Views） 可以在活动栏（左侧竖栏）添加一个自定义视图。\n配置（package.json）：\n{\r\"contributes\": {\r\"views\": {\r\"explorer\": [\r{\r\"id\": \"mySidebar\",\r\"name\": \"My Sidebar\"\r}\r]\r}\r}\r} 实现（extension.ts）：\nclass MyTreeDataProvider implements vscode.TreeDataProvider\u003cvscode.TreeItem\u003e {\rgetTreeItem(element: vscode.TreeItem): vscode.TreeItem {\rreturn element;\r}\rgetChildren(): vscode.TreeItem[] {\rreturn [\rnew vscode.TreeItem(\"Item 1\"),\rnew vscode.TreeItem(\"Item 2\")\r];\r}\r}\rvscode.window.registerTreeDataProvider(\"mySidebar\", new MyTreeDataProvider()); 说明：\n在 资源管理器面板 添加一个新视图 “My Sidebar”。\n用 TreeDataProvider 动态提供数据。\n6. 编辑器装饰（Decorations） 可以给代码添加背景色、高亮、提示信息等。\n实现（extension.ts）：\nconst decorationType = vscode.window.createTextEditorDecorationType({\rbackgroundColor: \"rgba(255,0,0,0.3)\"\r});\rconst editor = vscode.window.activeTextEditor;\rif (editor) {\rconst range = new vscode.Range(0, 0, 0, 5);\reditor.setDecorations(decorationType, [range]);\r} 说明：\ncreateTextEditorDecorationType 定义样式。 setDecorations 应用到代码范围。 7. 语言支持（Language Features） 可以扩展某种语言的代码补全、悬浮提示等。\n配置（package.json）：\n{\r\"contributes\": {\r\"languages\": [\r{\r\"id\": \"mylang\",\r\"aliases\": [\"MyLang\"],\r\"extensions\": [\".mlg\"],\r\"configuration\": \"./language-configuration.json\"\r}\r]\r}\r} 实现补全（extension.ts）：\nvscode.languages.registerCompletionItemProvider(\"mylang\", {\rprovideCompletionItems(document, position) {\rreturn [new vscode.CompletionItem(\"helloWorld\", vscode.CompletionItemKind.Keyword)];\r}\r}); 说明：\nlanguages 定义新语言（这里是 .mlg 后缀）。 registerCompletionItemProvider 提供自动补全。 8. 配置（Configuration） 插件可以在 VS Code 设置里增加配置项。\n配置（package.json）：\n{\r\"contributes\": {\r\"configuration\": {\r\"title\": \"My Extension\",\r\"properties\": {\r\"myExtension.enableFeature\": {\r\"type\": \"boolean\",\r\"default\": true,\r\"description\": \"是否启用我的功能\"\r},\r\"myExtension.apiEndpoint\": {\r\"type\": \"string\",\r\"default\": \"https://api.example.com\",\r\"description\": \"API 接口地址\"\r}\r}\r}\r}\r} 读取配置（extension.ts）：\nconst config = vscode.workspace.getConfiguration(\"myExtension\");\rconst enable = config.get(\"enableFeature\", true);\rconst api = config.get(\"apiEndpoint\", \"\"); 9. 文件系统监听（File System Watcher） 可以监听文件变化事件。\n实现（extension.ts）：\nconst watcher = vscode.workspace.createFileSystemWatcher(\"**/*.js\");\rwatcher.onDidChange(uri =\u003e console.log(\"修改: \" + uri.fsPath));\rwatcher.onDidCreate(uri =\u003e console.log(\"创建: \" + uri.fsPath));\rwatcher.onDidDelete(uri =\u003e console.log(\"删除: \" + uri.fsPath));\rcontext.subscriptions.push(watcher); 10. 任务（Tasks） 可以让插件在 VS Code 的“任务运行器”中提供任务。\n配置（package.json）：\n{\r\"contributes\": {\r\"taskDefinitions\": [\r{\r\"type\": \"myTask\",\r\"required\": [\"taskName\"],\r\"properties\": {\r\"taskName\": {\r\"type\": \"string\",\r\"description\": \"任务名称\"\r}\r}\r}\r]\r}\r} 实现（extension.ts）：\nvscode.tasks.registerTaskProvider(\"myTask\", {\rprovideTasks: () =\u003e {\rreturn [new vscode.Task(\r{ type: \"myTask\", taskName: \"sayHello\" },\rvscode.TaskScope.Workspace,\r\"sayHello\",\r\"myTask\",\rnew vscode.ShellExecution(\"echo Hello from task!\")\r)];\r},\rresolveTask: () =\u003e undefined\r}); 发布 打包插件 使用 vsce 打包插件：\n# 在插件项目根目录执行\rvsce package 执行成功后，会生成一个 .vsix 文件，例如：\nmy-extension-0.0.1.vsix 安装插件：\ncode --install-extension my-extension-0.0.1.vsix 或者到vscode插件中心右侧… install from vsix选择本地文件。\n发布到 VS Code Marketplace 前往 Azure DevOps 创建 Publisher。\n使用 vsce login \u003cpublisher-name\u003e 登录，并输入 Personal Access Token。\n发布插件：\nvsce publish 或者指定版本号：\nvsce publish minor 发布成功后，你的插件就会出现在 Visual Studio Marketplace 上，供所有用户下载。",
    "description": "概述 Visual Studio Code（简称 VS Code）是一款由 Microsoft 开发的开源轻量级编辑器，支持跨平台（Windows、macOS、Linux）。\n其最大的优势之一是强大的插件系统，开发者可以通过编写扩展（Extension）来增强 VS Code 的功能，比如支持新的编程语言、代码提示、调试器、界面主题等。\nVS Code 插件的主要原理是：\n插件运行在独立的进程（Extension Host）中，不会阻塞编辑器主线程。 插件通过 VS Code 提供的 API 与编辑器进行交互，比如注册命令、添加菜单、修改编辑器行为等。 插件开发语言主要是 TypeScript 或 JavaScript，并基于 Node.js 生态。 安装 VS Code 安装 打开 VS Code 官方下载页面。 选择对应操作系统（Windows、macOS 或 Linux）。 按提示进行安装，安装完成后可以通过 code 命令（需要在安装时勾选“添加到 PATH”）在命令行中启动 VS Code。 插件开发环境安装 插件开发需要以下工具：\nyo（Yeoman 脚手架工具） generator-code（VS Code 插件项目生成器） vsce（VS Code Extension CLI，用于打包和发布插件） 安装步骤：\n# 安装 yo 和 generator-code npm install -g yo generator-code # 安装 vsce npm install -g @vscode/vsce 开发 生成代码 使用 Yeoman 脚手架生成插件项目：",
    "tags": [],
    "title": "vscode插件开发教程",
    "uri": "/docs/programming/plugins/vscode/vscode_cross/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/docs/categories/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/docs/tags/index.html"
  }
]
