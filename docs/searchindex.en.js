var relearn_searchindex = [
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库",
    "content": "",
    "description": "",
    "tags": [],
    "title": "transformers",
    "uri": "/docs/programming/ai/tools_libraries/transformers/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers",
    "content": "",
    "description": "",
    "tags": [],
    "title": "transformers模型详解",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 插件开发",
    "content": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "description": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "tags": [],
    "title": "vscode插件",
    "uri": "/docs/programming/plugins/vscode/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能",
    "content": "",
    "description": "",
    "tags": [],
    "title": "工具库",
    "uri": "/docs/programming/ai/tools_libraries/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 编程语言",
    "content": "",
    "description": "",
    "tags": [],
    "title": "汇编语言",
    "uri": "/docs/programming/languages/assembly/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "深度基础",
    "uri": "/docs/programming/ai/deep_learning/basic/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客",
    "content": "",
    "description": "",
    "tags": [],
    "title": "编程开发",
    "uri": "/docs/programming/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发",
    "content": "",
    "description": "",
    "tags": [],
    "title": "编程语言",
    "uri": "/docs/programming/languages/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers",
    "content": "",
    "description": "",
    "tags": [],
    "title": "transformers实战",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发",
    "content": "",
    "description": "",
    "tags": [],
    "title": "人工智能",
    "uri": "/docs/programming/ai/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "框架学习",
    "uri": "/docs/programming/ai/deep_learning/frameworks/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能",
    "content": "",
    "description": "",
    "tags": [],
    "title": "深度学习",
    "uri": "/docs/programming/ai/deep_learning/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 插件开发",
    "content": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "description": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "tags": [],
    "title": "chrome插件",
    "uri": "/docs/programming/plugins/chrome/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "卷积神经网络",
    "uri": "/docs/programming/ai/deep_learning/cnn/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发",
    "content": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "description": "插件开发 插件开发文档 插件使用主题：\nhttps://github.com/McShelby/hugo-theme-relearn https://github.com/88250/hugo-theme-archie 插件列表 插件列表",
    "tags": [],
    "title": "插件开发",
    "uri": "/docs/programming/plugins/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "循环神经网络",
    "uri": "/docs/programming/ai/deep_learning/rnn/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习",
    "content": "",
    "description": "",
    "tags": [],
    "title": "生成对抗网络",
    "uri": "/docs/programming/ai/deep_learning/gans/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 编程语言 \u003e 汇编语言",
    "content": "简介 68000 汇编语言是为 Motorola 68000 微处理器设计的低级编程语言。68000 微处理器于 1979 年推出，因其强大的性能和灵活的架构而广泛应用于多种计算机系统和嵌入式设备中。以下是对 68000 汇编语言的背景、应用领域以及学习它的好处的详细介绍。\n产生背景 技术进步：\n68000 微处理器是 16 位架构，具有 32 位的地址总线，能够寻址高达 4GB 的内存。这使得它在当时的微处理器中具有较高的性能和灵活性。 其设计采用了复杂指令集计算（CISC）架构，支持多种寻址模式和丰富的指令集。 市场需求：\n1970 年代末和1980年代初，个人计算机和嵌入式系统的需求迅速增长。68000 微处理器因其性能和成本效益被广泛采用。 许多知名的计算机系统（如 Apple Macintosh、Atari ST 和 Sega Genesis）都使用了 68000 处理器。 应用领域 个人计算机：\n68000 微处理器被用于早期的个人计算机，如 Apple Macintosh 和 Atari ST。这些系统的操作系统和应用程序通常使用 68000 汇编语言进行开发。 嵌入式系统：\n68000 处理器也被广泛应用于嵌入式系统，如工业控制、汽车电子和消费电子产品。 游戏机：\nSega Genesis 和其他游戏机使用 68000 处理器，许多经典游戏都是用 68000 汇编语言编写的，学习后可以做一些hackrom的实战。 实时系统：\n由于其高效的性能，68000 处理器在实时系统中也得到了应用，如医疗设备和航空航天系统。 语言学习 EASy68K EASy68K 是一个 68000 结构化汇编语言集成开发环境（IDE）。EASy68K 允许您在 Windows PC 或 Wine 上编辑、汇编和运行 68000 程序。无需额外的硬件。EASy68K 是一个开源项目，根据 GNU 通用公共使用许可分发。 使用easy68k方便我们学习整套68000的编程和调试，学习这件基础知识，对我们hackrom或者逆向的基础。 下载地址：http://www.easy68k.com/files/SetupEASy68K.exe 安装完成后的目录结构 帮助文档 helm.chm提供了完整的编程和调试工具以及68k语言的学习入门资料，可以直接从该文档入手。 IDE使用 打开EDIT68K.exe，菜单file-\u003enew x68 source file 。 在source里面实现一个功能，打印helloworld，并从空值台输入一个字符串并打印。\n关于指令，标签，寄存器其他相关的内容请移步后续章节。\n源代码\n*-----------------------------------------------------------\r* Title :\r* Written by :\r* Date :\r* Description:\r*-----------------------------------------------------------\rORG $1000 ;告诉编译器代码从1000位置开始，不指定默认从0开始\rSTART: ; first instruction of program\r* 将text字符串地址写给A1\rlea text,A1\r* 将14号task print 给d0,并执行，14号任务自动获取A1地址的数据并打印\rmove #14,D0\rtrap #15\r* 执行2号任务，从输入流获取输入，自动写入到A1\rmove #2,D0\rtrap #15\r* 打印A1地址内容\rmove #14,D0\rtrap #15\r* Put program code here\r*-----------------------------------------------------------\r*HELLO：这是一个标签，标识字符串数据的起始位置。\r*DC.B：这是一个伪指令，表示“定义常量（Define Constant）”，后面的 .B 表示定义的是字节（Byte）数据。\r*'Hello World'：这是一个字符串常量，表示字符数组。每个字符占用一个字节。\r*$D：这是一个十六进制常量，表示一个字节的值。$D 的十进制值是 13，通常表示回车符（Carriage Return）。\r*$A：这是一个十六进制常量，表示一个字节的值。$A 的十进制值是 10，通常表示换行符（Line Feed）。\r*0：这是一个字节的值，表示字符串的结束符（null terminator），在 C 语言中常用来标识字符串的结束。\r*----------------------------------------------------------- text dc.b 'helloworld',0\rSIMHALT ; halt simulator\r* Put variables and constants here\rEND START ; last line of source 点击工具栏运行按钮（如果由错误会有提示，根据情况修正） 会弹出一个确认框 点击execute 绿色圈圈点击变成红色可下断点，F9运行，F8 stepover,F7 stepinto,点击运行可调试。 在view可打开内存窗口，栈窗口等 编程语言 汇编语言程序由以下部分组成：\nlabels 标签 - 用户创建的名称，用于标记程序中的位置。 opcode 操作码 - 微处理器可以执行的特定指令，比如ADD，MOVE等。 operands 操作数 - 某些指令所需的附加数据，比如#1表示10进制立即数1，$1表示16进制的1。 directives 指令 - 发给汇编器的命令，比如ORG $1000，告诉编译器，代码的开始位置，代码段不占用空间，类似于c语言的宏，编译阶段使用。 macros 宏 - 用户创建的源代码集合，可以在编写程序时轻松重用。 comments 注释 - 用户创建的文本字符串，用于记录程序。 寄存器：汇编语言编程需要与微处理器进行直接交互。68000 微处理器包含八个数据寄存器 D0 到 D7。数据寄存器是通用的，可以视为 8 位、16 位或 32 位的整数变量。还有八个地址寄存器 A0 到 A7，地址寄存器的长度为 32 位。它们通常用于引用变量。状态寄存器（SR）包含状态标志，用于指示比较的结果。 以下是一个例子 comments 在 Motorola 68000（68k）汇编语言中，注释用于帮助程序员理解代码的功能和逻辑。68k 汇编语言的注释格式如下：（*或者;开头的为注释）\n* Date TRAP #15 ;将3任务执行，自动打印D1的内容 operands 操作数 #,$,%区别 你可能也注意到了出现在32和0000001E前面的#和$符号，$符号是为了告诉汇编器这个数字是“十六进制”数字(是个地址)，而不是“十进制”数字，例如：\nmove.b #32,$0000001E 汇编器在进行汇编时会将32（十进制）转换为0010 0000（二进制）。0010 0000 是 20 十六进制，因此写 32 和写 $20 是一样的，0010 0000将写入地址0000001E。如果你想要写二进制数，可以使用 % 符号。\nmove.b #%00100000,$0000001E\rmove.b #$20,$0000001E\rmove.b #32,$0000001E 以上所有内容完全相同，顶部是二进制版本（%），中间是十六进制（$），底部是十进制。在本教程中，我们将更多地使用十六进制和二进制，而不是十进制，以帮助你更好地理解和掌握它们。\n另一方面，# 符号告诉汇编器，该数字是一个“立即”值，而不是一个偏移量。那么什么是“立即”值呢？稍安勿躁，让我们先看一个没有 # 符号的例子：\nmove.b $00000010,$0000002D 这将读取偏移量00000010处的字节，并将其复制到偏移量0000002D处，如果偏移量00000010处的字节是49，则0000002D处现在也将是49： 而现在回到“立即数”，在我看来，这只不过是“直接数字”的一个花哨名称，#符号告诉68k这个数字不是偏移量/地址。\n操作数移动 给个例子\nmove.w #$10,$0020 ;将立即数16进制10 写入内存地址$0020\rmove $0020,D0 ;将内存地址$0020的值10赋值给D0\rmove $0020,A0 ;将将内存地址$0020的值10赋值给地址寄存器A0\rmove #$0020,A1 ;将立即数$0020赋值给地址寄存器A1\rmove A1,D1 ;将A1地址#$0020赋予给D1\rmove (A1),D2 ;将A1地址#$0020内存的值10赋予给D2\rmove.w (a0),(a1) ;将a0地址的值赋给a1地址的内存\rmove.w d1,(a0)+ ;将d1的数据，写入a0+word(2个字节)，并且a0寄存器往后移动两位，比如a0=0000,执行完a0=0002\rmove.w d1,$10(a1) ;将d1数据写入a1+10个字节的位置，a1的指向不变，比如a1=0000，写入数据到0010，执行完a1=0000\rmove.b #$98,(a0)+ ;同上上，写入立即数\rmove.l $29(a0),$00120020 ;将a0+29位置的值写入$00120020位置\rmove.b $00120020,(a1)+ 注意：move.w $00000047,d0 这个会导致汇编程序崩溃，因为00000047是一个奇数（奇地址/偏移量），68k在处理时会有问题，并会因“地址错误”而崩溃，字w和双字l必须使用偶数地址，如果要使用奇数地址请使用字节b。\n你只能使用“字节”来访问奇地址上的数据：\nlabels 标签用于通过名称标识程序中的位置或内存位置。需要位置的指令或指令可以使用标签来指示该位置。标签通常在行的第一列开始，必须以空格、制表符或冒号结束。如果使用冒号，它不会成为标签的一部分。如果标签没有在第一列开始，则必须以冒号结束。标签的前 32 个字符是有效的。标签有两种类型：全局标签和局部标签。\n全局标签可以在程序的任何地方被引用。因此，全局标签必须是唯一的名称。全局标签应以字母开头，后面可以跟字母、数字或下划线。局部标签可以在程序中重复使用。局部标签必须以点 ‘.’ 开头，后面可以跟字母、数字或下划线。全局标签定义了局部标签的边界。当定义局部标签时，只有在遇到下一个全局标签之前，才能从局部标签上方或下方的代码中引用它。汇编器通过将局部标签名称附加到前面的全局标签并用冒号 ‘:’ 替换点来创建局部标签的唯一名称。结果名称的前 32 个字符是有效的。\n开始标签 标签可以用来指定程序的起始位置。如果标签 START 指定了程序的起始位置，那么 END 指令的写法如下：\nSTART: Start of program\rcode\rEND START 指令标签 标签常常放在某个指令前用来表示，定义变量，标签指向存储数据的首地址。 DC - DC 指令指示汇编器将后续的值放入当前内存位置。该指令有三种形式：DC.B 用于字节数据，DC.W 用于字（16 位）数据，DC.L 用于长（32 位）数据。定义常量指令不应与 C++ 中声明常量混淆。 例如\nORG $1000 start of the data region depart DC.B 'depart.wav',0 stores as a NULL terminated string in consecutive bytes DC.L $01234567 the value $01234567 is stored as a long word\rDC.W 1,2 two words are stored as $0001 and $0002\rDC.L 1,2 two long words are stored as $00000001 and $00000002 depart 就是一个label是这块内存区域的首地址。\n内存结果\n00001000 64 65 70 61 72 74 2E 77 61 76 00 0000100C 01234567 00001010 0001 0002 00001014 00000001 00000002 其他关于指令标签的用法参考，也可以到指令章节： 位置标签 可以定义一些位置标签，当进行特殊操作时，可以通过控制流opcode跳转到位置标签 实现一个从0，end_index的循环打印\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\r* 实现一个从0，end_index的循环打印\rmove #1,D1\rt:\rmove #3,D0\rTRAP #15 ;将3任务执行，自动打印D1的内容\radd.b #1,d1 ;让d1+1\rCMP #end_index,d1 ;比较d1和end_index的值\rBNE t ;如果不相等继续跳转到t label执行\rSIMHALT ; halt simulator\r* Put variables and constants here\rend_index equ 10\rEND START ; last line of source opcode 操作码 在 68K 汇编语言中，操作码（opcode）是指令的核心部分，定义了要执行的操作。以下是一些常用的 68K 操作码及其功能：\n常用操作码 注意大部分操作码都可以添加结尾.W表示字（2个字节16位）.L表示双字(4个字节32位)，.B（1个字节8位）\n数据传送 - `MOVE`：将数据从一个位置移动到另一个位置。\r- 例：`MOVE.W D0, D1`（将 D0 的值移动到 D1）\r- `MOVEA`：将地址从一个位置移动到另一个位置。\r- 例：`MOVEA.L A0, A1`（将 A0 的地址移动到 A1）\r算术运算 - `ADD`：将两个操作数相加。\r- 例：`ADD.W D0, D1`（将 D0 的值加到 D1）\r- `SUB`：从一个操作数中减去另一个操作数。\r- 例：`SUB.W D1, D0`（从 D0 中减去 D1）\r- `MULS`：有符号乘法。\r- 例：`MULS D0, D1`（将 D0 和 D1 相乘，结果存储在 D1）\r- `DIVS`：有符号除法。\r- 例：`DIVS D0, D1`（将 D1 除以 D0，结果存储在 D1）\r逻辑运算 - `AND`：按位与运算。\r- 例：`AND.W D0, D1`（D1 与 D0 按位与）\r- `OR`：按位或运算。\r- 例：`OR.W D0, D1`（D1 与 D0 按位或）\r- `EOR`：按位异或运算。\r- 例：`EOR.W D0, D1`（D1 与 D0 按位异或）\r- `NOT`：按位取反。\r- 例：`NOT.W D0`（D0 的值取反）\r控制流 常用如下：\r- `BRA`：无条件跳转。\r- 例：`BRA label`（跳转到指定标签）\r- `BEQ`：如果相等则跳转。\r- 例：`BEQ label`（如果零标志位被设置，则跳转）\r- `BNE`：如果不相等则跳转。\r- 例：`BNE label`（如果零标志位未设置，则跳转）\r- `JSR`：跳转到子程序。\r- 例：`JSR subroutine`（跳转到子程序并保存返回地址）\r- `RTS`：从子程序返回。\r- 例：`RTS`（返回到调用子程序的地址）\r分支跳转 该指令将在程序中引发分支，如果某些标志被设置。共有十五种检查标志的方法。每种方法都有一个由两个字母组成的符号，用于替换 “cc” 在 “Bcc” 中。\nBCC：分支如果进位标志清除 - 当 C 标志为 0 时分支。 BCS：分支如果进位标志设置 - 当 C 标志为 1 时分支。 BEQ：分支如果相等 - 当 Z 标志为 1 时分支。 BNE：分支如果不相等 - 当 Z 标志为 0 时分支。 BGE：分支如果大于或等于 - 当 N 和 V 相等时分支。 BGT：分支如果大于 - 当 N 和 V 相等且 Z=0 时分支。 BHI：分支如果高于 - 当 C 和 Z 都为 0 时分支。 BLE：分支如果小于或等于 - 当 Z=1 或 N 和 V 不同时分支。 BLS：分支如果小于或相同 - 当 C=1 或 Z=1 时分支。 BLT：分支如果小于 - 当 N 和 V 不同时分支。 BMI：分支如果负 - 当 N=1 时分支。 BPL：分支如果正 - 当 N=0 时分支。 BVC：分支如果溢出标志清除 - 当 V=0 时分支。 BVS：分支如果溢出标志设置 - 当 V=1 时分支。 BRA：无条件分支 - 始终分支。 上面这些opcode根据标志触发跳转，只能跳转到label，注意进入label后会往下执行，和函数调用不一样，函数调用会返回，继续执行之前代码的下一行，这个不会，是直接跳转过去不回来了。\n例子：\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rinput:\rmove.b #4,d0\rTRAP #15\rCMP #0,d1\rBNE input ;如果不等于0跳转到input标签，继续让输入数字\rBEQ exit ;如果等于0直接退出\rlabel\rexit:\rSIMHALT ; halt simulator\r* Put variables and constants here\rEND START ; last line of source 地址跳转 JMP（跳转）用于将程序控制转移到一个有效地址。它实际上相当于 MOVE.L xxx, PC，因为它将程序计数器更改为一个有效地址（计算得出）。\n注意JMP是无条件跳转，相对于B开头的跳转，他也支持 JMP label的语法，同时他也支持直接JMP 地址的跳转。\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rinput:\rmove.b #4,d0\rTRAP #15\rCMP #0,d1\rBNE input ;如果不等于0跳转到input标签，继续让输入数字\rBEQ exit ;如果等于0直接退出\rlabel\rexit:\rLEA quit,a0 ;=0跳转到这里后，将quit的地址给到a0，JMP直接跳转到地址,相当于：move.l a0,PC（这是伪代码）\rJMP (a0) ;如果想跳转到a0的下一个地址，可以1(a0) 或者n(a0),当然也可以直接JMP quit\rquit:\rSIMHALT ; halt simulator\r* Put variables and constants here\rEND START ; last line of source 子程序跳转 JSR/BSR（跳转到子例程）与 JMP（无条件跳转）类似，但在跳转之前，JSR 会将跳转指令后面的地址压入栈中，这样可以通过 RTS（返回子例程）指令返回，也就相当于调用函数，函数执行完了，执行代码的下一行。\nBSR适合同一代码段里的label直接调用，是相对掉哟个，JSR适合指定一个绝对地址调用(比如JSR $5000) ,但是实际上两个可以互相替换，没啥区别。\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rinput:\rJSR input_notion ;JSR执行完后会自动执行下一行代码，B开头的跳过去就不回来了\rmove.b #4,d0\rTRAP #15\rCMP #0,d1\rBNE input ;如果不等于0跳转到input标签，继续让输入数字\rBEQ exit ;如果等于0直接退出\rinput_notion: ;屏幕上输出提示语\rMOVE #14,D0\rLEA INPUT_STR,A1\rTRAP #15\rRTS ;注意返回了会运行调用这个函数的下一行\rconfirm_exit *屏幕上输出确认提示语\rMOVE #14,D0\rLEA CONFIRM_STR,A1\rTRAP #15\rRTS\rexit:\rJSR confirm_exit\rmove.b #4,d0\rTRAP #15\rCMP #0,d1\rBEQ quit\rBNE input\rquit:\rSIMHALT ; halt simulator\r* Put variables and constants here\rINPUT_STR: dc.b 'please input number(exit=0):',0\rCONFIRM_STR: dc.b 'confirm exit(:exit=0,not=1):',0\rEND START ; last line of source 效果 位操作 - `SHL`：左移。\r- 例：`SHL.W #1, D0`（D0 左移 1 位）\r- `SHR`：右移。\r- 例：`SHR.W #1, D0`（D0 右移 1 位）\r- `ROL`：循环左移。\r- 例：`ROL.W #1, D0`（D0 循环左移 1 位）\r- `ROR`：循环右移。\r- 例：`ROR.W #1, D0`（D0 循环右移 1 位）\r比较 - `CMP`：比较两个操作数。\r- 例：`CMP.W D0, D1`（比较 D0 和 D1 的值）\r堆栈操作 - `PUSH`：将数据压入堆栈。\r- 例：`PUSH.W D0`（将 D0 的值压入堆栈）\r- `POP`：从堆栈弹出数据。\r- 例：`POP.W D0`（从堆栈弹出值到 D0）\rIO操作码 TRAP #15 被用于触发 I/O. 不同的io流任务存储在： D0. 参考chm： 常用的输入输出任务：\n14: 将A1地址对应的字符串输出 以0结尾结束。 13：将A1地址对应的字符串输出 以0结尾结束，加上\\r\\n换行。 2: 从控制台获取一个字符串回车后存储在A1地址中 0结尾。 4：读取一个数字写入D1.L中。 例子\nSTART ORG $1000 Program load address.\rmove #14,D0 ;设置14号任务打印A1地址字符串\rlea text,A1 ;获text地址到A1\rtrap #15 ;激活任务\rSIMHALT text dc.b 'Hello World',0 ;0表示字符串结束\rEND START End of source with start address specified. 其他操作码 关于更加详情的指令参考chm directives 指令 指令是汇编器需要遵循的指令。它们占据源代码行中的第二个字段，与指令操作码占据的位置相同，但指令并不是 68000 操作码。 “DC” 和 “DCB” 是唯一会导致数据被添加到输出文件中的指令。指令还可以用于控制宏的汇编、条件汇编和结构化语法。\n在以下描述中，选项项用方括号 [] 表示。用斜体显示的项应替换为适当的语法。\nUsage:\r[label] directive[.size] [data,data,...]\r^ ^ ^\r\\_________________\\_________\\_____ varies by directive DC指令 全称：Define Constant（定义常量） 用途：用于定义并初始化数据常量。DC 指令可以用于定义一个或多个初始值，这些值会被存储在程序的输出文件中。 内存分配：DC 指令会在程序的内存中分配实际的存储空间，并将指定的值写入该空间。 示例： 使用语法： Usage:\r[label] DC.size data,data,... 例子：\nVALUE1 DC 10 ; 定义常量 VALUE1，值为 10\rVALUE2 DC 20, 30 ; 定义常量 VALUE2，值为 20 和 30 特性： 定义的值在程序运行时是不可更改的。 实际在内存中占用空间。 注意下面的代码修改地址的值是非法的，常量无法修改\nSTART: ; first instruction of program\rlea usercount,A0\rmove.b 20,(A0) ;修改A0地址的常量这是非法的。\r* Put program code here\rSIMHALT ; halt simulator\r* Put variables and constants here\rORG $1200\rusercount dc.b 10,20\rdc.w 23 EQU 指令 全称：Equate（等于）\n用途：用于定义一个符号并将其与一个值关联。EQU 定义的值在整个程序中是不可更改的，通常用于定义常量或符号地址，类似于c语言的#define在预编译将对应引用的地方替换为值。\n内存分配：EQU 不会在内存中分配实际的存储空间。它只是创建一个符号，所有使用该符号的地方都会被替换为其定义的值。\n示例：\nMAX_SIZE EQU 100 ; 定义常量 MAX_SIZE，值为 100\n特性：\n一旦定义，EQU 的值不能被修改。 不占用内存空间，编译时进行替换 ORG $1000 ; 程序起始地址\rSTART: ; 将立即数 10 移动到 D0 寄存器\r; 定义常量\rMAX_COUNT EQU 2 ; 定义 MAX_COUNT 为 100\rSTART_VALUE EQU 1 ; 定义 START_VALUE 为 10\rMOVE.B #10, D0\rADD.B #MAX_COUNT, D0 ; 将 MAX_COUNT (100) 加到 D0\rSUB.B #START_VALUE, D0 ; 将 START_VALUE (10) 从 D0 中减去\rSIMHALT ; 停止模拟器\rORG $1200 ; 数据段起始地址\rEND START SET 指令 用途：用于定义一个符号并赋予一个初始值，但与 DC 不同的是，SET 定义的值是可更改的。SET 通常用于在程序运行时动态地改变值。\n示例：\nCOUNT SET 0 ; 定义符号 COUNT，初始值为 0 COUNT SET COUNT + 1 ; 重新定义 COUNT，值为 COUNT + 1\n内存分配：SET 指令并不分配实际的存储空间来存储值，而是定义一个符号，允许在程序中动态地改变该符号的值。\nDS 指令 全称：Define Space（定义空间）\n用途：用于定义一块未初始化的内存空间。DS 指令只分配内存，但不初始化这些内存的值，随时可改。\n示例：\nBUFFER DS 256 ; 定义一个大小为 256 字节的缓冲区\n内存分配：DS 指令会在输出文件中分配指定大小的内存空间，但这些空间的初始值是未定义的（通常是随机值或零，具体取决于系统）。\n定义一个100字节的空间，可以理解为数组，将MULT_TABLE数字第一个位置设置为：12\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rmove.B #0,D0\rLEA MULT_TABLE, A0\rMOVE.B #12,(A0, D0)\rSIMHALT ; halt simulator\rORG $1200\r* Put variables and constants here\rMULT_TABLE: ; 乘法表的存储位置\rDS.B 10 * 10 ; 预留 10x10 的空间\rEND START ; last line of source 其他指令 参考chm 寄存器 程序计数器（PC） 程序计数器（有时在不同的体系结构中也称为指令指针或指令地址寄存器）保存下一条将要执行的指令的内存地址。每当 CPU 执行一条指令时，PC 的值会自动更新，以指向下一条指令。 更新机制：在大多数情况下，PC 在指令执行后自动加一（或加上指令的长度），以指向下一条指令的地址。 编写一个简单程序 运行，默认会从start:的写一条语句开始，PC寄存器指向初始代码的地址（注意有效的代码时左侧绿色点点的，其他都是指令或者注释） 按下F8执行到下一条 我这里将usercount的地址指向A0 ,同时加了ORG $1200从1200这个地址写入。点击A0的地址可以查看内存： 状态寄存器（SR） 在 68k（Motorola 68000）架构中，状态寄存器（SR，Status Register）是一个重要的寄存器，用于存储处理器的状态信息和控制标志。状态寄存器的内容影响程序的执行流程，特别是在条件跳转和中断处理时。以下是对 68k 状态寄存器的详细介绍：\n状态寄存器的结构 68k 的状态寄存器是一个 16 位的寄存器，包含多个标志位。主要的标志位包括：\nN（Negative）:\n表示最近一次运算的结果是否为负数。 如果结果的最高位（符号位）为 1，则 N 标志被设置。 Z（Zero）:\n表示最近一次运算的结果是否为零。 如果结果为 0，则 Z 标志被设置。 V（Overflow）:\n表示最近一次运算是否发生了溢出。 溢出通常发生在有符号数运算中，当结果超出可表示的范围时，V 标志被设置。 C（Carry）:\n表示最近一次运算是否产生了进位或借位。 在加法运算中，如果产生了进位，C 标志被设置；在减法运算中，如果发生了借位，C 标志也会被设置。 I（Interrupt Mask）:\n这是一个 3 位的中断屏蔽位，控制中断的响应。 I0、I1 和 I2 位用于设置中断优先级，值越大，响应的中断优先级越低。 T（Trace）:\n这是一个单个位，用于启用或禁用跟踪模式。 当 T 位被设置时，处理器将在每个指令执行后产生一个中断，适用于调试。 S（Supervisor）:\n这是一个单个位，指示当前处理器是否处于特权模式（超级用户模式）。 当 S 位被设置时，处理器处于超级用户模式，允许执行特权指令。 状态寄存器的作用 条件跳转: 状态寄存器中的标志位用于条件跳转指令（如 BEQ、BNE 等），根据运算结果的状态决定程序的执行路径。 中断处理: 中断标志位控制中断的响应，允许或禁止特定级别的中断。 运算结果的状态: 通过检查 N、Z、V 和 C 标志，程序可以根据运算结果的状态做出相应的处理。 示例 以下是一个简单的示例，展示如何使用状态寄存器的标志位：\nMOVE.L #5, D0 ; 将 5 加载到 D0\rMOVE.L #3, D1 ; 将 3 加载到 D1\rSUB.L D1, D0 ; D0 = D0 - D1，结果为 2\r; 检查 Z 标志\rBEQ zero_result ; 如果 Z 标志为 1，跳转到 zero_result\r; 检查 N 标志\rBPL positive_result ; 如果 N 标志为 0，跳转到 positive_result\rzero_result:\r; 处理结果为零的情况\r; ...\rpositive_result:\r; 处理结果为正的情况\r; ... 数据寄存器（D) 在 68000（68k）架构中，D 寄存器（数据寄存器）是用于存储数据和操作数的寄存器。68k 处理器有 8 个数据寄存器，分别为 D0 到 D7。\nD 寄存器的特点 数量:\n68k 处理器有 8 个数据寄存器，编号为 D0 到 D7。 大小:\n每个 D 寄存器的大小为 32 位（4 字节），可以存储 32 位的整数或指针。 用途:\nD 寄存器主要用于存储运算的操作数、结果以及临时数据。它们在算术运算、逻辑运算、数据传输等操作中被广泛使用。 寻址模式:\nD 寄存器可以与多种寻址模式结合使用，支持直接寻址、间接寻址等方式，方便数据的访问和操作。 操作:\nD 寄存器可以参与各种指令的操作，如加法、减法、位运算等。指令可以直接对 D 寄存器进行操作，也可以将 D 寄存器的值存储到内存中或从内存中加载数据。 D 寄存器的使用场景 算术运算: D 寄存器用于存储参与运算的数值。 数据传输: 在数据传输指令中，D 寄存器可以作为源或目标。 函数参数: 在调用子程序时，D 寄存器常用于传递参数。 示例 以下是一个简单的汇编代码示例，展示如何使用 D 寄存器进行基本的算术运算：\nMOVE.L #10, D0 ; 将 10 加载到 D0 寄存器\rMOVE.L #5, D1 ; 将 5 加载到 D1 寄存器\rADD.L D1, D0 ; D0 = D0 + D1，D0 现在为 15 地址寄存器（A) 68000（68k）架构中，A 寄存器（地址寄存器）是用于存储内存地址的寄存器。68k 处理器有 8 个地址寄存器，分别为 A0 到 A7。以下是对 A 寄存器的详细描述：\nA 寄存器的特点 数量:\n68k 处理器有 8 个地址寄存器，编号为 A0 到 A7。 大小:\n每个 A 寄存器的大小为 32 位（4 字节），可以存储 32 位的内存地址。 用途:\nA 寄存器主要用于存储内存地址，支持数据的加载和存储操作。它们在指令中用于指向数据或指令的内存位置。 寻址模式:\nA 寄存器可以与多种寻址模式结合使用，包括直接寻址、间接寻址、基址寻址和相对寻址等。这使得程序能够灵活地访问内存中的数据。 堆栈指针:\nA7 寄存器通常用作堆栈指针（SP），指向当前堆栈的顶部。堆栈用于存储函数调用的返回地址、局部变量等。 A 寄存器的使用场景 内存访问: A 寄存器用于指向数据在内存中的位置，支持数据的读取和写入。 函数调用: 在函数调用中，A 寄存器可以用于传递参数和返回地址。 堆栈管理: A7 寄存器作为堆栈指针，管理函数调用的堆栈帧。 示例 以下是一个简单的汇编代码示例，展示如何使用 A 寄存器进行内存操作：\nLEA array, A0 ; 将数组的地址加载到 A0 寄存器\rMOVE.L (A0), D0 ; 从 A0 指向的地址加载数据到 D0 寄存器\rADD.L #1, D0 ; D0 = D0 + 1\rMOVE.L D0, (A0) ; 将 D0 的值存储回 A0 指向的地址 堆栈寄存器（SS) 在68k架构中，堆栈寄存器是用于管理程序运行时的堆栈的关键组件。68k系列处理器使用一个专用的寄存器来指向当前堆栈的顶部，这个寄存器被称为堆栈指针（Stack Pointer）。\n在68k架构中，堆栈指针寄存器通常是 A7（地址寄存器7），它指向当前堆栈的顶部。 堆栈是一个后进先出（LIFO）的数据结构，用于存储临时数据，如函数调用的返回地址、局部变量和中断处理程序的上下文。\n堆栈操作 我们来看下堆栈指针的移动和数据写入逻辑。 在68k汇编语言中，-(A7) 和 (A7)+ 分别用于表示压栈和出栈操作。 执行代码\nmove.l #10,-(a7) 未执行前原始堆栈地址A7指向：01000000，没有任何数据 执行：move.l #10,-(a7) 执行：move.l #20,-(a7) 执行出栈：move.l (a7)+,d0 std函数模拟 我们知道c语言的std约定是：调用函数先压入执行代码的后一个位置，然后参数从右往左压入，在函数内部出栈从左（后入先出）往右获取参数，执行完成获取代码执行的位置，跳转。 我们来模拟这个过程： 假设函数: public int add(int a,int b) 用98k模拟堆栈实现：\nORG $1000\rSTART: ; first instruction of program\r* Put program code here\rmove.l #10,-(a7) #第二个参数压栈。\rmove.l #20,-(a7) #第一个参数压栈。\rLEA *+12, A0 *计算下LEA占用4个字节，一直到move.l d0,d2是12个字节，*+12就是从PC当前位置+12个就是下一个执行代码的位置\rmove.l a0,4(a7) *将下一个执行的地址压栈\rJMP add\rmove.l d0,d2\rSIMHALT ; halt simulator\radd:\rmove.l (a7)+,a0 ;地址出栈\rmove.l (a7)+,d0 ;第一个参数出栈\rmove.l (a7)+,d1 ;第二个参数出栈\radd.l d1,d0\rJMP (a0)\r* Put variables and constants here\rEND START ; last line of source 案例(9*9乘法表) *-----------------------------------------------------------\r* Title :\r* Written by :\r* Date :\r* Description:\r*-----------------------------------------------------------\rORG $1000\rSTART: ; first instruction of program\r* Put program code here\rmove.b #start_index,d2 ;行索引\rmove.b #start_index,d3 ;列索引\rrow:\rjsr print_str_line ;到row的部分就添加一个换行，jsr调用子程序，子程序需要RTS返回\radd.b #1,d2 ;每运行一次+1\rmove.b #start_index,d3\rcmp #end_index+1,d2 ;到达最后一行+1直接退出\rBEQ exit\rcol:\radd.b #1,d3\rmove.b d2,d1\rjsr print_num ;打印行的数字\rlea tmp_str,a1\rmove.b #'*',(a1) ;打印一个*\rjsr print_str\rmove.b d3,d1\rjsr print_num ;打印一个列的数字\rmove.b #'=',(a1) jsr print_str ;打印一个=\rmove.b #1,d4\rmuls d2,d4\rmuls d3,d4\rmove.b d4,d1\rjsr print_num ;打印一个列的数字\rmove.b #' ',(a1) jsr print_str ;打印一个空格\rcmp d3,d2\rBEQ row\rBNE col\rprint_num:\rmove.b #3,d0\rTRAP #15 RTS print_str:\rmove.b #0,1(a1) ;打印字符的结尾\rmove.b #14,d0\rTRAP #15 RTS print_str_line:\rmove.b #0,(a1) ;打印字符的结尾\rmove.b #13,d0\rTRAP #15 RTS exit:\rSIMHALT ; halt simulator\r* Put variables and constants here\rtmp_str ds.b 2\rend_index equ 9\rstart_index equ 0\rEND START ; last line of source 效果 其他68k速查的在线文档：\n指令列表：https://github.com/prb28/m68k-instructions-documentation?tab=readme-ov-file 0基础入门：https://mrjester.hapisan.com/04_MC68/ romhack相关所有资源（文档工具）：https://github.com/zengfr/romhack 常用指令备忘录：https://github.com/zengfr/romhack/blob/adf6412c2a969486918bb00c18a2c989abdeaad5/M68000/M68000%E6%8C%87%E4%BB%A4-%E5%A4%87%E5%BF%98%E8%A1%A5%E5%85%85(%E6%95%B4%E7%90%86zengfr).txt",
    "description": "简介 68000 汇编语言是为 Motorola 68000 微处理器设计的低级编程语言。68000 微处理器于 1979 年推出，因其强大的性能和灵活的架构而广泛应用于多种计算机系统和嵌入式设备中。以下是对 68000 汇编语言的背景、应用领域以及学习它的好处的详细介绍。\n产生背景 技术进步：\n68000 微处理器是 16 位架构，具有 32 位的地址总线，能够寻址高达 4GB 的内存。这使得它在当时的微处理器中具有较高的性能和灵活性。 其设计采用了复杂指令集计算（CISC）架构，支持多种寻址模式和丰富的指令集。 市场需求：\n1970 年代末和1980年代初，个人计算机和嵌入式系统的需求迅速增长。68000 微处理器因其性能和成本效益被广泛采用。 许多知名的计算机系统（如 Apple Macintosh、Atari ST 和 Sega Genesis）都使用了 68000 处理器。 应用领域 个人计算机：\n68000 微处理器被用于早期的个人计算机，如 Apple Macintosh 和 Atari ST。这些系统的操作系统和应用程序通常使用 68000 汇编语言进行开发。 嵌入式系统：\n68000 处理器也被广泛应用于嵌入式系统，如工业控制、汽车电子和消费电子产品。 游戏机：\nSega Genesis 和其他游戏机使用 68000 处理器，许多经典游戏都是用 68000 汇编语言编写的，学习后可以做一些hackrom的实战。 实时系统：\n由于其高效的性能，68000 处理器在实时系统中也得到了应用，如医疗设备和航空航天系统。 语言学习 EASy68K EASy68K 是一个 68000 结构化汇编语言集成开发环境（IDE）。EASy68K 允许您在 Windows PC 或 Wine 上编辑、汇编和运行 68000 程序。无需额外的硬件。EASy68K 是一个开源项目，根据 GNU 通用公共使用许可分发。 使用easy68k方便我们学习整套68000的编程和调试，学习这件基础知识，对我们hackrom或者逆向的基础。 下载地址：http://www.easy68k.com/files/SetupEASy68K.exe 安装完成后的目录结构",
    "tags": [],
    "title": "68000汇编实战01-编程基础",
    "uri": "/docs/programming/languages/assembly/68000_01_base/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "liaomin416100569博客",
    "uri": "/docs/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "Hugging Face Hugging Face是一个人工智能（AI）公司，它致力于开发和推广自然语言处理（NLP）相关的技术和工具。该公司以其开源项目和社区而闻名，其最知名的项目之一是Transformers库，它提供了一系列预训练的语言模型，包括BERT、GPT和RoBERTa等。这些模型已经在各种NLP任务中取得了显著的成功，并成为了许多研究和工业应用的基础。\n除了提供预训练的模型之外，Hugging Face还开发了一系列工具和平台，使得使用和部署这些模型变得更加简单。其中包括：\nTransformers库：提供了各种预训练的语言模型的接口和工具，使得开发者可以轻松地使用这些模型进行文本分类、命名实体识别、语言生成等任务。\nDatasets库：包含了各种NLP数据集的接口和工具，使得开发者可以方便地使用这些数据集进行模型训练和评估。\nTrainer库：提供了一个训练和微调模型的框架，使得开发者可以方便地使用自己的数据集对预训练模型进行微调，以适应特定的任务和应用场景。\nModel Hub：一个模型分享和发布平台，开发者可以在这里分享自己训练的模型，也可以找到其他人分享的模型，并且可以直接在自己的项目中使用这些模型。\ndatasets数据集处理，transformers预训练微调等相关教程请参考官网hugging face官方文档。\nTransformers Transformers 是由 Hugging Face 开发的一个 NLP 包，支持加载目前绝大部分的预训练模型。随着 BERT、GPT 等大规模语言模型的兴起，越来越多的公司和研究者采用 Transformers 库来构建 NLP 应用，官网地址。 它提供了各种预训练的 Transformer 模型，包括 BERT、GPT、RoBERTa、DistilBERT 等。这些模型在多个 NLP 任务上取得了 state-of-the-art 的性能，并且 Transformers 库提供了简单易用的接口，使得使用这些预训练模型变得非常便捷。\n安装 官网安装教程参考：https://huggingface.co/docs/transformers/installation\n您可以通过 pip 安装 Transformers 库。在终端或命令行界面中执行以下命令（我这里使用pytorch，如果需要tensorflow的版本参考官网）：\npip install 'transformers[torch]'\n这将会自动从 PyPI（Python Package Index）下载并安装最新版本的 Transformers 库及其依赖项。\n如果您使用的是 Anaconda 环境，您也可以通过 conda 安装：\nconda install -c huggingface transformers\n这将会从 Anaconda 仓库中下载并安装 Transformers 库及其依赖项。\n安装完成后，您就可以在 Python 环境中使用 Transformers 库了。您可以编写代码来加载预训练模型、执行各种 NLP 任务，或者使用 Transformers 提供的高级 API，如 pipelines 来快速完成任务。\n模型 自然语言处理（NLP）模型是指用于处理和理解自然语言文本的计算机模型。这些模型的设计和训练旨在使计算机能够自动处理和分析语言数据，以执行各种语言相关的任务。以下是几种常见的NLP模型类型及其功能：\n基于规则的模型：这些模型使用手工制定的规则和规则集来处理文本，例如语法分析或关键词提取。这种方法的局限性在于需要大量的人工工作和难以处理的复杂性。\n基于统计的模型：这些模型利用统计学习技术从大量文本数据中学习语言模式和规律。例如，n-gram语言模型可以预测给定单词序列的下一个单词，而隐马尔可夫模型则用于词性标注和语音识别。\n神经网络模型：随着深度学习的发展，神经网络在NLP中的应用越来越广泛。这些模型使用多层神经网络结构来学习复杂的语言特征和模式。例如，递归神经网络（RNN）、长短时记忆网络（LSTM）和变压器（Transformer）等模型已经在机器翻译、文本生成、情感分析等任务中取得了显著的成就。\n预训练语言模型：这些模型通过在大规模文本数据上进行自监督学习来预先训练，例如BERT（Bidirectional Encoder Representations from Transformers）、GPT（Generative Pre-trained Transformer）等。预训练模型在各种NLP任务中表现出色，并通过微调适应特定的下游任务。\nNLP模型的选择取决于任务的性质和复杂度，以及可用的训练数据和计算资源。随着技术的进步和研究的深入，NLP模型不断演进和改进，为语言处理领域带来了许多创新和新的应用可能性。\n文件结构 训练模型通常具有以下常见的目录文件结构和文件：\nvocab.json: 这是一个包含词汇表的文件，它将模型训练时使用的词汇映射到整数索引。这对于将文本转换为模型可以理解的输入格式（如整数索引或者词嵌入）非常重要。\ntokenizer_config.json: 这个文件包含有关模型使用的分词器（tokenizer）的配置信息，例如分词器的类型、参数设置等。分词器用于将文本划分为词语或子词的序列，并将其转换为模型可以处理的输入格式。\ntokenizer.json: 如果模型使用了特定的分词器，此文件可能包含分词器的具体实现和配置信息。这对于加载和使用模型的时候确保分词器能正确地工作非常重要。\nconfig.json: 这个文件包含了模型本身的配置信息，例如模型的类型（如BERT、GPT）、层数、隐藏单元数等超参数设置。这些信息对于构建和初始化模型极为关键。\npytorch_model.bin 或 tensorflow_model.h5：这是包含预训练模型权重的二进制文件，其格式取决于所使用的深度学习框架。这些权重是模型学习到的参数，用于实际的预测和推理任务。\nspecial_tokens_map.json: 如果模型包含了特殊标记（如填充标记、起始标记等），此文件将包含这些特殊标记的定义及其在模型中的使用方式。\nmerges.txt（对于BERT等子词级别的模型）：这个文件包含将词汇划分为子词或者字符的规则或者合并操作，这对于分词器的工作非常关键。\nREADME.md 或者 model_card.md：这些文件通常包含了关于模型的详细信息，如作者、许可证、训练数据集、性能评估等，对于了解和使用模型非常有帮助。\n每个模型的具体结构和文件可能会有所不同，但上述文件是构成大多数预训练模型的基本要素。通过理解和操作这些文件，可以更好地理解和使用预训练模型进行自然语言处理任务。\npipelines 在 Hugging Face Transformers 中，pipelines 是一种方便的高级 API，用于执行各种自然语言处理（NLP）任务，如文本分类、命名实体识别、问答等。使用 pipelines，您无需编写大量的代码来加载模型、预处理输入数据、执行推理等操作，而是可以通过简单的函数调用来完成这些任务。\nransformers 库将目前的 NLP 任务归纳为几下几类：\n文本分类：例如情感分析、句子对关系判断等； 对文本中的词语进行分类：例如词性标注 (POS)、命名实体识别 (NER) 等； 文本生成：例如填充预设的模板 (prompt)、预测文本中被遮掩掉 (masked) 的词语； 从文本中抽取答案：例如根据给定的问题从一段文本中抽取出对应的答案； 根据输入文本生成新的句子：例如文本翻译、自动摘要等。 Transformers 库最基础的对象就是 pipeline() 函数，它封装了预训练模型和对应的前处理和后处理环节。我们只需输入文本，就能得到预期的答案。目前常用的 pipelines 有：\naudio-classification（音频分类）：用于对音频进行分类，识别音频中的类别或属性。 automatic-speech-recognition（自动语音识别）：用于将音频转换为文本，实现语音识别的功能。 conversational（会话式处理）：用于构建和处理对话系统，实现对话式交互的功能。 depth-estimation（深度估计）：用于从单张图片或视频中估计场景的深度信息。 document-question-answering（文档问答）：用于从文档中回答问题，帮助用户获取文档内容中的相关信息。 feature-extraction（特征提取）：用于从文本、图片等数据中提取特征，用于后续的任务或分析。 fill-mask（填空）：用于给定带有空白的句子，预测并填补空白处的单词或短语。 image-classification（图片分类）：用于对图片进行分类，识别图片中的类别或属性。 image-feature-extraction（图片特征提取）：用于从图片中提取特征，用于后续的任务或分析。 image-segmentation（图片分割）：用于将图片分割成不同的区域或对象，进行图像分割任务。 image-to-image（图片到图片）：用于执行图片到图片的转换，如图像风格转换、图像去噪等。 image-to-text（图片到文本）：用于从图片中提取文本信息，实现图片中的文字识别功能。 mask-generation（遮罩生成）：用于生成图片中的遮罩或掩码，用于图像处理或分割任务。 object-detection（目标检测）：用于从图片或视频中检测和识别出图像中的目标对象。 question-answering（问答）：用于回答给定问题的模型，从文本中找出包含答案的部分。 summarization（摘要生成）：用于生成文本的摘要或总结，将文本内容压缩为简短的形式。 table-question-answering（表格问答）：用于从表格数据中回答问题，帮助用户从表格中获取信息。 text2text-generation（文本到文本生成）：用于生成文本的模型，可以执行文本到文本的转换或生成任务。 text-classification（文本分类）：(别名\"sentiment-analysis\" 可用，情感分析)用于将文本进行分类，识别文本中的类别或属性。 text-generation（文本生成）：用于生成文本的模型，可以生成连续的文本序列。 text-to-audio（文本到音频）：用于将文本转换为语音，实现文本到语音的功能。 token-classification（标记分类）：别名\"ner\" 可用，命名实体识别，用于将文本中的每个标记或单词进行分类，识别每个标记的类别或属性。 translation（翻译）：用于执行文本的翻译任务，将文本从一种语言翻译成另一种语言。 video-classification（视频分类）：用于对视频进行分类，识别视频中的类别或属性。 visual-question-answering（视觉问答）：用于从图片或视频中回答问题，结合视觉和文本信息进行问答。 zero-shot-classification（零样本分类）：用于执行零样本分类任务，即在没有见过该类别的情况下对新样本进行分类。 zero-shot-image-classification（零样本图片分类）：用于执行零样本分类任务，即在没有见过该类别的情况下对新图片进行分类。 zero-shot-audio-classification（零样本音频分类）：用于执行零样本分类任务，即在没有见过该类别的情况下对新音频进行分类。 zero-shot-object-detection（零样本目标检测）：用于执行零样本目标检测任务，即在没有见过该类别的情况下对新图片中的目标对象进行检测。 如果需要了解更多的task类型更新，参考官网pipeline： 下面我们以常见的几个 NLP 任务为例，展示如何调用这些 pipeline 模型。\n图片转文本 教程参考自官网：https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/pipelines#transformers.ImageToTextPipeline from transformers import pipeline\ritt=pipeline(\"image-to-text\",model=\"ydshieh/vit-gpt2-coco-en\") #model不指定会使用默认模型。\rrtn=itt(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\rprint(rtn) 可以看到输出是需要先下载模型（下载一次，自动缓存），下载在C:\\Users\\admin.cache\\huggingface\\hub目录下。\n:\\python\\evn311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--ydshieh--vit-gpt2-coco-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\rTo support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\rwarnings.warn(message) 最后输出：[{‘generated_text’: ’two birds are standing next to each other ‘}]\n如果希望使用其他的image-to-text模型可以在官网搜索 https://huggingface.co/models?pipeline_tag=image-to-text\u0026sort=trending 比如选择image-to-text右侧文本框输入chinese，看下是不是有中文描述的 使用这个模型来测试下\nimage_path=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\rfrom transformers import pipeline\ritt=pipeline(\"image-to-text\",model=\"IDEA-CCNL/Taiyi-BLIP-750M-Chinese\")\rrtn=itt(image_path)\rprint(rtn) 输出（效果没有英文的模型好，明明是两只鹦鹉啊，不过识别出了鹦鹉，英文的只是两只鸟） [{‘generated_text’: ‘一 只 鹦 鹉 的 黑 白 照 片 。’}]\n文本生成 我们首先根据任务需要构建一个模板 (prompt)，然后将其送入到模型中来生成后续文本。注意，由于文本生成具有随机性，因此每次运行都会得到不同的结果。\n#%%\rfrom transformers import pipeline\rgenerator = pipeline(\"text-generation\",model=\"openai-community/gpt2\")\rprint(generator(\"I can't believe you did such a \")) 输出：\n[{'generated_text': 'I can\\'t believe you did such a _____t!\"\\n\\n\"You know I\\'m kind of an asshole to you, I mean?\"\\n\\n\"Just because I had one thing to do doesn\\'t mean I hate you. I know you'}] 在huggerface上搜索一个古诗词生成的模型， 左侧选择tag Text Generation ，搜索poem，选择最多人喜欢。 from transformers import pipeline\rgenerator = pipeline(\"text-generation\",model=\"uer/gpt2-chinese-poem\")\rprint(generator(\"[CLS] 离 离 原 上 草 ，\")) 输出\n[{'generated_text': '[CLS] 离 离 原 上 草 ， 濯 濯 原 上 桑 。 春 风 吹 罗 衣 ， 行 人 泪 成 行 。 离 人 不 可 留 ， 况 乃 隔 河 梁 。 当 和 露 餐 ， 勿 复 怨 秋 凉 。 愿 言 崇 令 德 ， 以 配 君 子 光 。 毋 怀 远 心 ， 皓 月 鉴 我 伤 。 莫 怨 东 风 ， 飘 然 入 西 楼 。 举 手 倚 阑 干 ， 举 酒 相 劝 酬 。 良 时 焉 可 再 ， 逝 水 何 悠 悠 。 我 金 石 交 ， 沉 邈 焉 能 求'}] 情感分析 借助情感分析 pipeline，我们只需要输入文本，就可以得到其情感标签（积极/消极）以及对应的概率：\nfrom transformers import pipeline\rclassifier = pipeline(\"sentiment-analysis\")\rresult = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\rprint(result)\rresults = classifier(\r[\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\r)\rprint(results) No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\r[{'label': 'POSITIVE', 'score': 0.9598048329353333}]\r[{'label': 'POSITIVE', 'score': 0.9598048329353333}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}] pipeline 模型会自动完成以下三个步骤：\n将文本预处理为模型可以理解的格式； 将预处理好的文本送入模型； 对模型的预测值进行后处理，输出人类可以理解的格式。 pipeline 会自动选择合适的预训练模型来完成任务。例如对于情感分析，默认就会选择微调好的英文情感模型 distilbert-base-uncased-finetuned-sst-2-english。\nTransformers 库会在创建对象时下载并且缓存模型，只有在首次加载模型时才会下载，后续会直接调用缓存好的模型。\n零训练样本分类 零训练样本分类 pipeline 允许我们在不提供任何标注数据的情况下自定义分类标签。\nfrom transformers import pipeline\rclassifier = pipeline(\"zero-shot-classification\")\rresult = classifier(\r\"This is a course about the Transformers library\",\rcandidate_labels=[\"education\", \"politics\", \"business\"],\r)\rprint(result) No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\r{'sequence': 'This is a course about the Transformers library', 'labels': ['education', 'business', 'politics'], 'scores': [0.8445973992347717, 0.11197526752948761, 0.043427325785160065]} 可以看到，pipeline 自动选择了预训练好的 facebook/bart-large-mnli 模型来完成任务。\n遮盖词填充 给定一段部分词语被遮盖掉 (masked) 的文本，使用预训练模型来预测能够填充这些位置的词语。\nfrom transformers import pipeline\runmasker = pipeline(\"fill-mask\")\rresults = unmasker(\"This course will teach you all about \u003cmask\u003e models.\", top_k=2)\rprint(results) No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\r[{'sequence': 'This course will teach you all about mathematical models.', 'score': 0.19619858264923096, 'token': 30412, 'token_str': ' mathematical'}, {'sequence': 'This course will teach you all about computational models.', 'score': 0.04052719101309776, 'token': 38163, 'token_str': ' computational'}] 可以看到，pipeline 自动选择了预训练好的 distilroberta-base 模型来完成任务。\n命名实体识别 命名实体识别 (NER) pipeline 负责从文本中抽取出指定类型的实体，例如人物、地点、组织等等。\nfrom transformers import pipeline\rner = pipeline(\"ner\", grouped_entities=True)\rresults = ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\rprint(results) No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\r[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.97960186, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}] 可以看到，模型正确地识别出了 Sylvain 是一个人物，Hugging Face 是一个组织，Brooklyn 是一个地名。\n这里通过设置参数 grouped_entities=True，使得 pipeline 自动合并属于同一个实体的多个子词 (token)，例如这里将“Hugging”和“Face”合并为一个组织实体，实际上 Sylvain 也进行了子词合并，因为分词器会将 Sylvain 切分为 S、##yl 、##va 和 ##in 四个 token。\n自动问答 自动问答 pipeline 可以根据给定的上下文回答问题，例如：\nfrom transformers import pipeline\rquestion_answerer = pipeline(\"question-answering\")\ranswer = question_answerer(\rquestion=\"Where do I work?\",\rcontext=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\r)\rprint(answer) No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\r{'score': 0.6949771046638489, 'start': 33, 'end': 45, 'answer': 'Hugging Face'} 可以看到，pipeline 自动选择了在 SQuAD 数据集上训练好的 distilbert-base 模型来完成任务。这里的自动问答 pipeline 实际上是一个抽取式问答模型，即从给定的上下文中抽取答案，而不是生成答案。\n根据形式的不同，自动问答 (QA) 系统可以分为三种：\n**抽取式 QA (extractive QA)：**假设答案就包含在文档中，因此直接从文档中抽取答案； **多选 QA (multiple-choice QA)：**从多个给定的选项中选择答案，相当于做阅读理解题； **无约束 QA (free-form QA)：**直接生成答案文本，并且对答案文本格式没有任何限制。 自动摘要 自动摘要 pipeline 旨在将长文本压缩成短文本，并且还要尽可能保留原文的主要信息，例如：\nfrom transformers import pipeline\rsummarizer = pipeline(\"summarization\")\rresults = summarizer(\r\"\"\"\rAmerica has changed dramatically during recent years. Not only has the number of graduates in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering declined, but in most of the premier American universities engineering curricula now concentrate on and encourage largely the study of engineering science. As a result, there are declining offerings in engineering subjects dealing with infrastructure, the environment, and related issues, and greater concentration on high technology subjects, largely supporting increasingly complex scientific developments. While the latter is important, it should not be at the expense of more traditional engineering.\rRapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance the teaching of engineering. Both China and India, respectively, graduate six and eight times as many traditional engineers as does the United States. Other industrial countries at minimum maintain their output, while America suffers an increasingly serious decline in the number of engineering graduates and a lack of well-educated engineers.\r\"\"\"\r)\rprint(results) No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\r[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance engineering .'}] 可以看到，pipeline 自动选择了预训练好的 distilbart-cnn-12-6 模型来完成任务。与文本生成类似，我们也可以通过 max_length 或 min_length 参数来控制返回摘要的长度。\npipeline 背后做了什么？ 这些简单易用的 pipeline 模型实际上封装了许多操作，下面我们就来了解一下它们背后究竟做了啥。以第一个情感分析 pipeline 为例，我们运行下面的代码\nfrom transformers import pipeline\rclassifier = pipeline(\"sentiment-analysis\")\rresult = classifier(\"This course is amazing!\")\rprint(result) 就会得到结果：\n[{'label': 'POSITIVE', 'score': 0.9998824596405029}] 实际上它的背后经过了三个步骤：\n预处理 (preprocessing)，将原始文本转换为模型可以接受的输入格式； 将处理好的输入送入模型； 对模型的输出进行后处理 (postprocessing)，将其转换为人类方便阅读的格式。 使用分词器进行预处理 因为神经网络模型无法直接处理文本，因此首先需要通过预处理环节将文本转换为模型可以理解的数字。具体地，我们会使用每个模型对应的分词器 (tokenizer) 来进行：\n将输入切分为词语、子词或者符号（例如标点符号），统称为 tokens； 根据模型的词表将每个 token 映射到对应的 token 编号（就是一个数字）； 根据模型的需要，添加一些额外的输入。 我们对输入文本的预处理需要与模型自身预训练时的操作完全一致，只有这样模型才可以正常地工作。注意，每个模型都有特定的预处理操作，如果对要使用的模型不熟悉，可以通过 Model Hub 查询。这里我们使用 AutoTokenizer 类和它的 from_pretrained() 函数，它可以自动根据模型 checkpoint 名称来获取对应的分词器。\n情感分析 pipeline 的默认 checkpoint 是 distilbert-base-uncased-finetuned-sst-2-english，下面我们手工下载并调用其分词器：\nfrom transformers import AutoTokenizer\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\rprint(inputs) {\r'input_ids': tensor([\r[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102],\r[ 101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0,\r0, 0, 0, 0, 0, 0]\r]), 'attention_mask': tensor([\r[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\r])\r} 可以看到，输出中包含两个键 input_ids 和 attention_mask，其中 input_ids 对应分词之后的 tokens 映射到的数字编号列表，而 attention_mask 则是用来标记哪些 tokens 是被填充的（这里“1”表示是原文，“0”表示是填充字符）。\n先不要关注 padding、truncation 这些参数，以及 attention_mask 项，后面我们会详细介绍:)。\n将预处理好的输入送入模型 预训练模型的下载方式和分词器 (tokenizer) 类似，Transformers 包提供了一个 AutoModel 类和对应的 from_pretrained() 函数。下面我们手工下载这个 distilbert-base 模型：\nfrom transformers import AutoModel\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rmodel = AutoModel.from_pretrained(checkpoint) 预训练模型的本体只包含基础的 Transformer 模块，对于给定的输入，它会输出一些神经元的值，称为 hidden states 或者特征 (features)。对于 NLP 模型来说，可以理解为是文本的高维语义表示。这些 hidden states 通常会被输入到其他的模型部分（称为 head），以完成特定的任务，例如送入到分类头中完成文本分类任务。\n其实前面我们举例的所有 pipelines 都具有类似的模型结构，只是模型的最后一部分会使用不同的 head 以完成对应的任务。 Transformers库结构 Transformers 库封装了很多不同的结构，常见的有：\n*Model （返回 hidden states） *ForCausalLM （用于条件语言模型），是一种用于因果语言模型的Transformer接口类型。在Transformer架构中,ForCausalLM是一个特殊的模型头,它被设计用于生成文本,即从前一个词预测下一个词,这类模型主要用于以下任务：1.根据已有文本生成后续文本，例如自动写作、对话生成等,2.语言建模：预测给定文本序列中下一个词的概率。 *ForSeq2SeqLM代表用于序列到序列（Sequence-to-Sequence）任务的语言模型。这类模型通常用于以下任务：1.机器翻译：将一种语言的句子翻译成另一种语言,2.文本摘要：将长文本生成简短的摘要,3.问答系统：从上下文中生成回答。。 *ForMaskedLM （用于遮盖语言模型），在MLM任务中,输入序列中的某些词会被随机mask掉,模型的目标是预测这些被mask的词。 *ForMultipleChoice （用于多选任务） *ForQuestionAnswering （用于自动问答任务） *ForSequenceClassification （用于文本分类任务），类似api如：图像分类：*ForImageClassification，输出维度对应于整个序列的类别预测,即整个输入序列只有一个类别输出。 *ForTokenClassification （用于 token 分类任务，例如 NER），输出维度对应于每个token的类别预测,即每个输入token都有一个类别输出。 模块输出 Transformer 模块的输出是一个维度为 (Batch size, Sequence length, Hidden size) 的三维张量，其中 Batch size 表示每次输入的样本（文本序列）数量，即每次输入多少个句子，上例中为 2；Sequence length 表示文本序列的长度，即每个句子被分为多少个 token，上例中为 16；Hidden size 表示每一个 token 经过模型编码后的输出向量（语义表示）的维度。\n预训练模型编码后的输出向量的维度通常都很大，例如 Bert 模型 base 版本的输出为 768 维，一些大模型的输出维度为 3072 甚至更高。\n我们可以打印出这里使用的 distilbert-base 模型的输出维度：\nfrom transformers import AutoTokenizer, AutoModel\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rmodel = AutoModel.from_pretrained(checkpoint)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\rprint(outputs.last_hidden_state.shape) torch.Size([2, 16, 768]) Transformers 模型的输出格式类似 namedtuple 或字典，可以像上面那样通过属性访问，也可以通过键（outputs[\"last_hidden_state\"]），甚至索引访问（outputs[0]）。\n对于情感分析任务，很明显我们最后需要使用的是一个文本分类 head。因此，实际上我们不会使用 AutoModel 类，而是使用 AutoModelForSequenceClassification：\nfrom transformers import AutoTokenizer\rfrom transformers import AutoModelForSequenceClassification\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\rprint(outputs.logits.shape) torch.Size([2, 2]) 可以看到，对于 batch 中的每一个样本，模型都会输出一个两维的向量（每一维对应一个标签，positive 或 negative）。\n对模型输出进行后处理 由于模型的输出只是一些数值，因此并不适合人类阅读。例如我们打印出上面例子的输出：\nfrom transformers import AutoTokenizer\rfrom transformers import AutoModelForSequenceClassification\rcheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\rprint(outputs.logits) tensor([[-1.5607, 1.6123],\r[ 4.1692, -3.3464]], grad_fn=\u003cAddmmBackward0\u003e) 模型对第一个句子输出 [−1.5607,1.6123]，对第二个句子输出 [4.1692,−3.3464]，它们并不是概率值，而是模型最后一层输出的 logits 值。要将他们转换为概率值，还需要让它们经过一个 SoftMax 层，例如：\nimport torch\rpredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\rprint(predictions) tensor([[4.0195e-02, 9.5980e-01],\r[9.9946e-01, 5.4418e-04]], grad_fn=\u003cSoftmaxBackward0\u003e) 所有 Transformers 模型都会输出 logits 值，因为训练时的损失函数通常会自动结合激活函数（例如 SoftMax）与实际的损失函数（例如交叉熵 cross entropy）。\n这样模型的预测结果就是容易理解的概率值：第一个句子 [0.0402,0.9598]，第二个句子 [0.9995,0.0005]。最后，为了得到对应的标签，可以读取模型 config 中提供的 id2label 属性：\nprint(model.config.id2label) {0: 'NEGATIVE', 1: 'POSITIVE'} 于是我们可以得到最终的预测结果：\n第一个句子: NEGATIVE: 0.0402, POSITIVE: 0.9598 第二个句子: NEGATIVE: 0.9995, POSITIVE: 0.0005 本文部分文本引用自：https://transformers.run/",
    "description": "Hugging Face Hugging Face是一个人工智能（AI）公司，它致力于开发和推广自然语言处理（NLP）相关的技术和工具。该公司以其开源项目和社区而闻名，其最知名的项目之一是Transformers库，它提供了一系列预训练的语言模型，包括BERT、GPT和RoBERTa等。这些模型已经在各种NLP任务中取得了显著的成功，并成为了许多研究和工业应用的基础。\n除了提供预训练的模型之外，Hugging Face还开发了一系列工具和平台，使得使用和部署这些模型变得更加简单。其中包括：\nTransformers库：提供了各种预训练的语言模型的接口和工具，使得开发者可以轻松地使用这些模型进行文本分类、命名实体识别、语言生成等任务。\nDatasets库：包含了各种NLP数据集的接口和工具，使得开发者可以方便地使用这些数据集进行模型训练和评估。\nTrainer库：提供了一个训练和微调模型的框架，使得开发者可以方便地使用自己的数据集对预训练模型进行微调，以适应特定的任务和应用场景。\nModel Hub：一个模型分享和发布平台，开发者可以在这里分享自己训练的模型，也可以找到其他人分享的模型，并且可以直接在自己的项目中使用这些模型。\ndatasets数据集处理，transformers预训练微调等相关教程请参考官网hugging face官方文档。\nTransformers Transformers 是由 Hugging Face 开发的一个 NLP 包，支持加载目前绝大部分的预训练模型。随着 BERT、GPT 等大规模语言模型的兴起，越来越多的公司和研究者采用 Transformers 库来构建 NLP 应用，官网地址。 它提供了各种预训练的 Transformer 模型，包括 BERT、GPT、RoBERTa、DistilBERT 等。这些模型在多个 NLP 任务上取得了 state-of-the-art 的性能，并且 Transformers 库提供了简单易用的接口，使得使用这些预训练模型变得非常便捷。\n安装 官网安装教程参考：https://huggingface.co/docs/transformers/installation\n您可以通过 pip 安装 Transformers 库。在终端或命令行界面中执行以下命令（我这里使用pytorch，如果需要tensorflow的版本参考官网）：\npip install 'transformers[torch]'\n这将会自动从 PyPI（Python Package Index）下载并安装最新版本的 Transformers 库及其依赖项。\n如果您使用的是 Anaconda 环境，您也可以通过 conda 安装：\nconda install -c huggingface transformers\n这将会从 Anaconda 仓库中下载并安装 Transformers 库及其依赖项。\n安装完成后，您就可以在 Python 环境中使用 Transformers 库了。您可以编写代码来加载预训练模型、执行各种 NLP 任务，或者使用 Transformers 提供的高级 API，如 pipelines 来快速完成任务。",
    "tags": [],
    "title": "Transformers实战01-开箱即用的 pipelines",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_01/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "BERT（Bidirectional Encoder Representations from Transformers）是一种基于 Transformer 模型的预训练语言表示方法，由Google研究团队于2018年提出。BERT 通过在大规模文本语料上进行无监督的预训练，学习了通用的语言表示，并且在各种自然语言处理任务中取得了显著的性能提升。\nBERT仅使用了Transformer架构的Encoder部分。BERT自2018年由谷歌发布后，在多种NLP任务中（例如QA、文本生成、情感分析等等）都实现了更好的结果。\n“Word2vec与GloVe都有一个特点，就是它们是上下文无关（context-free）的词嵌入。所以它们没有解决：一个单词在不同上下文中代表不同的含义的问题。例如，对于单词bank，它在不同的上下文中，有银行、河畔这种差别非常大的含义。BERT的出现，解决了这个问题。\nBERT 的主要特点包括：\n双向性：BERT 使用双向 Transformer 模型来处理输入序列，从而能够同时考虑上下文的信息，而不仅仅是单向的上下文信息。这种双向性使得 BERT 能够更好地理解句子中的语义和语境。\n预训练-微调框架：BERT 使用了预训练-微调的方法。首先，在大规模文本语料上进行无监督的预训练，通过 Masked Language Model（MLM）和 Next Sentence Prediction（NSP）任务学习语言表示；然后，在特定的下游任务上微调模型参数，使其适应于特定的任务，如文本分类、命名实体识别等。\nTransformer 模型：BERT 基于 Transformer 模型结构，其中包括多层的编码器，每个编码器由自注意力机制和前馈神经网络组成。这种结构能够有效地捕获输入序列中的长距离依赖关系，有助于提高模型在各种自然语言处理任务中的性能。\n多层表示：BERT 提供了多层的语言表示，使得用户可以根据具体任务选择不同层的表示进行应用。较底层的表示通常更加接近原始输入，而较高层的表示则更加抽象，包含了更多的语义信息。\n开放源代码：BERT 的源代码和预训练模型已经在 GitHub 上开放，使得研究人员和开发者可以基于 BERT 进行进一步的研究和应用开发。\nBERT 通过预训练大规模文本语料上的通用语言表示，以及在各种下游任务上的微调，有效地提高了自然语言处理任务的性能，并且成为了当前领域内最具影响力的预训练模型之一。\ntransformer提供了不同领域中常见的机器学习模型类型：\nTEXT MODELS（文本模型）：用于处理和分析文本数据的模型，如自然语言处理（NLP）中的BERT、GPT等。\nVISION MODELS（视觉模型）：用于处理和分析图像数据的模型，如卷积神经网络（CNN）中的ResNet、VGG,Vision Transformer (ViT)等。\nAUDIO MODELS（音频模型）：用于处理和分析音频数据的模型，如声学模型、语音识别模型等。\nVIDEO MODELS（视频模型）：用于处理和分析视频数据的模型，如视频分类、目标检测、行为识别等。\nMULTIMODAL MODELS（多模态模型）：结合多种数据类型（如文本、图像、音频等）进行分析和预测的模型，如OpenAI的CLIP。\nREINFORCEMENT LEARNING MODELS（强化学习模型）：用于解决强化学习问题的模型，如Deep Q-Networks（DQN）、Actor-Critic等。\nTIME SERIES MODELS（时间序列模型）：用于分析和预测时间序列数据的模型，如循环神经网络（RNN）、长短期记忆网络（LSTM）等。\nGRAPH MODELS（图模型）：用于处理和分析图数据的模型，如图神经网络（GNN）、图卷积网络（GCN）等。\nBERT的基本原理 BERT基于的是Transformer模型，并且仅使用Transformer模型的Encoder部分。在Transformer模型中，Encoder的输入是一串序列，输出的是对序列中每个字符的表示。同样，在BERT中，输入的是一串序列，输出的是也是对应序列中每个单词的编码。 以“He got bit by Python”为例，BERT的输入输出如下图所示： 其中输入为序列“He got bit by Python”，输出的是对每个单词的编码$R_{word}$。这样在经过了BERT处理后，即得到了对每个单词包含的上下文表示$R_{word}$。\n分词 from transformers import AutoModel, BertTokenizer\rmodel_name=\"bert-base-chinese\" #bert-base-uncased\rmodel=AutoModel.from_pretrained(model_name)\rtokenizer=BertTokenizer.from_pretrained(model_name)\rprint(type(model),type(tokenizer))\rsequence = [\"我出生在湖南A阳,我得家在深圳.\",\"我得儿子是廖X谦\"]\r#输出中包含两个键 input_ids 和 attention_mask，其中 input_ids 对应分词之后的 tokens 映射到的数字编号列表，而 attention_mask 则是用来标记哪些 tokens #是被填充的（这里“1”表示是原文，“0”表示是填充字符）。\rprint(tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\",pair=True))\r#将输入切分为词语、子词或者符号（例如标点符号），统称为 tokens；\rprint(tokenizer.tokenize(sequence[0]),len(tokenizer.tokenize(sequence[0])))\r#我们通过 convert_tokens_to_ids() 将切分出的 tokens 转换为对应的 token IDs：\rprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence[0])))\r#可以通过 encode() 函数将这两个步骤合并，并且 encode() 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加[CLS] 和 [SEP]\rprint(tokenizer.encode(sequence[0]))\r#解码还原文字，可以看到encode前后加了[CLS] 和 [SEP]\rprint(tokenizer.decode(tokenizer.encode(sequence[1]))) 输出\n\u003cclass 'transformers.models.bert.modeling_bert.BertModel'\u003e \u003cclass 'transformers.models.bert.tokenization_bert.BertTokenizer'\u003e\r{'input_ids': tensor([[ 101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 2533,\r2157, 1762, 3918, 1766, 119, 102],\r[ 101, 2769, 2533, 1036, 2094, 3221, 2445, 3813, 6472, 102, 0, 0,\r0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\r['我', '出', '生', '在', '湖', '南', 'A', '阳', ',', '我', '得', '家', '在', '深', '圳', '.'] 16\r[2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 2533, 2157, 1762, 3918, 1766, 119]\r[101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 2533, 2157, 1762, 3918, 1766, 119, 102]\r[CLS] 我 得 儿 子 是 廖 X 谦 [SEP] 模型输出 #这里演示最终输出隐藏状态得输出\rfrom transformers import AutoModel,AutoTokenizer\rmodel_name=\"bert-base-chinese\" #bert-base-uncased\rmodel=AutoModel.from_pretrained(model_name)\rtokenizer=BertTokenizer.from_pretrained(model_name)\rraw_inputs = [\r\"I've been waiting for a HuggingFace course my whole life.\",\r\"I hate this so much!\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\rprint(\"词个数\",len(tokenizer.encode(raw_inputs[0])))\r\"\"\"\r在BERT模型中，last_hidden_state 的形状是 [batch_size, sequence_length, hidden_size]，其中：\rbatch_size 表示批量大小，即输入的样本数量。在你的例子中，batch_size 是 2，表示你有两个句子。\rsequence_length 表示序列长度，即输入文本中词元的数量。在你的例子中，sequence_length 是 19，表示每个句子包含 19 个词元,我爱中国，我就是一个词元，爱也是一个词元。\rhidden_size 表示隐藏状态的维度，通常是模型的隐藏层的大小。在BERT-base模型中，hidden_size 是 768，表示每个词元的隐藏状态是一个包含 768 个值的向量。\r\"\"\"\rprint(outputs.last_hidden_state.shape) 输出：torch.Size([2, 19, 768])\nBERT预训练的方法 BERT的预训练语料库使用的是Toronto BookCorpus和Wikipedia数据集。在准备训练数据时，首先从语料库中采样2条句子，例如Sentence-A与Sentence-B。这里需要注意的是：2条句子的单词之和不能超过512个。对于采集的这些句子，50%为两个句子是相邻句子，另50%为两个句子毫无关系。\n假设采集了以下2条句子：\nBeijing is a beautiful city\rI love Beijing 对这2条句子先做分词：\nTokens = [ [CLS], Beijing, is, a, beautiful, city, [SEP], I, love, Beijing, [SEP] ] 然后，以15%的概率遮挡单词，并遵循80%-10%-10%的规则。假设遮挡的单词为city，则：\nTokens = [ [CLS], Beijing, is, a, beautiful, [MASK], [SEP], I, love, Beijing, [SEP] ]\n接下来将Tokens送入到BERT中，并训练BERT预测被遮挡的单词，同时也要预测这2条句子是否为相邻（句子2是句子1的下一条句子）。也就是说，BERT是同时训练Masked Language Modeling和NSP任务。\nBERT的训练参数是：1000000个step，每个batch包含256条序列（256 * 512个单词 = 128000单词/batch）。使用的是Adam，learning rate为1e-4、β1 = 0.9、β2 = 0.999。L2正则权重的衰减参数为0.01。对于learning rete，前10000个steps使用了rate warmup，之后开始线性衰减learning rate（简单地说，就是前期训练使用一个较大的learning rate，后期开始线性减少）。对所有layer使用0.1概率的dropout。使用的激活函数为gelu，而非relu。 验证使用两条句子。\ncheckpoint = \"bert-base-chinese\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rraw_inputs = [\r\"拼多多得货物真是差劲.\",\r\"我喜欢天猫，天猫货物都很好\",\r]\rraw_inputs1 = [\r\"拼多多买了一件掉色衣服.\",\r\"我在天猫买的衣服颜色还行\",\r]\r#允许传入两个数组，相同索引会自动通过[SEP]拼接。\rinputs = tokenizer(raw_inputs,raw_inputs1, padding=True, truncation=True, return_tensors=\"pt\")\rprint(tokenizer.decode(inputs.input_ids[0]))\rprint(tokenizer.decode(inputs.input_ids[1])) 输出\n[CLS] 拼 多 多 得 货 物 真 是 差 劲. [SEP] 拼 多 多 买 了 一 件 掉 色 衣 服. [SEP] [PAD] [PAD]\r[CLS] 我 喜 欢 天 猫 ， 天 猫 货 物 都 很 好 [SEP] 我 在 天 猫 买 的 衣 服 颜 色 还 行 [SEP] 预测的整个过程\n#演示预测的整个过程。\rimport torch\rfrom transformers import AutoModelForSequenceClassification\r#情感分析任务\rcheckpoint = \"bert-base-chinese\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\rprint(type(model))\rraw_inputs = [\r\"拼多多得货物真是差劲.\",\r\"我喜欢天猫，天猫货物都很好\",\r]\rinputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\routputs = model(**inputs)\r#将分词的词反编码出来\rprint(tokenizer.decode(inputs.input_ids[0]),tokenizer.decode(inputs.input_ids[1]))\r#\"Logits\" 是指模型在分类问题中输出的未经过 softmax 或 sigmoid 函数处理的原始预测值。\rprint(\"分类输出形状:\",outputs.logits.shape)\rprint(\"分类输出:\",outputs.logits)\r#经过softmax就是预测的结果了\rpredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\r#预测的每一行是一个句子，第一列表示积极的概率，第二列表示不积极的概率\rprint(\"预测结果:\",predictions)\r#有两种分类0表示积极，1表示不积极\rprint(\"label和索引:\",print(model.config.id2label)) 输出\n[CLS] 拼 多 多 得 货 物 真 是 差 劲. [SEP] [PAD] [PAD] [CLS] 我 喜 欢 天 猫 ， 天 猫 货 物 都 很 好 [SEP]\r分类输出形状: torch.Size([2, 2])\r分类输出: tensor([[0.4789, 1.0043],\r[0.2907, 0.7432]], grad_fn=\u003cAddmmBackward0\u003e)\r预测结果: tensor([[0.3716, 0.6284],\r[0.3888, 0.6112]], grad_fn=\u003cSoftmaxBackward0\u003e)\r{0: 'LABEL_0', 1: 'LABEL_1'} BERT模型微调 加载数据集 我们以同义句判断任务为例（每次输入两个句子，判断它们是否为同义句），带大家构建我们的第一个 Transformers 模型。我们选择蚂蚁金融语义相似度数据集 AFQMC 作为语料，它提供了官方的数据划分，训练集（train.json） / 验证集（dev.json） / 测试集(test.json)分别包含 34334 / 4316 / 3861 个句子对，标签 0 表示非同义句，1 表示同义句：\n{\"sentence1\": \"还款还清了，为什么花呗账单显示还要还款\", \"sentence2\": \"花呗全额还清怎么显示没有还款\", \"label\": \"1\"} 训练集用于训练模型，验证集用于每次epoch后训练集的正确率，测试集用于验证最后生成模型的准确率。\nDataset Pytorch 通过 Dataset 类和 DataLoader 类处理数据集和加载样本。同样地，这里我们首先继承 Dataset 类构造自定义数据集，以组织样本和标签。AFQMC 样本以 json 格式存储，因此我们使用 json 库按行读取样本，并且以行号作为索引构建数据集。\nclass MyDataSet(Dataset):\rdef __init__(self,filePath):\rself.data={}\rcurrent_directory = os.getcwd()\rwith open(current_directory+\"/dataset/\"+filePath,\"rt\", encoding=\"utf-8\") as f:\rfor idx,line in enumerate(f):\rself.data[idx]=json.loads(line.strip())\rdef __getitem__(self, item):\rreturn self.data[item]\rdef __len__(self):\rreturn len(self.data)\rtrain_data=MyDataSet(\"train.json\")\rdev_data=MyDataSet(\"dev.json\")\rprint(dev_data[1]) 输出:\n{'id': 1, 'sentence1': '网商贷怎么转变成借呗', 'sentence2': '如何将网商贷切换为借呗'} 可以看到，我们编写的 AFQMC 类成功读取了数据集，每一个样本都以字典形式保存，分别以 sentence1、sentence2 和 label 为键存储句子对和标签。\n如果数据集非常巨大，难以一次性加载到内存中，我们也可以继承 IterableDataset 类构建迭代型数据集：\nclass MyDataSetIter(IterableDataset):\rdef __init__(self,filePath):\rself.filePath=filePath\rdef __iter__(self):\rcurrent_directory = os.getcwd()\rwith open(current_directory+\"/dataset/\"+self.filePath,\"rt\", encoding=\"utf-8\") as f:\rfor _,line in enumerate(f):\rdata=json.loads(line.strip())\ryield data\rprint(next(iter(MyDataSetIter(\"dev.json\")))) 输出：\n{'sentence1': '双十一花呗提额在哪', 'sentence2': '里可以提花呗额度', 'label': '0'} DataLoader 接下来就需要通过 DataLoader 库按批 (batch) 加载数据，并且将样本转换成模型可以接受的输入格式。对于 NLP 任务，这个环节就是将每个 batch 中的文本按照预训练模型的格式进行编码（包括 Padding、截断等操作）。\n我们通过手工编写 DataLoader 的批处理函数 collate_fn 来实现。首先加载分词器，然后对每个 batch 中的所有句子对进行编码，同时把标签转换为张量格式：\n#DataLoader处理数据为seq1 [SEP] seq2\rfrom transformers import AutoTokenizer\rimport torch\rfrom torch.utils.data import DataLoader\rcheckpoint = \"bert-base-chinese\"\rtokenizer = AutoTokenizer.from_pretrained(checkpoint)\rdef collote_fn(batch_samples):\rbatch_sentence_1, batch_sentence_2 = [], []\rbatch_label = []\rfor sample in batch_samples:\rbatch_sentence_1.append(sample['sentence1'])\rbatch_sentence_2.append(sample['sentence2'])\rbatch_label.append(int(sample['label']))\rX = tokenizer(\rbatch_sentence_1,\rbatch_sentence_2,\rpadding=True,\rtruncation=True,\rreturn_tensors=\"pt\"\r)\ry = torch.tensor(batch_label)\rreturn X, y\rtrain_loader=DataLoader(train_data,batch_size=4,shuffle=False,collate_fn=collote_fn)\rX,y=next(iter(train_loader))\rprint(\"label的维度\",y.shape)\rprint(\"s1,s2合并的维度\",X.input_ids.shape)\rfor idx,d in enumerate(X.input_ids):\rprint(\"第一批次4个元素中的第{}个：{},label={}\".format(idx,tokenizer.decode(X.input_ids[idx]),y[idx])) 输出\nlabel的维度 torch.Size([4])\rs1,s2合并的维度 torch.Size([4, 30])\r第一批次4个元素中的第0个：[CLS] 蚂 蚁 借 呗 等 额 还 款 可 以 换 成 先 息 后 本 吗 [SEP] 借 呗 有 先 息 到 期 还 本 吗 [SEP],label=0\r第一批次4个元素中的第1个：[CLS] 蚂 蚁 花 呗 说 我 违 约 一 次 [SEP] 蚂 蚁 花 呗 违 约 行 为 是 什 么 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD],label=0\r第一批次4个元素中的第2个：[CLS] 帮 我 看 一 下 本 月 花 呗 账 单 有 没 有 结 清 [SEP] 下 月 花 呗 账 单 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD],label=0\r第一批次4个元素中的第3个：[CLS] 蚂 蚁 借 呗 多 长 时 间 综 合 评 估 一 次 [SEP] 借 呗 得 评 估 多 久 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD],label=0 可以看到，DataLoader 按照我们设置的 batch size 每次对 4 个样本进行编码，并且通过设置 padding=True 和 truncation=True 来自动对每个 batch 中的样本进行补全和截断。这里我们选择 BERT 模型作为 checkpoint，所以每个样本都被处理成了“了“[CLS] sen1 [SEP] sen2 [SEP]”的形式。\n这种只在一个 batch 内进行补全的操作被称为动态补全 (Dynamic padding)，Hugging Face 也提供了 DataCollatorWithPadding 类来进行，如果感兴趣可以自行了解。\n训练模型 构建模型 常见的写法是继承 Transformers 库中的预训练模型来创建自己的模型。例如这里我们可以继承 BERT 模型（BertPreTrainedModel 类）来创建一个与上面模型结构完全相同的分类器：\n#构建模型\rfrom transformers import BertPreTrainedModel,BertModel,AutoConfig\rfrom torch import nn\rclass BertForPartwiseCLs(BertPreTrainedModel):\r\"\"\"\r定义模型继承自BertPreTrainedModel\r\"\"\"\rdef __init__(self,config):\r\"\"\"\r传入config，原始镜像的config\r\"\"\"\rsuper().__init__(config)\r#定义BertModel\rself.model=BertModel(config, add_pooling_layer=False)\r#丢弃10%\rself.dropout=nn.Dropout(config.hidden_dropout_prob)\r#全连接为2分类\rself.classifier=nn.Linear(768,2)\r#初始化权重参数\rself.post_init()\rdef forward(self,input):\r#执行模型产生一个(批次，词元,隐藏神经元)的输出\rbert_output=self.model(**input)\r#输出的数据有多个词元，取第一个[CLS]词元，因为每个词元通过注意力机制都包含了和其他词的语义信息，所以只需要一个即可\r#这里句子的维度编程了[批次,1,768]\rvector_data=bert_output.last_hidden_state[:,0,:]\rvector_data=self.dropout(vector_data)\rlogits=self.classifier(vector_data)\rreturn logits\rcheckpoint = \"bert-base-chinese\"\rconfig=AutoConfig.from_pretrained(checkpoint)\rmodel=BertForPartwiseCLs.from_pretrained(checkpoint,config=config)\rprint(model)\rX,y=next(iter(train_loader))\rprint(model(X).shape) 输出\nD:\\python\\evn311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\rwarnings.warn(\rSome weights of BertForPartwiseCLs were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['bert.classifier.bias', 'bert.classifier.weight', 'bert.model.embeddings.LayerNorm.bias', 'bert.model.embeddings.LayerNorm.weight', 'bert.model.embeddings.position_embeddings.weight', 'bert.model.embeddings.token_type_embeddings.weight', 'bert.model.embeddings.word_embeddings.weight', 'bert.model.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.0.attention.output.dense.bias', 'bert.model.encoder.layer.0.attention.output.dense.weight', 'bert.model.encoder.layer.0.attention.self.key.bias', 'bert.model.encoder.layer.0.attention.self.key.weight', 'bert.model.encoder.layer.0.attention.self.query.bias', 'bert.model.encoder.layer.0.attention.self.query.weight', 'bert.model.encoder.layer.0.attention.self.value.bias', 'bert.model.encoder.layer.0.attention.self.value.weight', 'bert.model.encoder.layer.0.intermediate.dense.bias', 'bert.model.encoder.layer.0.intermediate.dense.weight', 'bert.model.encoder.layer.0.output.LayerNorm.bias', 'bert.model.encoder.layer.0.output.LayerNorm.weight', 'bert.model.encoder.layer.0.output.dense.bias', 'bert.model.encoder.layer.0.output.dense.weight', 'bert.model.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.1.attention.output.dense.bias', 'bert.model.encoder.layer.1.attention.output.dense.weight', 'bert.model.encoder.layer.1.attention.self.key.bias', 'bert.model.encoder.layer.1.attention.self.key.weight', 'bert.model.encoder.layer.1.attention.self.query.bias', 'bert.model.encoder.layer.1.attention.self.query.weight', 'bert.model.encoder.layer.1.attention.self.value.bias', 'bert.model.encoder.layer.1.attention.self.value.weight', 'bert.model.encoder.layer.1.intermediate.dense.bias', 'bert.model.encoder.layer.1.intermediate.dense.weight', 'bert.model.encoder.layer.1.output.LayerNorm.bias', 'bert.model.encoder.layer.1.output.LayerNorm.weight', 'bert.model.encoder.layer.1.output.dense.bias', 'bert.model.encoder.layer.1.output.dense.weight', 'bert.model.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.10.attention.output.dense.bias', 'bert.model.encoder.layer.10.attention.output.dense.weight', 'bert.model.encoder.layer.10.attention.self.key.bias', 'bert.model.encoder.layer.10.attention.self.key.weight', 'bert.model.encoder.layer.10.attention.self.query.bias', 'bert.model.encoder.layer.10.attention.self.query.weight', 'bert.model.encoder.layer.10.attention.self.value.bias', 'bert.model.encoder.layer.10.attention.self.value.weight', 'bert.model.encoder.layer.10.intermediate.dense.bias', 'bert.model.encoder.layer.10.intermediate.dense.weight', 'bert.model.encoder.layer.10.output.LayerNorm.bias', 'bert.model.encoder.layer.10.output.LayerNorm.weight', 'bert.model.encoder.layer.10.output.dense.bias', 'bert.model.encoder.layer.10.output.dense.weight', 'bert.model.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.11.attention.output.dense.bias', 'bert.model.encoder.layer.11.attention.output.dense.weight', 'bert.model.encoder.layer.11.attention.self.key.bias', 'bert.model.encoder.layer.11.attention.self.key.weight', 'bert.model.encoder.layer.11.attention.self.query.bias', 'bert.model.encoder.layer.11.attention.self.query.weight', 'bert.model.encoder.layer.11.attention.self.value.bias', 'bert.model.encoder.layer.11.attention.self.value.weight', 'bert.model.encoder.layer.11.intermediate.dense.bias', 'bert.model.encoder.layer.11.intermediate.dense.weight', 'bert.model.encoder.layer.11.output.LayerNorm.bias', 'bert.model.encoder.layer.11.output.LayerNorm.weight', 'bert.model.encoder.layer.11.output.dense.bias', 'bert.model.encoder.layer.11.output.dense.weight', 'bert.model.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.2.attention.output.dense.bias', 'bert.model.encoder.layer.2.attention.output.dense.weight', 'bert.model.encoder.layer.2.attention.self.key.bias', 'bert.model.encoder.layer.2.attention.self.key.weight', 'bert.model.encoder.layer.2.attention.self.query.bias', 'bert.model.encoder.layer.2.attention.self.query.weight', 'bert.model.encoder.layer.2.attention.self.value.bias', 'bert.model.encoder.layer.2.attention.self.value.weight', 'bert.model.encoder.layer.2.intermediate.dense.bias', 'bert.model.encoder.layer.2.intermediate.dense.weight', 'bert.model.encoder.layer.2.output.LayerNorm.bias', 'bert.model.encoder.layer.2.output.LayerNorm.weight', 'bert.model.encoder.layer.2.output.dense.bias', 'bert.model.encoder.layer.2.output.dense.weight', 'bert.model.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.3.attention.output.dense.bias', 'bert.model.encoder.layer.3.attention.output.dense.weight', 'bert.model.encoder.layer.3.attention.self.key.bias', 'bert.model.encoder.layer.3.attention.self.key.weight', 'bert.model.encoder.layer.3.attention.self.query.bias', 'bert.model.encoder.layer.3.attention.self.query.weight', 'bert.model.encoder.layer.3.attention.self.value.bias', 'bert.model.encoder.layer.3.attention.self.value.weight', 'bert.model.encoder.layer.3.intermediate.dense.bias', 'bert.model.encoder.layer.3.intermediate.dense.weight', 'bert.model.encoder.layer.3.output.LayerNorm.bias', 'bert.model.encoder.layer.3.output.LayerNorm.weight', 'bert.model.encoder.layer.3.output.dense.bias', 'bert.model.encoder.layer.3.output.dense.weight', 'bert.model.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.4.attention.output.dense.bias', 'bert.model.encoder.layer.4.attention.output.dense.weight', 'bert.model.encoder.layer.4.attention.self.key.bias', 'bert.model.encoder.layer.4.attention.self.key.weight', 'bert.model.encoder.layer.4.attention.self.query.bias', 'bert.model.encoder.layer.4.attention.self.query.weight', 'bert.model.encoder.layer.4.attention.self.value.bias', 'bert.model.encoder.layer.4.attention.self.value.weight', 'bert.model.encoder.layer.4.intermediate.dense.bias', 'bert.model.encoder.layer.4.intermediate.dense.weight', 'bert.model.encoder.layer.4.output.LayerNorm.bias', 'bert.model.encoder.layer.4.output.LayerNorm.weight', 'bert.model.encoder.layer.4.output.dense.bias', 'bert.model.encoder.layer.4.output.dense.weight', 'bert.model.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.5.attention.output.dense.bias', 'bert.model.encoder.layer.5.attention.output.dense.weight', 'bert.model.encoder.layer.5.attention.self.key.bias', 'bert.model.encoder.layer.5.attention.self.key.weight', 'bert.model.encoder.layer.5.attention.self.query.bias', 'bert.model.encoder.layer.5.attention.self.query.weight', 'bert.model.encoder.layer.5.attention.self.value.bias', 'bert.model.encoder.layer.5.attention.self.value.weight', 'bert.model.encoder.layer.5.intermediate.dense.bias', 'bert.model.encoder.layer.5.intermediate.dense.weight', 'bert.model.encoder.layer.5.output.LayerNorm.bias', 'bert.model.encoder.layer.5.output.LayerNorm.weight', 'bert.model.encoder.layer.5.output.dense.bias', 'bert.model.encoder.layer.5.output.dense.weight', 'bert.model.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.6.attention.output.dense.bias', 'bert.model.encoder.layer.6.attention.output.dense.weight', 'bert.model.encoder.layer.6.attention.self.key.bias', 'bert.model.encoder.layer.6.attention.self.key.weight', 'bert.model.encoder.layer.6.attention.self.query.bias', 'bert.model.encoder.layer.6.attention.self.query.weight', 'bert.model.encoder.layer.6.attention.self.value.bias', 'bert.model.encoder.layer.6.attention.self.value.weight', 'bert.model.encoder.layer.6.intermediate.dense.bias', 'bert.model.encoder.layer.6.intermediate.dense.weight', 'bert.model.encoder.layer.6.output.LayerNorm.bias', 'bert.model.encoder.layer.6.output.LayerNorm.weight', 'bert.model.encoder.layer.6.output.dense.bias', 'bert.model.encoder.layer.6.output.dense.weight', 'bert.model.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.7.attention.output.dense.bias', 'bert.model.encoder.layer.7.attention.output.dense.weight', 'bert.model.encoder.layer.7.attention.self.key.bias', 'bert.model.encoder.layer.7.attention.self.key.weight', 'bert.model.encoder.layer.7.attention.self.query.bias', 'bert.model.encoder.layer.7.attention.self.query.weight', 'bert.model.encoder.layer.7.attention.self.value.bias', 'bert.model.encoder.layer.7.attention.self.value.weight', 'bert.model.encoder.layer.7.intermediate.dense.bias', 'bert.model.encoder.layer.7.intermediate.dense.weight', 'bert.model.encoder.layer.7.output.LayerNorm.bias', 'bert.model.encoder.layer.7.output.LayerNorm.weight', 'bert.model.encoder.layer.7.output.dense.bias', 'bert.model.encoder.layer.7.output.dense.weight', 'bert.model.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.8.attention.output.dense.bias', 'bert.model.encoder.layer.8.attention.output.dense.weight', 'bert.model.encoder.layer.8.attention.self.key.bias', 'bert.model.encoder.layer.8.attention.self.key.weight', 'bert.model.encoder.layer.8.attention.self.query.bias', 'bert.model.encoder.layer.8.attention.self.query.weight', 'bert.model.encoder.layer.8.attention.self.value.bias', 'bert.model.encoder.layer.8.attention.self.value.weight', 'bert.model.encoder.layer.8.intermediate.dense.bias', 'bert.model.encoder.layer.8.intermediate.dense.weight', 'bert.model.encoder.layer.8.output.LayerNorm.bias', 'bert.model.encoder.layer.8.output.LayerNorm.weight', 'bert.model.encoder.layer.8.output.dense.bias', 'bert.model.encoder.layer.8.output.dense.weight', 'bert.model.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.model.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.model.encoder.layer.9.attention.output.dense.bias', 'bert.model.encoder.layer.9.attention.output.dense.weight', 'bert.model.encoder.layer.9.attention.self.key.bias', 'bert.model.encoder.layer.9.attention.self.key.weight', 'bert.model.encoder.layer.9.attention.self.query.bias', 'bert.model.encoder.layer.9.attention.self.query.weight', 'bert.model.encoder.layer.9.attention.self.value.bias', 'bert.model.encoder.layer.9.attention.self.value.weight', 'bert.model.encoder.layer.9.intermediate.dense.bias', 'bert.model.encoder.layer.9.intermediate.dense.weight', 'bert.model.encoder.layer.9.output.LayerNorm.bias', 'bert.model.encoder.layer.9.output.LayerNorm.weight', 'bert.model.encoder.layer.9.output.dense.bias', 'bert.model.encoder.layer.9.output.dense.weight']\rYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\rBertForPartwiseCLs(\r(model): BertModel(\r(embeddings): BertEmbeddings(\r(word_embeddings): Embedding(21128, 768, padding_idx=0)\r(position_embeddings): Embedding(512, 768)\r(token_type_embeddings): Embedding(2, 768)\r(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(dropout): Dropout(p=0.1, inplace=False)\r)\r(encoder): BertEncoder(\r(layer): ModuleList(\r(0-11): 12 x BertLayer(\r(attention): BertAttention(\r(self): BertSelfAttention(\r(query): Linear(in_features=768, out_features=768, bias=True)\r(key): Linear(in_features=768, out_features=768, bias=True)\r(value): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.1, inplace=False)\r)\r(output): BertSelfOutput(\r(dense): Linear(in_features=768, out_features=768, bias=True)\r(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(dropout): Dropout(p=0.1, inplace=False)\r)\r)\r(intermediate): BertIntermediate(\r(dense): Linear(in_features=768, out_features=3072, bias=True)\r(intermediate_act_fn): GELUActivation()\r)\r(output): BertOutput(\r(dense): Linear(in_features=3072, out_features=768, bias=True)\r(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(dropout): Dropout(p=0.1, inplace=False)\r)\r)\r)\r)\r)\r(dropout): Dropout(p=0.1, inplace=False)\r(classifier): Linear(in_features=768, out_features=2, bias=True)\r)\rtorch.Size([4, 2]) 可以看到模型输出了一个 4×2 的张量，符合我们的预期（每个样本输出 2 维的 logits 值分别表示两个类别的预测分数，batch 内共 4 个样本）。\ntqdm使用 tqdm是一个Python库,用于在终端中显示进度条。它广泛应用于各种数据处理任务中,如循环、迭代器、pandas数据帧等。以下是对tqdm的简要介绍:\n简单易用: tqdm提供了简单直观的API,可以快速集成到代码中,只需要几行代码即可实现进度条显示。 丰富的功能: tqdm不仅可以显示进度条,还可以显示预估的剩余时间、完成百分比、已处理的数据量等信息。 自动检测环境: tqdm可以自动检测运行环境,在支持ANSI转义码的终端中使用动态进度条,在不支持的环境中使用静态进度条。 支持各种迭代器: tqdm支持各种Python内置迭代器,如list、range、enumerate等,也支持自定义迭代器。 可定制性强: tqdm提供了丰富的参数供用户自定义进度条的样式和行为,如颜色、宽度、刷新间隔等。 代码\n#tqdm进度条使用\rfrom tqdm.auto import tqdm\rimport time\r# 创建一个迭代对象，比如一个列表\ritems = range(10)\r# 使用tqdm来迭代这个对象，并显示进度条\rfor item in tqdm(items, desc='Processing'):\r# 在这里执行你的任务\rtime.sleep(0.1) # 模拟一些长时间运行的任务\r# range(10) 其实就是0-9\rprint([i for i in range(10)])\r#创建一个tqdm对象，传入得必须是range对象，range(10) 其实就是0-9\rprint(range(10),len(range(10)))\rtdm=tqdm(range(10), desc='Processing')\rfor item in range(10):\rtime.sleep(1) # 模拟一些长时间运行的任务\r#更新一次,其实就是进度条加上： 1/len(range(10))\rtdm.update(1) 效果 训练模型 在训练模型时，我们将每一轮 Epoch 分为训练循环和验证/测试循环。在训练循环中计算损失、优化模型的参数，在验证/测试循环中评估模型的性能，与 Pytorch 类似，Transformers 库同样实现了很多的优化器，并且相比 Pytorch 固定学习率，Transformers 库的优化器会随着训练过程逐步减小学习率（通常会产生更好的效果）。例如我们前面使用过的 AdamW 优化器 完整的训练过程，可与使用colab来进行训练。\n#训练模型和验证测试\r#定义损失函数\rfrom torch.nn import CrossEntropyLoss\rfrom transformers import get_scheduler\r#定义优化函数,from torch.optim import AdamW\rfrom transformers import AdamW\rfrom tqdm.auto import tqdm\r#定义epoch训练次数\repochs = 3\r#默认学习率\rlearning_rate = 1e-5\r# batchsize\rbatch_size=4\r#AdamW是Adam优化器的一种变体，它在Adam的基础上进行了一些改进，旨在解决Adam优化器可能引入的权重衰减问题。\roptimizer=AdamW(model.parameters(),lr=1e-5)\r#定义交叉熵损失函数\rloss_fn=CrossEntropyLoss()\r#重新初始化数据集\rtrain_loader=DataLoader(train_data,batch_size=batch_size,shuffle=False,collate_fn=collote_fn)\rdev_loader=DataLoader(MyDataSet(\"dev.json\"),batch_size=batch_size,shuffle=False,collate_fn=collote_fn)\r#总步数=epoch*批次数(总记录数train_data/一批次多少条数据batch_size)\rnum_training_steps = epochs * len(train_loader)\r#默认情况下，优化器会线性衰减学习率，对于上面的例子，学习率会线性地从le-5 降到0\r#。为了正确地定义学习率调度器，我们需要知道总的训练步数 (step)，它等于训练轮数 (Epoch number) 乘以每一轮中的步数（也就是训练 dataloader 的大小）\rlr_scheduler = get_scheduler(\r\"linear\",\roptimizer=optimizer,\rnum_warmup_steps=0,\rnum_training_steps=num_training_steps,\r)\r#初始化模型\rdevice = 'cuda' if torch.cuda.is_available() else 'cpu'\rcheckpoint = \"bert-base-chinese\"\rconfig=AutoConfig.from_pretrained(checkpoint)\rmodel=BertForPartwiseCLs.from_pretrained(checkpoint,config=config).to(device)\r#定义总损失\rtotal_loss=0\r#完成总batch\rcomplete_batch_count=0\r#最好的正确率\rbest_acc = 0.\rcurrent_directory = os.getcwd()\rfor step in range(epochs):\r#进入训练模式\rmodel.train()\rprint(f\"Epoch {step+1}/{epochs}\\n-------------------------------\")\rprogress_bar=tqdm(range(len(train_loader)))\rfor batch,(X,y) in enumerate(train_loader):\rX,y=X.to(device),y.to(device)\r#获取预测结果\rpred=model(X)\r#计算损失函数\rloss=loss_fn(pred,y)\r#清空梯度\roptimizer.zero_grad()\r#前向传播\rloss.backward();\r#更新模型参数\roptimizer.step();\r#学习率线性下降,必须是更新模型参数之后，函数根据设定的规则来调整学习率。这个调整需要基于当前的模型状态,包括参数、损失函数值等,所以要放在optimizer.step()之后。\rlr_scheduler.step()\rtotal_loss+=loss.item()\rcomplete_batch_count+=1\ravg_loss=total_loss/complete_batch_count\rprogress_bar.set_description(\"loss:{}\".format(avg_loss))\rprogress_bar.update(1)\r#使用验证集验证模型正确性。\r#进入预测模式,当前这一次epoch训练数据的正确率\rmodel.eval()\rcorrect=0\r#加载验证集的数据\rfor batch,(X,y) in enumerate(dev_loader):\r#获取预测结果\rpred=model(X)\r#因为是[[0.9,0.1],[0.3,0.4]]所以取dim=1维度上最大值的索引，概率大的索引就是预测的类别，如果和label值y相等就加起来，算个数\rcorrect += (pred.argmax(dim=1) == y).type(torch.float).sum().item()\r#正确/总数就是争取率\rvalid_acc=correct/len(dev_loader.dataset)\rprint(f\"{step+1} Accuracy: {(100*valid_acc):\u003e0.1f}%\\n\")\rif valid_acc \u003e best_acc:\rbest_acc = valid_acc\rprint('saving new weights...\\n')\rtorch.save(model.state_dict(), current_directory+f'/epoch_{step+1}_valid_acc_{(100*valid_acc):0.1f}_model_weights.bin')\rprint(\"Done!\") 模型预测 最后，我们加载验证集上最优的模型权重，汇报其在测试集上的性能。由于 AFQMC 公布的测试集上并没有标签，无法评估性能，这里我们暂且用验证集代替进行演示：\ncurrent_directory = os.getcwd()\rmodel.load_state_dict(torch.load(current_directory+'/model_weights.bin'))\rmodel.eval()\rtest_loader=DataLoader(test_data,batch_size=4,shuffle=False,collate_fn=collote_fn)\rX,y=next(iter(test_loader))\rX, y = X.to(device), y.to(device)\rpred = model(X)\rprint(pred.argmax(1) == y) 文章部分文字引用：https://transformers.run/c2/2021-12-17-transformers-note-4/",
    "description": "BERT（Bidirectional Encoder Representations from Transformers）是一种基于 Transformer 模型的预训练语言表示方法，由Google研究团队于2018年提出。BERT 通过在大规模文本语料上进行无监督的预训练，学习了通用的语言表示，并且在各种自然语言处理任务中取得了显著的性能提升。\nBERT仅使用了Transformer架构的Encoder部分。BERT自2018年由谷歌发布后，在多种NLP任务中（例如QA、文本生成、情感分析等等）都实现了更好的结果。\n“Word2vec与GloVe都有一个特点，就是它们是上下文无关（context-free）的词嵌入。所以它们没有解决：一个单词在不同上下文中代表不同的含义的问题。例如，对于单词bank，它在不同的上下文中，有银行、河畔这种差别非常大的含义。BERT的出现，解决了这个问题。\nBERT 的主要特点包括：\n双向性：BERT 使用双向 Transformer 模型来处理输入序列，从而能够同时考虑上下文的信息，而不仅仅是单向的上下文信息。这种双向性使得 BERT 能够更好地理解句子中的语义和语境。\n预训练-微调框架：BERT 使用了预训练-微调的方法。首先，在大规模文本语料上进行无监督的预训练，通过 Masked Language Model（MLM）和 Next Sentence Prediction（NSP）任务学习语言表示；然后，在特定的下游任务上微调模型参数，使其适应于特定的任务，如文本分类、命名实体识别等。\nTransformer 模型：BERT 基于 Transformer 模型结构，其中包括多层的编码器，每个编码器由自注意力机制和前馈神经网络组成。这种结构能够有效地捕获输入序列中的长距离依赖关系，有助于提高模型在各种自然语言处理任务中的性能。\n多层表示：BERT 提供了多层的语言表示，使得用户可以根据具体任务选择不同层的表示进行应用。较底层的表示通常更加接近原始输入，而较高层的表示则更加抽象，包含了更多的语义信息。\n开放源代码：BERT 的源代码和预训练模型已经在 GitHub 上开放，使得研究人员和开发者可以基于 BERT 进行进一步的研究和应用开发。\nBERT 通过预训练大规模文本语料上的通用语言表示，以及在各种下游任务上的微调，有效地提高了自然语言处理任务的性能，并且成为了当前领域内最具影响力的预训练模型之一。\ntransformer提供了不同领域中常见的机器学习模型类型：\nTEXT MODELS（文本模型）：用于处理和分析文本数据的模型，如自然语言处理（NLP）中的BERT、GPT等。\nVISION MODELS（视觉模型）：用于处理和分析图像数据的模型，如卷积神经网络（CNN）中的ResNet、VGG,Vision Transformer (ViT)等。\nAUDIO MODELS（音频模型）：用于处理和分析音频数据的模型，如声学模型、语音识别模型等。\nVIDEO MODELS（视频模型）：用于处理和分析视频数据的模型，如视频分类、目标检测、行为识别等。\nMULTIMODAL MODELS（多模态模型）：结合多种数据类型（如文本、图像、音频等）进行分析和预测的模型，如OpenAI的CLIP。\nREINFORCEMENT LEARNING MODELS（强化学习模型）：用于解决强化学习问题的模型，如Deep Q-Networks（DQN）、Actor-Critic等。\nTIME SERIES MODELS（时间序列模型）：用于分析和预测时间序列数据的模型，如循环神经网络（RNN）、长短期记忆网络（LSTM）等。\nGRAPH MODELS（图模型）：用于处理和分析图数据的模型，如图神经网络（GNN）、图卷积网络（GCN）等。\nBERT的基本原理 BERT基于的是Transformer模型，并且仅使用Transformer模型的Encoder部分。在Transformer模型中，Encoder的输入是一串序列，输出的是对序列中每个字符的表示。同样，在BERT中，输入的是一串序列，输出的是也是对应序列中每个单词的编码。 以“He got bit by Python”为例，BERT的输入输出如下图所示： 其中输入为序列“He got bit by Python”，输出的是对每个单词的编码$R_{word}$。这样在经过了BERT处理后，即得到了对每个单词包含的上下文表示$R_{word}$。",
    "tags": [],
    "title": "Transformers实战02-BERT预训练模型微调",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_02/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "简介 PEFT PEFT（Parameter-Efficient Fine-Tuning）是一个用于高效地将大型预训练模型适配到各种下游应用的库，而无需对模型的所有参数进行微调，因为这在计算上是非常昂贵的。PEFT 方法只微调少量的（额外的）模型参数——显著降低了计算和存储成本——同时其性能与完全微调的模型相当。这使得在消费者硬件上训练和存储大型语言模型（LLMs）变得更加可行。\nPEFT 集成了 Transformers、Diffusers 和 Accelerate 库，以提供更快、更简单的方法来加载、训练和使用大型模型进行推理。\nLORA方法 一种高效训练大型模型的流行方法是在注意力块中插入较小的可训练矩阵，这些矩阵是微调期间要学习的增量权重矩阵的低秩分解。预训练模型的原始权重矩阵被冻结，仅更新较小的矩阵。这减少了可训练参数的数量，降低了内存使用和训练时间，而这些在大型模型中可能非常昂贵。\n有几种不同的方法可以将权重矩阵表示为低秩分解，但最常见的方法是低秩适应（LoRA原理）。PEFT 库支持几种其他 LoRA 变体，例如低秩Hadamard积（LoHa）、低秩Kronecker积（LoKr）和自适应低秩适应（AdaLoRA）。你可以在适配器指南中了解这些方法的概念工作原理。如果你有兴趣将这些方法应用于其他任务和用例，比如语义分割、标记分类，可以看看我们的笔记本集合！\nVision Transformer (ViT) Vision Transformer（ViT）模型是由Alexey Dosovitskiy，Lucas Beyer，Alexander Kolesnikov，Dirk Weissenborn，Xiaohua Zhai，Thomas Unterthiner，Mostafa Dehghani，Matthias Minderer，Georg Heigold，Sylvain Gelly，Jakob Uszkoreit，Neil Houlsby在《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》中提出的。这是第一篇成功在ImageNet上训练Transformer编码器并获得非常好结果的论文。\n这篇论文的摘要是：\n虽然Transformer架构已经成为自然语言处理任务的事实标准，但它在计算机视觉中的应用仍然有限。在视觉领域，注意力要么与卷积网络一起应用，要么用来替换卷积网络的某些组件，同时保持其总体结构不变。我们展示了在这种对CNN的依赖不是必要的，纯Transformer直接应用于图像块序列可以在图像分类任务上表现得非常好。当在大量数据上进行预训练并转移到多个中等规模或小型图像识别基准数据集（ImageNet，CIFAR-100，VTAB等）时，Vision Transformer（ViT）与最先进的卷积网络相比取得了出色的结果，同时训练所需的计算资源大大减少。\n具体关于该模型得结构参考：https://huggingface.co/docs/transformers/model_doc/vit\nlora方法实战 图像分类微调 本指南将向你展示如何快速训练一个图像分类模型——使用低秩分解方法——来识别图像中显示的食物类别。 案例来自官网：https://huggingface.co/docs/peft/task_guides/lora_based_methods\n模型选择 google/vit-base-patch16-224-in21k 是一个基于Transformer编码器的模型（类似于BERT），在监督方式下，即ImageNet-21k上以224x224像素的分辨率预训练了大量图像。\n图像被呈现给模型作为固定大小的补丁序列（分辨率为16x16），这些补丁被线性嵌入。在序列的开头还添加了一个[CLS]标记，用于分类任务。在将序列馈送到Transformer编码器的层之前，还会添加绝对位置嵌入。\n需要注意的是，这个模型不提供任何经过微调的头部，因为这些头部被Google研究人员清零了。但是，模型包括预训练的汇聚层，可以用于下游任务（如图像分类）。\n通过预训练模型，它学习了图像的内部表示，然后可以用于提取对下游任务有用的特征：例如，如果您有一个带标签的图像数据集，可以通过在预训练编码器顶部放置一个线性层来训练标准分类器。通常将线性层放置在[CLS]标记的顶部，因为该标记的最后隐藏状态可以被视为整个图像的表示。\nfrom transformers import ViTImageProcessor, FlaxViTModel\rfrom PIL import Image\rimport requests\rurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\rimage = Image.open(requests.get(url, stream=True).raw)\rprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\rmodel = FlaxViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\rinputs = processor(images=image, return_tensors=\"np\")\routputs = model(**inputs)\rlast_hidden_states = outputs.last_hidden_state\rprint(last_hidden_states.shape) 不包含分类信息，不包含label信息\n模块 一个模型通常有很多个模块和层，模块是nn.Module是一个更高级别的组织单元，可以包含多个层、子模块或其他操作，层（Layer） 是模块的基本组成部分，用于执行特定的数学运算或神经网络中的某一步骤。\n# 自定义一个简单的模块，包含两个线性层\rclass MyModule(nn.Module):\rdef __init__(self):\rsuper(MyModule, self).__init__()\rself.layer1 = nn.Linear(10, 20)\rself.layer2 = nn.Linear(20, 5)\rdef forward(self, x):\rx = self.layer1(x)\rx = self.layer2(x)\rreturn x 其中MyModule是模块，self.layer1就是层，都可以直接运行，反向传播。 我们可以看看’google/vit-base-patch16-224-in21k’有哪些模块和层\nfrom transformers import AutoModelForImageClassification, TrainingArguments, Trainer\rmodel = AutoModelForImageClassification.from_pretrained(\r\"google/vit-base-patch16-224-in21k\",\rignore_mismatched_sizes=True,\r)\r# 打印模型的结构，查看有哪些模块\rfor name, module in model.named_modules():\rprint(name,\"=\", module) 输出，其中ViTForImageClassification就是用于图形分类的模块，如果通过AutoModelForImageClassification，加载的必然是这个模块，后面我们通过lora优化的是其中的模块的和层，输出的是最后的classifier层。\n= ViTForImageClassification(\r(vit): ViTModel(\r(embeddings): ViTEmbeddings(\r(patch_embeddings): ViTPatchEmbeddings(\r(projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(encoder): ViTEncoder(\r(layer): ModuleList(\r(0-11): 12 x ViTLayer(\r(attention): ViTSdpaAttention(\r(attention): ViTSdpaSelfAttention(\r(query): Linear(in_features=768, out_features=768, bias=True)\r(key): Linear(in_features=768, out_features=768, bias=True)\r(value): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(output): ViTSelfOutput(\r(dense): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r)\r(intermediate): ViTIntermediate(\r(dense): Linear(in_features=768, out_features=3072, bias=True)\r(intermediate_act_fn): GELUActivation()\r)\r(output): ViTOutput(\r(dense): Linear(in_features=3072, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r)\r)\r)\r(layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r)\r(classifier): Linear(in_features=768, out_features=2, bias=True)\r)\rvit = ViTModel(\r(embeddings): ViTEmbeddings(\r(patch_embeddings): ViTPatchEmbeddings(\r(projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(encoder): ViTEncoder(\r(layer): ModuleList(\r(0-11): 12 x ViTLayer(\r(attention): ViTSdpaAttention(\r(attention): ViTSdpaSelfAttention(\r(query): Linear(in_features=768, out_features=768, bias=True)\r(key): Linear(in_features=768, out_features=768, bias=True)\r(value): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(output): ViTSelfOutput(\r(dense): Linear(in_features=768, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r)\r(intermediate): ViTIntermediate(\r(dense): Linear(in_features=768, out_features=3072, bias=True)\r(intermediate_act_fn): GELUActivation()\r)\r(output): ViTOutput(\r(dense): Linear(in_features=3072, out_features=768, bias=True)\r(dropout): Dropout(p=0.0, inplace=False)\r)\r(layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r(layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r)\r)\r)\r(layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\r)\rvit.embeddings = ViTEmbeddings(\r(patch_embeddings): ViTPatchEmbeddings(\r(projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r)\r(dropout): Dropout(p=0.0, inplace=False)\r)\rvit.embeddings.patch_embeddings = ViTPatchEmbeddings(\r(projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\r)\r。。。\rclassifier = Linear(in_features=768, out_features=2, bias=True) google/vit-base-patch16-224 是一个在大规模图像数据集上进行监督预训练的转换器编码器模型（类似于BERT），即ImageNet-21k，分辨率为224x224像素。接下来，该模型在ImageNet上进行微调（也称为ILSVRC2012），这是一个包含100万张图像和1000个类别的数据集，分辨率也为224x224。\n图像被呈现给模型作为固定大小补丁（分辨率为16x16）的序列，这些补丁被线性嵌入。还在序列开始处添加了一个[CLS]标记，以用于分类任务。在将序列馈送到Transformer编码器的层之前，还会添加绝对位置嵌入。\n通过对模型进行预训练，它学习了图像的内部表示，然后可以用于提取对下游任务有用的特征：例如，如果您有一个带标签的图像数据集，您可以在预训练编码器之上放置一个标准分类器的线性层。通常将线性层放置在[CLS]标记的顶部，因为该标记的最后隐藏状态可以被视为整个图像的表示。\nfrom transformers import AutoImageProcessor, ViTForImageClassification\rfrom PIL import Image\rimport requests,torch\rurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\rimage = Image.open(requests.get(url, stream=True).raw)\rprocessor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')\rmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\rinputs = processor(images=image, return_tensors=\"pt\")\rprint(inputs)\rprint(inputs[\"pixel_values\"].shape)\routputs = model(**inputs)\rwith torch.no_grad():\rlogits = model(**inputs).logits\rpredicted_label = logits.argmax(-1).item()\rprint(model.config.id2label[predicted_label]) 输出：\n{'pixel_values': tensor([[[[ 0.1137, 0.1686, 0.1843, ..., -0.1922, -0.1843, -0.1843],\r[ 0.1373, 0.1686, 0.1843, ..., -0.1922, -0.1922, -0.2078],\r[ 0.1137, 0.1529, 0.1608, ..., -0.2314, -0.2235, -0.2157],\r...,\r[ 0.8353, 0.7882, 0.7333, ..., 0.7020, 0.6471, 0.6157],\r[ 0.8275, 0.7961, 0.7725, ..., 0.5843, 0.4667, 0.3961],\r[ 0.8196, 0.7569, 0.7569, ..., 0.0745, -0.0510, -0.1922]],\r[[-0.8039, -0.8118, -0.8118, ..., -0.8902, -0.8902, -0.8980],\r[-0.7882, -0.7882, -0.7882, ..., -0.8745, -0.8745, -0.8824],\r[-0.8118, -0.8039, -0.7882, ..., -0.8902, -0.8902, -0.8902],\r...,\r[-0.2706, -0.3176, -0.3647, ..., -0.4275, -0.4588, -0.4824],\r[-0.2706, -0.2941, -0.3412, ..., -0.4824, -0.5451, -0.5765],\r[-0.2784, -0.3412, -0.3490, ..., -0.7333, -0.7804, -0.8353]],\r[[-0.5451, -0.4667, -0.4824, ..., -0.7412, -0.6941, -0.7176],\r[-0.5529, -0.5137, -0.4902, ..., -0.7412, -0.7098, -0.7412],\r[-0.5216, -0.4824, -0.4667, ..., -0.7490, -0.7490, -0.7647],\r...,\r[ 0.5686, 0.5529, 0.4510, ..., 0.4431, 0.3882, 0.3255],\r[ 0.5451, 0.4902, 0.5137, ..., 0.3020, 0.2078, 0.1294],\r[ 0.5686, 0.5608, 0.5137, ..., -0.2000, -0.4275, -0.5294]]]])}\rtorch.Size([1, 3, 224, 224])\r{0: 'tench, Tinca tinca', 1: 'goldfish, Carassius auratus', 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias', 3: 'tiger shark, Galeocerdo cuvieri', 4: 'hammerhead, hammerhead shark', 5: 'electric ray, crampfish, numbfish, torpedo', 6: 'stingray', 7: 'cock', 8: 'hen', 9: 'ostrich, Struthio camelus', 10: 'brambling, Fringilla montifringilla', 11: 'goldfinch, Carduelis carduelis', 12: 'house finch, linnet, Carpodacus mexicanus', 13: 'junco, snowbird', 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea', 15: 'robin, American robin, Turdus migratorius', 16: 'bulbul', 17: 'jay', 18: 'magpie', 19: 'chickadee', 20: 'water ouzel, dipper', 21: 'kite', 22: 'bald eagle, American eagle, Haliaeetus leucocephalus', 23: 'vulture', 24: 'great grey owl, great gray owl, Strix nebulosa', 25: 'European fire salamander, Salamandra salamandra', 26: 'common newt, Triturus vulgaris', 27: 'eft', 28: 'spotted salamander, Ambystoma maculatum', 29: 'axolotl, mud puppy, Ambystoma mexicanum', 30: 'bullfrog, Rana catesbeiana', 31: 'tree frog, tree-frog', 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui', 33: 'loggerhead, loggerhead turtle, Caretta caretta', 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea', 35: 'mud turtle', 36: 'terrapin', 37: 'box turtle, box tortoise', 38: 'banded gecko', 39: 'common iguana, iguana, Iguana iguana', 40: 'American chameleon, anole, Anolis carolinensis', 41: 'whiptail, whiptail lizard', 42: 'agama', 43: 'frilled lizard, Chlamydosaurus kingi', 44: 'alligator lizard', 45: 'Gila monster, Heloderma suspectum', 46: 'green lizard, Lacerta viridis', 47: 'African chameleon, Chamaeleo chamaeleon', 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis', 49: 'African crocodile, Nile crocodile, Crocodylus niloticus', 50: 'American alligator, Alligator mississipiensis', 51: 'triceratops', 52: 'thunder snake, worm snake, Carphophis amoenus', 53: 'ringneck snake, ring-necked snake, ring snake', 54: 'hognose snake, puff adder, sand viper', 55: 'green snake, grass snake', 56: 'king snake, kingsnake', 57: 'garter snake, grass snake', 58: 'water snake', 59: 'vine snake', 60: 'night snake, Hypsiglena torquata', 61: 'boa constrictor, Constrictor constrictor', 62: 'rock python, rock snake, Python sebae', 63: 'Indian cobra, Naja naja', 64: 'green mamba', 65: 'sea snake', 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus', 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus', 68: 'sidewinder, horned rattlesnake, Crotalus cerastes', 69: 'trilobite', 70: 'harvestman, daddy longlegs, Phalangium opilio', 71: 'scorpion', 72: 'black and gold garden spider, Argiope aurantia', 73: 'barn spider, Araneus cavaticus', 74: 'garden spider, Aranea diademata', 75: 'black widow, Latrodectus mactans', 76: 'tarantula', 77: 'wolf spider, hunting spider', 78: 'tick', 79: 'centipede', 80: 'black grouse', 81: 'ptarmigan', 82: 'ruffed grouse, partridge, Bonasa umbellus', 83: 'prairie chicken, prairie grouse, prairie fowl', 84: 'peacock', 85: 'quail', 86: 'partridge', 87: 'African grey, African gray, Psittacus erithacus', 88: 'macaw', 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita', 90: 'lorikeet', 91: 'coucal', 92: 'bee eater', 93: 'hornbill', 94: 'hummingbird', 95: 'jacamar', 96: 'toucan', 97: 'drake', 98: 'red-breasted merganser, Mergus serrator', 99: 'goose', 100: 'black swan, Cygnus atratus', 101: 'tusker', 102: 'echidna, spiny anteater, anteater', 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus', 104: 'wallaby, brush kangaroo', 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus', 106: 'wombat', 107: 'jellyfish', 108: 'sea anemone, anemone', 109: 'brain coral', 110: 'flatworm, platyhelminth', 111: 'nematode, nematode worm, roundworm', 112: 'conch', 113: 'snail', 114: 'slug', 115: 'sea slug, nudibranch', 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore', 117: 'chambered nautilus, pearly nautilus, nautilus', 118: 'Dungeness crab, Cancer magister', 119: 'rock crab, Cancer irroratus', 120: 'fiddler crab', 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica', 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus', 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish', 124: 'crayfish, crawfish, crawdad, crawdaddy', 125: 'hermit crab', 126: 'isopod', 127: 'white stork, Ciconia ciconia', 128: 'black stork, Ciconia nigra', 129: 'spoonbill', 130: 'flamingo', 131: 'little blue heron, Egretta caerulea', 132: 'American egret, great white heron, Egretta albus', 133: 'bittern', 134: 'crane', 135: 'limpkin, Aramus pictus', 136: 'European gallinule, Porphyrio porphyrio', 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana', 138: 'bustard', 139: 'ruddy turnstone, Arenaria interpres', 140: 'red-backed sandpiper, dunlin, Erolia alpina', 141: 'redshank, Tringa totanus', 142: 'dowitcher', 143: 'oystercatcher, oyster catcher', 144: 'pelican', 145: 'king penguin, Aptenodytes patagonica', 146: 'albatross, mollymawk', 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus', 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca', 149: 'dugong, Dugong dugon', 150: 'sea lion', 151: 'Chihuahua', 152: 'Japanese spaniel', 153: 'Maltese dog, Maltese terrier, Maltese', 154: 'Pekinese, Pekingese, Peke', 155: 'Shih-Tzu', 156: 'Blenheim spaniel', 157: 'papillon', 158: 'toy terrier', 159: 'Rhodesian ridgeback', 160: 'Afghan hound, Afghan', 161: 'basset, basset hound', 162: 'beagle', 163: 'bloodhound, sleuthhound', 164: 'bluetick', 165: 'black-and-tan coonhound', 166: 'Walker hound, Walker foxhound', 167: 'English foxhound', 168: 'redbone', 169: 'borzoi, Russian wolfhound', 170: 'Irish wolfhound', 171: 'Italian greyhound', 172: 'whippet', 173: 'Ibizan hound, Ibizan Podenco', 174: 'Norwegian elkhound, elkhound', 175: 'otterhound, otter hound', 176: 'Saluki, gazelle hound', 177: 'Scottish deerhound, deerhound', 178: 'Weimaraner', 179: 'Staffordshire bullterrier, Staffordshire bull terrier', 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier', 181: 'Bedlington terrier', 182: 'Border terrier', 183: 'Kerry blue terrier', 184: 'Irish terrier', 185: 'Norfolk terrier', 186: 'Norwich terrier', 187: 'Yorkshire terrier', 188: 'wire-haired fox terrier', 189: 'Lakeland terrier', 190: 'Sealyham terrier, Sealyham', 191: 'Airedale, Airedale terrier', 192: 'cairn, cairn terrier', 193: 'Australian terrier', 194: 'Dandie Dinmont, Dandie Dinmont terrier', 195: 'Boston bull, Boston terrier', 196: 'miniature schnauzer', 197: 'giant schnauzer', 198: 'standard schnauzer', 199: 'Scotch terrier, Scottish terrier, Scottie', 200: 'Tibetan terrier, chrysanthemum dog', 201: 'silky terrier, Sydney silky', 202: 'soft-coated wheaten terrier', 203: 'West Highland white terrier', 204: 'Lhasa, Lhasa apso', 205: 'flat-coated retriever', 206: 'curly-coated retriever', 207: 'golden retriever', 208: 'Labrador retriever', 209: 'Chesapeake Bay retriever', 210: 'German short-haired pointer', 211: 'vizsla, Hungarian pointer', 212: 'English setter', 213: 'Irish setter, red setter', 214: 'Gordon setter', 215: 'Brittany spaniel', 216: 'clumber, clumber spaniel', 217: 'English springer, English springer spaniel', 218: 'Welsh springer spaniel', 219: 'cocker spaniel, English cocker spaniel, cocker', 220: 'Sussex spaniel', 221: 'Irish water spaniel', 222: 'kuvasz', 223: 'schipperke', 224: 'groenendael', 225: 'malinois', 226: 'briard', 227: 'kelpie', 228: 'komondor', 229: 'Old English sheepdog, bobtail', 230: 'Shetland sheepdog, Shetland sheep dog, Shetland', 231: 'collie', 232: 'Border collie', 233: 'Bouvier des Flandres, Bouviers des Flandres', 234: 'Rottweiler', 235: 'German shepherd, German shepherd dog, German police dog, alsatian', 236: 'Doberman, Doberman pinscher', 237: 'miniature pinscher', 238: 'Greater Swiss Mountain dog', 239: 'Bernese mountain dog', 240: 'Appenzeller', 241: 'EntleBucher', 242: 'boxer', 243: 'bull mastiff', 244: 'Tibetan mastiff', 245: 'French bulldog', 246: 'Great Dane', 247: 'Saint Bernard, St Bernard', 248: 'Eskimo dog, husky', 249: 'malamute, malemute, Alaskan malamute', 250: 'Siberian husky', 251: 'dalmatian, coach dog, carriage dog', 252: 'affenpinscher, monkey pinscher, monkey dog', 253: 'basenji', 254: 'pug, pug-dog', 255: 'Leonberg', 256: 'Newfoundland, Newfoundland dog', 257: 'Great Pyrenees', 258: 'Samoyed, Samoyede', 259: 'Pomeranian', 260: 'chow, chow chow', 261: 'keeshond', 262: 'Brabancon griffon', 263: 'Pembroke, Pembroke Welsh corgi', 264: 'Cardigan, Cardigan Welsh corgi', 265: 'toy poodle', 266: 'miniature poodle', 267: 'standard poodle', 268: 'Mexican hairless', 269: 'timber wolf, grey wolf, gray wolf, Canis lupus', 270: 'white wolf, Arctic wolf, Canis lupus tundrarum', 271: 'red wolf, maned wolf, Canis rufus, Canis niger', 272: 'coyote, prairie wolf, brush wolf, Canis latrans', 273: 'dingo, warrigal, warragal, Canis dingo', 274: 'dhole, Cuon alpinus', 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus', 276: 'hyena, hyaena', 277: 'red fox, Vulpes vulpes', 278: 'kit fox, Vulpes macrotis', 279: 'Arctic fox, white fox, Alopex lagopus', 280: 'grey fox, gray fox, Urocyon cinereoargenteus', 281: 'tabby, tabby cat', 282: 'tiger cat', 283: 'Persian cat', 284: 'Siamese cat, Siamese', 285: 'Egyptian cat', 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 287: 'lynx, catamount', 288: 'leopard, Panthera pardus', 289: 'snow leopard, ounce, Panthera uncia', 290: 'jaguar, panther, Panthera onca, Felis onca', 291: 'lion, king of beasts, Panthera leo', 292: 'tiger, Panthera tigris', 293: 'cheetah, chetah, Acinonyx jubatus', 294: 'brown bear, bruin, Ursus arctos', 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus', 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus', 297: 'sloth bear, Melursus ursinus, Ursus ursinus', 298: 'mongoose', 299: 'meerkat, mierkat', 300: 'tiger beetle', 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle', 302: 'ground beetle, carabid beetle', 303: 'long-horned beetle, longicorn, longicorn beetle', 304: 'leaf beetle, chrysomelid', 305: 'dung beetle', 306: 'rhinoceros beetle', 307: 'weevil', 308: 'fly', 309: 'bee', 310: 'ant, emmet, pismire', 311: 'grasshopper, hopper', 312: 'cricket', 313: 'walking stick, walkingstick, stick insect', 314: 'cockroach, roach', 315: 'mantis, mantid', 316: 'cicada, cicala', 317: 'leafhopper', 318: 'lacewing, lacewing fly', 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\", 320: 'damselfly', 321: 'admiral', 322: 'ringlet, ringlet butterfly', 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus', 324: 'cabbage butterfly', 325: 'sulphur butterfly, sulfur butterfly', 326: 'lycaenid, lycaenid butterfly', 327: 'starfish, sea star', 328: 'sea urchin', 329: 'sea cucumber, holothurian', 330: 'wood rabbit, cottontail, cottontail rabbit', 331: 'hare', 332: 'Angora, Angora rabbit', 333: 'hamster', 334: 'porcupine, hedgehog', 335: 'fox squirrel, eastern fox squirrel, Sciurus niger', 336: 'marmot', 337: 'beaver', 338: 'guinea pig, Cavia cobaya', 339: 'sorrel', 340: 'zebra', 341: 'hog, pig, grunter, squealer, Sus scrofa', 342: 'wild boar, boar, Sus scrofa', 343: 'warthog', 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius', 345: 'ox', 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis', 347: 'bison', 348: 'ram, tup', 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis', 350: 'ibex, Capra ibex', 351: 'hartebeest', 352: 'impala, Aepyceros melampus', 353: 'gazelle', 354: 'Arabian camel, dromedary, Camelus dromedarius', 355: 'llama', 356: 'weasel', 357: 'mink', 358: 'polecat, fitch, foulmart, foumart, Mustela putorius', 359: 'black-footed ferret, ferret, Mustela nigripes', 360: 'otter', 361: 'skunk, polecat, wood pussy', 362: 'badger', 363: 'armadillo', 364: 'three-toed sloth, ai, Bradypus tridactylus', 365: 'orangutan, orang, orangutang, Pongo pygmaeus', 366: 'gorilla, Gorilla gorilla', 367: 'chimpanzee, chimp, Pan troglodytes', 368: 'gibbon, Hylobates lar', 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus', 370: 'guenon, guenon monkey', 371: 'patas, hussar monkey, Erythrocebus patas', 372: 'baboon', 373: 'macaque', 374: 'langur', 375: 'colobus, colobus monkey', 376: 'proboscis monkey, Nasalis larvatus', 377: 'marmoset', 378: 'capuchin, ringtail, Cebus capucinus', 379: 'howler monkey, howler', 380: 'titi, titi monkey', 381: 'spider monkey, Ateles geoffroyi', 382: 'squirrel monkey, Saimiri sciureus', 383: 'Madagascar cat, ring-tailed lemur, Lemur catta', 384: 'indri, indris, Indri indri, Indri brevicaudatus', 385: 'Indian elephant, Elephas maximus', 386: 'African elephant, Loxodonta africana', 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens', 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca', 389: 'barracouta, snoek', 390: 'eel', 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch', 392: 'rock beauty, Holocanthus tricolor', 393: 'anemone fish', 394: 'sturgeon', 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus', 396: 'lionfish', 397: 'puffer, pufferfish, blowfish, globefish', 398: 'abacus', 399: 'abaya', 400: \"academic gown, academic robe, judge's robe\", 401: 'accordion, piano accordion, squeeze box', 402: 'acoustic guitar', 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier', 404: 'airliner', 405: 'airship, dirigible', 406: 'altar', 407: 'ambulance', 408: 'amphibian, amphibious vehicle', 409: 'analog clock', 410: 'apiary, bee house', 411: 'apron', 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin', 413: 'assault rifle, assault gun', 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack', 415: 'bakery, bakeshop, bakehouse', 416: 'balance beam, beam', 417: 'balloon', 418: 'ballpoint, ballpoint pen, ballpen, Biro', 419: 'Band Aid', 420: 'banjo', 421: 'bannister, banister, balustrade, balusters, handrail', 422: 'barbell', 423: 'barber chair', 424: 'barbershop', 425: 'barn', 426: 'barometer', 427: 'barrel, cask', 428: 'barrow, garden cart, lawn cart, wheelbarrow', 429: 'baseball', 430: 'basketball', 431: 'bassinet', 432: 'bassoon', 433: 'bathing cap, swimming cap', 434: 'bath towel', 435: 'bathtub, bathing tub, bath, tub', 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon', 437: 'beacon, lighthouse, beacon light, pharos', 438: 'beaker', 439: 'bearskin, busby, shako', 440: 'beer bottle', 441: 'beer glass', 442: 'bell cote, bell cot', 443: 'bib', 444: 'bicycle-built-for-two, tandem bicycle, tandem', 445: 'bikini, two-piece', 446: 'binder, ring-binder', 447: 'binoculars, field glasses, opera glasses', 448: 'birdhouse', 449: 'boathouse', 450: 'bobsled, bobsleigh, bob', 451: 'bolo tie, bolo, bola tie, bola', 452: 'bonnet, poke bonnet', 453: 'bookcase', 454: 'bookshop, bookstore, bookstall', 455: 'bottlecap', 456: 'bow', 457: 'bow tie, bow-tie, bowtie', 458: 'brass, memorial tablet, plaque', 459: 'brassiere, bra, bandeau', 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty', 461: 'breastplate, aegis, egis', 462: 'broom', 463: 'bucket, pail', 464: 'buckle', 465: 'bulletproof vest', 466: 'bullet train, bullet', 467: 'butcher shop, meat market', 468: 'cab, hack, taxi, taxicab', 469: 'caldron, cauldron', 470: 'candle, taper, wax light', 471: 'cannon', 472: 'canoe', 473: 'can opener, tin opener', 474: 'cardigan', 475: 'car mirror', 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig', 477: \"carpenter's kit, tool kit\", 478: 'carton', 479: 'car wheel', 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM', 481: 'cassette', 482: 'cassette player', 483: 'castle', 484: 'catamaran', 485: 'CD player', 486: 'cello, violoncello', 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone', 488: 'chain', 489: 'chainlink fence', 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour', 491: 'chain saw, chainsaw', 492: 'chest', 493: 'chiffonier, commode', 494: 'chime, bell, gong', 495: 'china cabinet, china closet', 496: 'Christmas stocking', 497: 'church, church building', 498: 'cinema, movie theater, movie theatre, movie house, picture palace', 499: 'cleaver, meat cleaver, chopper', 500: 'cliff dwelling', 501: 'cloak', 502: 'clog, geta, patten, sabot', 503: 'cocktail shaker', 504: 'coffee mug', 505: 'coffeepot', 506: 'coil, spiral, volute, whorl, helix', 507: 'combination lock', 508: 'computer keyboard, keypad', 509: 'confectionery, confectionary, candy store', 510: 'container ship, containership, container vessel', 511: 'convertible', 512: 'corkscrew, bottle screw', 513: 'cornet, horn, trumpet, trump', 514: 'cowboy boot', 515: 'cowboy hat, ten-gallon hat', 516: 'cradle', 517: 'crane', 518: 'crash helmet', 519: 'crate', 520: 'crib, cot', 521: 'Crock Pot', 522: 'croquet ball', 523: 'crutch', 524: 'cuirass', 525: 'dam, dike, dyke', 526: 'desk', 527: 'desktop computer', 528: 'dial telephone, dial phone', 529: 'diaper, nappy, napkin', 530: 'digital clock', 531: 'digital watch', 532: 'dining table, board', 533: 'dishrag, dishcloth', 534: 'dishwasher, dish washer, dishwashing machine', 535: 'disk brake, disc brake', 536: 'dock, dockage, docking facility', 537: 'dogsled, dog sled, dog sleigh', 538: 'dome', 539: 'doormat, welcome mat', 540: 'drilling platform, offshore rig', 541: 'drum, membranophone, tympan', 542: 'drumstick', 543: 'dumbbell', 544: 'Dutch oven', 545: 'electric fan, blower', 546: 'electric guitar', 547: 'electric locomotive', 548: 'entertainment center', 549: 'envelope', 550: 'espresso maker', 551: 'face powder', 552: 'feather boa, boa', 553: 'file, file cabinet, filing cabinet', 554: 'fireboat', 555: 'fire engine, fire truck', 556: 'fire screen, fireguard', 557: 'flagpole, flagstaff', 558: 'flute, transverse flute', 559: 'folding chair', 560: 'football helmet', 561: 'forklift', 562: 'fountain', 563: 'fountain pen', 564: 'four-poster', 565: 'freight car', 566: 'French horn, horn', 567: 'frying pan, frypan, skillet', 568: 'fur coat', 569: 'garbage truck, dustcart', 570: 'gasmask, respirator, gas helmet', 571: 'gas pump, gasoline pump, petrol pump, island dispenser', 572: 'goblet', 573: 'go-kart', 574: 'golf ball', 575: 'golfcart, golf cart', 576: 'gondola', 577: 'gong, tam-tam', 578: 'gown', 579: 'grand piano, grand', 580: 'greenhouse, nursery, glasshouse', 581: 'grille, radiator grille', 582: 'grocery store, grocery, food market, market', 583: 'guillotine', 584: 'hair slide', 585: 'hair spray', 586: 'half track', 587: 'hammer', 588: 'hamper', 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier', 590: 'hand-held computer, hand-held microcomputer', 591: 'handkerchief, hankie, hanky, hankey', 592: 'hard disc, hard disk, fixed disk', 593: 'harmonica, mouth organ, harp, mouth harp', 594: 'harp', 595: 'harvester, reaper', 596: 'hatchet', 597: 'holster', 598: 'home theater, home theatre', 599: 'honeycomb', 600: 'hook, claw', 601: 'hoopskirt, crinoline', 602: 'horizontal bar, high bar', 603: 'horse cart, horse-cart', 604: 'hourglass', 605: 'iPod', 606: 'iron, smoothing iron', 607: \"jack-o'-lantern\", 608: 'jean, blue jean, denim', 609: 'jeep, landrover', 610: 'jersey, T-shirt, tee shirt', 611: 'jigsaw puzzle', 612: 'jinrikisha, ricksha, rickshaw', 613: 'joystick', 614: 'kimono', 615: 'knee pad', 616: 'knot', 617: 'lab coat, laboratory coat', 618: 'ladle', 619: 'lampshade, lamp shade', 620: 'laptop, laptop computer', 621: 'lawn mower, mower', 622: 'lens cap, lens cover', 623: 'letter opener, paper knife, paperknife', 624: 'library', 625: 'lifeboat', 626: 'lighter, light, igniter, ignitor', 627: 'limousine, limo', 628: 'liner, ocean liner', 629: 'lipstick, lip rouge', 630: 'Loafer', 631: 'lotion', 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system', 633: \"loupe, jeweler's loupe\", 634: 'lumbermill, sawmill', 635: 'magnetic compass', 636: 'mailbag, postbag', 637: 'mailbox, letter box', 638: 'maillot', 639: 'maillot, tank suit', 640: 'manhole cover', 641: 'maraca', 642: 'marimba, xylophone', 643: 'mask', 644: 'matchstick', 645: 'maypole', 646: 'maze, labyrinth', 647: 'measuring cup', 648: 'medicine chest, medicine cabinet', 649: 'megalith, megalithic structure', 650: 'microphone, mike', 651: 'microwave, microwave oven', 652: 'military uniform', 653: 'milk can', 654: 'minibus', 655: 'miniskirt, mini', 656: 'minivan', 657: 'missile', 658: 'mitten', 659: 'mixing bowl', 660: 'mobile home, manufactured home', 661: 'Model T', 662: 'modem', 663: 'monastery', 664: 'monitor', 665: 'moped', 666: 'mortar', 667: 'mortarboard', 668: 'mosque', 669: 'mosquito net', 670: 'motor scooter, scooter', 671: 'mountain bike, all-terrain bike, off-roader', 672: 'mountain tent', 673: 'mouse, computer mouse', 674: 'mousetrap', 675: 'moving van', 676: 'muzzle', 677: 'nail', 678: 'neck brace', 679: 'necklace', 680: 'nipple', 681: 'notebook, notebook computer', 682: 'obelisk', 683: 'oboe, hautboy, hautbois', 684: 'ocarina, sweet potato', 685: 'odometer, hodometer, mileometer, milometer', 686: 'oil filter', 687: 'organ, pipe organ', 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO', 689: 'overskirt', 690: 'oxcart', 691: 'oxygen mask', 692: 'packet', 693: 'paddle, boat paddle', 694: 'paddlewheel, paddle wheel', 695: 'padlock', 696: 'paintbrush', 697: \"pajama, pyjama, pj's, jammies\", 698: 'palace', 699: 'panpipe, pandean pipe, syrinx', 700: 'paper towel', 701: 'parachute, chute', 702: 'parallel bars, bars', 703: 'park bench', 704: 'parking meter', 705: 'passenger car, coach, carriage', 706: 'patio, terrace', 707: 'pay-phone, pay-station', 708: 'pedestal, plinth, footstall', 709: 'pencil box, pencil case', 710: 'pencil sharpener', 711: 'perfume, essence', 712: 'Petri dish', 713: 'photocopier', 714: 'pick, plectrum, plectron', 715: 'pickelhaube', 716: 'picket fence, paling', 717: 'pickup, pickup truck', 718: 'pier', 719: 'piggy bank, penny bank', 720: 'pill bottle', 721: 'pillow', 722: 'ping-pong ball', 723: 'pinwheel', 724: 'pirate, pirate ship', 725: 'pitcher, ewer', 726: \"plane, carpenter's plane, woodworking plane\", 727: 'planetarium', 728: 'plastic bag', 729: 'plate rack', 730: 'plow, plough', 731: \"plunger, plumber's helper\", 732: 'Polaroid camera, Polaroid Land camera', 733: 'pole', 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria', 735: 'poncho', 736: 'pool table, billiard table, snooker table', 737: 'pop bottle, soda bottle', 738: 'pot, flowerpot', 739: \"potter's wheel\", 740: 'power drill', 741: 'prayer rug, prayer mat', 742: 'printer', 743: 'prison, prison house', 744: 'projectile, missile', 745: 'projector', 746: 'puck, hockey puck', 747: 'punching bag, punch bag, punching ball, punchball', 748: 'purse', 749: 'quill, quill pen', 750: 'quilt, comforter, comfort, puff', 751: 'racer, race car, racing car', 752: 'racket, racquet', 753: 'radiator', 754: 'radio, wireless', 755: 'radio telescope, radio reflector', 756: 'rain barrel', 757: 'recreational vehicle, RV, R.V.', 758: 'reel', 759: 'reflex camera', 760: 'refrigerator, icebox', 761: 'remote control, remote', 762: 'restaurant, eating house, eating place, eatery', 763: 'revolver, six-gun, six-shooter', 764: 'rifle', 765: 'rocking chair, rocker', 766: 'rotisserie', 767: 'rubber eraser, rubber, pencil eraser', 768: 'rugby ball', 769: 'rule, ruler', 770: 'running shoe', 771: 'safe', 772: 'safety pin', 773: 'saltshaker, salt shaker', 774: 'sandal', 775: 'sarong', 776: 'sax, saxophone', 777: 'scabbard', 778: 'scale, weighing machine', 779: 'school bus', 780: 'schooner', 781: 'scoreboard', 782: 'screen, CRT screen', 783: 'screw', 784: 'screwdriver', 785: 'seat belt, seatbelt', 786: 'sewing machine', 787: 'shield, buckler', 788: 'shoe shop, shoe-shop, shoe store', 789: 'shoji', 790: 'shopping basket', 791: 'shopping cart', 792: 'shovel', 793: 'shower cap', 794: 'shower curtain', 795: 'ski', 796: 'ski mask', 797: 'sleeping bag', 798: 'slide rule, slipstick', 799: 'sliding door', 800: 'slot, one-armed bandit', 801: 'snorkel', 802: 'snowmobile', 803: 'snowplow, snowplough', 804: 'soap dispenser', 805: 'soccer ball', 806: 'sock', 807: 'solar dish, solar collector, solar furnace', 808: 'sombrero', 809: 'soup bowl', 810: 'space bar', 811: 'space heater', 812: 'space shuttle', 813: 'spatula', 814: 'speedboat', 815: \"spider web, spider's web\", 816: 'spindle', 817: 'sports car, sport car', 818: 'spotlight, spot', 819: 'stage', 820: 'steam locomotive', 821: 'steel arch bridge', 822: 'steel drum', 823: 'stethoscope', 824: 'stole', 825: 'stone wall', 826: 'stopwatch, stop watch', 827: 'stove', 828: 'strainer', 829: 'streetcar, tram, tramcar, trolley, trolley car', 830: 'stretcher', 831: 'studio couch, day bed', 832: 'stupa, tope', 833: 'submarine, pigboat, sub, U-boat', 834: 'suit, suit of clothes', 835: 'sundial', 836: 'sunglass', 837: 'sunglasses, dark glasses, shades', 838: 'sunscreen, sunblock, sun blocker', 839: 'suspension bridge', 840: 'swab, swob, mop', 841: 'sweatshirt', 842: 'swimming trunks, bathing trunks', 843: 'swing', 844: 'switch, electric switch, electrical switch', 845: 'syringe', 846: 'table lamp', 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle', 848: 'tape player', 849: 'teapot', 850: 'teddy, teddy bear', 851: 'television, television system', 852: 'tennis ball', 853: 'thatch, thatched roof', 854: 'theater curtain, theatre curtain', 855: 'thimble', 856: 'thresher, thrasher, threshing machine', 857: 'throne', 858: 'tile roof', 859: 'toaster', 860: 'tobacco shop, tobacconist shop, tobacconist', 861: 'toilet seat', 862: 'torch', 863: 'totem pole', 864: 'tow truck, tow car, wrecker', 865: 'toyshop', 866: 'tractor', 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi', 868: 'tray', 869: 'trench coat', 870: 'tricycle, trike, velocipede', 871: 'trimaran', 872: 'tripod', 873: 'triumphal arch', 874: 'trolleybus, trolley coach, trackless trolley', 875: 'trombone', 876: 'tub, vat', 877: 'turnstile', 878: 'typewriter keyboard', 879: 'umbrella', 880: 'unicycle, monocycle', 881: 'upright, upright piano', 882: 'vacuum, vacuum cleaner', 883: 'vase', 884: 'vault', 885: 'velvet', 886: 'vending machine', 887: 'vestment', 888: 'viaduct', 889: 'violin, fiddle', 890: 'volleyball', 891: 'waffle iron', 892: 'wall clock', 893: 'wallet, billfold, notecase, pocketbook', 894: 'wardrobe, closet, press', 895: 'warplane, military plane', 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin', 897: 'washer, automatic washer, washing machine', 898: 'water bottle', 899: 'water jug', 900: 'water tower', 901: 'whiskey jug', 902: 'whistle', 903: 'wig', 904: 'window screen', 905: 'window shade', 906: 'Windsor tie', 907: 'wine bottle', 908: 'wing', 909: 'wok', 910: 'wooden spoon', 911: 'wool, woolen, woollen', 912: 'worm fence, snake fence, snake-rail fence, Virginia fence', 913: 'wreck', 914: 'yawl', 915: 'yurt', 916: 'web site, website, internet site, site', 917: 'comic book', 918: 'crossword puzzle, crossword', 919: 'street sign', 920: 'traffic light, traffic signal, stoplight', 921: 'book jacket, dust cover, dust jacket, dust wrapper', 922: 'menu', 923: 'plate', 924: 'guacamole', 925: 'consomme', 926: 'hot pot, hotpot', 927: 'trifle', 928: 'ice cream, icecream', 929: 'ice lolly, lolly, lollipop, popsicle', 930: 'French loaf', 931: 'bagel, beigel', 932: 'pretzel', 933: 'cheeseburger', 934: 'hotdog, hot dog, red hot', 935: 'mashed potato', 936: 'head cabbage', 937: 'broccoli', 938: 'cauliflower', 939: 'zucchini, courgette', 940: 'spaghetti squash', 941: 'acorn squash', 942: 'butternut squash', 943: 'cucumber, cuke', 944: 'artichoke, globe artichoke', 945: 'bell pepper', 946: 'cardoon', 947: 'mushroom', 948: 'Granny Smith', 949: 'strawberry', 950: 'orange', 951: 'lemon', 952: 'fig', 953: 'pineapple, ananas', 954: 'banana', 955: 'jackfruit, jak, jack', 956: 'custard apple', 957: 'pomegranate', 958: 'hay', 959: 'carbonara', 960: 'chocolate sauce, chocolate syrup', 961: 'dough', 962: 'meat loaf, meatloaf', 963: 'pizza, pizza pie', 964: 'potpie', 965: 'burrito', 966: 'red wine', 967: 'espresso', 968: 'cup', 969: 'eggnog', 970: 'alp', 971: 'bubble', 972: 'cliff, drop, drop-off', 973: 'coral reef', 974: 'geyser', 975: 'lakeside, lakeshore', 976: 'promontory, headland, head, foreland', 977: 'sandbar, sand bar', 978: 'seashore, coast, seacoast, sea-coast', 979: 'valley, vale', 980: 'volcano', 981: 'ballplayer, baseball player', 982: 'groom, bridegroom', 983: 'scuba diver', 984: 'rapeseed', 985: 'daisy', 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\", 987: 'corn', 988: 'acorn', 989: 'hip, rose hip, rosehip', 990: 'buckeye, horse chestnut, conker', 991: 'coral fungus', 992: 'agaric', 993: 'gyromitra', 994: 'stinkhorn, carrion fungus', 995: 'earthstar', 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa', 997: 'bolete', 998: 'ear, spike, capitulum', 999: 'toilet tissue, toilet paper, bathroom tissue'}\rEgyptian cat 经过转码后得json结构是一个key=pixel_values的像素数组，维度是：[批次，通道数，宽度，高度]。 通过model.config.id2label可以看到总共1000个label。\n数据集 food101包含多种食物类别，数据集地址：https://huggingface.co/datasets/ethz/food101。\nfrom datasets import load_dataset\rds = load_dataset(\"food101\")\rprint(\"数据集\",ds)\r#获取训练集数据\rds = load_dataset(\"food101\",split=\"train\")\rprint(\"训练集\",ds)\rprint(\"第一个数据集\",ds[0])\r#获取所有label\rlabels = ds.features[\"label\"].names\rprint(labels)\rprint(len(labels)) 输出\n数据集 DatasetDict({\rtrain: Dataset({\rfeatures: ['image', 'label'],\rnum_rows: 75750\r})\rvalidation: Dataset({\rfeatures: ['image', 'label'],\rnum_rows: 25250\r})\r})\r训练集 Dataset({\rfeatures: ['image', 'label'],\rnum_rows: 75750\r})\r第一个数据集 {'image': \u003cPIL.Image.Image image mode=RGB size=384x512 at 0x7A1FCE415750\u003e, 'label': 6}\r['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\r101 总共101个food总类。 显示第二张图片和label\nimport matplotlib.pyplot as plt\rplt.imshow(ds[1][\"image\"])\rplt.axis('off') # 关闭坐标轴\rplt.show()\rprint(labels[ds[1][\"label\"]]) 显示 这里我们看到第二个数据集的label=6,也就是beignets。\n我们需要生成label和id关系的字典。\nlabels = ds[\"train\"].features[\"label\"].names\rlabel2id, id2label = dict(), dict()\rfor i, label in enumerate(labels):\rlabel2id[label] = i\rid2label[i] = label\rprint(id2label)\rprint(label2id) 输出：\n{0: 'apple_pie', 1: 'baby_back_ribs', 2: 'baklava', 3: 'beef_carpaccio', 4: 'beef_tartare', 5: 'beet_salad', 6: 'beignets', 7: 'bibimbap', 8: 'bread_pudding', 9: 'breakfast_burrito', 10: 'bruschetta', 11: 'caesar_salad', 12: 'cannoli', 13: 'caprese_salad', 14: 'carrot_cake', 15: 'ceviche', 16: 'cheesecake', 17: 'cheese_plate', 18: 'chicken_curry', 19: 'chicken_quesadilla', 20: 'chicken_wings', 21: 'chocolate_cake', 22: 'chocolate_mousse', 23: 'churros', 24: 'clam_chowder', 25: 'club_sandwich', 26: 'crab_cakes', 27: 'creme_brulee', 28: 'croque_madame', 29: 'cup_cakes', 30: 'deviled_eggs', 31: 'donuts', 32: 'dumplings', 33: 'edamame', 34: 'eggs_benedict', 35: 'escargots', 36: 'falafel', 37: 'filet_mignon', 38: 'fish_and_chips', 39: 'foie_gras', 40: 'french_fries', 41: 'french_onion_soup', 42: 'french_toast', 43: 'fried_calamari', 44: 'fried_rice', 45: 'frozen_yogurt', 46: 'garlic_bread', 47: 'gnocchi', 48: 'greek_salad', 49: 'grilled_cheese_sandwich', 50: 'grilled_salmon', 51: 'guacamole', 52: 'gyoza', 53: 'hamburger', 54: 'hot_and_sour_soup', 55: 'hot_dog', 56: 'huevos_rancheros', 57: 'hummus', 58: 'ice_cream', 59: 'lasagna', 60: 'lobster_bisque', 61: 'lobster_roll_sandwich', 62: 'macaroni_and_cheese', 63: 'macarons', 64: 'miso_soup', 65: 'mussels', 66: 'nachos', 67: 'omelette', 68: 'onion_rings', 69: 'oysters', 70: 'pad_thai', 71: 'paella', 72: 'pancakes', 73: 'panna_cotta', 74: 'peking_duck', 75: 'pho', 76: 'pizza', 77: 'pork_chop', 78: 'poutine', 79: 'prime_rib', 80: 'pulled_pork_sandwich', 81: 'ramen', 82: 'ravioli', 83: 'red_velvet_cake', 84: 'risotto', 85: 'samosa', 86: 'sashimi', 87: 'scallops', 88: 'seaweed_salad', 89: 'shrimp_and_grits', 90: 'spaghetti_bolognese', 91: 'spaghetti_carbonara', 92: 'spring_rolls', 93: 'steak', 94: 'strawberry_shortcake', 95: 'sushi', 96: 'tacos', 97: 'takoyaki', 98: 'tiramisu', 99: 'tuna_tartare', 100: 'waffles'}\r{'apple_pie': 0, 'baby_back_ribs': 1, 'baklava': 2, 'beef_carpaccio': 3, 'beef_tartare': 4, 'beet_salad': 5, 'beignets': 6, 'bibimbap': 7, 'bread_pudding': 8, 'breakfast_burrito': 9, 'bruschetta': 10, 'caesar_salad': 11, 'cannoli': 12, 'caprese_salad': 13, 'carrot_cake': 14, 'ceviche': 15, 'cheesecake': 16, 'cheese_plate': 17, 'chicken_curry': 18, 'chicken_quesadilla': 19, 'chicken_wings': 20, 'chocolate_cake': 21, 'chocolate_mousse': 22, 'churros': 23, 'clam_chowder': 24, 'club_sandwich': 25, 'crab_cakes': 26, 'creme_brulee': 27, 'croque_madame': 28, 'cup_cakes': 29, 'deviled_eggs': 30, 'donuts': 31, 'dumplings': 32, 'edamame': 33, 'eggs_benedict': 34, 'escargots': 35, 'falafel': 36, 'filet_mignon': 37, 'fish_and_chips': 38, 'foie_gras': 39, 'french_fries': 40, 'french_onion_soup': 41, 'french_toast': 42, 'fried_calamari': 43, 'fried_rice': 44, 'frozen_yogurt': 45, 'garlic_bread': 46, 'gnocchi': 47, 'greek_salad': 48, 'grilled_cheese_sandwich': 49, 'grilled_salmon': 50, 'guacamole': 51, 'gyoza': 52, 'hamburger': 53, 'hot_and_sour_soup': 54, 'hot_dog': 55, 'huevos_rancheros': 56, 'hummus': 57, 'ice_cream': 58, 'lasagna': 59, 'lobster_bisque': 60, 'lobster_roll_sandwich': 61, 'macaroni_and_cheese': 62, 'macarons': 63, 'miso_soup': 64, 'mussels': 65, 'nachos': 66, 'omelette': 67, 'onion_rings': 68, 'oysters': 69, 'pad_thai': 70, 'paella': 71, 'pancakes': 72, 'panna_cotta': 73, 'peking_duck': 74, 'pho': 75, 'pizza': 76, 'pork_chop': 77, 'poutine': 78, 'prime_rib': 79, 'pulled_pork_sandwich': 80, 'ramen': 81, 'ravioli': 82, 'red_velvet_cake': 83, 'risotto': 84, 'samosa': 85, 'sashimi': 86, 'scallops': 87, 'seaweed_salad': 88, 'shrimp_and_grits': 89, 'spaghetti_bolognese': 90, 'spaghetti_carbonara': 91, 'spring_rolls': 92, 'steak': 93, 'strawberry_shortcake': 94, 'sushi': 95, 'tacos': 96, 'takoyaki': 97, 'tiramisu': 98, 'tuna_tartare': 99, 'waffles': 100} 加载一个图像处理器，以正确调整大小并对训练和评估图像的像素值进行归一化。\nfrom transformers import AutoImageProcessor\rimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\") 您还可以使用图像处理器来准备一些转换函数，用于数据增强和像素缩放。\nfrom torchvision.transforms import (\rCenterCrop,\rCompose,\rNormalize,\rRandomHorizontalFlip,\rRandomResizedCrop,\rResize,\rToTensor,\r)\rnormalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\rtrain_transforms = Compose(\r[\rRandomResizedCrop(image_processor.size[\"height\"]),\rRandomHorizontalFlip(),\rToTensor(),\rnormalize,\r]\r)\rval_transforms = Compose(\r[\rResize(image_processor.size[\"height\"]),\rCenterCrop(image_processor.size[\"height\"]),\rToTensor(),\rnormalize,\r]\r)\rdef preprocess_train(example_batch):\rexample_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\rreturn example_batch\rdef preprocess_val(example_batch):\rexample_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\rreturn example_batch 定义训练和验证数据集，并使用set_transform函数在运行时应用转换。\ntrain_ds = ds[\"train\"]\rval_ds = ds[\"validation\"]\rtrain_ds.set_transform(preprocess_train)\rval_ds.set_transform(preprocess_val) 最后，您需要一个数据整理器来创建训练和评估数据的批次，并将标签转换为torch.tensor对象。\nimport torch\rdef collate_fn(examples):\rpixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\rlabels = torch.tensor([example[\"label\"] for example in examples])\rreturn {\"pixel_values\": pixel_values, \"labels\": labels} 模型 现在让我们加载一个预训练模型作为基础模型。本指南使用了google/vit-base-patch16-224-in21k模型，但您可以使用任何您想要的图像分类模型。将label2id和id2label字典传递给模型，以便它知道如何将整数标签映射到它们的类标签，并且如果您正在微调已经微调过的检查点，可以选择传递ignore_mismatched_sizes=True参数。\nfrom transformers import AutoModelForImageClassification, TrainingArguments, Trainer\rmodel = AutoModelForImageClassification.from_pretrained(\r\"google/vit-base-patch16-224-in21k\",\rlabel2id=label2id,\rid2label=id2label,\rignore_mismatched_sizes=True,\r) PEFT configuration and model 每个 PEFT 方法都需要一个配置，其中包含了指定 PEFT 方法应该如何应用的所有参数。一旦配置设置好了，就将其传递给 get_peft_model() 函数，同时还要传递基础模型，以创建一个可训练的 PeftModel。\n调用 print_trainable_parameters() 方法来比较 PeftModel 的参数数量与基础模型的参数数量！\nLoRA将权重更新矩阵分解为两个较小的矩阵。这些低秩矩阵的大小由其秩或r确定。更高的秩意味着模型有更多的参数需要训练，但也意味着模型有更大的学习能力。您还需要指定 target_modules，确定较小矩阵插入的位置。对于本指南，您将针对注意力块的查询和值矩阵进行目标指定。设置的其他重要参数包括 lora_alpha（缩放因子）、bias（是否应该训练none、all或只有 LoRA 偏置参数）、modules_to_save（除了 LoRA 层之外需要训练和保存的模块）。所有这些参数 - 以及更多 - 都可以在 LoraConfig 中找到。\nfrom peft import LoraConfig, get_peft_model\rconfig = LoraConfig(\rr=16,\rlora_alpha=16,\rtarget_modules=[\"query\", \"value\"],\rlora_dropout=0.1,\rbias=\"none\",\rmodules_to_save=[\"classifier\"],\r)\rmodel = get_peft_model(model, config)\rmodel.print_trainable_parameters() 输出：“trainable params: 667,493 || all params: 86,543,818 || trainable%: 0.7712775047664294”\n在LoRA中，为了简化和精简，可能只针对查询和值矩阵进行权重分解，而不对键矩阵进行处理。这样可以在一定程度上减少计算量和参数数量，同时仍然提高模型的学习能力和灵活性。\n参数说明：\ntask_type：指定任务类型。如：条件生成任务（SEQ_2_SEQ_LM），因果语言建模（CAUSAL_LM）等。 inference_mode：是否在推理模式下使用Peft模型。 r： LoRA低秩矩阵的维数。关于秩的选择，通常，使用4，8，16即可。 lora_alpha： LoRA低秩矩阵的缩放系数，为一个常数超参，调整alpha与调整学习率类似。 lora_dropout：LoRA 层的丢弃（dropout）率，取值范围为[0, 1)。 target_modules：要替换为 LoRA 的模块名称列表或模块名称的正则表达式。针对不同类型的模型，模块名称不一样，因此，我们需要根据具体的模型进行设置，比如，LLaMa的默认模块名为[q_proj, v_proj]，我们也可以自行指定为：[q_proj,k_proj,v_proj,o_proj]。 在 PEFT 中支持的模型默认的模块名如下所示： TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\r\"t5\": [\"q\", \"v\"],\r\"mt5\": [\"q\", \"v\"],\r\"bart\": [\"q_proj\", \"v_proj\"],\r\"gpt2\": [\"c_attn\"],\r\"bloom\": [\"query_key_value\"],\r\"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"],\r\"opt\": [\"q_proj\", \"v_proj\"],\r\"gptj\": [\"q_proj\", \"v_proj\"],\r\"gpt_neox\": [\"query_key_value\"],\r\"gpt_neo\": [\"q_proj\", \"v_proj\"],\r\"bert\": [\"query\", \"value\"],\r\"roberta\": [\"query\", \"value\"],\r\"xlm-roberta\": [\"query\", \"value\"],\r\"electra\": [\"query\", \"value\"],\r\"deberta-v2\": [\"query_proj\", \"value_proj\"],\r\"deberta\": [\"in_proj\"],\r\"layoutlm\": [\"query\", \"value\"],\r\"llama\": [\"q_proj\", \"v_proj\"],\r\"chatglm\": [\"query_key_value\"],\r\"gpt_bigcode\": [\"c_attn\"],\r\"mpt\": [\"Wqkv\"],\r} 训练 对于训练，让我们使用Transformers中的Trainer类。Trainer类包含一个PyTorch训练循环，当您准备好时，调用train开始训练。要自定义训练运行，请在TrainingArguments类中配置训练超参数。对于类似LoRA的方法，您可以承受更高的批量大小和学习率。\nbatch_size = 128\rargs = TrainingArguments(\r#peft_model_id,\routput_dir=\"/kaggle/working\",\rremove_unused_columns=False,\revaluation_strategy=\"epoch\",\rsave_strategy=\"epoch\",\rlearning_rate=5e-3,\rreport_to=\"none\",\rper_device_train_batch_size=batch_size,\rgradient_accumulation_steps=4,\rper_device_eval_batch_size=batch_size,\rfp16=True,\rnum_train_epochs=5,\rlogging_steps=10,\rload_best_model_at_end=True,\rlabel_names=[\"labels\"],\r) 这里是对TrainingArguments中参数的解释：\noutput_dir: 指定训练过程中输出模型和日志的目录。 remove_unused_columns: 控制是否在训练过程中删除未使用的列。 evaluation_strategy: 指定评估策略，这里设置为“epoch”表示在每个epoch结束时进行评估。 save_strategy: 指定模型保存策略，这里设置为“epoch”表示在每个epoch结束时保存模型。 learning_rate: 学习率设置为5e-3，即0.005。 report_to: 控制训练过程中的报告输出，这里设置为“none”表示不输出任何报告。 per_device_train_batch_size: 每个设备上的训练批量大小。 gradient_accumulation_steps: 梯度累积步数。 per_device_eval_batch_size: 每个设备上的评估批量大小。 fp16: 控制是否使用混合精度训练。 num_train_epochs: 训练的总epoch数。 logging_steps: 控制日志输出的步数。 load_best_model_at_end: 在训练结束时是否加载最佳模型。 label_names: 标签的名称列表。 这些参数是用来配置训练过程的，例如指定训练和评估的批量大小、学习率、训练时长等等。 开始训练\ntrainer = Trainer(\rmodel,\rargs,\rtrain_dataset=train_ds,\reval_dataset=val_ds,\rtokenizer=image_processor,\rdata_collator=collate_fn,\r)\rtrainer.train() 使用kaggle的免费gpu T4*2(双倍时间消耗累计)，gpu基本100%，gpu是一周30hhrs免费时间的,我为了节省时间，用2epoch，batch_size=128,因为kaggle的session有效期在12hours内，越快越好，否则session断开就白训练了，简单看下效果，大概1个小时左右，也可以save session让他在后台跑。 看下速度 第一次epoch完成 查看输出 点击后面的三个点下载所有的文件，然后将模型下载下来，点击输入的上传-new model 输入model名称，选择私有，点击create model 输入平台：transformer，点击addnewvariation 定义附件名称，选择协议，点击右下侧的create 关闭后点击return to notebook，就可以看到输入的模型了，点击右侧的复制路径即可。 这里input的是持久的不会丢失，output数据再页面关闭session关闭后就丢失，所以尽快保存下来，或者上传到huggingface。\n预测 切换到p100(按分钟算，省钱)验证 代码\nmodel_name=\"/kaggle/input/image-classifity/transformers/version1/1/checkpoint-74\" #复制输入的路径\rfrom peft import PeftConfig, PeftModel\rfrom transformers import AutoImageProcessor, AutoModelForImageClassification\rfrom PIL import Image\rimport requests,torch\rfrom datasets import load_dataset\rds = load_dataset(\"food101\")\rlabels = ds[\"train\"].features[\"label\"].names\rlabel2id, id2label = dict(), dict()\rfor i, label in enumerate(labels):\rlabel2id[label] = i\rid2label[i] = label\rconfig = PeftConfig.from_pretrained(model_name)\rmodel = AutoModelForImageClassification.from_pretrained(\rconfig.base_model_name_or_path,#google/vit-base-patch16-224-in21k\rlabel2id=label2id,\rid2label=id2label,\rignore_mismatched_sizes=True,\r)\rmodel = PeftModel.from_pretrained(model, model_name)\rurl = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg\"\rimage = Image.open(requests.get(url, stream=True).raw)\rprint(image)\rimage_processor = AutoImageProcessor.from_pretrained(config.base_model_name_or_path)\rencoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")\rwith torch.no_grad():\routputs = model(**encoding)\rlogits = outputs.logits\rpredicted_class_idx = logits.argmax(-1).item()\rprint(\"Predicted class:\", model.config.id2label[predicted_class_idx]) 输出：beignets",
    "description": "简介 PEFT PEFT（Parameter-Efficient Fine-Tuning）是一个用于高效地将大型预训练模型适配到各种下游应用的库，而无需对模型的所有参数进行微调，因为这在计算上是非常昂贵的。PEFT 方法只微调少量的（额外的）模型参数——显著降低了计算和存储成本——同时其性能与完全微调的模型相当。这使得在消费者硬件上训练和存储大型语言模型（LLMs）变得更加可行。\nPEFT 集成了 Transformers、Diffusers 和 Accelerate 库，以提供更快、更简单的方法来加载、训练和使用大型模型进行推理。\nLORA方法 一种高效训练大型模型的流行方法是在注意力块中插入较小的可训练矩阵，这些矩阵是微调期间要学习的增量权重矩阵的低秩分解。预训练模型的原始权重矩阵被冻结，仅更新较小的矩阵。这减少了可训练参数的数量，降低了内存使用和训练时间，而这些在大型模型中可能非常昂贵。\n有几种不同的方法可以将权重矩阵表示为低秩分解，但最常见的方法是低秩适应（LoRA原理）。PEFT 库支持几种其他 LoRA 变体，例如低秩Hadamard积（LoHa）、低秩Kronecker积（LoKr）和自适应低秩适应（AdaLoRA）。你可以在适配器指南中了解这些方法的概念工作原理。如果你有兴趣将这些方法应用于其他任务和用例，比如语义分割、标记分类，可以看看我们的笔记本集合！\nVision Transformer (ViT) Vision Transformer（ViT）模型是由Alexey Dosovitskiy，Lucas Beyer，Alexander Kolesnikov，Dirk Weissenborn，Xiaohua Zhai，Thomas Unterthiner，Mostafa Dehghani，Matthias Minderer，Georg Heigold，Sylvain Gelly，Jakob Uszkoreit，Neil Houlsby在《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》中提出的。这是第一篇成功在ImageNet上训练Transformer编码器并获得非常好结果的论文。\n这篇论文的摘要是：\n虽然Transformer架构已经成为自然语言处理任务的事实标准，但它在计算机视觉中的应用仍然有限。在视觉领域，注意力要么与卷积网络一起应用，要么用来替换卷积网络的某些组件，同时保持其总体结构不变。我们展示了在这种对CNN的依赖不是必要的，纯Transformer直接应用于图像块序列可以在图像分类任务上表现得非常好。当在大量数据上进行预训练并转移到多个中等规模或小型图像识别基准数据集（ImageNet，CIFAR-100，VTAB等）时，Vision Transformer（ViT）与最先进的卷积网络相比取得了出色的结果，同时训练所需的计算资源大大减少。\n具体关于该模型得结构参考：https://huggingface.co/docs/transformers/model_doc/vit\nlora方法实战 图像分类微调 本指南将向你展示如何快速训练一个图像分类模型——使用低秩分解方法——来识别图像中显示的食物类别。 案例来自官网：https://huggingface.co/docs/peft/task_guides/lora_based_methods\n模型选择 google/vit-base-patch16-224-in21k 是一个基于Transformer编码器的模型（类似于BERT），在监督方式下，即ImageNet-21k上以224x224像素的分辨率预训练了大量图像。\n图像被呈现给模型作为固定大小的补丁序列（分辨率为16x16），这些补丁被线性嵌入。在序列的开头还添加了一个[CLS]标记，用于分类任务。在将序列馈送到Transformer编码器的层之前，还会添加绝对位置嵌入。\n需要注意的是，这个模型不提供任何经过微调的头部，因为这些头部被Google研究人员清零了。但是，模型包括预训练的汇聚层，可以用于下游任务（如图像分类）。\n通过预训练模型，它学习了图像的内部表示，然后可以用于提取对下游任务有用的特征：例如，如果您有一个带标签的图像数据集，可以通过在预训练编码器顶部放置一个线性层来训练标准分类器。通常将线性层放置在[CLS]标记的顶部，因为该标记的最后隐藏状态可以被视为整个图像的表示。\nfrom transformers import ViTImageProcessor, FlaxViTModel\rfrom PIL import Image\rimport requests\rurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\rimage = Image.open(requests.get(url, stream=True).raw)\rprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\rmodel = FlaxViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\rinputs = processor(images=image, return_tensors=\"np\")\routputs = model(**inputs)\rlast_hidden_states = outputs.last_hidden_state\rprint(last_hidden_states.shape) 不包含分类信息，不包含label信息",
    "tags": [],
    "title": "Transformers实战03-PEFT库使用LORA方法微调VIT图像分类。",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_03/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "简介 GPT-2（Generative Pre-trained Transformer 2）是由OpenAI开发的一种基于Transformer架构的自然语言处理模型。以下是关于GPT-2的一些关键特点和信息：\nTransformer架构：GPT-2基于Transformer模型架构，这是一种使用自注意力机制来捕捉输入序列中词语之间依赖关系的深度学习模型。\n预训练：GPT-2是一个预训练的语言模型，意味着它在大规模的文本数据上进行了预训练，以学习文本数据的统计特性和语言模式。\n无监督学习：在预训练过程中，GPT-2采用了无监督学习的方式，即模型仅仅通过文本数据本身来学习，而没有使用人工标注的标签或监督信号。\n生成式任务：GPT-2被设计用于生成式任务，如文本生成、对话生成和摘要生成等。它可以根据给定的上下文生成连贯的文本，并且在语言理解和生成方面表现出色。\n多层次架构：GPT-2具有多层的Transformer编码器，其中包含数百万个参数，使得模型能够捕获复杂的语言结构和语义关系。\n大小变种：GPT-2有多个大小的变种，从117M到1.5B个参数不等，每个变种都具有不同的性能和资源要求。更大的模型往往在生成更加准确和流畅的文本方面表现更好，但同时也需要更多的计算资源。\n开放许可：GPT-2是在OpenAI的研究下开发的，其模型和相关资源以开放许可的形式发布，使得研究人员和开发者可以自由地使用和构建基于GPT-2的应用。\n总的来说，GPT-2是一种强大的语言模型，具有广泛的应用潜力，可用于自然语言生成、理解、翻译等各种NLP任务。\n案例 该案例来源huggingface学习中心nlp-course，Training a causal language model from scratch 文章\n描述 我们将构建一个缩减版的代码生成模型：我们将专注于一行补全，而不是完整的函数或类，使用Python代码的一个子集。在Python中处理数据时，您会频繁接触到Python数据科学栈，包括matplotlib、seaborn、pandas和scikit-learn库。在使用这些框架时，经常需要查找特定的命令，因此如果我们可以使用一个模型来为我们完成这些调用，那将是很好的。\n收集数据 我们使用huggingface收集得content包含：“pandas”, “sklearn”, “matplotlib”, “seaborn” 这些关键字python代码 这个数据集是从github公共仓库爬取，比如 from datasets import load_dataset, DatasetDict\rds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\rds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\rraw_datasets = DatasetDict(\r{\r\"train\": ds_train, # .shuffle().select(range(50000)),\r\"valid\": ds_valid, # .shuffle().select(range(500))\r}\r) 让我们看一个数据集中的例子。我们只需显示每个字段的前200个字符：\nfor key in raw_datasets[\"train\"][0]:\rprint(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\") 输出\n'REPO_NAME: kmike/scikit-learn'\r'PATH: sklearn/utils/__init__.py'\r'COPIES: 3'\r'SIZE: 10094'\r'''CONTENT: \"\"\"\rThe :mod:`sklearn.utils` module includes various utilites.\r\"\"\"\rfrom collections import Sequence\rimport numpy as np\rfrom scipy.sparse import issparse\rimport warnings\rfrom .murmurhash import murm\rLICENSE: bsd-3-clause''' 数据集处理 首先要对数据进行标记化，这样我们才能用它进行训练。由于我们的目标主要是自动补全短函数调用，所以我们可以保持上下文大小相对较小。这样做的好处是我们可以更快地训练模型，并且需要的内存量明显较少。如果你的应用程序需要更多的上下文（例如，如果你希望模型能够基于包含函数定义的文件编写单元测试），请确保增加该数字，但也要记住这会增加GPU的内存占用。目前，让我们将上下文大小固定为128个标记，而不是 GPT-2 或 GPT-3 中分别使用的 1,024 或 2,048。\n回顾预处理 input_ids和attention_mask： input_ids是tokenizer处理后得到的输入特征，它将文本转换为模型能够处理的数字序列。每个单词或者标记（token）都会被映射成对应的唯一整数。这些整数序列就是模型的实际输入。 示例：假设原始文本经过tokenizer处理后，生成的input_ids可能是一个整数序列，如[101, 2023, 2003, 1037, 2814, 2242, 102]，每个整数对应一个token。\nattention_mask用于告诉模型哪些部分是真实的输入，哪些部分是填充（padding）的，以便模型在计算时能够正确处理。 对于输入中的真实token，对应位置的attention_mask值为1；对于填充的位置，attention_mask值为0。 示例：如果input_ids是[101, 2023, 2003, 1037, 2814, 2242, 102]，那么对应的attention_mask可能是[1, 1, 1, 1, 1, 1, 1]，表示所有位置都是真实的输入，如果某个句子词元比他小，可能就需要填充。\n#这里演示分词器\rfrom transformers import AutoModel, BertTokenizer\rmodel_name=\"bert-base-chinese\" #bert-base-uncased\rmodel=AutoModel.from_pretrained(model_name)\rtokenizer=BertTokenizer.from_pretrained(model_name)\rprint(type(model),type(tokenizer))\rsequence = [\"我出生在湖南岳阳,我的家在深圳.\",\"我得儿子是小谦谦\"]\r#输出中包含两个键 input_ids 和 attention_mask，其中 input_ids 对应分词之后的 tokens 映射到的数字编号列表，而 attention_mask 则是用来标记哪些 tokens #是被填充的（这里“1”表示是原文，“0”表示是填充字符）。\rprint(tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\",pair=True))\r# 获取填充token的id\rpad_token_id = tokenizer.pad_token_id\r# 获取填充token的字符串表示\rpad_token = tokenizer.convert_ids_to_tokens(pad_token_id)\rprint(f\"实际填充是id,padid={pad_token_id},padtoken={pad_token}\")\r#获取词汇表大小\rvocab = tokenizer.get_vocab()\rvocab_size = len(vocab)\rprint(\"词汇表大小:\", vocab_size,len(tokenizer))\r# 打印词汇表内容（可选）\rprint(\"词汇表内容:\", vocab)\r#将输入切分为词语、子词或者符号（例如标点符号），统称为 tokens；\rprint(tokenizer.tokenize(sequence[0]),len(tokenizer.tokenize(sequence[0])))\r#我们通过 convert_tokens_to_ids() 将切分出的 tokens 转换为对应的 token IDs：\rprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence[0])))\r#可以通过 encode() 函数将这两个步骤合并，并且 encode() 会自动添加模型需要的特殊 token，例如 BERT 分词器会分别在序列的首尾添加[CLS] 和 [SEP]\rprint(tokenizer.encode(sequence[0]))\r#解码还原文字，可以看到encode前后加了[CLS] 和 [SEP]\rprint(tokenizer.decode(tokenizer.encode(sequence[1]))) 输出\n\u003cclass 'transformers.models.bert.modeling_bert.BertModel'\u003e \u003cclass 'transformers.models.bert.tokenization_bert.BertTokenizer'\u003e\r{'input_ids': tensor([[ 101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638,\r2157, 1762, 3918, 1766, 119, 102],\r[ 101, 2769, 2533, 1036, 2094, 3221, 2207, 6472, 6472, 102, 0, 0,\r0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\r[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\r实际填充是id,padid=0,padtoken=[PAD]\r词汇表大小: 21128 21128\r词汇表内容: {'[PAD]': 0, '[unused1]': 1, '[unused2]': 2, '[unused3]': 3, '[unused4]': 4, '[unused5]': 5, '[unused6]': 6, '[unused7]': 7, '[unused8]': 8, '[unused9]': 9, '[unused10]': 10, '[unused11]': 11,。。。。。。。。。。。\r['我', '出', '生', '在', '湖', '南', '岳', '阳', ',', '我', '的', '家', '在', '深', '圳', '.'] 16\r[2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638, 2157, 1762, 3918, 1766, 119]\r[101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638, 2157, 1762, 3918, 1766, 119, 102]\r[CLS] 我 得 儿 子 是 小 谦 谦 [SEP] special token Tokenizer 的特殊标记（special tokens）是在处理文本数据时经常用到的一些特殊符号或者字符串，它们在自然语言处理中起着重要的作用。这些特殊标记通常包括以下几类：\nPadding token ([PAD]) pad_token：\n在进行批量处理时，序列长度不一致是很常见的情况。为了保证输入数据的统一性，我们通常会使用 [PAD] 标记来填充较短的序列，使其与其他序列的长度相同。\nStart of sequence token ([CLS]) bos_token：\n在许多自然语言处理任务（如文本分类）中，需要在输入序列的开头添加一个特殊标记，例如 [CLS]，用于模型理解这是一个序列的起始点，gpt2的开始token是：\u003c|endoftext|\u003e。\nEnd of sequence token ([SEP]) eos_token：\n类似地，[SEP] 标记通常用于表示序列的结束，特别是在处理多个句子或文本对时，可以用 [SEP] 分隔它们。\nMask token ([MASK]) mask_token：\n在预训练语言模型中，为了进行语言模型的掩码语言建模（Masked Language Modeling），我们需要将一些单词或子词随机地用 [MASK] 标记替换掉，让模型预测被掩码的部分。\nunk_token 是 tokenizer 中的一个特殊标记，通常用来表示未登录词（Unknown Token）。在自然语言处理中，未登录词指的是在训练数据中没有出现过的词汇或者子词。当模型在处理输入文本时遇到未登录词，它会用 unk_token 来替代这些词，以便继续进行处理或预测。\nsep_token 是 tokenizer 中的另一个特殊标记，通常用来表示序列的分隔符。在自然语言处理（NLP）任务中，sep_token 主要用于以下几个方面： 某些预训练语言模型（如 BERT）要求输入数据按照特定格式组织，包括使用 sep_token 来分隔输入的各个部分。例如，在文本对分类任务中，可以用 [SEP] 标记分隔两个句子： [CLS] Sentence A [SEP] Sentence B [SEP]\ncls_token 是 tokenizer 中的另一个特殊标记，通常用来表示序列的开头或者分类任务中的特殊标记。\n这些特殊标记在不同的任务和模型中具有不同的用途，但它们的共同作用是帮助模型更好地处理文本数据，处理输入序列的长度变化，以及在特定任务中引导模型学习和预测。通过适当使用特殊标记，可以有效地增强模型对语言数据的理解和处理能力。\n#特殊token\rfrom transformers import GPT2Tokenizer,AutoTokenizer\r# 初始化 GPT-2 分词器\rtokenizer = GPT2Tokenizer.from_pretrained('gpt2')\rtokenizer1 = AutoTokenizer.from_pretrained('bert-base-chinese')\r# 打印所有特殊标记\rprint(\"gpt2特殊标记:\")\rfor token_name, token_value in tokenizer.special_tokens_map.items():\rprint(f\"{token_name}: {token_value}\")\rprint(\"bert-base-chinese特殊标记:\")\rfor token_name, token_value in tokenizer1.special_tokens_map.items():\rprint(f\"{token_name}: {token_value}\") 输出\ngpt2特殊标记:\rbos_token: \u003c|endoftext|\u003e\reos_token: \u003c|endoftext|\u003e\runk_token: \u003c|endoftext|\u003e\r--------------------------\rbert-base-chinese特殊标记:\runk_token: [UNK]\rsep_token: [SEP]\rpad_token: [PAD]\rcls_token: [CLS]\rmask_token: [MASK] chunk 当你有多个句子或文本段落需要处理时，你可以将它们划分成固定长度的小块（chunks），以便输入到模型中进行处理。这个过程通常用于处理较长的文本，以确保模型可以有效地处理输入数据，特别是在使用Transformer等模型时，其输入长度通常是有限制的。\nchunk的逻辑是，输入数据的每一行句子，超过max_length 都会被截断，当前句子被拆分成的chuck的个数为：len(句子)%max_length +1，当前有些模型会添加一些开始和分割字符 比如[CLS][SEQ]等也要算入长度。\n注意tokenizer拆分小块的开启由 truncation=True,决定，如果是False max_length等就无效了。\n#truck的问题。\rcontent = [\"This is the first sentence. This is the second sentence.\",\"i am a stupid man\"]\rfrom transformers import AutoTokenizer\r# 选择一个预训练模型和对应的tokenizer\rmodel_name = \"bert-base-uncased\"\rtokenizer = AutoTokenizer.from_pretrained(model_name)\r# 最大的字符长度，因为字符的最前面会加一个[CLS],最后会补一个[SEP]，每一个句子都会被拆分一次，也就是一个truck行只能10个字符，content【0】因为超过10个字符，所以被切割成2个truck。\r# 输出的trucklength是[10,6]，第二个句子不满10个只有7个，最后length=[10, 6, 7]\rmax_length = 10\r# 进行tokenization，并返回结果\routputs = tokenizer(\rcontent,\rtruncation=True,\rmax_length=max_length,\rreturn_overflowing_tokens=True,\rreturn_length=True,\r)\r# 输出结果\rprint(outputs)\rprint(tokenizer.decode(outputs['input_ids'][0]))\rprint(tokenizer.decode(outputs['input_ids'][1])) 输出\n{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 2023, 2003, 102], [101, 1996, 2117, 6251, 1012, 102], [101, 1045, 2572, 1037, 5236, 2158, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]], 'length': [10, 6, 7], 'overflow_to_sample_mapping': [0, 0, 1]}\r[CLS] this is the first sentence. this is [SEP]\r[CLS] the second sentence. [SEP] 注意overflow_to_sample_mapping中是标识每个小chuck属于之前哪个句子索引，第1-2个chuck是属于第0个索引也就是第一个句子，3个第二个句子。\n如果加了 padding=True,所有的子句都会自动补上padding_id，最终length都会是10，结果就变成\n{'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 2023, 2003, 102], [101, 1996, 2117, 6251, 1012, 102, 0, 0, 0, 0], [101, 1045, 2572, 1037, 5236, 2158, 102, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], 'length': [10, 10, 10], 'overflow_to_sample_mapping': [0, 0, 1]}\r[CLS] this is the first sentence. this is [SEP]\r[CLS] the second sentence. [SEP] [PAD] [PAD] [PAD] [PAD] 其他更详细的预处理参考：https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb\ndatacollator DataCollatorForLanguageModeling 的主要功能是为掩码语言模型（Masked Language Modeling，MLM）任务准备数据。它的主要作用是随机地掩盖输入中的一些标记，并生成相应的标签，以便模型在训练时能够预测这些被掩盖的标记。 DataCollatorWithPadding：对输入进行填充，使得输入张量具有相同的长度。 更多相关类的实现，请参考官方api\n以下是一个例子\nimport torch\rfrom transformers import BertTokenizer, DataCollatorForLanguageModeling\r# 初始化BERT分词器\rtokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r# 定义示例文本\rtexts = [\"Hello, how are you?\", \"I am fine, thank you.\"]\r# 对文本进行编码\rinputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\r# 打印编码后的输入\rprint(\"Encoded inputs:\", inputs)\r# 将输入转换为列表，以适应DataCollatorForLanguageModeling的输入格式,他的格式要求有多少个句子就多少行[{'input_ids':,'attention_mask':},{'input_ids':,'attention_mask':}]\r# tokenizer encode的格式是字典 {'input_ids': [[],[]]是在二维数组体现，所以强制转一下\rbatch = [{key: val[i] for key, val in inputs.items()} for i in range(len(texts))]\rprint(\"collator需要格式\",batch)\r# 初始化数据整理器，指定进行掩码语言模型任务\rdata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\r# 对输入数据进行整理\rcollated_inputs = data_collator(batch)\r# 打印整理后的输入,这里因为mlm=True是自动掩盖，有15%的数据被掩盖，被掩盖的数据在input_ids被替换成103，然后在生成的labels上，没有被掩盖的数据都变成-100，被掩盖的数据替换为之前的数据\r# labels是最后的标签，通过训练反向就能很好的优化模型，这就是masked模型数据处理\rprint(\"Collated inputs:\", collated_inputs)\rdata_collator1 = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\rcollated_inputs = data_collator1(batch)\r#mlm=False，不会产生遮盖，所有的输入生成的是输出相同的labels，如果是padding字符，labels是-100\rprint(\"Collated inputs:\", collated_inputs) 输出\nEncoded inputs: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0],\r[ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\r[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\rcollator需要格式 [{'input_ids': tensor([ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0])}, {'input_ids': tensor([ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012, 102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])}]\rCollated inputs: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0],\r[ 101, 1045, 2572, 2986, 103, 4067, 103, 1012, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\r[1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100],\r[-100, -100, -100, -100, 1010, -100, 2017, -100, -100]])}\rCollated inputs: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0],\r[ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\r[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\r[1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029, 102, -100],\r[ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012, 102]])} map 在使用 transformers 库时，datasets 中的 map 方法是一个非常有用的工具，用于对数据集进行预处理、特征提取、数据增强等操作。下面是一个示例，展示如何使用 map 方法对数据集进行预处理，以便于将其用于训练一个文本分类模型。 详细处理参考：https://huggingface.co/docs/datasets/use_dataset map 函数是 datasets 库中一个非常强大的工具，它允许你对数据集的每个样本或批次进行操作和变换。以下是 map 函数的几个关键参数及其解释：\nfunction 这是一个用户定义的函数，它将应用于数据集的每个样本或批次。函数可以接受一个样本或一组样本作为输入，并返回一个或多个新的字段。\ndef preprocess_function(examples): # 你的预处理逻辑 return examples\nbatched 类型：bool 默认值：False 解释：如果设置为 True，function 将会批量应用到数据集中。这意味着 function 将接收一个包含多个样本的字典作为输入。 dataset.map(preprocess_function, batched=True)\nbatch_size 类型：int 默认值：1000 解释：指定批量处理时的批次大小。仅当 batched=True 时有效。 dataset.map(preprocess_function, batched=True, batch_size=32)\nremove_columns 类型：list or str 默认值：None 解释：指定要从数据集中移除的列。这对于清理不需要的字段非常有用。 dataset.map(preprocess_function, remove_columns=[\"column_name\"])\n# 导入必要的库\rfrom datasets import Dataset\r# 创建一个简单的数据集\rdata = {\r'text': [\r\"This is the first sentence.\",\r\"Here's the second sentence.\",\r\"And this is the third one.\"\r],\r'label': [1, 0, 1]\r}\r# 转换为 Dataset 对象\rdataset = Dataset.from_dict(data)\r# 打印原始数据集\rprint(\"原始数据集：\")\rprint(dataset)\r# 导入必要的库\rfrom transformers import AutoTokenizer\r# 加载预训练的分词器\rtokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\r# 定义预处理函数\rdef preprocess_function(examples):\rprint(\"传入数据集\",examples)\r# 使用分词器对文本进行编码\rencoded_tokenizer = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=8)\rprint(\"分词数据集\",encoded_tokenizer)\r#返回的字典数据会被累加到原始数据集上。\rreturn encoded_tokenizer\r# 使用 map 方法应用预处理函数\rencoded_dataset = dataset.map(preprocess_function, batched=True,batch_size=2)\r# 打印预处理后的数据集\rprint(\"\\n预处理后的数据集结构：\",encoded_dataset)\rprint(\"\\n预处理后的数据集：\",encoded_dataset[0:3])\r# 使用 map 方法应用预处理函数,remove_columns表示删除某些列是个数组。\rencoded_dataset = dataset.map(preprocess_function, batched=True,batch_size=2,remove_columns=dataset.features)\r# 打印预处理后的数据集\rprint(\"\\n预处理后的数据集：\",encoded_dataset[0:3]) 输出：\n原始数据集：\rDataset({\rfeatures: ['text', 'label'],\rnum_rows: 3\r})\rMap: 100%\r3/3 [00:00\u003c00:00, 138.43 examples/s]\r传入数据集 {'text': ['This is the first sentence.', \"Here's the second sentence.\", 'And this is the third one.'], 'label': [1, 0, 1]}\r分词数据集 {'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\r预处理后的数据集结构： Dataset({\rfeatures: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\rnum_rows: 3\r})\r预处理后的数据集： {'text': ['This is the first sentence.', \"Here's the second sentence.\", 'And this is the third one.'], 'label': [1, 0, 1], 'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\r预处理后的数据集： {'input_ids': [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]} 预处理 大多数文档的标记数远超过 128 个，因此简单地将输入截断到最大长度会消除我们数据集的很大一部分。相反，我们将使用 return_overflowing_tokens 选项来对整个输入进行标记，并将其拆分为几个块。我们还将使用 return_length 选项自动返回每个创建块的长度。通常，最后一个块会小于上下文大小，我们将去掉这些部分以避免填充问题；实际上我们不需要它们，因为我们有很多数据。 让我们通过查看前两个例子来看看这到底是如何工作的：\nfrom transformers import AutoTokenizer\rcontext_length = 128\r#这个分词器专门为代码搜索和理解任务设计。它主要用于处理编程语言（如 Python、JavaScript、Java 等）的源代码。\rtokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\routputs = tokenizer(\r#获取0，1这两个数据集的脚本内容\rraw_datasets[\"train\"][:2][\"content\"],\rtruncation=True,\rmax_length=context_length,\rreturn_overflowing_tokens=True,\rreturn_length=True,\r)\rprint(f\"Input IDs length: {len(outputs['input_ids'])}\")\rprint(f\"Input chunk lengths: {(outputs['length'])}\")\rprint(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\") huggingface-course/code-search-net-tokenizer\n设计目标：这个分词器专门为代码搜索和理解任务设计。它主要用于处理编程语言（如 \u003ePython、JavaScript、Java 等）的源代码。 训练数据：该分词器使用 CodeSearchNet 数据集进行训练，数据集中包含了大量的代码示例\u003e和注释。 应用领域：适用于代码搜索、代码补全、代码生成和其他与代码相关的任务。 词汇表：词汇表中包含了大量的编程语言特定的标记（如关键字、操作符、变量名等），以及\u003e常见的编程语言语法和结构。 注意：分词器模型的作用是将单词转换为一个个的数字，训练时使用的数字计算数字之间的上下文关系，最后推算对应的数字后，反向通过词典解析成文字，所以如果需要训练中文，你只需要有一个中文分词模型即可，训练只和数字相关。 输出：\nInput IDs length: 34\rInput chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\rChunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 我们可以看到从这两个例子中总共得到了 34 个片段。查看片段长度，我们可以看到两个文档末尾的片段都少于 128 个标记（分别为 117 和 41）。这些仅占我们拥有的总片段的一小部分，因此我们可以安全地丢弃它们。使用 overflow_to_sample_mapping 字段，我们还可以重建哪些片段属于哪些输入样本。\n通过这个操作，我们利用了 🤗 Datasets 中 Dataset.map() 函数的一个便利功能，即它不需要一一对应的映射，我们可以创建比输入批次多或少的元素批次。当进行数据增强或数据过滤等会改变元素数量的操作时，这非常有用。在我们的例子中，当将每个元素标记为指定上下文大小的块时，我们从每个文档中创建了许多样本。我们只需要确保删除现有列，因为它们的大小不一致。如果我们想保留它们，可以适当重复并在 Dataset.map() 调用中返回它们：\ndef tokenize(element):\routputs = tokenizer(\relement[\"content\"],\rtruncation=True,\rmax_length=context_length,\rreturn_overflowing_tokens=True,\rreturn_length=True,\r)\rinput_batch = []\r#获取当前input_ids和长度，末尾chuck不等于context_length，就不需要加入了\rfor length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\rif length == context_length:\rinput_batch.append(input_ids)\rreturn {\"input_ids\": input_batch}\rtokenized_datasets = raw_datasets.map(\rtokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\r)\rtokenized_datasets 输出：\nDatasetDict({\rtrain: Dataset({\rfeatures: ['input_ids'],\rnum_rows: 16702061\r})\rvalid: Dataset({\rfeatures: ['input_ids'],\rnum_rows: 93164\r})\r}) 我们现在有 1670 万个例子，每个例子有 128 个标记，总共对应大约 21 亿个标记。供参考，OpenAI 的 GPT-3 和 Codex 模型分别在 300 和 1000 亿个标记上训练，其中 Codex 模型是从 GPT-3 检查点初始化的。我们在这一部分的目标不是与这些模型竞争，这些模型可以生成长而连贯的文本，而是创建一个缩减版本，为数据科学家提供快速自动补全功能。 现在我们已经准备好数据集，接下来让我们设置模型！\n初始化模型 回顾模型 参数计算 在 PyTorch 中，t.numel() 是一个张量方法，用于返回张量中所有元素的数量。它等价于计算张量的大小（shape）的所有维度的乘积。例如，一个形状为 (3, 4, 5) 的张量有 3 * 4 * 5 = 60 个元素。\n在你提供的代码中：\nmodel_size = sum(t.numel() for t in model.parameters())\n这里 model.parameters() 返回模型中所有参数的一个生成器。通过 t.numel() 计算每个参数张量中的元素数量，然后使用 sum() 函数将所有这些数量加起来，得到整个模型中所有参数的总元素数量，即模型的总大小。 示例 假设有一个简单的神经网络模型：\nimport torch\rimport torch.nn as nn\rclass SimpleModel(nn.Module):\rdef __init__(self):\rsuper(SimpleModel, self).__init__()\rself.fc1 = nn.Linear(10, 20)\rself.fc2 = nn.Linear(20, 30)\rdef forward(self, x):\rx = self.fc1(x)\rx = self.fc2(x)\rreturn x\rmodel = SimpleModel() 计算模型大小的代码如下：\nmodel_size = sum(t.numel() for t in model.parameters())\rprint(model_size) 在这个例子中，model.parameters() 会返回 fc1 和 fc2 的参数张量。\nfc1 的权重张量形状为 (20, 10)，有 20 * 10 = 200 个元素。 fc1 的偏置张量形状为 (20,)，有 20 个元素。 fc2 的权重张量形状为 (30, 20)，有 30 * 20 = 600 个元素。 fc2 的偏置张量形状为 (30,)，有 30 个元素。 总计模型中有 200 + 20 + 600 + 30 = 850 个参数元素。因此，model_size 的值将是 850。\n初始化 我们的第一步是初始化一个GPT-2模型。我们将为我们的模型使用与小型GPT-2模型相同的配置，因此我们加载预训练的配置，确保标记器大小与模型词汇大小匹配，并传递bos和eos（序列开始和结束）令牌ID：\nfrom transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\rconfig = AutoConfig.from_pretrained(\r\"gpt2\",\rvocab_size=len(tokenizer), #获取词汇表大小\rn_ctx=context_length,\rbos_token_id=tokenizer.bos_token_id,\reos_token_id=tokenizer.eos_token_id,\r) 因为使用了不同的分词器，所以重新加载配置\n通过该配置，我们可以加载一个新模型。请注意，这是我们第一次不使用 from_pretrained() 函数，因为我们实际上是在自己初始化一个模型：\nmodel = GPT2LMHeadModel(config)\rmodel_size = sum(t.numel() for t in model.parameters())\rprint(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\") 输出\nGPT-2 size: 124.2M parameters 我们的模型有 124M 个参数需要调优。在开始训练之前，我们需要设置一个数据整理器，来处理创建批次的工作。我们可以使用 DataCollatorForLanguageModeling 整理器，它是专门为语言建模设计的（正如其名称微妙地暗示的那样）。除了堆叠和填充批次外，它还负责创建语言模型标签——在因果语言建模中，输入也作为标签（仅偏移一个元素），这个数据整理器在训练过程中实时创建它们，因此我们不需要重复 input_ids。 请注意，DataCollatorForLanguageModeling 支持掩码语言建模 (MLM) 和因果语言建模 (CLM)。默认情况下，它为 MLM 准备数据，但我们可以通过设置参数 mlm=False 切换到 CLM：\nfrom transformers import DataCollatorForLanguageModeling\rtokenizer.pad_token = tokenizer.eos_token\rdata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False) 让我们来看一个例子：\nout = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\rfor key in out:\rprint(f\"{key} shape: {out[key].shape}\") 输出\ninput_ids shape: torch.Size([5, 128])\rattention_mask shape: torch.Size([5, 128])\rlabels shape: torch.Size([5, 128]) 我们可以看到示例已经被堆叠，所有张量形状相同。\n剩下的就是配置训练参数并启动训练器。我们将使用余弦学习率调度，并进行一些预热，实际批量大小为256（per_device_train_batch_size * gradient_accumulation_steps）。当单个批次无法适应内存时，会使用梯度累积，它通过多次前向/反向传递逐步累积梯度。当我们使用🤗 Accelerate 创建训练循环时，我们将看到这一点的实际应用。\nfrom transformers import Trainer, TrainingArguments\rargs = TrainingArguments(\routput_dir=\"codeparrot-ds\",\rper_device_train_batch_size=32,\rper_device_eval_batch_size=32,\revaluation_strategy=\"steps\",\reval_steps=5_000,\rlogging_steps=5_000,\rgradient_accumulation_steps=8,\rnum_train_epochs=1,\rweight_decay=0.1,\rwarmup_steps=1_000,\rlr_scheduler_type=\"cosine\",\rlearning_rate=5e-4,\rsave_steps=5_000,\rfp16=True,\rpush_to_hub=True,\r)\rtrainer = Trainer(\rmodel=model,\rtokenizer=tokenizer,\rargs=args,\rdata_collator=data_collator,\rtrain_dataset=tokenized_datasets[\"train\"],\reval_dataset=tokenized_datasets[\"valid\"],\r) 现在我们可以启动训练器并等待训练完成。根据您是在完整的训练集上运行还是在子集上运行，这将分别需要 20 小时或 2 小时，所以准备几杯咖啡和一本好书来阅读吧！\ntrainer.train() 完整代码 from datasets import load_dataset, DatasetDict\rds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\rds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\rraw_datasets = DatasetDict(\r{\r\"train\": ds_train, # .shuffle().select(range(50000)),\r\"valid\": ds_valid, # .shuffle().select(range(500))\r}\r)\rfrom transformers import AutoTokenizer\rcontext_length = 128\r#这个分词器模型专门为代码搜索和理解任务设计。它主要用于处理编程语言（如 Python、JavaScript、Java 等）的源代码，分词器的目的是将对应词元转换为数字，让模型通过计算来理解数字和数字之间的关系，选择模型的分词器非常重要。\rtokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\rdef tokenize(element):\routputs = tokenizer(\relement[\"content\"],\rtruncation=True,\rmax_length=context_length,\rreturn_overflowing_tokens=True,\rreturn_length=True,\r)\rinput_batch = []\r#获取当前input_ids和长度，末尾chuck不等于context_length，就不需要加入了\rfor length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\rif length == context_length:\rinput_batch.append(input_ids)\rreturn {\"input_ids\": input_batch}\rtokenized_datasets = raw_datasets.map(\rtokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\r)\rfrom transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\rconfig = AutoConfig.from_pretrained(\r\"gpt2\",\rvocab_size=len(tokenizer), #获取词汇表大小\rn_ctx=context_length,\rbos_token_id=tokenizer.bos_token_id,\reos_token_id=tokenizer.eos_token_id,\r)\rmodel = GPT2LMHeadModel(config)\rfrom transformers import DataCollatorForLanguageModeling\rtokenizer.pad_token = tokenizer.eos_token\rdata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\rfrom transformers import Trainer, TrainingArguments\rargs = TrainingArguments(\routput_dir=\"/kaggle/working\",\rper_device_train_batch_size=32,\rper_device_eval_batch_size=32,\revaluation_strategy=\"steps\",\reval_steps=5_000,\rlogging_steps=5_000,\rgradient_accumulation_steps=8,\rnum_train_epochs=1,\rweight_decay=0.1,\rwarmup_steps=1_000,\rlr_scheduler_type=\"cosine\",\rlearning_rate=5e-4,\rsave_steps=5_000,\rfp16=True,\rreport_to=\"none\",\rpush_to_hub=False,\r)\rtrainer = Trainer(\rmodel=model,\rtokenizer=tokenizer,\rargs=args,\rdata_collator=data_collator,\rtrain_dataset=tokenized_datasets[\"train\"],\reval_dataset=tokenized_datasets[\"valid\"],\r)\rtrainer.train() 测试 由于使用kaggle的gpu无法在12小时训练完成，所以这里只能用官方已经训练好的镜像测试了。\n现在是见证结果的时刻：让我们看看训练好的模型实际表现如何！我们可以在日志中看到损失值一直在稳定下降，但为了真正测试模型的效果，我们来看看它在一些提示信息上的表现。为此，我们将模型封装到一个文本生成管道中，并如果条件允许的话，将其部署到 GPU 上以实现快速生成：\nimport torch\rfrom transformers import pipeline\rdevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\rpipe = pipeline(\r\"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\r) 让我们从创建散点图的简单任务开始：\ntxt = \"\"\"\\\r# create some data\rx = np.random.randn(100)\ry = np.random.randn(100)\r# create scatter plot with x, y\r\"\"\"\rprint(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) 输出：\n# create some data\rx = np.random.randn(100)\ry = np.random.randn(100)\r# create scatter plot with x, y\rplt.scatter(x, y)\r# create scatter 结果看起来是正确的。对于 pandas 的操作是否也适用呢？我们来看看能否从两个数组创建一个 DataFrame：\ntxt = \"\"\"\\\r# create some data\rx = np.random.randn(100)\ry = np.random.randn(100)\r# create dataframe from x and y\r\"\"\"\rprint(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) 输出\n# create some data\rx = np.random.randn(100)\ry = np.random.randn(100)\r# create dataframe from x and y\rdf = pd.DataFrame({'x': x, 'y': y})\rdf.insert(0,'x', x)\rfor 好的，这是正确的答案——尽管随后又插入了列 x。由于生成的令牌数量有限，下面的 for 循环被截断了。我们来看看能否做一些更复杂的事情，并让模型帮助我们使用 groupby 操作：\ntxt = \"\"\"\\\r# dataframe with profession, income and name\rdf = pd.DataFrame({'profession': x, 'income':y, 'name': z})\r# calculate the mean income per profession\r\"\"\"\rprint(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) 输出\n# dataframe with profession, income and name\rdf = pd.DataFrame({'profession': x, 'income':y, 'name': z})\r# calculate the mean income per profession\rprofession = df.groupby(['profession']).mean()\r# compute the 还不错；这样做是对的。最后，让我们看看是否也能用它来为 scikit-learn 设置一个随机森林模型：\ntxt = \"\"\"\r# import random forest regressor from scikit-learn\rfrom sklearn.ensemble import RandomForestRegressor\r# fit random forest model with 300 estimators on X, y:\r\"\"\"\rprint(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) 输出\n# import random forest regressor from scikit-learn\rfrom sklearn.ensemble import RandomForestRegressor\r# fit random forest model with 300 estimators on X, y:\rrf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)\rrf.fit(X, y)\rrf 查看这几个例子，模型似乎学到了一些 Python 数据科学套件的语法。",
    "description": "简介 GPT-2（Generative Pre-trained Transformer 2）是由OpenAI开发的一种基于Transformer架构的自然语言处理模型。以下是关于GPT-2的一些关键特点和信息：\nTransformer架构：GPT-2基于Transformer模型架构，这是一种使用自注意力机制来捕捉输入序列中词语之间依赖关系的深度学习模型。\n预训练：GPT-2是一个预训练的语言模型，意味着它在大规模的文本数据上进行了预训练，以学习文本数据的统计特性和语言模式。\n无监督学习：在预训练过程中，GPT-2采用了无监督学习的方式，即模型仅仅通过文本数据本身来学习，而没有使用人工标注的标签或监督信号。\n生成式任务：GPT-2被设计用于生成式任务，如文本生成、对话生成和摘要生成等。它可以根据给定的上下文生成连贯的文本，并且在语言理解和生成方面表现出色。\n多层次架构：GPT-2具有多层的Transformer编码器，其中包含数百万个参数，使得模型能够捕获复杂的语言结构和语义关系。\n大小变种：GPT-2有多个大小的变种，从117M到1.5B个参数不等，每个变种都具有不同的性能和资源要求。更大的模型往往在生成更加准确和流畅的文本方面表现更好，但同时也需要更多的计算资源。\n开放许可：GPT-2是在OpenAI的研究下开发的，其模型和相关资源以开放许可的形式发布，使得研究人员和开发者可以自由地使用和构建基于GPT-2的应用。\n总的来说，GPT-2是一种强大的语言模型，具有广泛的应用潜力，可用于自然语言生成、理解、翻译等各种NLP任务。\n案例 该案例来源huggingface学习中心nlp-course，Training a causal language model from scratch 文章\n描述 我们将构建一个缩减版的代码生成模型：我们将专注于一行补全，而不是完整的函数或类，使用Python代码的一个子集。在Python中处理数据时，您会频繁接触到Python数据科学栈，包括matplotlib、seaborn、pandas和scikit-learn库。在使用这些框架时，经常需要查找特定的命令，因此如果我们可以使用一个模型来为我们完成这些调用，那将是很好的。\n收集数据 我们使用huggingface收集得content包含：“pandas”, “sklearn”, “matplotlib”, “seaborn” 这些关键字python代码 这个数据集是从github公共仓库爬取，比如 from datasets import load_dataset, DatasetDict\rds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\rds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\rraw_datasets = DatasetDict(\r{\r\"train\": ds_train, # .shuffle().select(range(50000)),\r\"valid\": ds_valid, # .shuffle().select(range(500))\r}\r) 让我们看一个数据集中的例子。我们只需显示每个字段的前200个字符：",
    "tags": [],
    "title": "Transformers实战04-微调gpt-2生成python代码。",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_04/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers实战",
    "content": "简介 模型量化（Model Quantization）是一种优化技术，旨在减少机器学习模型的计算资源需求和存储空间，同时在精度损失最小化的前提下提高推理效率。量化通过将模型权重和激活函数的数值从高精度（如 32 位浮点数）转换为低精度（如 8 位整数），显著减少了模型大小和计算复杂度。\n主要类型 静态量化（Post-Training Quantization, PTQ）\n在模型训练完成后进行量化。 通过分析训练数据的分布，将权重和激活函数映射到低精度表示。 不需要重新训练模型。 适用于对性能影响较小的场景。 动态量化（Dynamic Quantization）\n在推理时动态地将浮点数转换为低精度整数。 在运行过程中对激活函数进行量化。 比静态量化更简单，因为不需要分析训练数据。 对推理速度有显著提升，尤其是对模型输入依赖较少的层（如全连接层）。 量化感知训练（Quantization-Aware Training, QAT）\n在训练过程中模拟量化影响。 模型在训练过程中考虑量化误差，以便在量化后保持更高的精度。 比静态量化和动态量化需要更多的计算资源，但精度损失最小。 适用于对精度要求较高的应用。 这里例子就演示下动态量化，bitsandbytes本身以上三种都支持。\n量化的优点 减小模型大小：通过将权重和激活函数表示从 32 位浮点数转换为 8 位整数，模型大小可以显著减少。 加快推理速度：低精度运算速度更快，可以显著提高推理效率。 降低内存带宽需求：低精度表示占用更少的内存，减少了内存带宽的需求。 量化的缺点 精度损失：由于数值表示的精度降低，模型可能会经历一定程度的精度损失，具体程度取决于模型结构和数据分布。 复杂性增加：在某些情况下，量化过程可能会增加模型部署的复杂性，尤其是需要进行量化感知训练时。 量化过程 以下过程只是一种最简单的思路，方便理解，实际要比这更复杂。\n量化过程 确定值域： 首先，确定要量化的数据的值域范围。例如，假设我们有一组数据的值域为 $[min,max]$。\n确定量化级别： 确定量化的级别或分辨率，这决定了将值域划分成多少个区间。在4位整数的情况下，共有 $2^4=16$ 个可能的值。\n线性映射： 将原始数据映射到4位整数的范围内。通常使用线性映射来实现，计算公式如下： $$\\text{quantized_value} = \\frac{\\text{original_value} - \\text{min}}{\\text{max} - \\text{min}} \\times (\\text{number of levels} - 1)$$\n这里的 number of levels 是16（对应4位整数的值域范围）。\n反量化过程 解码反量化： 在使用量化数据进行计算之前，需要将其解码回原始的数据表示形式（如32位浮点数或其他高精度表示）。解码公式通常为： $$\\text{original_value} = \\text{quantized_value} \\times \\frac{\\text{max} - \\text{min}}{\\text{number of levels} - 1} + \\text{min}$$ 这里的 quantized_value是是量化后的4位整数值,min和max是原始数据的最小值和最大值。\n两个不同的原始值在量化后可能相同，被还原为同一个值。这种情况表明精度损失是不可避免的。为了减少这种精度损失带来的影响，通常采取以下策略：\n增加量化级别： 增加量化级别（如使用8位、16位量化）以减少不同原始值被量化为同一个值的概率。\n量化感知训练（Quantization-aware training）： 在训练过程中模拟量化误差，以提高模型在量化后的精度表现。\n非线性量化： 使用对数量化或其他非线性量化方法，使得量化更适应数据的分布特性，从而减少精度损失。\n精细调节量化参数： 通过精细调整量化的最小值、最大值和比例因子，尽量减少量化误差对关键值的影响。\n精度和参数 模型中每个参数常见的存储类型包括：\nFP32（32-bit Floating Point）: 每个参数占用 4 字节（32 位），单精度浮点数（32位浮点数），范围大约：$[-3.4 \\times 10^{38}, 3.4 \\times 10^{38}]$。 FP16（16-bit Floating Point）: 每个参数占用 2 字节（16 位），半精度浮点数使用16位（1位符号、5位指数、10位尾数），FP16的数值范围大约是 [−65504,65504]，大约 3 位有效数字。 INT8（8-bit Integer）: 每个参数占用 1 字节（8 位），将模型的权重和激活值量化为8位整数（范围通常是0到255），相对于32位浮点数，精度的损失较小。8-bit量化比4-bit提供更好的精度，并且通常可以更接近原始模型的性能。 INT4（4-bit Integer）: 每个参数占用4位，将模型的权重和激活值量化为4位整数（范围通常是-8到7或者0到15），因此相对于32位浮点数，它的精度显著降低。这种量化可以显著减小模型的大小和计算需求，但可能会损失一定的模型精度。 如何获取某个模型的精度了\nimport torch\rfrom transformers import AutoModel, BertTokenizer\rmodel_name=\"bert-base-chinese\" #bert-base-uncased\rmodel=AutoModel.from_pretrained(model_name)\r#获取模型参数的精度\r\"\"\"\rFP32（32-bit Floating Point）: 每个参数占用 4 字节（32 位）。\rFP16（16-bit Floating Point）: 每个参数占用 2 字节（16 位）。\rINT8（8-bit Integer）: 每个参数占用 1 字节（8 位）。\r\"\"\"\rdtype=list(model.parameters())[0].dtype\rprint(\"精度:\",dtype)\rtotal_params = sum(p.numel() for p in model.parameters())\rdtype_to_bytes = {\rtorch.float32: 4, # FP32: 4字节\rtorch.float16: 2, # FP16: 2字节\rtorch.int8: 1, # INT8: 1字节\rtorch.int32: 4, # INT32: 4字节\rtorch.int64: 8, # INT64: 8字节\rtorch.float64: 8, # FP64 (double): 8字节\r}\rmodel_size = total_params * dtype_to_bytes[dtype]\rprint(f'Model size: {model_size / (1024**2):.2f} MB') 输出\n精度: torch.float32\rModel size: 390.12 MB 量化实例 bitsandbytes bitsandbytes 通过 PyTorch 的 k 位量化技术使大型语言模型的访问变得可行。bitsandbytes 提供了三个主要功能以显著降低推理和训练时的内存消耗：\n8 位优化器采用区块式量化技术，在极小的内存成本下维持 32 位的表现。 LLM.Int() 或 8 位量化使大型语言模型推理只需一半的内存需求，并且不会有任何性能下降。该方法基于向量式的量化技术将大部分特性量化到 8 位，并且用 16 位矩阵乘法单独处理异常值。 QLoRA 或 4 位量化使大型语言模型训练成为可能，它结合了几种节省内存的技术，同时又不牺牲性能。该方法将模型量化至 4 位，并插入一组可训练的低秩适应（LoRA）权重来允许训练。 安装bitsandbytes bitsandbytes 仅支持 CUDA 版本 11.0 - 12.5 的 CUDA GPU。\n!pip install -U bitsandbytes\r!pip install transformers\r!pip install accelerate 4bit量化(加载) 加载并量化一个模型到4位，并使用bfloat16数据类型进行计算：\n您使用 bnb_4bit_compute_dtype=torch.bfloat16，这意味着计算过程中会反量化使用 bfloat16 数据类型，而存储时则可能使用4位表示。这解释了为什么您看到的 dtype 仍然是 fp16 或者 bfloat16。\nBigScience 是一个全球性的开源AI研究合作项目，旨在推动大型语言模型（LLM）的发展。bloom-1b7 是 BigScience 项目下的一部分，具体来说，是一个包含约17亿参数的语言模型。\nimport torch\rfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\rmodel_name=\"bigscience/bloom-1b7\" quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\rmodel = AutoModelForCausalLM.from_pretrained(\rmodel_name,\rdevice_map=\"auto\",\r)\rmodel_4bit = AutoModelForCausalLM.from_pretrained(\rmodel_name,\rdevice_map=\"auto\",\rquantization_config=quantization_config,\r)\rdtype=list(model.parameters())[0].dtype\rprint(\"原始精度:\",dtype)\rdest_dtype=list(model_4bit.parameters())[0].dtype\rprint(\"量化精度:\",dest_dtype)\r# 检查模型的量化配置\rprint(\"量化配置:\", model_4bit.config.quantization_config)\rdef print_model_info(model):\rtotal_params = 0\rfor name, param in model.named_parameters():\rtotal_params += param.numel()\r#print(f\"Total parameters: {total_params / 1e6}M\")\rreturn total_params\rtotal_model_size=print_model_info(model)\rtotal_model_4bit_size=print_model_info(model_4bit)\rprint(\"模型参数个数：\",total_model_size)\rprint(\"量化后的模型参数个数：\",total_model_4bit_size)\rdtype_to_bytes = {\rtorch.float32: 4, # FP32: 4字节\rtorch.float16: 2, # FP16: 2字节\rtorch.int8: 1, # INT8: 1字节\rtorch.int32: 4, # INT32: 4字节\rtorch.int64: 8, # INT64: 8字节\rtorch.float64: 8, # FP64 (double): 8字节\r}\rmodel_size = total_model_size * dtype_to_bytes[dtype]\rmodel_size = total_model_size * dtype_to_bytes[dtype]\rprint(f'origin Model size: {model_size / (1024**2):.2f} MB')\rmodel_size = total_model_4bit_size * dtype_to_bytes[dest_dtype]\rprint(f'quan Model size: {model_size / (1024**2):.2f} MB')\rmodel_4bit.save_pretrained(\"/tmp/p\")\rmodel.save_pretrained(\"/tmp/o\") 输出：\n原始精度: torch.float32\r量化精度: torch.float16\r量化配置: BitsAndBytesConfig {\r\"_load_in_4bit\": true,\r\"_load_in_8bit\": false,\r\"bnb_4bit_compute_dtype\": \"bfloat16\",\r\"bnb_4bit_quant_storage\": \"uint8\",\r\"bnb_4bit_quant_type\": \"fp4\",\r\"bnb_4bit_use_double_quant\": false,\r\"llm_int8_enable_fp32_cpu_offload\": false,\r\"llm_int8_has_fp16_weight\": false,\r\"llm_int8_skip_modules\": null,\r\"llm_int8_threshold\": 6.0,\r\"load_in_4bit\": true,\r\"load_in_8bit\": false,\r\"quant_method\": \"bitsandbytes\"\r}\r模型参数信息： 1722408960\r量化后的模型参数信息： 1118429184\rorigin Model size: 6570.47 MB\rquan Model size: 2133.23 MB 总的参数个数减少。这通常是由于量化过程中进行了优化或者参数压缩的操作。 量化在深度学习中通常是指将模型中的浮点数参数转换为更低精度的整数或定点数表示，以节省内存和提高计算效率。\n为啥量化模型的dtype是fp16了而不是int4，以下是对量化模型加载过程中 dtype 问题的一些解释：\n参数存储与计算类型的区别：\n存储时，模型参数可能被压缩或量化为较低位宽的整数类型（如4位整数）。 加载时，为了方便后续计算，这些参数可能会被解码为较高精度的浮点类型（如 fp16 或 bfloat16）。 量化过程的具体实现：\n许多量化库在加载模型时，会将低位宽的量化参数解码为浮点类型，以便在计算时可以直接使用这些参数。 这就是为什么即使您使用了 load_in_4bit=True，在加载后检查参数的 dtype 时仍然看到的是 fp16。 通过查看模型保存的就可以确定了 查看量化的模型：\n!ls /tmp/p -al --block-size=M | grep model 输出:\n-rw-r--r-- 1 root root 1630M Aug 6 08:04 model.safetensors 可以看到我们之前在内存中打印的是2133.23（内存中计算还是会被反量化到bnb_4bit_compute_dtype指定类型，但是参数都是压缩后去掉了一些参数） ，存储后变成了1630M，比之前计算的少一些，说明存储使用了4bit。 在看下没有量化的模型：\n!ls /tmp/o -al --block-size=M | grep model 输出了：\n-rw-r--r-- 1 root root 4714M Aug 6 08:05 model-00001-of-00002.safetensors\r-rw-r--r-- 1 root root 1857M Aug 6 08:05 model-00002-of-00002.safetensors\r-rw-r--r-- 1 root root 1M Aug 6 08:05 model.safetensors.index.json 可以看到我们之前在内存中打印的是6570.47 MB ，存储后没变，分文件存储了4714M+1857M 。\n8bit量化(加载) 代码和4bit相似，调整下配置即可\nquantization_config = BitsAndBytesConfig(load_in_8bit=True) 同4bit代码，输出\n原始精度: torch.float32\r量化精度: torch.float16\r量化配置: BitsAndBytesConfig {\r\"_load_in_4bit\": false,\r\"_load_in_8bit\": true,\r\"bnb_4bit_compute_dtype\": \"float32\",\r\"bnb_4bit_quant_storage\": \"uint8\",\r\"bnb_4bit_quant_type\": \"fp4\",\r\"bnb_4bit_use_double_quant\": false,\r\"llm_int8_enable_fp32_cpu_offload\": false,\r\"llm_int8_has_fp16_weight\": false,\r\"llm_int8_skip_modules\": null,\r\"llm_int8_threshold\": 6.0,\r\"load_in_4bit\": false,\r\"load_in_8bit\": true,\r\"quant_method\": \"bitsandbytes\"\r}\r模型参数信息： 1722408960\r量化后的模型参数信息： 1722408960\rorigin Model size: 6570.47 MB\rquan Model size: 3285.23 MB 可以看到8bit不需要指定内存计算的类型，量化内存计算精度默认就是fp16。 查看模型保存大小\n!ls /tmp/p -al --block-size=M | grep model\r#----------------------------------------------------------------------------------------------------\r!ls /tmp/o -al --block-size=M | grep model 输出\n-rw-r--r-- 1 root root 2135M Aug 6 08:30 model.safetensors\r#----------------------------------------------------------------------------------------------------\r-rw-r--r-- 1 root root 4714M Aug 6 08:30 model-00001-of-00002.safetensors\r-rw-r--r-- 1 root root 1857M Aug 6 08:31 model-00002-of-00002.safetensors\r-rw-r--r-- 1 root root 1M Aug 6 08:31 model.safetensors.index.json 验证效果 这里用之前的4bit模型来和原始模型比较\nimport time\rdef benchmark_model(model, input_text, tokenizer):\rinputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\rstart_time = time.time()\rwith torch.no_grad():\routputs = model.generate(**inputs)\r# 解码并打印生成的文本\rgenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\rprint(\"Generated text:\", generated_text)\rend_time = time.time()\rinference_time = end_time - start_time\rprint(f\"Inference time: {inference_time:.2f} seconds\")\rfrom transformers import AutoTokenizer\rtokenizer = AutoTokenizer.from_pretrained(model_name)\rinput_text = \"Hello, how are you?\"\rprint(\"未量化模型性能测试：\")\rbenchmark_model(model, input_text, tokenizer)\rprint(\"量化模型性能测试：\")\rbenchmark_model(model_4bit, input_text, tokenizer) 输出\n未量化模型性能测试：\rGenerated text: Hello, how are you? I hope you are doing well. I am a newbie in this\rInference time: 0.31 seconds\r量化模型性能测试：\rGenerated text: Hello, how are you?\"\r\"I'm fine,\" I said.\r\"I'm just a\rInference time: 0.62 seconds 这里看到量化的模型反而推理需要更多的时间，量化模型在理论上应该提高推理速度和减少内存占用,这里使用float16gpu显存占用肯定少了一半以上，但是推理速度比较慢，在实际应用中，可能会因为多个因素导致性能下降。",
    "description": "简介 模型量化（Model Quantization）是一种优化技术，旨在减少机器学习模型的计算资源需求和存储空间，同时在精度损失最小化的前提下提高推理效率。量化通过将模型权重和激活函数的数值从高精度（如 32 位浮点数）转换为低精度（如 8 位整数），显著减少了模型大小和计算复杂度。\n主要类型 静态量化（Post-Training Quantization, PTQ）\n在模型训练完成后进行量化。 通过分析训练数据的分布，将权重和激活函数映射到低精度表示。 不需要重新训练模型。 适用于对性能影响较小的场景。 动态量化（Dynamic Quantization）\n在推理时动态地将浮点数转换为低精度整数。 在运行过程中对激活函数进行量化。 比静态量化更简单，因为不需要分析训练数据。 对推理速度有显著提升，尤其是对模型输入依赖较少的层（如全连接层）。 量化感知训练（Quantization-Aware Training, QAT）\n在训练过程中模拟量化影响。 模型在训练过程中考虑量化误差，以便在量化后保持更高的精度。 比静态量化和动态量化需要更多的计算资源，但精度损失最小。 适用于对精度要求较高的应用。 这里例子就演示下动态量化，bitsandbytes本身以上三种都支持。\n量化的优点 减小模型大小：通过将权重和激活函数表示从 32 位浮点数转换为 8 位整数，模型大小可以显著减少。 加快推理速度：低精度运算速度更快，可以显著提高推理效率。 降低内存带宽需求：低精度表示占用更少的内存，减少了内存带宽的需求。 量化的缺点 精度损失：由于数值表示的精度降低，模型可能会经历一定程度的精度损失，具体程度取决于模型结构和数据分布。 复杂性增加：在某些情况下，量化过程可能会增加模型部署的复杂性，尤其是需要进行量化感知训练时。 量化过程 以下过程只是一种最简单的思路，方便理解，实际要比这更复杂。\n量化过程 确定值域： 首先，确定要量化的数据的值域范围。例如，假设我们有一组数据的值域为 $[min,max]$。\n确定量化级别： 确定量化的级别或分辨率，这决定了将值域划分成多少个区间。在4位整数的情况下，共有 $2^4=16$ 个可能的值。\n线性映射： 将原始数据映射到4位整数的范围内。通常使用线性映射来实现，计算公式如下： $$\\text{quantized_value} = \\frac{\\text{original_value} - \\text{min}}{\\text{max} - \\text{min}} \\times (\\text{number of levels} - 1)$$\n这里的 number of levels 是16（对应4位整数的值域范围）。",
    "tags": [],
    "title": "Transformers实战05-模型量化",
    "uri": "/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_05/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "前言 Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。\n在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。\nAttention is All You Need：Attention Is All You Need\nTransformer 整体结构 首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构： 可以看到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：\n第一步：获取输入句子的每一个单词的表示向量 X，X由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。 第二步：将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 x) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 C，如下图。单词向量矩阵用 $X_{n\\times d}$ 表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。 第三步：将 Encoder 输出的编码信息矩阵 C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。 上图 Decoder 接收了 Encoder 的编码矩阵 C，然后首先输入一个翻译开始符 “\"，预测第一个单词 “I”；然后输入翻译开始符 “” 和单词 “I”，预测单词 “have”，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。\nTransformer 的输入 Transformer 中单词的输入表示 x由单词 Embedding 和位置 Embedding （Positional Encoding）相加得到。 单词 Embedding 单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。\n原理 什么是Word Embedding（词嵌入）？\n词嵌入是自然语言处理中语言模型与表征技术技术的统称。讲人话就是： 就是把词语（字符串类型）这一文本数据转换成 计算机能认识 的数字表征的数据（一般为浮点型数据）。因为我们的机器学习模型或者深度学习模型，需要的数据都是数字类型的，无法处理文本类型的数据，所以我们需要把单词转换成数字类型。 词嵌入为 文本AI系统的上游任务，只有通过词嵌入模型才能得到文本AI系统才能得到数字类型的输入数据。 现有的词嵌入模型有：word2vec，GloVe，ELMo，BERT等 以下使用word2vec的原理来解释下词embedding实现逻辑\nword2vec是词向量化技术的一种，通过神经网络来实现。其在表面上看起来是一种无监督学习技术，但本质上仍然是有监督学习。 利用文本的上下文信息构造有监督数据集，通过这一数据集来训练神经网络，最后取得训练好的神经网络两个网络层之间的权重 矩阵作为的词向量表（每个单词对应其中一行数据）。\nword2vec 有两个模型：\nSkip-gram模型：其特点为，根据当前单词预测上下文单词，使用中心词来预测上下文词。 CBOW模型：全称为 Continuous Bag-of-Word，连续词袋模型，该模型的特点是，输入已知的上下文，输出对当前单词的预测，其实就是利用中心两侧的词来预测中心的词。 以下两幅图展现了CBOW模型和Skip-gram模型。 CBOW 模型 如果对以下神经网络连接不太清楚的，可以先去看看：https://blog.csdn.net/liaomin416100569/article/details/130572559?spm=1001.2014.3001.5501\none-hot 参考：rnn中关于one-hot和nn.embedding章节\nOne-hot编码又称一位有效编码，是将文字数字化的过程。假如我们有一个语料库：”I drink coffee everyday“。我们对其以” “（空格）进行分词，则我们会得到4个单词，假设这4个单词是我们所有的单词种类（也就是说，我们的字典中只有这四个单词），这样我们对其进行one-hot编码后，可以得到如下编码结果： 表1\n单词 One-hot编码 I [1, 0, 0, 0] drink [0, 1, 0, 0] coffee [0, 0, 1, 0] everyday [0, 0, 0, 1] 这里使用one-hot的原因是列的个数就是单词的格式，最后使用隐藏层的w作为嵌入结果，刚好是(列数，隐藏层神经元个数)\n构建 CBOW 训练数据集 cbow是使用两侧的词语，预测中心的词语，预测窗口大小为 2，输入就是左侧和右侧的两个单词，预测的单词就是中心的单词。 我们语料库仍然为：”I drink coffee everyday“，假设我们的预测窗口大小为 2，通过语料库我们可以构建以下训练集，表2\n输入词 预测词 [drink, coffee] I [I, coffee, everyday] drink [I, drink, everyday] coffee [drink, coffee] everyday 构建 CBOW 神经网络 从上可知，我们的输入层有4个输入单元（one-hot的4列，因为one-hot所以就是原始单词个数），输出层神经元的个数应该跟输入层保持一致，输出层也是4个神经元，加入我们想要每个单词为一个五维的向量表示，那么我们的隐藏层则为五个神经元。由此，我们可以构建一个输入层为4，隐藏层为5，输出层为4的全连接神经网络，如下图所示，训练好的模型的权重矩阵w1可以作为我们的词向量化表。 训练 CBOW 神经网络 这时我们可以根据构建的CBOW数据集对模型进行训练了，假设我们要预测的词是coffee，那么由表2可知，我们输入词为[I, drink, everyday]，我们可以得到如下训练过程。 首先，我们将输入词[I, drink, everyday]转换为对应的one-hot编码向量。假设我们的词汇表中有四个词（I, drink, coffee, everyday），则输入词的one-hot编码分别为：\nI: [1, 0, 0, 0]\rdrink: [0, 1, 0, 0]\reveryday: [0, 0, 0, 1] 接下来，我们将每个one-hot编码向量乘以词嵌入矩阵，以获取词嵌入向量。假设我们已经有了每个词的词嵌入矩阵（这些矩阵在实际应用中是通过训练得到的）,这也是我们经过多次训练之后，最终得到的嵌入矩阵，因为初始化肯定是一个初始值，经过训练反向传播得到一个最佳值，这里假设它们分别为： $$ W = \\begin{bmatrix} 0.1 \u0026 0.2 \u0026 0.3 \u0026 0.4 \u0026 0.5 \\ 0.2 \u0026 0.3 \u0026 0.4 \u0026 0.5 \u0026 0.6 \\ 0.3 \u0026 0.4 \u0026 0.5 \u0026 0.6 \u0026 0.7 \\ 0.4 \u0026 0.5 \u0026 0.6 \u0026 0.7 \u0026 0.8 \\ \\end{bmatrix} $$ 接下来，我们将每个one-hot编码向量乘以词嵌入矩阵，以获取词嵌入向量。例如：\n输入词I的词嵌入向量：$$[1, 0, 0, 0] \\times W = [0.1, 0.2, 0.3, 0.4, 0.5] $$ 输入词drink的词嵌入向量：$$[0, 1, 0, 0] \\times W = [0.2, 0.3, 0.4, 0.5, 0.6] $$ 输入词everyday的词嵌入向量：$$ [0, 0, 0, 1] \\times W = [0.4, 0.5, 0.6, 0.7, 0.8] $$ 接下来，我们将上下文单词的词嵌入向量加起来或求平均以获取一个特征向量。在这个例子中，我们将对它们求平均。\n平均特征向量 = $$\\text{平均特征向量} = \\frac{( \\text{词嵌入向量(I)} + \\text{词嵌入向量(drink)} + \\text{词嵌入向量(everyday)} )}{3}$$ $$= \\frac{( [0.1, 0.2, 0.3, 0.4, 0.5] + [0.2, 0.3, 0.4, 0.5, 0.6] + [0.4, 0.5, 0.6, 0.7, 0.8] )}{3}$$ $$= \\left[ \\frac{(0.1 + 0.2 + 0.4)}{3}, \\frac{(0.2 + 0.3 + 0.5)}{3}, \\frac{(0.3 + 0.4 + 0.6)}{3}, \\frac{(0.4 + 0.5 + 0.7)}{3}, \\frac{(0.5 + 0.6 + 0.8)}{3} \\right]$$ $$= [0.233, 0.333, 0.433, 0.533, 0.633]$$ 现在，我们得到了一个特征向量$$ [0.233, 0.333, 0.433, 0.533, 0.633]$$它表示了上下文单词[I, drink, everyday]的语义信息。\n理解CBOW模型中将上下文单词的词嵌入向量加起来或求平均的原因需要考虑两个方面： 1.上下文信息的整合：CBOW模型的目标是通过上下文单词来预测目标词。因此，对于一个给定的目标词，在预测时需要综合考虑其周围的上下文信息。将上下文单词的词嵌入向量加起来或求平均，可以将这些单词的语义信息整合到一个特征向量中，使得该特征向量更全面地表示了整个句子的语境信息，而不仅仅是单个词的信息。这样可以帮助模型更准确地捕捉句子的语义信息，从而提高模型在目标词预测任务上的性能。 2.语义信息的提取：虽然CBOW模型是用来预测目标词的，但实际上，在训练过程中，模型会学习到每个词的词嵌入向量，这些词嵌入向量包含了每个单词的语义信息。当将上下文单词的词嵌入向量加起来或求平均时，实际上是在利用这些已经学习到的词嵌入向量来提取整个句子的语义信息。由于词嵌入向量是通过大规模语料库训练得到的，其中包含了丰富的语义信息，因此将它们加起来或求平均可以帮助提取句子的语义特征，而不仅仅是单个词的语义特征。\n接下来，我们将特征向量输入到一个全连接层（也称为投影层），并应用softmax函数以获取预测概率。假设全连接层的权重矩阵为： $$W_{proj} = \\begin{bmatrix} 0.1 \u0026 0.2 \u0026 0.3 \u0026 0.4 \u0026 0.5 \\ 0.2 \u0026 0.3 \u0026 0.4 \u0026 0.5 \u0026 0.6 \\ 0.3 \u0026 0.4 \u0026 0.5 \u0026 0.6 \u0026 0.7 \\ 0.4 \u0026 0.5 \u0026 0.6 \u0026 0.7 \u0026 0.8 \\ 0.5 \u0026 0.6 \u0026 0.7 \u0026 0.8 \u0026 0.9 \\ \\end{bmatrix}$$ 我们将特征向量乘以权重矩阵，并应用softmax函数，以获取每个词作为预测目标的概率。 $$z = [0.233, 0.333, 0.433, 0.533, 0.633] \\times W_{proj}$$\n经过训练之后，我们拿 W1( 4*5 权重矩阵) 作为我们的词向量化表，我们可以得到**如下词向量化表（假设）。\n单词索引 向量 I [0.11, 0.22, 0.23, 0.25, 0.31] drink [0.32, 0.22, 0.33, 0.11, 0.32] coffee [0.23, 0.03, 0.62, 0.12, 0.17] everyday [0.05, 0.25, 0.55, 0.17, 0.47 ] 假如我们要词向量化”I drink coffee“这句话，我们便可以直接查询上表，拿到我们的词向量矩阵，即为$$[ [0.11, 0.22, 0.23, 0.25, 0.31],\\ [0.32, 0.22, 0.33, 0.11, 0.32], \\ [0.23, 0.03, 0.62, 0.12, 0.17] ]$$\nSkip-gram 模型 one-hot 参考：rnn中关于one-hot和nn.embedding章节\nOne-hot编码又称一位有效编码，是将文字数字化的过程。假如我们有一个语料库：”I drink coffee everyday“。我们对其以” “（空格）进行分词，则我们会得到4个单词，假设这4个单词是我们所有的单词种类（也就是说，我们的字典中只有这四个单词），这样我们对其进行one-hot编码后，可以得到如下编码结果： 表1\n单词 One-hot编码 I [1, 0, 0, 0] drink [0, 1, 0, 0] coffee [0, 0, 1, 0] everyday [0, 0, 0, 1] 这里使用one-hot的原因是列的个数就是单词的格式，最后使用隐藏层的w作为嵌入结果，刚好是(列数，隐藏层神经元个数)\n构建 Skip-gram训练数据集 cbow是使用两侧的词语，预测中心的词语，预测窗口大小为 2，输入就是左侧和右侧的两个单词，预测的单词就是中心的单词。 skip-gram是使用中心的词语，预测两侧的词语，预测窗口大小为 2，输入就是中心词语，预测的单词就是左侧和右侧的两个单词。 我们语料库仍然为：”I drink coffee everyday“，假设我们的预测窗口大小为 2，通过语料库我们可以构建以下训练集，表2\n预测词 输入词 I drink I coffee drink I drink coffee drink everyday coffee I coffee drink coffee everyday everyday drink everyday coffee 注意输入是一个词，输出是一个词\n训练 Skip-gram神经网络 这时我们可以根据构建的Skip-gram数据集对模型进行训练了，假设我们要预测的词是coffee，那么由表2可知，我们输入词为[I, drink, everyday]中的任何一个，由表2可知，对其进行one-hot编码后的结果为 [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1]], **我们选择其中一个就可以得到一个 1*4 的输入向量，那么我们可以得到如下训练过程。 经过训练之后，我们拿 W1( 4*5 权重矩阵) 作为我们的词向量化表。 训练过程不表,类似于CBOW 。\nWord2Vec实例 数据训练 导入必要的库： #安装 pip install gensim jieba from gensim.models import Word2Vec\rimport logging # 用来设置日志输出\rimport jieba 准备文本数据： context = [\"word2vec是监督学习算法，其会通过句子中词的前后顺序构建有标签数据集，通过数据集 训练神经网络模型 得到这一数据集的 词向量 表（可以理解成我们的新华字典）。\"\r,\"word2vec是用来进行 对词语进行向量化 的模型，也就是对文本类型的数据进行 特征提取\"\r,\"word2vec一般为一个3层（输入层、隐藏层、输出层） 的 全连接神经网络。\"\r,\"本文主要从原理、代码实现 理论结合实战两个角度来剖析word2vec算法\"\r,\"理论部分主要是关于 什么是 word2vec，其两种常见的模型\"\r,\"实战部分主要是通过Gensim库中的word2vec模型，实现文本特征提取\"] 中文分词：\n使用jieba库对文本进行中文分词，并将分词结果保存在context列表中。 for i in range(len(context)):\rsplit_s = context[i]\rcontext[i] = \" \".join(jieba.cut(split_s, HMM=True))\rcontext = [e.split(\" \") for e in context] 配置日志：\n配置日志输出格式和级别。\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) 训练Word2Vec模型：\n使用Word2Vec类来训练模型，传入分词后的文本数据以及一些参数：\nsentences: 分词后的文本数据。 workers: 训练时使用的线程数。 window: 上下文窗口大小，表示一个词周围的上下文词数量。 vector_size: 词向量的维度大小。 epochs: 训练轮数。 min_count: 忽略词频低于此值的词语。 model = Word2Vec(sentences=context, workers=8, window=4, vector_size=10, epochs=30, min_count=3) 查看词汇表和词向量： print(model.wv.key_to_index) # 打印词汇表\rprint(model.wv[\"word2vec\"]) model.wv.key_to_index用于查看词汇表，而model.wv[\"word2vec\"]则用于查看特定词的词向量，这里是查询单词word2vec的词向量。 输出结果\n{'': 0, '的': 1, 'word2vec': 2, '，': 3, '是': 4, '层': 5, '模型': 6, '数据': 7, '主要': 8, '、': 9, '进行': 10, '集': 11, '通过': 12}\r[ 0.07315318 0.05167933 0.06995787 0.00852275 0.0644208 -0.03653978\r-0.00503093 0.06105096 -0.081814 -0.04047652] 可以使用Gensim提供的save()方法将训练好的Word2Vec模型保存到文件。这样可以在之后加载模型并重用它。以下是保存模型的示例代码：\n注意：词汇表里单词都是词频次数超过min_count的词。\n保存和加载 保存模型\nmodel.save(\"word2vec_model.bin\") 这将把训练好的模型保存到名为\"word2vec_model.bin\"的文件中。然后，您可以使用以下代码加载保存的模型：\nfrom gensim.models import Word2Vec\r# 加载模型\rloaded_model = Word2Vec.load(\"word2vec_model.bin\")",
    "description": "前言 Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。\n在本文中，我们将试图把模型简化一点，并逐一介绍里面的核心概念，希望让普通读者也能轻易理解。\nAttention is All You Need：Attention Is All You Need\nTransformer 整体结构 首先介绍 Transformer 的整体结构，下图是 Transformer 用于中英文翻译的整体结构： 可以看到 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：\n第一步：获取输入句子的每一个单词的表示向量 X，X由单词的 Embedding（Embedding就是从原始数据提取出来的Feature） 和单词位置的 Embedding 相加得到。 第二步：将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 x) 传入 Encoder 中，经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 C，如下图。单词向量矩阵用 $X_{n\\times d}$ 表示， n 是句子中单词个数，d 是表示向量的维度 (论文中 d=512)。每一个 Encoder block 输出的矩阵维度与输入完全一致。 第三步：将 Encoder 输出的编码信息矩阵 C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。 上图 Decoder 接收了 Encoder 的编码矩阵 C，然后首先输入一个翻译开始符 “\"，预测第一个单词 “I”；然后输入翻译开始符 “” 和单词 “I”，预测单词 “have”，以此类推。这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节。",
    "tags": [],
    "title": "Transformer模型详解01-Word Embedding",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_01/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "什么是位置编码 在transformer的encoder和decoder的输入层中，使用了Positional Encoding，使得最终的输入满足： $$input = input_embedding + positional_encoding $$\nword embedding：理解参考\n这里，input_embedding是通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model，由于是相加关系，自然而然地，这里的positional_encoding也是一个d_model维度的向量。（在原论文里，d_model = 512） 注意：在Transformer模型中，“token”（标记）是指输入序列中的每个元素，它通常是一个单词、一个子词或一个字符，假设我们有一个句子：“The cat sat on the mat.\"，单词级别的标记： [“The”, “cat”, “sat”, “on”, “the”, “mat”, “.\"]。然后被转换成词嵌入（word embeddings）和位置嵌入（position embeddings），然后这两种嵌入会被相加起来形成输入嵌入（input embeddings）。这个输入嵌入会作为模型的输入，并传递到Transformer的神经网络中进行处理,token本身不会再作为数据传递到模型中。\nInput Embedding为什么解决的是语义问题，没有解决位置问题？？，语义不是有顺序才有吗？？\nInput Embedding (输入嵌入):\ninput_embedding 主要解决的是词汇语义的表示问题。通过将单词映射为连续的低维向量空间，词嵌入技术（如Word2Vec、GloVe等）可以捕获单词之间的语义关系，比如单词的近义词、反义词等。这使得神经网络在处理文本时能够更好地理解单词的含义，从而提高了对语义的建模能力。 但是，词嵌入并没有直接解决词序的问题。即使单词被嵌入到向量空间中，神经网络在处理这些向量时仍然不知道它们在句子中的位置。这就是为什么我们需要进一步引入位置编码的原因。 Positional Encoding (位置编码):\npositional_encoding 解决的是序列数据的位置信息丢失问题。在自然语言处理中，文本是由单词或字符组成的序列，这些单词的排列顺序对句子的含义至关重要。通过引入位置编码，我们可以向神经网络提供关于单词在序列中位置的信息，从而使网络能够区分不同位置的单词并更好地处理序列数据。 位置编码通常是与词嵌入相加的方式来融合位置信息和语义信息。这样，神经网络在处理输入数据时既能考虑单词的语义关系，又能考虑单词在句子中的位置关系，从而更全面地理解文本数据。 因此，input_embedding 和 positional_encoding 两者都是为了帮助神经网络更好地理解文本数据，但它们解决的是不同层面的问题：input_embedding 解决的是语义表示问题，而 positional_encoding 解决的是位置信息丢失问题。这两者结合起来能够提高神经网络对文本数据的建模能力。\n在transformer的self-attention模块中，序列的输入输出如下（不了解self-attention没关系，这里只要关注它的输入输出就行）： 在self-attention模型中，输入是一整排的tokens，对于人来说，我们很容易知道tokens的位置信息，比如：\n（1）绝对位置信息。a1是第一个token，a2是第二个token……\n（2）相对位置信息。a2在a1的后面一位，a4在a2的后面两位……\n（3）不同位置间的距离。a1和a3差两个位置，a1和a4差三个位置….\n但是这些对于self-attention来说，是无法分辩的信息，因为self-attention的运算是无向的。因为，我们要想办法，把tokens的位置信息，喂给模型。\n连续有界 有界又连续的概念是数学中对函数或者集合的性质进行描述的。一个函数或者集合被称为有界的意思是它在某个范围内有限，即它的值不能无限增长或减小；而连续则表示函数或者集合中的元素在某个区间内没有断裂或跳跃。\n举个例子，考虑函数 (f(x) = \\sin(x))。这个函数是有界的，因为正弦函数的值范围在 ([-1, 1]) 之间，不会超出这个范围。而且，正弦函数在定义域内是连续的，没有断点或跳跃。因此，正弦函数 (f(x) = \\sin(x)) 是一个有界又连续的函数。\n另一个例子是闭区间 ([0, 1]) 上的实数集合。这个集合是有界的，因为它的元素都在区间 ([0, 1]) 内；同时，这个集合是连续的，因为在闭区间内没有任何间隔或断裂。\n总的来说，有界又连续的概念在数学中非常常见，许多函数、集合以及数学对象都可以被描述为有界又连续的。\n为什么要有界 在Transformer等模型中，位置编码用于为序列中的不同位置提供唯一的标识，以便模型能够区分不同位置的词语。通常情况下，位置编码是与词嵌入向量相加的，因此需要确保位置编码与词嵌入向量的范围相匹配，以避免结果的数值过大或过小。\n此外，由于模型的输入通常是通过词嵌入向量表示的，而词嵌入向量通常是有限范围的，因此位置编码的范围也被限制在一个合理的范围内，以保持整个输入的稳定性和可训练性。\n因此，尽管位置编码并不一定必须是有界的，但在实践中，为了保持模型的稳定性和可训练性，通常会设计位置编码为有界的。\n为什么要连续 位置编码必须是连续的，因为它们用于表示序列中的位置信息，而序列中的位置是连续的。在自然语言处理任务中，如语言模型或机器翻译，序列中的每个词或标记都对应着一个连续的位置。\n如果位置编码不是连续的，那么模型将无法正确地理解序列中各个位置之间的关系。例如，如果某个位置的位置编码与其相邻位置的位置编码之间存在不连续性，模型可能会误解序列中的顺序关系，从而影响其性能。\n另外，连续的位置编码有助于模型更好地捕捉序列中的局部和全局关系，因为它们可以在连续的空间中表示位置信息，使模型能够更准确地理解序列中不同位置之间的距离和关联。\n综上所述，位置编码必须是连续的，以确保模型能够有效地理解序列中的位置信息，并正确地捕捉序列中的关系和结构。\n位置编码的演变 用整型值标记位置 一种自然而然的想法是，给第一个token标记0，给第二个token标记1…，以此类推。 这种方法产生了以下几个主要问题：\n模型可能遇见比训练时所用的序列更长的序列。不利于模型的泛化。 模型的位置表示是无界的。随着序列长度的增加，位置值会越来越大。 用[0,1]范围标记位置 为了解决整型值带来的问题，可以考虑将位置值的范围限制在[0, 1]之内，其中，0表示第一个token，1表示最后一个token。比如有3个token，那么位置信息就表示成[0, 0.5, 1]；若有四个token，位置信息就表示成[0, 0.33, 0.69, 1]。 但这样产生的问题是，当序列长度不同时，token间的相对距离是不一样的。例如在序列长度为3时，token间的相对距离为0.5；在序列长度为4时，token间的相对距离就变为0.33。 因此，我们需要这样一种位置表示方式，满足于：\n它能用来表示一个token在序列中的绝对位置 在序列长度不同的情况下，不同序列中token的相对位置/距离也要保持一致 可以用来表示模型在训练过程中从来没有看到过的句子长度。 用二进制向量标记位置 考虑到位置信息作用在input embedding上，因此比起用单一的值，更好的方案是用一个和input embedding维度一样的向量来表示位置。这时我们就很容易想到二进制编码。如下图，假设d_model = 3，那么我们的位置向量可以表示成： 这下所有的值都是有界的（位于0，1之间），且transformer中的d_model本来就足够大，基本可以把我们要的每一个位置都编码出来了。 但是这种编码方式也存在问题：这样编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。假设d_model = 2，我们有4个位置需要编码，这四个位置向量可以表示成[0,0],[0,1],[1,0],[1,1]。我们把它的位置向量空间做出来： 用周期函数（sin）来表示位置 sin函数 先回顾下sin函数的几个概念，因为下面要用到sin函数：\n周期 周期是从一个最高点到下一个最高点（或任何一点到下一个相对点）： 振幅，相移，垂直位移 振幅是从中（平）线到最高点的高度（或到最低点），也是从最高点到最低点的距离除以2。 相移是函数比通常的位置水平向右移了多远。 垂直位移是函数比通常的位置垂直向上移了多远。 我们可以全部放进一个方程里：\ny = A sin(Bx + C) + D\n振幅是：A 周期是：2π/B 相移是：−C/B 垂直移位是：D 例子：sin(x) 这是正弦的基本公式。A = 1, B = 1, C = 0 and D = 0\n所以振幅是1，周期是2π，没有相移或垂直移位： 振幅 1，周期 2pi，没有相移或垂直移位 频率 频率是在一个时间单位里发生多少次（每 “1”）。 例子：这个正弦函数在0到1之间重复了4次： 所以频率是 4 周期是 $\\frac{1}{4}$ 其实周期和频率是相连的,周期越大，频率越小： 频率 = $\\frac{1}{周期}$ 周期 = $\\frac{1}{频率}$\n波长 波长λ=vT，其中v是波速，T是周长。波长是一个周期内波前进的距离，而这段周期内波都是匀速直线前进的，所以直接使用匀速直线运动的位移公式即可。\nsin表示位置 回想一下，现在我们需要一个有界又连续的函数，最简单的，正弦函数sin就可以满足这一点。我们可以考虑把位置向量当中的每一个元素都用一个sin函数来表示，则第t个token的位置向量($d_{model}$表示嵌入向量维度)可以表示为： $$ PE_t = [sin(\\frac{1}{2^0}t),sin(\\frac{1}{2^1}t)…,sin(\\frac{1}{2^{i-1}}t), …,sin(\\frac{1}{2^{d_{model}-1}}t)]\\ $$\nPE:位置编码：Positional Encoding，t表示第t个token，i表示位置编码是第几个列。 列sin函数，越往右波长（入*(2π/B)）越长，频率越低。\n说个题外话，说说音量调节，后面会有用： 假设你在调节音量。如果你向右旋转音量旋钮，音量（精度）可能会从低到高逐渐增加。一开始，当音量较低时，每次向右旋转可能只会增加一点音量，这时候你可能希望更细微地调整音量。但是，当音量已经相对较高时，每次向右旋转可能会增加更多的音量，这时候你可能不希望调整得太大，因此需要更小的步进来精确地调整音量。 因此，可以概括，向右旋转旋钮会增加调整参数的精度，也就是每次移动的步幅会变小，以便更精细地调整参数的值。\n言归正传： 结合下图，来理解一下这样设计的含义。图中每一行表示一个$PE_t$，每一列表示$PE_t$中的第i个元素。旋钮用于调整精度，越往右边的旋钮，需要调整的精度越大，因此指针移动的步伐越小。每一排的旋钮都在上一排的基础上进行调整（函数中t的作用）。通过频率sin(12i−1t)sin(\\frac{1}{2^{i-1}}t)sin(2i−11​t)来控制sin函数的波长，频率不断减小，则波长不断变大，此时sin函数对t的变动越不敏感，以此来达到越向右的旋钮，指针移动步伐越小的目的。 由于sin是周期函数，因此从纵向来看，如果当函数的频率增大并导致波长缩短时，意味着波形在相同时间内完成了更多的周期，则不同t下的位置向量可能出现重合的情况。比如在下图中(d_model = 3），图中的点表示每个token的位置向量，颜色越深，token的位置越往后，在频率偏大的情况下，位置相连点连成了一个闭环，靠前位置（黄色）和靠后位置（棕黑色）竟然靠得非常近： 为了避免这种情况，我们尽量将函数的波长拉长。一种简单的解决办法是同一把所有的频率都设成一个非常小的值。因此在transformer的论文中，采用了$\\frac{1}{10000^{i/(d_{model}-1)}}$这个频率（这里i其实不是表示第i个位置，但是大致意思差不多，下面会细说） 总结一下，到这里我们把位置向量表示为： $$ PE_t = [sin(w_0t),sin(w_1t)…,sin(w_{i-1}t), …,sin(w_{d_{model}-1}t)]\\ $$ 其中，$w_{i} = \\frac{1}{10000^{i/(d_{model}-1)}}$\n用sin和cos交替来表示位置 先来回顾下线性变化旋转的相关概念，后续用到。\n线形变换——旋转 在二维坐标系中，一个位置向量的旋转公式可以由三角函数的几何意义推出。 如上图假设：\n已知：假设向量$R_{A}$=(x0，y0) 角度为：A，向右旋转了角度B，新向量$R_{A+B}$角度为：A+B，模：$|\\mathbf{R}|=|\\mathbf{R_{A}}|= |\\mathbf{R_{A+B}}| = \\sqrt{x_0^2 + y_0^2}$。 未知：旋转后向量为:(x1,y1) 上面的命题就是向量$R_{A}$旋转了角度B，求新向量$R_{A+B}$（模大小相同）,$R_{A}$和$R_{A+B}$之间有绝对关系也有相对关系，相对一个角度B，我们需要通过公式来获得一个$R_{A+B}$和$R_{A}$的关系 $R_{A+B}=T_B*R_A$,$T_B$表示一个线性变换矩阵,我们可以通过公式推算出来。 在左图中，我们有关系： $x0 = |R| * cosA =\u003e cosA = x0 / |R|$ $y0 = |R| * sinA =\u003e sinA = y0 / |R|$ 在右图中，我们有关系： $x1 = |R| * cos（A+B）$ $y1 = |R| * sin（A+B）$ 其中（x1， y1）就是（x0， y0）旋转角B后得到的点。我们展开cos（A+B）和sin（A+B），得到： $x1 = |R| * （cosAcosB - sinAsinB）$ $y1 = |R| * （sinAcosB + cosAsinB）$ 现在把 $cosA = x0 / |R|$ 和 $sinA = y0 / |R|$ 代入上面的式子，得到： $x1 = |R| * （x0 * cosB / |R| - y0 * sinB / |R|） =\u003e x1 = x0 * cosB - y0 * sinB$ $y1 = |R| * （y0 * cosB / |R| + x0 * sinB / |R|） =\u003e y1 = x0 * sinB + y0 * cosB$ 这样我们就得到了二维坐标下向量围绕圆点的逆时针旋转公式。顺时针旋转就把角度变为负： $x1 = x0 * cos（-B） - y0 * sin（-B） =\u003e x1 = x0 * cosB + y0 * sinB$ $y1 = x0 * sin（-B） + y0 * cos（-B）=\u003e y1 = -x0 * sinB + y0 * cosB$ 现在我要把这个旋转公式写成矩阵的形式，有一个概念我简单提一下，平面或空间里的每个线性变换（这里就是旋转变换）都对应一个矩阵，叫做变换矩阵。对一个点实施线性变换就是通过乘上该线性变换的矩阵完成的。好了，打住，不然就跑题了。\n现在来将下面公式转换成矩阵 逆时针： $x1 = x0 * cosB - y0 * sinB$ $y1 = x0 * sinB + y0 * cosB$ 所以二维旋转变换矩阵就是： $$ [x, y] * \\left[\\begin{matrix}cosB \u0026 sinB \\ -sinB \u0026 cosB\\end{matrix}\\right] = [xcosB-ysinB ,xsinB+ycosB] $$ 变换矩阵为$\\left[\\begin{matrix}cosB \u0026 sinB \\ -sinB \u0026 cosB\\end{matrix}\\right]$ 顺时针： $x1 = x0 * cosB + y0 * sinB$ $y1 = -x0 * sinB + y0 * cosB$ 同理变换矩阵为：$\\left[\\begin{matrix}cosB \u0026 -sinB \\ sinB \u0026 cosB\\end{matrix}\\right]$\nsin和cos交替表示位置 目前为止，我们的位置向量实现了如下功能：\n每个token的向量唯一（每个sin函数的频率足够小） 位置向量的值是有界的，且位于连续空间中。模型在处理位置向量时更容易泛化，即更好处理长度和训练数据分布不一致的序列（sin函数本身的性质） 那现在我们对位置向量再提出一个要求，不同的位置向量是可以通过线性转换得到的。这样，我们不仅能表示一个token的绝对位置，还可以表示一个token的相对位置(也就是两个token之间得线性关系)，即我们想要： $$ PE_{t+\\bigtriangleup t} = T_{\\bigtriangleup t} * PE_{t} $$ 这里，T表示一个线性变换矩阵。观察下面这个目标式子，联想到在向量空间中一种常用的线形变换——旋转。在这里，我们将t想象为一个角度，那么 $\\bigtriangleup t$就是其旋转的角度，则上面的式子可以进一步写成： $$\\begin{pmatrix} \\sin(t + \\bigtriangleup t)\\ \\cos((t + \\bigtriangleup t) \\end{pmatrix}=\\begin{pmatrix} \\cos\\bigtriangleup t\u0026\\sin\\bigtriangleup t \\ -\\sin\\bigtriangleup t\u0026\\cos\\bigtriangleup t \\end{pmatrix}\\begin{pmatrix} \\sin t\\ \\cos t \\end{pmatrix} $$ 有了这个构想，我们就可以把原来元素全都是sin函数的 $PE_{t}$ 做一个替换，我们让位置两两一组，分别用sin和cos的函数对来表示它们，则现在我们有： $$ PE_t = [sin(w_0t),cos(w_0t), sin(w_1t),cos(w_1t),…,sin(w_{\\frac{d_{model}}{2}-1}t), cos(w_{\\frac{d_{model}}{2}-1}t)]\\ $$ 在这样的表示下，我们可以很容易用一个线性变换，把 $PE_{t}$ 转变为 $PE_{t+\\bigtriangleup t}$ $$PE_{t+\\bigtriangleup t} = T_{\\bigtriangleup t} * PE_{t} =\\begin{pmatrix} \\begin{bmatrix} cos(w_0\\bigtriangleup t)\u0026 sin(w_0\\bigtriangleup t)\\ -sin(w_0\\bigtriangleup t)\u0026 cos(w_0\\bigtriangleup t) \\end{bmatrix}\u0026…\u00260 \\ …\u0026 …\u0026 …\\ 0\u0026 …\u0026 \\begin{bmatrix} cos(w_{\\frac{d_{model}}{2}-1 }\\bigtriangleup t)\u0026 sin(w_{\\frac{d_{model}}{2}-1}\\bigtriangleup t)\\ -sin(w_{\\frac{d_{model}}{2}-1}\\bigtriangleup t)\u0026 cos(w_{\\frac{d_{model}}{2}-1}\\bigtriangleup t) \\end{bmatrix} \\end{pmatrix}\\begin{pmatrix} sin(w_0t)\\ cos(w_0t)\\ …\\ sin(w_{\\frac{d_{model}}{2}-1}t)\\ cos(w_{\\frac{d_{model}}{2}-1}t) \\end{pmatrix} = \\begin{pmatrix} sin(w_0(t+\\bigtriangleup t))\\ cos(w_0(t+\\bigtriangleup t))\\ …\\ sin(w_{\\frac{d_{model}}{2}-1}(t+\\bigtriangleup t))\\ cos(w_{\\frac{d_{model}}{2}-1}(t+\\bigtriangleup t)) \\end{pmatrix}$$ 变换矩阵，也是两个一组和$PE_{t}$进行点乘，变换数组一行就有多组，最后也是个由转换角度+参数(常量)的线性变换。\nTransformer中位置编码方法 Transformer 位置编码定义 有了上面的演变过程后，现在我们就可以正式来看transformer中的位置编码方法了。\n定义：\nt是这个token在序列中的实际位置（例如第一个token为1，第二个token为2…）\n-$PE_t\\in\\mathbb{R}^d$是这个token的位置向量， $PE_{t}^{(i)}$表示这个位置向量里的第i个元素 $d_{model}$是这个token的维度（在论文中，是512) 则 $PE_{t}^{(i)}$ 可以表示为： $$PE_{t}^{(i)} = \\left{\\begin{matrix} \\sin(w_kt),\u0026if\\ i=2k ( 偶数行 ) \\ \\cos(w_kt),\u0026if\\ i = 2k+1(奇数行) \\end{matrix}\\right.$$ 这里： $w_k = \\frac{1}{10000^{2k/d_{model}}}$ $i = 0,1,2,3,…,\\frac{d_{model}}{2} -1$\n注意：当使用 $w_k = \\frac{1}{10000^{2k/d_{\\text{model}}}}$作为位置编码的调节因子时，当 k 增大时，分母中的指数项会变得非常大，可能导致数值溢出或者数值精度问题。为了避免这种情况，可以使用其对数形式$-\\frac{\\log(10000.0)}{d_{\\text{model}}}$这样做有以下几个优点：\n数值稳定性： 对数形式避免了指数项过大导致的数值溢出或者数值精度问题。 计算效率： 对数形式的计算更加高效，避免了重复计算指数项。 一致性： 使用对数形式可以保持代码中的一致性，因为在其他部分可能也会涉及到对数形式的处理。 把512维的向量两两一组，每组都是一个sin和一个cos，这两个函数共享同一个频率$w_i$ ，一共有256组，由于我们从0开始编号，所以最后一组编号是255。sin/cos函数的波长（由 $w_i$ 决定）则从 $2\\pi$增长到 $2\\pi*10000$,下面是代码实现\nclass PositionalEncoding(nn.Module): def __init__(self, d_model, dropout, max_len=5000): \"\"\" 位置编码器类的初始化函数 共有三个参数，分别是 d_model：词嵌入维度 dropout: dropout触发比率 max_len：每个句子的最大长度 \"\"\" super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings # 注意下面代码的计算方式与公式中给出的是不同的，但是是等价的，你可以尝试简单推导证明一下。 # 这样计算是为了避免中间的数值计算结果超出float的范围， pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer('pe', pe) def forward(self, x): x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) return self.dropout(x) 确认是否维度越往后，是否波长越长\nplt.figure(figsize=(15, 5))\rpe = PositionalEncoding(20, 0)\ry = pe.forward(torch.zeros(1, 100, 20))\rplt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\rplt.legend([\"dim %d\"%p for p in [4,5,6,7]]) Transformer位置编码可视化 下图是一串序列长度为100，位置编码维度为512的位置编码可视化结果： 途中y轴表示单词的位置，从0开始到100，横坐标表示每个单词的512维度，颜色表示值，sin，cos函数的值在【-1，1】之间\n可以发现，由于sin/cos函数的性质，位置向量的每一个值都位于[-1, 1]之间。同时，纵向来看，图的右半边几乎都是黄色的，这是因为越往后的位置，频率越小，波长越长，所以不同的t对最终的结果影响不大。而越往左边走，颜色交替的频率越频繁。 代码：\nimport matplotlib.pyplot as plt\rimport numpy as np\r# 设置序列长度和模型维度\rsequence_length = 100 # 序列长度\rd_model = 512 # 模型维度\r# 初始化位置编码矩阵\rpositional_encoding = np.zeros((sequence_length, d_model))\r# 计算位置编码\rfor pos in range(sequence_length):\rfor i in range(d_model):\rif i % 2 == 0:\r# 偶数索引使用正弦函数\rpositional_encoding[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\relse:\r# 奇数索引使用余弦函数\rpositional_encoding[pos, i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\r# 绘制位置编码的图像\rplt.figure(figsize=(10, 8))\rplt.imshow(positional_encoding, cmap='hot', interpolation='nearest')\rplt.title('Positional Encoding')\rplt.xlabel('Depth')\rplt.ylabel('Position')\rplt.colorbar()\rplt.show()",
    "description": "什么是位置编码 在transformer的encoder和decoder的输入层中，使用了Positional Encoding，使得最终的输入满足： $$input = input_embedding + positional_encoding $$\nword embedding：理解参考\n这里，input_embedding是通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model，由于是相加关系，自然而然地，这里的positional_encoding也是一个d_model维度的向量。（在原论文里，d_model = 512） 注意：在Transformer模型中，“token”（标记）是指输入序列中的每个元素，它通常是一个单词、一个子词或一个字符，假设我们有一个句子：“The cat sat on the mat.\"，单词级别的标记： [“The”, “cat”, “sat”, “on”, “the”, “mat”, “.\"]。然后被转换成词嵌入（word embeddings）和位置嵌入（position embeddings），然后这两种嵌入会被相加起来形成输入嵌入（input embeddings）。这个输入嵌入会作为模型的输入，并传递到Transformer的神经网络中进行处理,token本身不会再作为数据传递到模型中。\nInput Embedding为什么解决的是语义问题，没有解决位置问题？？，语义不是有顺序才有吗？？\nInput Embedding (输入嵌入):\ninput_embedding 主要解决的是词汇语义的表示问题。通过将单词映射为连续的低维向量空间，词嵌入技术（如Word2Vec、GloVe等）可以捕获单词之间的语义关系，比如单词的近义词、反义词等。这使得神经网络在处理文本时能够更好地理解单词的含义，从而提高了对语义的建模能力。 但是，词嵌入并没有直接解决词序的问题。即使单词被嵌入到向量空间中，神经网络在处理这些向量时仍然不知道它们在句子中的位置。这就是为什么我们需要进一步引入位置编码的原因。 Positional Encoding (位置编码):\npositional_encoding 解决的是序列数据的位置信息丢失问题。在自然语言处理中，文本是由单词或字符组成的序列，这些单词的排列顺序对句子的含义至关重要。通过引入位置编码，我们可以向神经网络提供关于单词在序列中位置的信息，从而使网络能够区分不同位置的单词并更好地处理序列数据。 位置编码通常是与词嵌入相加的方式来融合位置信息和语义信息。这样，神经网络在处理输入数据时既能考虑单词的语义关系，又能考虑单词在句子中的位置关系，从而更全面地理解文本数据。 因此，input_embedding 和 positional_encoding 两者都是为了帮助神经网络更好地理解文本数据，但它们解决的是不同层面的问题：input_embedding 解决的是语义表示问题，而 positional_encoding 解决的是位置信息丢失问题。这两者结合起来能够提高神经网络对文本数据的建模能力。",
    "tags": [],
    "title": "Transformer模型详解02-Positional Encoding（位置编码）",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_02/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "简介 下图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 Multi-Head Attention，是由多个 Self-Attention组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add \u0026 Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。\n因为 Self-Attention是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。 基础知识 向量的内积 向量的内积是什么，如何计算，最重要的，其几何意义是什么？\n内积的计算方法是将两个向量对应分量相乘，然后将结果相加。 内积的几何意义是非常重要的。在二维空间中，两个向量的内积等于两个向量的模（长度）之积乘以它们之间的夹角的余弦值。具体来说，如果 θ 是两个向量之间的夹角，则它们的内积为： $$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| |\\mathbf{b}| \\cos(\\theta)$$ 这个公式表明，内积可以用来衡量两个向量的相似程度。当两个向量的夹角为 0时(cos0=1)，它们的内积取得最大值，表示它们的方向相同；当夹角为 90时(cos90=0)，内积为 0，表示它们的方向垂直；当夹角为180(cos180=-1) 时，内积取得最小值，表示它们的方向相反。\n矩阵与转置相乘 一个矩阵 与其自身的转置相乘，得到的结果有什么意义？ 矩阵的对称性指的是矩阵在某种变换下保持不变的性质。对称矩阵是一种特殊的矩阵，它满足以下性质：矩阵的转置等于它自身。 具体来说，对称矩阵 A 满足以下条件： $$\\mathbf{A} = \\mathbf{A}^\\intercal$$ 这意味着矩阵的主对角线上的元素保持不变，而其他元素关于主对角线对称。\n例如，如果一个矩阵 A 的元素为： $$\\mathbf{A} = \\begin{pmatrix} a \u0026 b \u0026 c \\ b \u0026 d \u0026 e \\ c \u0026 e \u0026 f \\end{pmatrix}$$ 矩阵中的元素对称于主对角线。 对称矩阵在数学和工程领域中非常重要，因为它们具有许多有用的性质，比如特征值都是实数、可以通过正交变换对角化等。在应用中，对称矩阵广泛用于描述对称系统、表示物理现象等。\n当一个矩阵与其自身的转置相乘时，得到的结果矩阵具有重要的性质，其中最显著的是结果矩阵是一个对称矩阵。这个性质在许多领域中都有重要的应用，比如在统计学中用于协方差矩阵的计算，以及在机器学习中用于特征提取和数据降维。\n让我们用一个具体的矩阵示例来演示这个性质。考虑一个 $3 \\times 2$的矩阵 的矩阵$\\mathbf{A}$： $$\\mathbf{A} = \\begin{pmatrix} 1 \u0026 2 \\ 3 \u0026 4 \\ 5 \u0026 6 \\end{pmatrix}$$ 首先，我们计算 A 的转置 $\\mathbf{A}^\\intercal$ $$\\mathbf{A}^\\intercal = \\begin{pmatrix} 1 \u0026 3 \u0026 5 \\ 2 \u0026 4 \u0026 6 \\end{pmatrix}$$ 然后，我们将 𝐴 相乘$\\mathbf{A}^\\intercal$，得到结果矩阵 $\\mathbf{A} \\mathbf{A}^\\intercal$ $$\\mathbf{A} \\mathbf{A}^\\intercal = \\begin{pmatrix} 1 \u0026 2 \\ 3 \u0026 4 \\ 5 \u0026 6 \\end{pmatrix} \\begin{pmatrix} 1 \u0026 3 \u0026 5 \\ 2 \u0026 4 \u0026 6 \\end{pmatrix} = \\begin{pmatrix} 5 \u0026 11 \u0026 17 \\ 11 \u0026 25 \u0026 39 \\ 17 \u0026 39 \u0026 61 \\end{pmatrix}$$\n可以观察到，结果矩阵 $\\mathbf{A} \\mathbf{A}^\\intercal$是一个对称矩阵。\nQ,K,V 在Transformer模型中，Q（Query）、K（Key）和V（Value）在注意力机制中起着关键作用。让我们通过一个简单的例子来理解它们的物理意义：\n假设我们要翻译一段文本，比如将英文句子 “The cat sat on the mat” 翻译成法文。在这个例子中，Q、K 和 V 可以被解释为：\nQuery（查询）：在翻译时，Query表示当前正在翻译的单词或者短语。例如，当我们尝试翻译 “sat” 这个词时，“sat” 就是当前的 Query。\nKey（键）：Key表示源语言（英文）中其他位置的信息，用于与当前 Query 进行比较。在翻译任务中，Key可以是源语言句子中的其他单词或者短语。比如，在翻译 “sat” 时，Key 可能是源语言句子中的 “The”、“cat”、“on” 等单词。\nValue（值）：Value包含了与 Key 相关的实际数值信息。在翻译任务中，Value 可以是源语言句子中与 Key 对应的词语的嵌入向量或者表示。比如，与 Key “The” 相关的 Value 可能是 “Le”，与 Key “cat” 相关的 Value 可能是 “chat”，等等。\n在注意力机制中，系统会计算当前 Query（如 “sat”）与所有 Key（如 “The”、“cat”、“on”）之间的相关性得分，然后使用这些得分对 Value（如 “Le”、“chat”）进行加权求和，以产生最终的翻译输出。这样，模型可以根据输入的 Query（即要翻译的单词或短语）选择性地关注源语言句子中与之相关的信息，并生成相应的翻译结果。\n通过上面的例子稍微理解Q,K,V概念后，对后续理解公式有帮助。\n什么是Attention 所谓Attention，顾名思义：注意力，意思是处理一个问题的时候把\"注意力\"放到重要的地方上。Attention思想其实是从人类的习惯中提取出来的。人们在第一次看一张照片的时候，第一眼一定落到这张照片的某个位置上，可能是个显著的建筑物，或者是一个有特点的人等等，总之，人们通常并没有看清图片的全部内容，而是将注意力集中在了图片的焦点上。\n2017年的某一天,Google 机器翻译团队发表了《Attention is All You Need》这篇论文，犹如一道惊雷，Attention横空出世了！（有一说一，这标题也太他喵嚣张了，不过人家有这个资本(oﾟ▽ﾟ)o ）\nAttention 机制最早是在计算机视觉里应用的，随后在NLP领域也开始应用了，真正发扬光大是在NLP领域，由于2018年GPT模型的效果显著，Transformer和Attention这些核心才开始被大家重点关注。\n下面举个例子上说明一下注意力和自注意力，可能不够严谨，但足以说明注意力和自注意力是什么了。 首先我们不去考虑得到注意力分数的细节，而是把这个操作认为是一个封装好的函数。比如定义为attention_score(a,b)，表示词a和b的注意力分数。现在有两个句子A=“you are beautiful”和B=“你很漂亮”，我们想让B句子中的词“你”更加关注A句子中的词“you”，该怎么做呢？答案是对于每一个A句子中的词，计算一下它与“you”的注意力分数。也就是把\nattention_score(“you”,“你”)\rattention_score(“are”,“你”)\rattention_score(“beautiful”,“你”) 都计算一遍，在实现attention_score这个函数的时候，底层的运算会让相似度比较大的两个词分数更高，因此attention_score(“you”,“你”)的分数最高，也相当于告诉了计算机，在对B句子中“你”进行某些操作的时候，你应该更加关注A句子中的“you”，而不是“are”或者“beautiful”。 以上这种方式就是注意力机制，两个不同的句子去进行注意力的计算。而当句子只有一个的时候，只能去计算自己与自己的注意力，这种方式就是自注意力机制。比如只看A句子，去计算\nattention_score(“you”,“you”)\rattention_score(“are”,“you”)\rattention_score(“beautiful”,“you”) 这种方式可以把注意力放在句子内部各个单词之间的联系，非常适合寻找一个句子内部的语义关系。\n再举个例子比如这句话“这只蝴蝶真漂亮，停在花朵上，我很喜欢它”，我们怎么知道这个“它”指的是“蝴蝶”还是“花朵”呢？答案是用自注意力机制计算出这个“它”和其他所有输入词的“分数”，这个“分数”一定程度上决定了其他单词与这个联系。可以理解成越相似的，分就越高（通过权重来控制）。通过计算，发现对于“它”这个字，“蝴蝶”比“花朵”打的分高。所以对于“它”来说，“蝴蝶”更重要，我们可以认为这个“它”指的就是蝴蝶。\nSelf Attention 原理 通俗易懂理解 在人类的理解中，对待问题是有明显的侧重。具体举个例子来说：“我喜欢踢足球，更喜欢打篮球。”，对于人类来说，显然知道这个人更喜欢打篮球。但对于深度学习来说，在不知道”更“这个字的含义前，是没办法知道这个结果的。所以在训练模型的时候，我们会加大“更”字的权重，让它在句子中的重要性获得更大的占比。比如： $$C(seq) = F(0.1d(我)，0.1d(喜)，…，0.8d(更)，0.2d(喜)，…) $$ 在知道了attention在机器学习中的含义之后（下文都称之为注意力机制）。人为设计的注意力机制，是非常主观的，而且没有一个准则来评定，这个权重设置为多少才好。所以，如何让模型自己对变量的权重进行自赋值成了一个问题，这个权重自赋值的过程也就是self-attention。\n定义：假设有四个输入变量$a^1$，$a^2$，$a^3$，$a^4$，希望它们经过一个self-attention layer之后变为$b^1$，$b^2$，$b^3$，$b^4$ 拿$a^1$和$b^1$做例子,$b^1$这个结果是综合了$a^1$，$a^2$，$a^3$，$a^4$而得出来的一个结果。既然得到一个b 是要综合所有的a才行，那么最直接的做法就是$a^1$与$a^2$，$a^3$，$a^4$ 都做一次运算，得到的结果就代表了这个变量的注意力系数。直接做乘法太暴力了，所以选择一个更柔和的方法：引入三个变量$W^q$,$W^k$,$W^v$这三个变量与$a^1$相乘得到$q^1$,$k^1$,$v^1$ 同样的方法对$a^2$，$a^3$，$a^4$都做一次,至于这里的q , k , v具体代表什么，下面就慢慢展开讲解。 然后拿自己的q与别人的k相乘就可以得到一个系数$\\alpha$。这里$q^1$在和其他的k做内积时，可近似的看成是在做相似度计算(前面基础向量的内积)。比如： $$ \\alpha_{1,1} =q^1\\cdot k^1\\ \\alpha_{1,2} =q^1\\cdot k^2\\ \\alpha_{1,3} =q^1\\cdot k^3\\ \\alpha_{1,4} =q^1\\cdot k^4\\ $$\n在实际的神经网络计算过程中，还得除于一个缩放系数$\\sqrt{d}$这个d是指q和k的维度,因为q和k会做内积，所以维度是一样的。之所以要除$\\sqrt{d}$​，是因为做完内积之后，，$\\alpha$会随着它们的维度增大而增大，除$\\sqrt{d}$相当于标准化。 得到了四个$\\alpha$之后，我们分别对其进行softmax，得到四个 $\\hat{\\alpha}_1$，增加模型的非线性。 四个$\\alpha$分别是 $\\hat{\\alpha}_1$,$\\hat{\\alpha}2$,$\\hat{\\alpha}3$,$\\hat{\\alpha}4$,别忘了还有我们一开始计算出来的 ${v}^1$,${v}^2$,${v}^3$,${v}^4$，直接把各个$\\hat{\\alpha}1$直接与各个a 相乘不就得出了最后的结果了吗？虽然这么说也没错，但为了增加网络深度，将a变成v也可以减少原始的a对最终注意力计算的影响。 那么距离最后计算出$b^1$只剩最后一步，我们将所有的$\\hat{\\alpha}1$ 与所有的v分别相乘，然后求和，就得出$b^1$啦！具体计算如下： $$b^1=\\hat{\\alpha}{1,1}*v^1+\\hat{\\alpha}{1,2}*v^2+\\hat{\\alpha}{1,3}*v^3+\\hat{\\alpha}{1,4}*v^4 $$ 公式简化为： $$b^1=\\sum_i\\hat{\\alpha}{1,i}*v^i $$ 同样的计算过程，我们对剩下的a都进行一次，就可以得到$b^2$,$b^3$,$b^4$,每个b都是综合了每个a之间的相关性计算出来的，这个相关性就是我们所说的注意力机制,。那么我们将这样的计算层称为self-attention layer。 我们把一个句子中的每个字代入上图的 $x^1$，$x^2$ ,$x^3$， $x^4$\n矩阵计算 通过注意力计算出来的结果是每个位置单词的上下文表示，每个位置的上下文表示是指在自注意力机制中，通过将每个位置的词嵌入向量与注意力权重进行加权求和，得到的每个位置的语义表示。这个语义表示包含了输入序列中每个位置的语义信息，经过加权后更加全局和丰富。\n举个例子来说明：\n假设我们有一个输入序列：“The cat sat on the mat.\"，并且使用 Transformer 模型进行编码，其中每个单词对应一个位置。在自注意力机制中，模型会计算每个位置对其他位置的注意力权重，然后将这些权重与对应位置的词嵌入向量进行加权求和，得到每个位置的上下文表示。\n考虑位置 3，对应单词 “sat”。在计算注意力权重时，模型会考虑 “sat” 与其他单词之间的关联程度。假设在这个例子中，“sat” 与 “cat”、“mat” 之间有较高的注意力权重，而与 “on” 的关联较低。因此，经过加权求和后，位置 3 的上下文表示将会强调 “sat” 与 “cat”、“mat” 之间的语义关系。\n通过这种方式，每个位置的上下文表示会受到整个输入序列中所有位置的影响，从而更好地捕捉输入序列的语义结构和信息。\n上图是 Self-Attention 的结构，在计算的时候需要用到矩阵Q(查询),K(键值),V(值)。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而Q,K,V正是通过 Self-Attention 的输入进行线性变换得到的。\nQ，K，V计算 输入矩阵X=position encoding+word embedding,其中维度d_model，行为句子中单词的个数。 需要知道x的格式参考：Word2Vec实例\nSelf-Attention 的输入用矩阵X进行表示，则可以使用线性变阵矩阵WQ,WK,WV计算得到Q,K,V。计算 如下图所示，注意 X, Q, K, V 的每一行都表示一个单词，WQ，WK，WV是一个d_model(输入矩阵的列)行的线性变阵参数，X的每一行都会都会有自己的QKV，比如$X_1$对应$Q_1,K_1,V_1$,即$X_n$对应$Q_n,K_n,V_n$，所以 X, Q, K, V 的每一行都表示一个单词。 这里通过线性变换的Q,K,V的物理意义是包含了原始数据的信息，可能关注的特征点不一样。\nSelf-Attention 的输出 得到矩阵 Q, K, V之后就可以计算出 Self-Attention 的输出了，计算的公式如下： 公式中计算矩阵Q和K每一行向量的内积，为了防止内积过大，因此除以$d_k$的平方根，缩放注意力，以使得注意力分布的方差在不同维度上保持一致，从而更好地控制梯度的稳定性。。Q乘以K的转置后，得到的矩阵行列数都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度。下图为Q乘以$K^T$, 1234 表示的是句子中的单词。 $QK^T$其实终算出来的是，每一个单词与其他所有的单词的注意力系数，因为Q代表单词本身假如 我有一只猫，分词后1=我，2=有，3=一只，4=猫。 因为K代表其他单词，转至相乘最终$QK^T$矩阵的(1,1)这个各自就是标识我和我的注意力权重，(1,2)就是我和有的注意力权重，(2,4)就是有和猫的注意力权重\n得到$QK^T$之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1. 得到 Softmax 矩阵之后可以和V相乘，得到最终的输出Z。 上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数，最终单词 1 的输出$Z_1$等于所有单词 i 的值$V_i$根据 attention 系数的比例加在一起得到，如下图所示： 这里算出来的$Z_1$第一行就是第一个单词和其他单词的注意力系数权重，\n优势 从self-attention的原理中可以看出，这一层需要学习的参数只有$W^q$ ,$W^k$, $W^v$，大部分变量来自于内部计算得出来的，所以它的参数量少但每个参数所涵盖的信息多，这是它的第一个优点。 每个b的计算都是独立的，这一点相比之前的RNN来说很不一样，RNN是需要等前面的$a^1$算完了才能算$a^2$，是串行的。所以RNN无论是训练还是推理，都会因为不能计算并行而变慢，这是它的的第二个优点。 RNN的一个最大的问题是：前面的变量在经过多次RNN计算后，已经失去了原有的特征。越到后面，最前面的变量占比就越小，这是一个很反人类的设计。而self-attention在每次计算中都能保证每个输入变量 a的初始占比是一样的，这样才能保证经过self-attention layer计算后他的注意力系数是可信的。 所以总结下来，它的三个优点分别是：\n需要学习的参数量少 可以并行计算 能够保证每个变量初始占比是一样的 代码实现 import torch\rimport torch.nn as nn\rimport torch.nn.functional as F\rclass SelfAttention(nn.Module):\rdef __init__(self, embed_size, num_heads):\r\"\"\"\r初始化 SelfAttention 层\r参数:\rembed_size (int): 输入特征的维度\rnum_heads (int): 注意力头的数量\r\"\"\"\rsuper(SelfAttention, self).__init__()\rself.embed_size = embed_size\rself.num_heads = num_heads\rself.head_dim = embed_size // num_heads\r# 确保 embed_size 能被 num_heads 整除\rassert (\rself.head_dim * num_heads == embed_size\r), \"Embedding size needs to be divisible by heads\"\r# 初始化线性层\rself.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\rself.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\rself.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\rself.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\rdef forward(self, values, keys, query):\r\"\"\"\r前向传播函数\r参数:\rvalues (Tensor): 值的张量，形状为 (batch_size, value_len, embed_size)\rkeys (Tensor): 键的张量，形状为 (batch_size, key_len, embed_size)\rquery (Tensor): 查询的张量，形状为 (batch_size, query_len, embed_size)\r返回:\rout (Tensor): 输出张量，形状为 (batch_size, query_len, embed_size)\r\"\"\"\r# 获取张量的大小\rN = query.shape[0]\rvalue_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\r# 将输入张量按头数和头维度进行切分\rvalues = values.reshape(N, value_len, self.num_heads, self.head_dim)\rkeys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\rqueries = query.reshape(N, query_len, self.num_heads, self.head_dim)\r# 通过线性层进行变换\rvalues = self.values(values)\rkeys = self.keys(keys)\rqueries = self.queries(queries)\r# 计算点积注意力\renergy = torch.einsum(\"nqhd,nkhd-\u003enhqk\", [queries, keys]) # batch_size, num_heads, query_len, key_len\r# 计算注意力权重\rattention = torch.nn.functional.softmax(energy / (self.embed_size ** (1/2)), dim=3)\r# 将注意力权重应用到值上\rout = torch.einsum(\"nhql,nlhd-\u003enqhd\", [attention, values]).reshape(\rN, query_len, self.num_heads * self.head_dim\r)\r# 合并多个头并通过线性层进行变换\rout = self.fc_out(out)\rreturn out Multi-head self-attention 为什么要多头 在多头注意力机制中，每个注意力头学习不同的特征表示，这是为了提高模型的表征能力和泛化能力。这种设计允许模型在不同抽象级别上关注输入的不同部分，从而更好地捕获输入之间的关系。\n具体来说，每个注意力头都有自己的权重矩阵（通常是通过学习得到的），这些权重矩阵决定了每个头对输入的不同部分的关注程度。通过允许多个头并且每个头学习不同的特征表示，模型可以同时关注输入的不同方面，从而更好地捕获输入之间的复杂关系。\n举例来说，考虑一个用于自然语言处理的 Transformer 模型。在这种情况下，每个注意力头可以学习关注句子中的不同单词或短语，其中一些头可能更关注主语-谓语关系，另一些头可能更关注宾语-谓语关系，而其他头可能关注句子中的修饰词或者语法结构等。通过允许每个头学习不同的特征表示，模型可以更好地捕获句子中不同部分之间的语义关系，从而提高了模型的性能。\n总的来说，多头注意力机制允许模型以多个不同的视角来观察输入数据，从而提高了模型对输入数据的表征能力和泛化能力。\n原理 通俗易懂理解 multi-head self-attention，所谓head也就是指一个a 衍生出几个q , k , v 。上述所讲解的self-attention是基于single-head的。以2 head为例： 首先，$a^i$先生成$q^1$,$k^1$,$v^1$,然后，接下来就和single-head不一样了，$q^i$生成$q^{i,1},q^{i,2}$生成的方式有两种：\n$q^i$乘上一个$W^{q,1}$得到$q^{i,2}$，这个和single-head的生成是差不多的； $q^i$直接从通道维，平均拆分成两个，得到$q^{i,1},q^{i,2}$ 这两种方式，在最后结果上都差不多。至于为啥，后面会讲一下原因。 那么这里的图解使用第1个方式，先得到$q^{i,1}$,$k^{i,1}$,$v^{i,1}$。对$a^j$做同样的操作得到 ,对$a^j$做同样的操作得到$q^{j,1}$,$k^{j,1}$,$v^{j,1}$。这边需要注意的一点，$q^{i,1}$是要和$k^{j,1}$做矩阵乘法，而非$k^{j,2}$，一一对应。后面计算就和single-head一样了，最后得到$b^{i,1}$ 第二步，对$q^{i,2}$,$k^{i,2}$,$v^{i,2}$做一样的操作，得到$b^{i,2}$ 这里我们算出的$b^{i,1}$,$b^{i,2}$是同维度的，我们可以将其concat在一起，再通过一个$W^0$把他转成想要的维度。这也就不难理解，为什么说multi-head的两种生成方式是一样的，因为最终决定是输出维度的是$W^o$。我们可以将multi-head的过程看成是cnn中的隐藏层，multi-head的数量也就对应着Conv2D的filter数量，每一个head各司其职，提取不同的特征。 矩阵计算 我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 Z，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。 从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入X分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵Z。下图是 h=8 时候的情况，此时会得到 8 个输出矩阵Z。 得到 8 个输出矩阵$Z_1$到$Z_8$之后，Multi-Head Attention 将它们拼接在一起 (Concat)，然后传入一个Linear层，得到 Multi-Head Attention 最终的输出Z。 可以看到 Multi-Head Attention 输出的矩阵Z与其输入的矩阵X的维度是一样的。\n代码实现 import torch\rimport torch.nn.functional as F\rclass MultiHeadSelfAttention(torch.nn.Module):\rdef __init__(self, d_model, num_heads):\rsuper(MultiHeadSelfAttention, self).__init__()\rself.num_heads = num_heads\rself.d_model = d_model\rassert d_model % num_heads == 0 # 确保 d_model 可以被 num_heads 整除\r# 初始化 Q、K、V 矩阵和输出矩阵\rself.W_q = torch.nn.Linear(d_model, d_model)\rself.W_k = torch.nn.Linear(d_model, d_model)\rself.W_v = torch.nn.Linear(d_model, d_model)\rself.W_o = torch.nn.Linear(d_model, d_model)\rdef forward(self, x):\rbatch_size, seq_len, d_model = x.size()\r# 将输入 x 拆分成 num_heads 个头\rQ = self.W_q(x).view(batch_size, seq_len, self.num_heads, d_model // self.num_heads)\rK = self.W_k(x).view(batch_size, seq_len, self.num_heads, d_model // self.num_heads)\rV = self.W_v(x).view(batch_size, seq_len, self.num_heads, d_model // self.num_heads)\r# 对每个头进行 scaled dot-product attention\rattention_scores = torch.matmul(Q, K.transpose(1, 2)) / (d_model ** 0.5)\rattention_probs = F.softmax(attention_scores, dim=-1)\rattention_output = torch.matmul(attention_probs, V)\r# 将每个头的输出拼接起来\rattention_output = attention_output.view(batch_size, seq_len, d_model)\r# 经过线性变换得到最终的输出\routput = self.W_o(attention_output)\rreturn output\r# 为了测试\rd_model = 512 # 模型的维度\rnum_heads = 8 # 头的数量\rseq_len = 10 # 序列长度\rbatch_size = 4 # 批次大小\r# 创建一个随机输入张量\rx = torch.rand(batch_size, seq_len, d_model)\r# 创建 Multi-Head Self-Attention 模块并进行前向传播\rmultihead_attention = MultiHeadSelfAttention(d_model, num_heads)\routput = multihead_attention(x)\r# 打印输出张量的形状\rprint(\"Output shape:\", output.shape)",
    "description": "简介 下图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 Multi-Head Attention，是由多个 Self-Attention组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add \u0026 Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。\n因为 Self-Attention是 Transformer 的重点，所以我们重点关注 Multi-Head Attention 以及 Self-Attention，首先详细了解一下 Self-Attention 的内部逻辑。 基础知识 向量的内积 向量的内积是什么，如何计算，最重要的，其几何意义是什么？\n内积的计算方法是将两个向量对应分量相乘，然后将结果相加。 内积的几何意义是非常重要的。在二维空间中，两个向量的内积等于两个向量的模（长度）之积乘以它们之间的夹角的余弦值。具体来说，如果 θ 是两个向量之间的夹角，则它们的内积为： $$\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| |\\mathbf{b}| \\cos(\\theta)$$ 这个公式表明，内积可以用来衡量两个向量的相似程度。当两个向量的夹角为 0时(cos0=1)，它们的内积取得最大值，表示它们的方向相同；当夹角为 90时(cos90=0)，内积为 0，表示它们的方向垂直；当夹角为180(cos180=-1) 时，内积取得最小值，表示它们的方向相反。",
    "tags": [],
    "title": "Transformer模型详解03-Self-Attention（自注意力机制）",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_03/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "简介 Transformer 模型中的 Encoder 层主要负责将输入序列进行编码，将输入序列中的每个词或标记转换为其对应的向量表示，并且捕获输入序列中的语义和关系。\n具体来说，Transformer Encoder 层的作用包括：\n词嵌入（Word Embedding）：将输入序列中的每个词或标记映射为其对应的词嵌入向量。这些词嵌入向量包含了词语的语义信息，并且可以在模型中进行学习。\n位置编码（Positional Encoding）：因为 Transformer 模型不包含任何关于序列顺序的信息，为了将位置信息引入模型，需要添加位置编码。位置编码是一种特殊的向量，用于表示输入序列中每个词的位置信息，以便模型能够区分不同位置的词。\n多头自注意力机制（Multi-Head Self-Attention）：自注意力机制允许模型在处理每个词时，同时考虑到输入序列中所有其他词之间的关系。多头自注意力机制通过将输入进行多次线性变换并计算多组注意力分数，从而允许模型在不同的表示子空间中学习到不同的语义信息。\n残差连接（Residual Connection）：为了减轻梯度消失和加速训练，Transformer Encoder 层使用了残差连接。残差连接允许模型直接学习到输入序列的增量变换，而不是完全替代原始输入。\n层归一化（Layer Normalization）：在残差连接后应用层归一化，有助于提高模型的训练稳定性，加快训练速度。\nTransformer Encoder 层的主要作用是将输入序列转换为其对应的向量表示，并且捕获输入序列中的语义和位置信息，以便后续的模型能够更好地理解和处理输入序列。\n前面我们已经详解了三个点的计算过程，现在了解一下 Add \u0026 Norm 和 Feed Forward 部分。\n基础知识 归一化 归一化是将数据转换为具有统一尺度的过程，常用于机器学习、数据挖掘和统计分析中。归一化可以确保不同特征或变量之间具有相似的数值范围，有助于提高模型的性能和收敛速度。\n作用 让我用一个简单的例子来说明归一化的作用。\n假设你有一个数据集，其中包含两个特征：年龄和收入。年龄的范围是 0 到 100 岁，而收入的范围是 1000 到 100000 美元。这两个特征的范围差异很大。\n现在，你想要使用这些特征来训练一个机器学习模型，比如线性回归模型，来预测一个人是否会购买某种产品。由于特征的范围差异较大，这可能会导致某些问题：\n收入的范围比年龄大得多，这可能会使得模型过度关注收入而忽略年龄，因为收入的变化可能会对预测产生更大的影响。 模型可能会受到数值范围的影响，而不是特征本身的重要性。 这时候，归一化就可以派上用场了。通过归一化，你可以将不同特征的值缩放到相似的范围内，从而消除数值范围差异带来的影响。比如，你可以将年龄和收入都缩放到 0 到 1 之间的范围内，或者使用其他归一化方法，如标准化 (standardization)。\n通过归一化，你可以确保模型不会因为特征值的范围差异而偏向某个特定的特征，而是可以更平衡地利用所有的特征信息来进行预测。\n常用归一化 下面是几种常用的归一化方式及其公式：\nMin-Max 归一化： Min-Max 归一化将数据线性映射到一个指定的范围内，通常是 [0, 1] 或 [-1, 1]。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X - X_{\\text{min}}}}{{X_{\\text{max}} - X_{\\text{min}}}}]$$\n其中，$(X_{\\text{norm}})$ 是归一化后的数据，(X) 是原始数据，$(X_{\\text{min}})$ 和 $(X_{\\text{max}})$分别是数据的最小值和最大值。\nZ-Score 标准化： Z-Score 标准化将数据转换为均值为 0，标准差为 1 的正态分布。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X - \\mu}}{{\\sigma}}]$$\n其中，$(X_{\\text{norm}})$是归一化后的数据，$(X)$ 是原始数据，$\\mu$是数据的均值，$(\\sigma)$是数据的标准差。\nDecimal Scaling 归一化： Decimal Scaling 归一化将数据缩放到[-1,1]或者[0,1]的范围内，通过除以数据中的最大绝对值来实现。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X}}{{\\max(|X|)}}]$$\n其中，$(X_{\\text{norm}})$ 是归一化后的数据，$(X)$ 是原始数据，$(\\max(|X|))$ 是数据中的最大绝对值。\nRobust Scaling： Robust Scaling 是一种针对离群值鲁棒的归一化方法，通过除以数据的四分位距（IQR）来缩放数据。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X - Q_1}}{{Q_3 - Q_1}}]$$\n其中，$(X_{\\text{norm}})$ 是归一化后的数据，$(X)$是原始数据，$(Q_1)$ 是数据的第一四分位数（25th percentile），$(Q_3)$ 是数据的第三四分位数（75th percentile）。\n这些是常用的归一化方式，选择适合你的数据和模型的归一化方法可以提高模型的性能和稳定性。\n残差连接 残差连接（Residual Connection）是一种在深度神经网络中用于解决梯度消失和梯度爆炸问题的技术。它通过将输入直接添加到神经网络的某些层的输出中，从而允许梯度直接通过残差路径传播，减轻了梯度消失的问题，加速了训练过程。\n具体来说，假设我们有一个包含多个层的神经网络，每个层都由输入 $x$ 经过一些变换 $F(x)$得到输出 $H(x)$。传统的神经网络会直接将 $H(x)$ 作为下一层的输入，而残差连接则是将 $x$ 与 $H(x)$ 相加，即 $H(x)+x$，然后再输入到下一层。这样做可以使得网络学习到的变换是相对于输入的增量，而不是完全替代原始输入。\n残差连接的作用包括：\n缓解梯度消失：通过保留原始输入的信息，使得梯度可以更容易地传播到较浅层，从而减轻了梯度消失问题。 加速训练：残差连接可以使得神经网络更快地收敛，因为它减少了训练过程中的信息丢失。 提高模型性能：残差连接使得神经网络可以更深，更复杂，从而能够更好地捕捉输入数据的特征和模式。 举个例子，考虑一个包含残差连接的深度残差网络（Residual Network，ResNet）。在这个网络中，每个残差块都由两个或多个卷积层组成，其中第一个卷积层产生特征图 $H(x)$，而第二个卷积层则对 $H(x)$ 进行进一步变换。然后，原始输入 $x$ 被添加到 $H(x)$ 上，得到 $F(x)=H(x)+x$。这样，输出 $F(x)$ 就包含了相对于输入 $x$ 的增量，网络可以更轻松地学习到残差部分，从而更有效地优化模型。\nAdd \u0026 Norm Add \u0026 Norm 层由 Add 和 Norm 两部分组成，其计算公式如下： 第一个Add\u0026Norm中Add指 X+MultiHeadAttention(X)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 ResNet 中经常用到： Norm指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。\nFeed Forward Feed Forward 层比较简单，是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下。 $$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$ 也就是： 在这个公式中：\n$(X)$ 是输入的隐藏表示，维度为 $(d_{\\text{model}})$，是Add\u0026Norm输出； $(W_1)$ 和 $(W_2)$ 是权重矩阵，分别用于第一层和第二层的线性变换，维度分别为 $(d_{\\text{model}} \\times d_{\\text{ff}})$ 和 $(d_{\\text{ff}} \\times d_{\\text{model}})$； $(b_1)$ 和 $(b_2)$ 是偏置项； $(\\text{ReLU})$ 表示修正线性单元，是一种非线性激活函数，用于引入模型的非线性性。 Feed Forward 最终得到的输出矩阵的维度与X一致。\nFeed Forward 层在深度学习模型中具有重要意义，它主要有以下几个方面的作用：\n特征变换与组合： Feed Forward 层通过线性变换和非线性激活函数将输入数据进行特征变换和组合，使得模型能够学习到更高级、更复杂的特征表示。这有助于模型更好地理解数据的内在结构和规律。\n引入非线性： 非线性激活函数（如 ReLU、sigmoid、tanh 等）可以引入非线性变换，从而使得模型能够学习到非线性关系，提高模型的表达能力。如果没有非线性变换，多个线性变换的组合仍然只会得到线性变换，模型的表达能力将受到限制。\n增加模型的深度： Feed Forward 层通常是深度神经网络中的一个组成部分，通过堆叠多个 Feed Forward 层可以构建深度模型。深度模型能够学习到更多层次、更抽象的特征表示，从而提高模型的性能和泛化能力。\n提高模型的泛化能力： Feed Forward 层通过特征变换和非线性变换有助于模型学习到数据的高级抽象表示，这有助于提高模型对新样本的泛化能力，使得模型更好地适应未见过的数据。\n组成 Encoder 通过上面描述的 Multi-Head Attention, Feed Forward, Add \u0026 Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵$X_(nd)$, 并输出一个矩阵$O_(nd)$,通过多个 Encoder block 叠加就可以组成 Encoder。 第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是编码信息矩阵 C，这一矩阵后续会用到 Decoder 中。 代码实现 import torch\rimport torch.nn as nn\rimport torch.nn.functional as F\rclass TransformerEncoderLayer(nn.Module):\rdef __init__(self, d_model, n_heads, d_ff, dropout=0.1):\rsuper(TransformerEncoderLayer, self).__init__()\rself.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\rself.linear1 = nn.Linear(d_model, d_ff)\rself.linear2 = nn.Linear(d_ff, d_model)\rself.dropout = nn.Dropout(dropout)\rself.norm1 = nn.LayerNorm(d_model)\rself.norm2 = nn.LayerNorm(d_model)\rdef forward(self, src, src_mask=None):\r# Multi-head self-attention\rsrc2 = self.self_attn(src, src, src, attn_mask=src_mask)[0]\rsrc = src + self.dropout(src2)\rsrc = self.norm1(src)\r# Feed Forward Layer\rsrc2 = self.linear2(F.relu(self.linear1(src)))\rsrc = src + self.dropout(src2)\rsrc = self.norm2(src)\rreturn src\rclass TransformerEncoder(nn.Module):\rdef __init__(self, num_layers, d_model, n_heads, d_ff, dropout=0.1):\rsuper(TransformerEncoder, self).__init__()\rself.layers = nn.ModuleList([TransformerEncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])\rdef forward(self, src, src_mask=None):\rfor layer in self.layers:\rsrc = layer(src, src_mask)\rreturn src",
    "description": "简介 Transformer 模型中的 Encoder 层主要负责将输入序列进行编码，将输入序列中的每个词或标记转换为其对应的向量表示，并且捕获输入序列中的语义和关系。\n具体来说，Transformer Encoder 层的作用包括：\n词嵌入（Word Embedding）：将输入序列中的每个词或标记映射为其对应的词嵌入向量。这些词嵌入向量包含了词语的语义信息，并且可以在模型中进行学习。\n位置编码（Positional Encoding）：因为 Transformer 模型不包含任何关于序列顺序的信息，为了将位置信息引入模型，需要添加位置编码。位置编码是一种特殊的向量，用于表示输入序列中每个词的位置信息，以便模型能够区分不同位置的词。\n多头自注意力机制（Multi-Head Self-Attention）：自注意力机制允许模型在处理每个词时，同时考虑到输入序列中所有其他词之间的关系。多头自注意力机制通过将输入进行多次线性变换并计算多组注意力分数，从而允许模型在不同的表示子空间中学习到不同的语义信息。\n残差连接（Residual Connection）：为了减轻梯度消失和加速训练，Transformer Encoder 层使用了残差连接。残差连接允许模型直接学习到输入序列的增量变换，而不是完全替代原始输入。\n层归一化（Layer Normalization）：在残差连接后应用层归一化，有助于提高模型的训练稳定性，加快训练速度。\nTransformer Encoder 层的主要作用是将输入序列转换为其对应的向量表示，并且捕获输入序列中的语义和位置信息，以便后续的模型能够更好地理解和处理输入序列。\n前面我们已经详解了三个点的计算过程，现在了解一下 Add \u0026 Norm 和 Feed Forward 部分。\n基础知识 归一化 归一化是将数据转换为具有统一尺度的过程，常用于机器学习、数据挖掘和统计分析中。归一化可以确保不同特征或变量之间具有相似的数值范围，有助于提高模型的性能和收敛速度。\n作用 让我用一个简单的例子来说明归一化的作用。\n假设你有一个数据集，其中包含两个特征：年龄和收入。年龄的范围是 0 到 100 岁，而收入的范围是 1000 到 100000 美元。这两个特征的范围差异很大。\n现在，你想要使用这些特征来训练一个机器学习模型，比如线性回归模型，来预测一个人是否会购买某种产品。由于特征的范围差异较大，这可能会导致某些问题：\n收入的范围比年龄大得多，这可能会使得模型过度关注收入而忽略年龄，因为收入的变化可能会对预测产生更大的影响。 模型可能会受到数值范围的影响，而不是特征本身的重要性。 这时候，归一化就可以派上用场了。通过归一化，你可以将不同特征的值缩放到相似的范围内，从而消除数值范围差异带来的影响。比如，你可以将年龄和收入都缩放到 0 到 1 之间的范围内，或者使用其他归一化方法，如标准化 (standardization)。\n通过归一化，你可以确保模型不会因为特征值的范围差异而偏向某个特定的特征，而是可以更平衡地利用所有的特征信息来进行预测。\n常用归一化 下面是几种常用的归一化方式及其公式：\nMin-Max 归一化： Min-Max 归一化将数据线性映射到一个指定的范围内，通常是 [0, 1] 或 [-1, 1]。其公式如下：\n$$[X_{\\text{norm}} = \\frac{{X - X_{\\text{min}}}}{{X_{\\text{max}} - X_{\\text{min}}}}]$$",
    "tags": [],
    "title": "Transformer模型详解04-Encoder 结构",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_04/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 工具库 \u003e transformers \u003e transformers模型详解",
    "content": "@[toc]\n简介 Transformer 模型由编码器（Encoder）和解码器（Decoder）两部分组成。这里我会着重描述解码器的结构以及在预训练、输入输出和预测时的输入输出。\n解码器结构：\n自注意力层（Self-Attention Layers）：与编码器类似，解码器也包含多个自注意力层，用于在解码器端对输出序列的不同位置进行关注，解码器中的自注意力层被修改为接受一个遮盖（masking）向量，以便在计算注意力权重时将未来的信息屏蔽掉，只关注当前位置之前的信息。。\n编码器-解码器注意力层（Encoder-Decoder Attention Layers）：除了自注意力层外，解码器还包含编码器-解码器注意力层，用于将编码器端的信息与解码器端的信息进行交互，帮助解码器更好地理解输入序列。\n前馈神经网络（Feed-Forward Neural Networks）：与编码器一样，解码器也包含前馈神经网络层，用于对特征进行映射和转换。\n位置编码（Positional Encoding）：解码器也需要位置编码来将位置信息融入模型中，以便模型能够理解输入序列的顺序信息。\nDecoder在预训练、输入输出和预测时的输入输出：\n预训练：\n输入：在预训练期间，解码器的输入通常是由目标序列（target sequence）以及可选的编码器端输出的上下文信息组成。这些输入经过嵌入（embedding）和位置编码后，被送入解码器中。 输出：解码器预训练的目标是生成目标序列的下一个词的概率分布。因此，在每个时间步，解码器会生成一个预测概率分布，以便训练模型。 输入输出：\n输入：在进行输入输出（Inference）时，解码器的输入通常是由上一个时间步生成的词以及编码器端的上下文信息组成。这些输入通过嵌入和位置编码后，传递给解码器。 输出：解码器在每个时间步生成的输出通常是一个概率分布，用于预测下一个词的概率。根据应用场景，可以使用不同的策略（如贪婪搜索、束搜索等）来选择最终的输出序列。 预测：\n输入：在预测阶段，解码器的输入通常是由起始符号（如）以及编码器端的上下文信息组成。这些输入经过嵌入和位置编码后，传递给解码器。 输出：解码器生成的输出是一个概率分布，用于预测下一个词的概率。根据应用需求，可以根据生成的概率分布采样得到最终的预测结果。 结构 上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：\n包含两个 Multi-Head Attention 层。 第一个 Multi-Head Attention 层采用了 Masked 操作。 第二个 Multi-Head Attention 层的K, V矩阵使用 Encoder 的编码信息矩阵C进行计算，而Q使用上一个 Decoder block 的输出计算。 最后有一个 Softmax 层计算下一个翻译单词的概率。 先理解:自注意力的计算过程\n原理 第一个 Multi-Head Attention Decoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。下面以 “我有一只猫” 翻译成 “I have a cat” 为例，了解一下 Masked 操作。\n下面的描述中使用了类似 Teacher Forcing 的概念，不熟悉 Teacher Forcing 的童鞋可以参考以下上一篇文章Seq2Seq 模型详解。在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 “” 预测出第一个单词为 “I”，然后根据输入 “ I” 预测下一个单词 “have”。 Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 ( I have a cat) 和对应输出 (I have a cat ) 传递到 Decoder。那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 “ I have a cat \"。\n第一步：是 Decoder 的输入矩阵和 Mask 矩阵，输入矩阵包含 “ I have a cat” (0, 1, 2, 3, 4) 五个单词的表示向量，Mask 是一个 5×5 的矩阵。在 Mask 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。 第二步：接下来的操作和之前的 Self-Attention 一样，通过输入矩阵X计算得到Q,K,V矩阵。然后计算Q和$K^T$的乘积$QK^T$ 第三步：在得到 $QK^T$之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用Mask矩阵遮挡住每一个单词之后的信息，遮挡操作如下： 得到 Mask $QK^T$之后在 Mask$QK^T$上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。 第四步：使用 Mask $QK^T$与矩阵 V相乘，得到输出 Z，则单词 1 的输出向量 $Z_1$是只包含单词 1 信息的。 第五步：通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 $Z_i$，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出$Z_i$然后计算得到第一个 Multi-Head Attention 的输出Z，Z与输入X维度一样。\n第二个 Multi-Head Attention Decoder block 第二个 Multi-Head Attention 变化不大， 主要的区别在于其中 Self-Attention 的 K, V矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 Encoder 的编码信息矩阵 C 计算的。\n根据 Encoder 的输出 C计算得到 K, V，根据上一个 Decoder block 的输出 Z 计算 Q (如果是第一个 Decoder block 则使用输入矩阵 X 进行计算)，后续的计算方法与之前描述的一致。\n这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 Mask)。\n什么使用Encoder计算k,v decoder计算Q\n在 Transformer 模型的解码器中，使用了编码器的键（key）和值（value），而使用解码器的查询（query）。这种结构是为了充分利用编码器端对输入序列的理解，同时使得解码器端能够更好地根据自身生成的部分序列来做出决策。这种设计的物理意义可以从以下几个方面来理解：\n利用编码器的上下文信息：编码器对输入序列进行编码，生成了对输入序列全局理解的表示。因此，使用编码器的键和值可以提供丰富的上下文信息，帮助解码器更好地理解输入序列。\n解码器的自注意力：解码器的自注意力机制中，查询用于计算注意力权重，而键和值则用于构建注意力分布。使用解码器的查询意味着模型在计算注意力时更关注当前正在生成的部分序列，这有助于确保生成的序列在语法和语义上的连贯性。\n解耦编码器和解码器：使用不同的键、值和查询将编码器和解码器的功能分开，使得模型更具灵活性和泛化能力。解码器可以独立地根据当前正在生成的序列来调整自己的注意力，而不受编码器端信息的限制。\n总之，通过在解码器中使用编码器的键和值，以及使用解码器的查询，Transformer 模型能够更好地利用编码器端对输入序列的理解，并在解码器端根据当前正在生成的序列来做出决策，从而提高了生成序列的质量和连贯性。\nSoftmax 预测输出单词 Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下： Softmax 根据输出矩阵的每一行预测下一个单词： 这就是 Decoder block 的定义，与 Encoder 一样，Decoder 是由多个 Decoder block 组合而成。",
    "description": "@[toc]\n简介 Transformer 模型由编码器（Encoder）和解码器（Decoder）两部分组成。这里我会着重描述解码器的结构以及在预训练、输入输出和预测时的输入输出。\n解码器结构：\n自注意力层（Self-Attention Layers）：与编码器类似，解码器也包含多个自注意力层，用于在解码器端对输出序列的不同位置进行关注，解码器中的自注意力层被修改为接受一个遮盖（masking）向量，以便在计算注意力权重时将未来的信息屏蔽掉，只关注当前位置之前的信息。。\n编码器-解码器注意力层（Encoder-Decoder Attention Layers）：除了自注意力层外，解码器还包含编码器-解码器注意力层，用于将编码器端的信息与解码器端的信息进行交互，帮助解码器更好地理解输入序列。\n前馈神经网络（Feed-Forward Neural Networks）：与编码器一样，解码器也包含前馈神经网络层，用于对特征进行映射和转换。\n位置编码（Positional Encoding）：解码器也需要位置编码来将位置信息融入模型中，以便模型能够理解输入序列的顺序信息。\nDecoder在预训练、输入输出和预测时的输入输出：\n预训练：\n输入：在预训练期间，解码器的输入通常是由目标序列（target sequence）以及可选的编码器端输出的上下文信息组成。这些输入经过嵌入（embedding）和位置编码后，被送入解码器中。 输出：解码器预训练的目标是生成目标序列的下一个词的概率分布。因此，在每个时间步，解码器会生成一个预测概率分布，以便训练模型。 输入输出：\n输入：在进行输入输出（Inference）时，解码器的输入通常是由上一个时间步生成的词以及编码器端的上下文信息组成。这些输入通过嵌入和位置编码后，传递给解码器。 输出：解码器在每个时间步生成的输出通常是一个概率分布，用于预测下一个词的概率。根据应用场景，可以使用不同的策略（如贪婪搜索、束搜索等）来选择最终的输出序列。 预测：\n输入：在预测阶段，解码器的输入通常是由起始符号（如）以及编码器端的上下文信息组成。这些输入经过嵌入和位置编码后，传递给解码器。 输出：解码器生成的输出是一个概率分布，用于预测下一个词的概率。根据应用需求，可以根据生成的概率分布采样得到最终的预测结果。 结构 上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：\n包含两个 Multi-Head Attention 层。 第一个 Multi-Head Attention 层采用了 Masked 操作。 第二个 Multi-Head Attention 层的K, V矩阵使用 Encoder 的编码信息矩阵C进行计算，而Q使用上一个 Decoder block 的输出计算。 最后有一个 Softmax 层计算下一个翻译单词的概率。 先理解:自注意力的计算过程",
    "tags": [],
    "title": "Transformer模型详解05-Decoder 结构",
    "uri": "/docs/programming/ai/tools_libraries/transformers/basic/transformers_basic_05/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 插件开发 \u003e vscode插件",
    "content": "概述 Visual Studio Code（简称 VS Code）是一款由 Microsoft 开发的开源轻量级编辑器，支持跨平台（Windows、macOS、Linux）。\n其最大的优势之一是强大的插件系统，开发者可以通过编写扩展（Extension）来增强 VS Code 的功能，比如支持新的编程语言、代码提示、调试器、界面主题等。\nVS Code 插件的主要原理是：\n插件运行在独立的进程（Extension Host）中，不会阻塞编辑器主线程。 插件通过 VS Code 提供的 API 与编辑器进行交互，比如注册命令、添加菜单、修改编辑器行为等。 插件开发语言主要是 TypeScript 或 JavaScript，并基于 Node.js 生态。 安装 VS Code 安装 打开 VS Code 官方下载页面。 选择对应操作系统（Windows、macOS 或 Linux）。 按提示进行安装，安装完成后可以通过 code 命令（需要在安装时勾选“添加到 PATH”）在命令行中启动 VS Code。 插件开发环境安装 插件开发需要以下工具：\nyo（Yeoman 脚手架工具） generator-code（VS Code 插件项目生成器） vsce（VS Code Extension CLI，用于打包和发布插件） 安装步骤：\n# 安装 yo 和 generator-code npm install -g yo generator-code # 安装 vsce npm install -g @vscode/vsce 开发 生成代码 使用 Yeoman 脚手架生成插件项目：\nyo code 执行后会有交互式提示，例如：\n选择插件类型（TypeScript / JavaScript） 插件名称 描述 初始化 Git 仓库等 生成完成后，项目目录大致结构如下：\nmy-extension/\r├── .vscode/ # VS Code 调试配置\r├── src/ # 插件源码\r│ └── extension.ts # 插件入口文件\r├── package.json # 插件描述文件，配置命令、激活事件、依赖等\r├── tsconfig.json # TypeScript 配置（如果是 TS 项目）\r└── README.md # 插件说明文档 package.json：插件的核心配置文件，用来描述插件元信息和扩展点。 extension.ts：插件入口文件，负责注册命令和功能。 package.json 核心配置 package.json 是插件的描述文件，控制插件如何被 VS Code 加载。主要字段：\n{\r\"name\": \"my-extension\",\r\"displayName\": \"My Extension\",\r\"description\": \"一个简单的 VS Code 插件示例\",\r\"version\": \"0.0.1\",\r\"publisher\": \"your-name\",\r\"engines\": {\r\"vscode\": \"^1.80.0\"\r},\r\"activationEvents\": [\r\"onCommand:extension.helloWorld\"\r],\r\"main\": \"./out/extension.js\",\r\"contributes\": {\r\"commands\": [\r{\r\"command\": \"extension.helloWorld\",\r\"title\": \"Hello World\"\r}\r]\r},\r\"scripts\": {\r\"vscode:prepublish\": \"npm run compile\",\r\"compile\": \"tsc -p ./\",\r\"watch\": \"tsc -watch -p ./\",\r\"test\": \"npm run compile \u0026\u0026 node ./out/test/runTest.js\"\r},\r\"devDependencies\": {\r\"typescript\": \"^5.0.0\",\r\"vscode\": \"^1.1.37\"\r}\r} 核心字段说明：\nname：插件的唯一 ID（发布后不可更改）。 displayName：VS Code Marketplace 上显示的名称。 version：插件版本。 publisher：发布者名称（需与 Marketplace 发布者一致）。 engines.vscode：兼容的 VS Code 版本范围。 activationEvents：触发插件激活的事件（如 onCommand、onLanguage、*）。 main：插件的入口文件（一般是编译后的 extension.js）。 contributes：插件扩展点，例如命令、菜单、快捷键、配置等。 extension.ts 核心函数 extension.ts 是插件的入口文件，负责插件的生命周期和功能实现。\nimport * as vscode from 'vscode';\r/**\r* 插件被激活时调用\r* @param context 插件上下文对象，包含订阅、全局存储等\r*/\rexport function activate(context: vscode.ExtensionContext) {\rconsole.log('插件已激活！');\r// 注册命令\rlet disposable = vscode.commands.registerCommand('extension.helloWorld', () =\u003e {\rvscode.window.showInformationMessage('Hello World from My Extension!');\r});\r// 将命令注册到插件上下文，确保插件卸载时清理资源\rcontext.subscriptions.push(disposable);\r}\r/**\r* 插件被停用时调用\r* 通常用于清理资源、保存数据\r*/\rexport function deactivate() {} 核心点解释：\nactivate：插件激活时执行（如首次运行命令、打开特定文件类型）。 deactivate：插件停用时执行，用于清理资源。 vscode.commands.registerCommand：注册一个命令（命令 ID 必须和 package.json 中一致）。 vscode.window.showInformationMessage：在 VS Code 界面右下角弹出提示消息。 context.subscriptions：插件上下文，保存所有注册的资源，确保在插件停用时能正确释放。 Hello World 示例 编辑 src/extension.ts，添加一个最简单的命令： import * as vscode from 'vscode';\rexport function activate(context: vscode.ExtensionContext) {\rconsole.log('插件已激活！');\rlet disposable = vscode.commands.registerCommand('extension.helloWorld', () =\u003e {\rvscode.window.showInformationMessage('Hello World from My Extension!');\r});\rcontext.subscriptions.push(disposable);\r}\rexport function deactivate() {} 在 package.json 中配置命令： {\r\"contributes\": {\r\"commands\": [\r{\r\"command\": \"extension.helloWorld\",\r\"title\": \"Hello World\"\r}\r]\r}\r} 运行调试： 按 F5 启动调试，会打开一个新的 VS Code 窗口（Extension Development Host）。 打开命令面板（Ctrl+Shift+P / Cmd+Shift+P），输入并运行 Hello World。 会弹出消息 “Hello World from My Extension!\"。 拓展介绍 VS Code 插件 API 非常丰富，常见扩展能力包括：\n编辑器扩展：代码高亮、自动补全、格式化器。\nUI 扩展：状态栏、活动栏、侧边栏视图。\n调试扩展：调试适配器，用于支持新的调试语言。\n文件系统扩展：实现虚拟文件系统。\n常见配置示例（在 package.json 中添加）：\n1. 命令（Commands） 命令是最常见的扩展方式，用户可以在命令面板（Ctrl+Shift+P）或绑定快捷键来触发。\n配置（package.json）：\n{ \"contributes\": { \"commands\": [ { \"command\": \"extension.helloWorld\", \"title\": \"Hello World\" } ] } } 实现（extension.ts）：\nvscode.commands.registerCommand('extension.helloWorld', () =\u003e {\rvscode.window.showInformationMessage('Hello World!');\r}); 2. 菜单（Menus） 可以把命令挂载到编辑器右键菜单、资源管理器右键菜单等位置。\n配置（package.json）：\n{\r\"contributes\": {\r\"commands\": [\r{\r\"command\": \"extension.helloWorld\",\r\"title\": \"hello\"\r}，\r\"menus\": {\r\"editor/context\": [\r{\r\"command\": \"extension.helloWorld\",\r\"when\": \"editorLangId == javascript\",\r\"group\": \"navigation\"\r}\r]\r}\r}\r} 说明：\neditor/context 表示编辑器内右键菜单。 when 条件限制了命令只在 JavaScript 文件中出现。 group 决定菜单项分组（navigation = 导航相关）。 菜单本身没有名字，只能通过命令 title 来显示，菜单本省command会关联到commands的命令通过command的title显示菜单名称。 菜单位置由 menus 的 key 决定，比如：\n菜单位置 key:\r`editor/context` 编辑器右键菜单\r`editor/title` 编辑器标题栏按钮\r`editor/title/context` 编辑器标题栏右键菜单\r`explorer/context` 资源管理器右键菜单\r`commandPalette` 命令面板（Ctrl+Shift+P）\r`view/title` 视图面板标题栏按钮\r`scm/title` 版本控制标题栏按钮 3. 快捷键（Keybindings） 可以为命令绑定快捷键。\n配置（package.json）：\n{\r\"contributes\": {\r\"keybindings\": [\r{\r\"command\": \"extension.helloWorld\",\r\"key\": \"ctrl+alt+h\",\r\"when\": \"editorTextFocus\"\r}\r]\r}\r} 说明：\nkey：快捷键组合。 when：触发条件，这里是“编辑器有焦点时”。 4. 状态栏（Status Bar Items） 可以在底部状态栏添加一个按钮。\n实现（extension.ts）：\nlet statusBar = vscode.window.createStatusBarItem(vscode.StatusBarAlignment.Right, 100);\rstatusBar.text = \"$(smiley) Hello\";\rstatusBar.command = \"extension.helloWorld\";\rstatusBar.show();\rcontext.subscriptions.push(statusBar); 说明：\ncreateStatusBarItem 用于创建状态栏元素。 text 可以包含图标（如 $(smiley)）。 command 绑定点击事件。 5. 侧边栏视图（Views） 可以在活动栏（左侧竖栏）添加一个自定义视图。\n配置（package.json）：\n{\r\"contributes\": {\r\"views\": {\r\"explorer\": [\r{\r\"id\": \"mySidebar\",\r\"name\": \"My Sidebar\"\r}\r]\r}\r}\r} 实现（extension.ts）：\nclass MyTreeDataProvider implements vscode.TreeDataProvider\u003cvscode.TreeItem\u003e {\rgetTreeItem(element: vscode.TreeItem): vscode.TreeItem {\rreturn element;\r}\rgetChildren(): vscode.TreeItem[] {\rreturn [\rnew vscode.TreeItem(\"Item 1\"),\rnew vscode.TreeItem(\"Item 2\")\r];\r}\r}\rvscode.window.registerTreeDataProvider(\"mySidebar\", new MyTreeDataProvider()); 说明：\n在 资源管理器面板 添加一个新视图 “My Sidebar”。\n用 TreeDataProvider 动态提供数据。\n6. 编辑器装饰（Decorations） 可以给代码添加背景色、高亮、提示信息等。\n实现（extension.ts）：\nconst decorationType = vscode.window.createTextEditorDecorationType({\rbackgroundColor: \"rgba(255,0,0,0.3)\"\r});\rconst editor = vscode.window.activeTextEditor;\rif (editor) {\rconst range = new vscode.Range(0, 0, 0, 5);\reditor.setDecorations(decorationType, [range]);\r} 说明：\ncreateTextEditorDecorationType 定义样式。 setDecorations 应用到代码范围。 7. 语言支持（Language Features） 可以扩展某种语言的代码补全、悬浮提示等。\n配置（package.json）：\n{\r\"contributes\": {\r\"languages\": [\r{\r\"id\": \"mylang\",\r\"aliases\": [\"MyLang\"],\r\"extensions\": [\".mlg\"],\r\"configuration\": \"./language-configuration.json\"\r}\r]\r}\r} 实现补全（extension.ts）：\nvscode.languages.registerCompletionItemProvider(\"mylang\", {\rprovideCompletionItems(document, position) {\rreturn [new vscode.CompletionItem(\"helloWorld\", vscode.CompletionItemKind.Keyword)];\r}\r}); 说明：\nlanguages 定义新语言（这里是 .mlg 后缀）。 registerCompletionItemProvider 提供自动补全。 8. 配置（Configuration） 插件可以在 VS Code 设置里增加配置项。\n配置（package.json）：\n{\r\"contributes\": {\r\"configuration\": {\r\"title\": \"My Extension\",\r\"properties\": {\r\"myExtension.enableFeature\": {\r\"type\": \"boolean\",\r\"default\": true,\r\"description\": \"是否启用我的功能\"\r},\r\"myExtension.apiEndpoint\": {\r\"type\": \"string\",\r\"default\": \"https://api.example.com\",\r\"description\": \"API 接口地址\"\r}\r}\r}\r}\r} 读取配置（extension.ts）：\nconst config = vscode.workspace.getConfiguration(\"myExtension\");\rconst enable = config.get(\"enableFeature\", true);\rconst api = config.get(\"apiEndpoint\", \"\"); 9. 文件系统监听（File System Watcher） 可以监听文件变化事件。\n实现（extension.ts）：\nconst watcher = vscode.workspace.createFileSystemWatcher(\"**/*.js\");\rwatcher.onDidChange(uri =\u003e console.log(\"修改: \" + uri.fsPath));\rwatcher.onDidCreate(uri =\u003e console.log(\"创建: \" + uri.fsPath));\rwatcher.onDidDelete(uri =\u003e console.log(\"删除: \" + uri.fsPath));\rcontext.subscriptions.push(watcher); 10. 任务（Tasks） 可以让插件在 VS Code 的“任务运行器”中提供任务。\n配置（package.json）：\n{\r\"contributes\": {\r\"taskDefinitions\": [\r{\r\"type\": \"myTask\",\r\"required\": [\"taskName\"],\r\"properties\": {\r\"taskName\": {\r\"type\": \"string\",\r\"description\": \"任务名称\"\r}\r}\r}\r]\r}\r} 实现（extension.ts）：\nvscode.tasks.registerTaskProvider(\"myTask\", {\rprovideTasks: () =\u003e {\rreturn [new vscode.Task(\r{ type: \"myTask\", taskName: \"sayHello\" },\rvscode.TaskScope.Workspace,\r\"sayHello\",\r\"myTask\",\rnew vscode.ShellExecution(\"echo Hello from task!\")\r)];\r},\rresolveTask: () =\u003e undefined\r}); 发布 打包插件 使用 vsce 打包插件：\n# 在插件项目根目录执行\rvsce package 执行成功后，会生成一个 .vsix 文件，例如：\nmy-extension-0.0.1.vsix 安装插件：\ncode --install-extension my-extension-0.0.1.vsix 或者到vscode插件中心右侧… install from vsix选择本地文件。\n发布到 VS Code Marketplace 前往 Azure DevOps 创建 Publisher。\n使用 vsce login \u003cpublisher-name\u003e 登录，并输入 Personal Access Token。\n发布插件：\nvsce publish 或者指定版本号：\nvsce publish minor 发布成功后，你的插件就会出现在 Visual Studio Marketplace 上，供所有用户下载。",
    "description": "概述 Visual Studio Code（简称 VS Code）是一款由 Microsoft 开发的开源轻量级编辑器，支持跨平台（Windows、macOS、Linux）。\n其最大的优势之一是强大的插件系统，开发者可以通过编写扩展（Extension）来增强 VS Code 的功能，比如支持新的编程语言、代码提示、调试器、界面主题等。\nVS Code 插件的主要原理是：\n插件运行在独立的进程（Extension Host）中，不会阻塞编辑器主线程。 插件通过 VS Code 提供的 API 与编辑器进行交互，比如注册命令、添加菜单、修改编辑器行为等。 插件开发语言主要是 TypeScript 或 JavaScript，并基于 Node.js 生态。 安装 VS Code 安装 打开 VS Code 官方下载页面。 选择对应操作系统（Windows、macOS 或 Linux）。 按提示进行安装，安装完成后可以通过 code 命令（需要在安装时勾选“添加到 PATH”）在命令行中启动 VS Code。 插件开发环境安装 插件开发需要以下工具：\nyo（Yeoman 脚手架工具） generator-code（VS Code 插件项目生成器） vsce（VS Code Extension CLI，用于打包和发布插件） 安装步骤：\n# 安装 yo 和 generator-code npm install -g yo generator-code # 安装 vsce npm install -g @vscode/vsce 开发 生成代码 使用 Yeoman 脚手架生成插件项目：",
    "tags": [],
    "title": "vscode插件开发教程",
    "uri": "/docs/programming/plugins/vscode/vscode_cross/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 框架学习",
    "content": "@[toc]\n简介 TensorFlow是一种端到端开源机器学习平台，它提供了一个全面而灵活的生态系统，包含各种工具、库和社区资源，能够助力研究人员推动先进机器学习技术的发展。在TensorFlow机器学习框架下，开发者能够轻松地构建和部署由机器学习提供支持的应用。[2]\nKeras是一个高层次神经网络 API，适用于快速构建原型、高级研究和生产。它作为TensorFlow的一个接口，可以兼容多种深度学习框架。Keras 的核心数据结构是 model，一种组织网络层的方式。最简单的模型是 Sequential 顺序模型，它由多个网络层线性堆叠。对于更复杂的结构，你应该使用 Keras 函数式 API，它允许构建任意的神经网络图。 Keras最开始是为研究人员开发的，其目的在于快速实验，具有相同的代码可以在CPU或GPU上无缝切换运行的特点，同时也具有用户友好的API，方便快速地开发深度学习模型的原型。 Keras使用类sklearn的api接口来调用tensorflow，从sklearn机器学习中切换过来，更加容易上手。 Tenforflow2.0后直接内置可keras。\n运行硬件 TensorFlow支持在CPU和GPU上运行。GPU（图形处理单元）是一种专门用于加速计算的硬件，它可以大大提高深度学习模型的训练速度。相对而言，CPU（中央处理器）的每个核心具有更强大的处理能力，但它们的数量通常非常有限，因此在处理大数据时它们表现不佳。\nTensorFlow GPU和CPU的主要区别在于如何使用硬件来处理计算任务，以及处理速度的差异。在CPU上，TensorFlow利用所有可用的CPU内核并将任务分配给它们，这可能需要几分钟或几小时来完成。在GPU上，TensorFlow使用CUDA（Compute Unified Device Architecture）技术来利用GPU进行并行计算并加速训练过程，因为GPU拥有数百到数千个小型核心，这比CPU的几十个核心要多得多。这使得TensorFlow能够在GPU上实现更快的训练速度和更高的吞吐量，尤其是在处理大规模的深度学习任务时。\n另外需要注意的是，如果你的计算机没有安装专门的GPU，则无法使用TensorFlow GPU。在这种情况下，TensorFlow会使用CPU作为默认选项，但是训练过程会比在GPU上慢得多。因此，如果你需要进行大量的深度学习训练任务，建议使用具有至少一张GPU的计算机来加速训练。\n总之，TensorFlow GPU和CPU之间的区别在于它们的硬件架构、并行计算能力以及处理速度等方面。当进行大规模的深度学习训练时，使用GPU可以显著提高训练速度和吞吐量，而对于较小的任务或者没有专门GPU的计算机，则应该使用CPU。\ncuda和cuddn CUDA（Compute Unified Device Architecture）是一种由NVIDIA公司开发的并行计算平台和编程模型，它允许开发人员使用标准C/C++语言编写基于GPU的高性能应用程序。CUDA包括一个可编程的内核语言（CUDA C/C++），一个并行计算库（CUDA Toolkit），以及驱动程序和硬件架构，支持对NVIDIA GPU进行高性能并行计算。与CPU相比，GPU在并行处理任务时的性能要高得多，因此CUDA被广泛用于深度学习、科学计算和高性能计算等领域。[2]\ncuDNN（CUDA Deep Neural Network library）是NVIDIA CUDA的一个加速库，它提供了一组高度优化的本地函数，用于加速深度神经网络模型的训练和推理。cuDNN主要用于卷积神经网络（CNNs）和递归神经网络（RNNs）等深度学习模型的优化，从而实现更快的训练和推理速度。cuDNN支持多种深度学习框架，包括TensorFlow，PyTorch和Caffe等。[1]\n因此，CUDA是一种GPU计算平台和编程模型，cuDNN是其中一个加速库，专门用于加速深度学习模型的训练和推理。这两个技术结合起来，可以实现对GPU的高性能并行计算和深度学习模型的优化，从而提高深度学习任务的整体性能。\n不同的tensorflow版本需要不同的cuda和cuddn版本，google官网可查看 https://tensorflow.google.cn/install/source_windows?hl=en 如果电脑有gpu建议安装tensorflow-gpu,如果电脑没有gpu安装性能较差的tensorflow\n打开任务管理器-性能,查看你是否支持gpu 从这里我们可以看到我的cpu是英伟达(nvidia)的gtx1050 如果电脑已经安装显卡驱动，cuda肯定是自带的，我们可以使用nvidia-smi命令查看 tensorflow安装。 tensorflow版本 tensorflow-gpu需要至少4GB的GPU显存才能运行，并且在训练模型时需要大量的计算资源。而tensorflow-cpu则是专门为CPU设计的版本，能够在CPU上高效地运行，同时不需要GPU显存。一般pc电脑的环境中，建议使用tensorflow-cpu会更加适合。当然，如果你未来有升级GPU的计划，可以考虑使用tensorflow-gpu。\n安装Anaconda 首先我们安装Anaconda，教程参考： 打开命令行 同时这里建议使用anaconda的替代方案mamba，因为conda包多了之后安装超级慢，会让你崩溃的，mamba基本匹配pip的速度，官网。\n在window上打开powershell，执行web命令下载压缩包\nmkdir d:\\green\\mamba \u0026\u0026 cd d:\\green\\mamba\rInvoke-Webrequest -URI https://micro.mamba.pm/api/micromamba/win-64/latest -OutFile micromamba.tar.bz2\rtar xf micromamba.tar.bz2 比如我的下载这里，直接将解压路径添加到path环境变量中 其他命令就和conda一样了，比如\nmicromamba env list\rmicomamba activate base 创建python环境 在其他盘创建一个环境，假设使用python3.7版本\nconda create --prefix=d:\\condaenv\\env_name python=3.7 micromamba create --prefix=d:\\condaenv\\env_name python=3.7 -c conda-forge 切换\nactivate d:\\condaenv\\env_name\r(d:\\condaenv\\env_name) C:\\Users\\liaomin\u003epython --version\rPython 3.7.4 安装tensorflow-gpu 通过版本关系图 可以确定我们可以使用python3.7版本安装tensorflow-gpu 2.0.0以上所有版本，我们选择一个2.6.0\n#设置镜像源\rconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\rconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\rconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/\r#安装tensorflow-gpu\rconda install tensorflow-gpu==2.6.0\r#或者（建议用mamba）\rmicomamba install tensorflow-gpu==2.6.0\r#如果是cpu版本直接\rmicomamba install tensorflow==2.6.0 安装完成我们先不急着安装cuddn和cuda可以先写个helloworld测试下\npycharm配置 配置conda环境 创建一个项目pure python项目，点击project interpreter 选择Existing interpreter,点击右侧的.. 选择conda environment 然后点击ok 在interpreter下拉框中选择刚新建的那个\n编写一段helloworld代码\nimport tensorflow as tf\r# 创建一个常量张量\rx = tf.constant([1, 2, 3])\r# 创建一个变量张量\ry = tf.Variable([3, 2, 1])\r# 计算两个张量的和\rz = tf.add(x, y)\r# 输出结果\rprint(z.numpy()) 运行，虽然能输出结果[4 4 4]，但是有红色警告\n2023-05-06 16:43:35.610912: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r#下面这段是安装好cuda后报的错。\r2023-05-06 17:37:38.727999: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\r2023-05-06 17:37:38.728345: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\rSkipping registering GPU devices...\r2023-05-06 17:37:38.729310: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX AVX2\rTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 显然缺少cuda和cudnn，注意这里缺少cudart64_110.dll并不是说cuda就是110版本，实际你11.2安装后也是这个dll。\n配置juypternotebook 打开anaconda prompt，并且激活你新穿件的环境，安装jupyter\nconda install jupyter notebook\r#或者（建议用mamba）\rmicomamba install jupyter notebook 在pycharm中右键创建一个notebook 输入之前的helloword代码，选择运行或者调试 右侧输出结果\n安装cuda 通过版本关系图，我们知道tensorflow-gpu 2.6.0需要安装11.2的cuda\n如果是cpu版本无需安装\ncuda历史版本下载位置,下载对应版本安装 这里有三个版本选择最高的11.2.2 点击进入后 选择window版本 默认安装路径是C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA 打开电脑设置——\u003e系统——\u003e系统高级设置——\u003e环境变量——\u003e系统变量——\u003ePATH 将C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin加入到环境变量PATH中 cmd重新执行nvidia-smi，发现版本更新了 发现之前缺失的cudart64_110.dll确实在C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin 安装cudnn 通过版本关系图，我们知道tensorflow-gpu 2.6.0需要安装8.1的cudnn cudnn历史版本下载位置,下载对应版本8.1，下载cudnn需要注册nvidia 选择cudnn library for window(x86)点击下载 打开cudnn文件夹 将上述cudnn里面的文件移动或copy到cuda对应文件夹目录下即可！ 此时在运行helloworld程序使用gpu正常运行\n2023-05-06 19:01:23.403530: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX AVX2\rTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r2023-05-06 19:01:24.075663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1320 MB memory: -\u003e device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\r[4 4 4] 安装blas TensorFlow 是一个使用 C++ 编写的开源机器学习框架，它支持在 CPU 和 GPU 上运行计算。在 TensorFlow 中，BLAS（Basic Linear Algebra Subprograms）库是用于执行线性代数计算的关键库，例如矩阵乘法和向量加法。由于这些操作在机器学习中非常常见，因此 BLAS 库对于 TensorFlow 的性能和稳定性至关重要。 切换到你的python环境，使用以下命令来安装 BLAS 库：\nconda install blas\r#或者（建议用mamba）\rmicomamba install openblas 安装完成后，因为安装的是动态链接库最终仍然会安装在你的anaconda的base环境下，找到动态链接库的位置\nC:\\Users\\你的用户名\\anaconda3\\pkgs\\blas-2.116-blis\\Library\\bin 将上面的路径添加到环境变量PATH中，如果不添加tensorflow会报错找不到blas 执行以下代码测试\npython -c \"import tensorflow as tf; print(tf.linalg.matmul(tf.ones([1, 2]), tf.ones([2, 2])))\" 运行结果\n(d:\\condaenv\\env_name) C:\\Users\\liaomin\u003epython -c \"import tensorflow as tf; print(tf.linalg.matmul(tf.ones([1, 2]), tf.ones([2, 2])))\"\r2023-05-10 09:39:23.654612: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX AVX2\rTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r2023-05-10 09:39:24.372107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1320 MB memory: -\u003e device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\rtf.Tensor([[2. 2.]], shape=(1, 2), dtype=float32) 云服务器运行 云服务器提供高性能的CPU和GPU运算服务器，对个人开发者来说非常便宜一般几十块钱半个月的秒杀团很多，可以提供强大的计算能力。对于需要大量计算资源的TensorFlow程序，使用腾讯云服务器可以提高计算效率\n云服务器选择 我这里首选autodl（关机不扣费，gpu按小时收费，0.5-2块钱一个小时，而且数据集和镜像【chatglm镜像等】，cuda都安装好了）,其次是腾讯云（新用户有秒杀,15天60元还是划算的），对个人友好，可以微信登录，微信支付。 右侧的gpu服务器是gpu 8gb的适合个人学习来跑神经网络。 购买的时候选择系统为：unbuntu的tensorflow版本，我选择的是：TensorFlow 2.8.0 Ubuntu 18.04 GPU基础镜像（预装460驱动），不要选window自己安装环境，麻烦，因为我们的pycharm支持远程ssh编程，用服务器跑，结果显示在pycharm中。 重置你的ubuntu密码（系统默认的账号是ubuntu，不是root），主机会给你配个公网地址 pycharm配置 代码自动同步 我们在/homt/ubuntu用户目录下新建一个deeplearn目录用于映射本地代码 点击tools-deployment-configuation 选择sftp，输入账号密码测试连接 点击mappings目录映射本地目录和远程目录 确定后在享有右键deployments-\u003eupload 上传代码 点击tools-deployments-browe remote host查看远程目录是否上传（勾上Automatic upload(always)保存代码自动上传） 远程interpreter 点击File-Settings-Project（项目名）-Project interpreter add一个 输入密码后下一步进入配置python目录，我们可以使用shell登录到远程服务器执行\nubuntu@VM-0-5-ubuntu:~$ which python3\r/usr/local/miniconda3/bin//python3 在interpreter上输入python3的路径即可 确认项目选择了该interpreter 接下来打开神经网络代码,，代码右键运行，可看到运行是ssh运行的 执行后的模型实际上是在远程服务器的可以使用brwoer remote host右键下载下来覆盖本地",
    "description": "@[toc]\n简介 TensorFlow是一种端到端开源机器学习平台，它提供了一个全面而灵活的生态系统，包含各种工具、库和社区资源，能够助力研究人员推动先进机器学习技术的发展。在TensorFlow机器学习框架下，开发者能够轻松地构建和部署由机器学习提供支持的应用。[2]\nKeras是一个高层次神经网络 API，适用于快速构建原型、高级研究和生产。它作为TensorFlow的一个接口，可以兼容多种深度学习框架。Keras 的核心数据结构是 model，一种组织网络层的方式。最简单的模型是 Sequential 顺序模型，它由多个网络层线性堆叠。对于更复杂的结构，你应该使用 Keras 函数式 API，它允许构建任意的神经网络图。 Keras最开始是为研究人员开发的，其目的在于快速实验，具有相同的代码可以在CPU或GPU上无缝切换运行的特点，同时也具有用户友好的API，方便快速地开发深度学习模型的原型。 Keras使用类sklearn的api接口来调用tensorflow，从sklearn机器学习中切换过来，更加容易上手。 Tenforflow2.0后直接内置可keras。\n运行硬件 TensorFlow支持在CPU和GPU上运行。GPU（图形处理单元）是一种专门用于加速计算的硬件，它可以大大提高深度学习模型的训练速度。相对而言，CPU（中央处理器）的每个核心具有更强大的处理能力，但它们的数量通常非常有限，因此在处理大数据时它们表现不佳。\nTensorFlow GPU和CPU的主要区别在于如何使用硬件来处理计算任务，以及处理速度的差异。在CPU上，TensorFlow利用所有可用的CPU内核并将任务分配给它们，这可能需要几分钟或几小时来完成。在GPU上，TensorFlow使用CUDA（Compute Unified Device Architecture）技术来利用GPU进行并行计算并加速训练过程，因为GPU拥有数百到数千个小型核心，这比CPU的几十个核心要多得多。这使得TensorFlow能够在GPU上实现更快的训练速度和更高的吞吐量，尤其是在处理大规模的深度学习任务时。\n另外需要注意的是，如果你的计算机没有安装专门的GPU，则无法使用TensorFlow GPU。在这种情况下，TensorFlow会使用CPU作为默认选项，但是训练过程会比在GPU上慢得多。因此，如果你需要进行大量的深度学习训练任务，建议使用具有至少一张GPU的计算机来加速训练。\n总之，TensorFlow GPU和CPU之间的区别在于它们的硬件架构、并行计算能力以及处理速度等方面。当进行大规模的深度学习训练时，使用GPU可以显著提高训练速度和吞吐量，而对于较小的任务或者没有专门GPU的计算机，则应该使用CPU。\ncuda和cuddn CUDA（Compute Unified Device Architecture）是一种由NVIDIA公司开发的并行计算平台和编程模型，它允许开发人员使用标准C/C++语言编写基于GPU的高性能应用程序。CUDA包括一个可编程的内核语言（CUDA C/C++），一个并行计算库（CUDA Toolkit），以及驱动程序和硬件架构，支持对NVIDIA GPU进行高性能并行计算。与CPU相比，GPU在并行处理任务时的性能要高得多，因此CUDA被广泛用于深度学习、科学计算和高性能计算等领域。[2]\ncuDNN（CUDA Deep Neural Network library）是NVIDIA CUDA的一个加速库，它提供了一组高度优化的本地函数，用于加速深度神经网络模型的训练和推理。cuDNN主要用于卷积神经网络（CNNs）和递归神经网络（RNNs）等深度学习模型的优化，从而实现更快的训练和推理速度。cuDNN支持多种深度学习框架，包括TensorFlow，PyTorch和Caffe等。[1]\n因此，CUDA是一种GPU计算平台和编程模型，cuDNN是其中一个加速库，专门用于加速深度学习模型的训练和推理。这两个技术结合起来，可以实现对GPU的高性能并行计算和深度学习模型的优化，从而提高深度学习任务的整体性能。\n不同的tensorflow版本需要不同的cuda和cuddn版本，google官网可查看 https://tensorflow.google.cn/install/source_windows?hl=en 如果电脑有gpu建议安装tensorflow-gpu,如果电脑没有gpu安装性能较差的tensorflow\n打开任务管理器-性能,查看你是否支持gpu 从这里我们可以看到我的cpu是英伟达(nvidia)的gtx1050 如果电脑已经安装显卡驱动，cuda肯定是自带的，我们可以使用nvidia-smi命令查看",
    "tags": [],
    "title": "深度学习01-tensorflow开发环境搭建",
    "uri": "/docs/programming/ai/deep_learning/frameworks/dl_01_tensorflow/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 深度基础",
    "content": "神经网络 简介 神经网络是一种基于生物神经系统结构和功能特点而设计的人工神经网络模型，具有很强的自适应性和非线性映射能力。神经网络由多个神经元（或称节点）组成，这些神经元通过连接权重相互连接，构成多层的网络结构。每个神经元接收到来自其它神经元的信号，并将这些信号加权线性组合后通过激活函数进行非线性转换，最终输出给下一层神经元或输出层。\n学习机器学习后，学习神经网络可以帮助你更深入地理解模式识别和人工智能领域的基础知识。神经网络在很多领域都有广泛的应用，例如计算机视觉、自然语言处理、语音识别等。学习神经网络可以让你掌握这些领域中最前沿的技术，并且能够应用这些技术来解决具体的问题。同时，神经网络的学习方法和算法也是机器学习的重要组成部分，学习神经网络可以帮助你更好地理解机器学习的原理和技术，从而更好地应用机器学习来解决实际问题。\n学习路径 如果你已经学过机器学习，那么开始学习神经网络，可以从多层感知器（Multilayer Perceptron，简称 MLP）神经网络入手。 MLP 是最基本的神经网络模型之一，它的结构比较简单，易于理解和实现，同时又有很好的可扩展性和通用性，可以应用于分类、回归等多种任务。学习 MLP 之后，你可以进一步学习卷积神经网络（Convolutional Neural Networks，简称 CNN）和循环神经网络（Recurrent Neural Networks，简称 RNN），它们分别用于计算机视觉和自然语言处理等特定领域的问题。总之，建议先从 MLP 入手，逐渐深入学习其他类型的神经网络。\n分类 神经网络可以分为多种不同的类型，下面列举一些常见的神经网络类型：\n前馈神经网络（Feedforward Neural Network）：前馈神经网络是最基本的神经网络类型，也是深度学习中最常见的神经网络类型。它由若干个神经元按照一定的层次结构组成，每个神经元接收上一层的输出，产生本层的输出，从而实现信息的传递和处理。\n卷积神经网络（Convolutional Neural Network）：卷积神经网络是一种专门用于图像处理和计算机视觉任务的神经网络类型。它通过卷积和池化等操作，可以提取图像中的特征，从而实现图像分类、目标检测、图像分割等任务。\n循环神经网络（Recurrent Neural Network）：循环神经网络是一种能够处理序列数据的神经网络类型。它通过记忆单元和门控机制等方式，可以处理任意长度的序列数据，从而实现自然语言处理、语音识别等任务。\n自编码器（Autoencoder）：自编码器是一种无监督学习的神经网络类型，它的目标是将输入数据进行压缩和解压缩，从而实现特征提取和降维等任务。\n深度置信网络（Deep Belief Network）：深度置信网络是一种由多个受限玻尔兹曼机组成的神经网络类型。它可以通过逐层贪心预训练和微调等方式，实现高效的特征学习和分类任务。\n除了以上列举的几种神经网络类型，还有众多其他的神经网络类型，如反向传播神经网络、Hopfield网络、Boltzmann机等。不同的神经网络类型适用于不同的任务和数据类型，需要根据具体的问题选择合适的神经网络类型。\n多层感知器（MLP） MLP神经网络属于前馈神经网络（Feedforward Neural Network）的一种。在网络训练过程中，需要通过反向传播算法计算梯度，将误差从输出层反向传播回输入层，用于更新网络参数。这个过程中需要使用反向传播算法来计算梯度，并且在某些类型的神经网络中，例如循环神经网络（RNN），也存在反馈回路。除了MLP，其他常见的前馈神经网络包括卷积神经网络（CNN）和循环神经网络（RNN）等。\n神经网络认识 我们以一个简单的例子来认识神经网络，只是为了理解其中的一些概念。 我们已知四个数据点(1,1)(-1,1)(-1,-1)(1,-1)，这四个点分别对应I~IV象限（也就是数据属于的类别），如果这时候给我们一个新的坐标点（比如(2,2)），那么它应该属于哪个象限呢？（没错，当然是第I象限，但我们的任务是要让机器知道，机器不知道有象限这个东西啊，他只能根据历史数据的经验推断），如果机器只是知道一堆数据 比如(-2,3)属于2象限，机器就需要通过这些数据总结出一个特征，这个特征可能就是根据x和y坐标的正负来判断象限了。 两层神经网络 这里我们构建一个两层神经网络，理论上两层神经网络已经可以拟合任意函数。这个神经网络的结构如下图： 首先我们去掉途中难懂的东西 输入层 在我们的例子中，输入层是坐标值，例如（1,1），这是一个包含两个元素的数组，也可以看作是一个12的矩阵。输入层的元素维度与输入量的特征息息相关，如果输入的是一张3232像素的灰度图像，那么输入层的维度就是32*32。 因为整个神经网络的目的是为了训练出一个模型，所以输入的是历史数据，历史数据有一个确定的输出label，模型出来后，直接使用模型就可以分类出输入的数据的输出 这里输入的数据为:\n[\r[1,1],\r[-1,1],\r[-1,-1],\r[1,-1]\r] 从输入层到隐藏层 连接输入层和隐藏层的是W1和b1。由X计算得到H十分简单，就是矩阵运算： $H=wx+b$ 果你学过线性代数，对这个式子一定不陌生,可以理解为w是一个权重(权重越高，这个特征也就越重要)，b是一个偏置，如果有多个特征那么就有个w，还记得$w^T*x$。 如上图中所示，在设定隐藏层为50维（也可以理解成50个神经元）之后，矩阵H的大小为（450）的矩阵。 也就是说50个神经元就是一个矩阵50个特征，每一行就是他的w值，这里输入层总共两个维度，所有只有w1和w2，b值这里就不说了，假设为0 这里我们可以简化最终得到4*50的意义 从隐藏层到输出层 连接隐藏层和输出层的是W2和b2,输入就是隐藏层输入的H值。同样是通过矩阵运算进行的： $Y=w2H+b2$ 最终输出层，最终是4个象限 H是450的矩阵，输出层的w2矩阵就是个504,最终得到一个44的矩阵 这里不详细画图了，大概意义如下。\nH是4*50的矩阵其实是一个列是神经元50个，行是4个数据集的经过第一轮计算的输出值H，隐藏层的目的就是计算出一个H值。 输出层的 w矩阵是50*4，目的是为了将50个神经元压缩到4个输出特征，也就是每一个数据集在4个象限的概率。所以最终输出是4*4 激活层 通过上述两个线性方程的计算，我们就能得到最终的输出Y了，但是如果你还对线性代数的计算有印象的话，应该会知道：一系列线性方程的运算最终都可以用一个线性方程表示。也就是说，上述两个式子联立后可以用一个线性方程表达。对于两次神经网络是这样，就算网络深度加到100层，也依然是这样。这样的话神经网络就失去了意义。 所以这里要对网络注入灵魂：激活层。 简而言之，激活层是为矩阵运算的结果添加非线性的 具体为什么需要，请view：https://blog.csdn.net/liaomin416100569/article/details/130597944?spm=1001.2014.3001.5501\n激活层是神经网络中的一种层，其作用是在输入信号和输出信号之间添加一个非线性的转换函数，使得网络可以更好地学习和表示复杂的非线性关系。激活层的意义在于增加模型的非线性表达能力，使得神经网络可以更好地处理复杂的输入数据，例如图像、文本和语音等。激活函数的选择也非常重要，不同的激活函数具有不同的特点。 激活层常用的激活函数三种，分别是阶跃函数、Sigmoid和ReLU，如下图： 阶跃函数：当输入小于等于0时，输出0；当输入大于0时，输出1。 Sigmoid：当输入趋近于正无穷/负无穷时，输出无限接近于1/0。 ReLU：当输入小于0时，输出0；当输入大于0时，输出等于输入。 其中，阶跃函数输出值是跳变的，且只有二值，较少使用；Sigmoid函数在当x的绝对值较大时，曲线的斜率变化很小（梯度消失），并且计算较复杂；ReLU是当前较为常用的激活函数。\n激活函数具体是怎么计算的呢？ 假如经过公式H=X*W1+b1计算得到的H值为：(1,-2,3,-4,7…)，那么经过阶跃函数激活层后就会变为(1,0,1,0,1…)，经过ReLU激活层之后会变为(1,0,3,0,7…)。\n需要注意的是，每个隐藏层计算（矩阵线性运算）之后，都需要加一层激活层，要不然该层线性计算是没有意义的。 此时的神经网络变成了如下图所示的形式： 我们都知道（？）神经网络是分为“训练”和“使用”两个步骤的。如果是在“使用”的步骤，图4就已经完成整个过程了，在求得的Y（大小为4*4）矩阵中，当前样本数值最大的就代表着当前分类。\n但是对于用于“训练”的网络，上图还远远不够。起码当前的输出Y，还不够“漂亮”。\n输出的正规化 假设某个样本输出Y的值可能会是(3,1,0.1,0.5)这样的矩阵，诚然我们可以找到里边的最大值“3”，从而找到对应的分类为I，但是这并不直观。我们想让最终的输出为概率，也就是说可以生成像(90%,5%,2%,3%)这样的结果，这样做不仅可以找到最大概率的分类，而且可以知道各个分类计算的概率值。 具体是怎么计算的呢？ 计算公式如下： 简单来说分三步进行：（1）以e为底对所有元素求指数幂；（2）将所有指数幂求和；（3）分别将这些指数幂与该和做商。\n这样求出的结果中，所有元素的和一定为1，而每个元素可以代表概率值。\n我们将使用这个计算公式做输出结果正规化处理的层叫做“Softmax”层。此时的神经网络将变成如下图所示： 如何衡量输出的好坏 通过Softmax层之后，我们得到了I，II，III和IV这四个类别分别对应的概率，但是要注意，这是神经网络计算得到的概率值结果，而非真实的情况。\n比如，Softmax输出的结果是(90%,5%,3%,2%)，真实的结果是(100%,0,0,0)。虽然输出的结果可以正确分类，但是与真实结果之间是有差距的，一个优秀的网络对结果的预测要无限接近于100%，为此，我们需要将Softmax输出结果的好坏程度做一个“量化”。\n一种直观的解决方法，是用1减去Softmax输出的概率，比如1-90%=0.1。不过更为常用且巧妙的方法是，求对数的负数。\n还是用90%举例，对数的负数就是：-log0.9=0.046\n可以想见，概率越接近100%，该计算结果值越接近于0，说明结果越准确，该输出叫做“交叉熵损失（Cross Entropy Error）”。\n我们训练神经网络的目的，就是尽可能地减少这个“交叉熵损失”。 反向传播与参数优化 上面的过程其实就是神经网络的正向传播过程 ，一句话复习一下：神经网络的传播都是形如Y=WX+b的矩阵运算；为了给矩阵运算加入非线性，需要在隐藏层中加入激活层；输出层结果需要经过Softmax层处理为概率值，并通过交叉熵损失来量化当前网络的优劣。\n算出交叉熵损失后，就要开始反向传播了。其实反向传播就是一个参数优化的过程，优化对象就是网络中的所有W和b（因为其他所有参数都是确定的）。\n神经网络的神奇之处，就在于它可以自动做W和b的优化，在深度学习中，参数的数量有时会上亿，不过其优化的原理和我们这个两层神经网络是一样的。\n神经网络需要反复迭代。如上述例子中，第一次计算得到的概率是90%，交叉熵损失值是0.046；将该损失值反向传播，使W1,b1,W2,b2做相应微调；再做第二次运算，此时的概率可能就会提高到92%，相应地，损失值也会下降，然后再反向传播损失值，微调参数W1,b1,W2,b2。依次类推，损失值越来越小，直到我们满意为止。\n此时我们就得到了理想的W1,b1,W2,b2。\n具体参考 BP算法怎推导章节\n内容参考：https://zhuanlan.zhihu.com/p/65472471\n过拟合 Dropout是一种在神经网络中用于防止过拟合的技术。它是通过在训练期间随机将一些节点的输出设置为0来实现的。具体来说，每个节点有一定的概率被“关闭”，即其输出被设置为0。这样，节点之间的连接就会被随机断开，从而迫使网络学习更加鲁棒的特征，而不是依赖特定的节点或连接。这种随机性可以被看作是一种正则化技术，可以有效地防止过拟合。\n过拟合是指模型在训练数据上表现很好，但在测试数据上表现不佳的现象。这通常是由于模型过于复杂，而训练数据又过少或过于噪声导致的。通过使用Dropout技术，我们可以减少模型的复杂度，并使其更加适应不同的训练数据。这样，我们就可以更好地泛化模型，从而在测试数据上获得更好的表现。\n假设我们有一个二分类任务，需要从图像中识别猫和狗。我们使用卷积神经网络进行训练，但由于数据集较小，容易出现过拟合的问题。\n为了解决这个问题，我们可以在卷积神经网络中添加Dropout层。例如，我们可以在全连接层之前添加一个Dropout层，将其输出概率设置为0.5。这意味着在每个训练批次中，该层中的一半节点的输出将被随机设置为0。这样，网络就不会过于依赖特定节点或连接，并且可以更好地适应不同的训练数据。\nBP算法推导 定义 首先来一个反向传播算法的定义（转自维基百科）：反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。 该方法对网络中所有权重计算损失函数的梯度。 这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。（误差的反向传播）\n算法讲解 如果去问一下了解BP算法的人“BP算法怎推导？”，大概率得到的回答是“不就是链式求导法则嘛”，我觉得这种答案对于提问题的人来说没有任何帮助。BP的推导需要链式求导不错，但提问者往往想得到的是直观的回答，毕竟理解才是王道。直观的答案，非图解莫属了。 注：下图的确是反向传播算法，但不是深度学习中的backprop，不过backward的大体思想是一样的，毕竟误差没法从前往后计算啊。（在深度学习中操作的是计算图—Computational graph），如果暂时不理解上面那句话，你可以当我没说过，不要紧~（手动?）\n下面通过两组图来进行神经网络前向传播和反向传播算法的讲解，第一组图来自国外某网站，配图生动形象。如果对你来说，单纯的讲解理解起来比较费劲，那么可以参考第二组图——一个具体的前向传播和反向传播算法的例子。相信就算是刚刚入门的小白（只要有一点点高等数学基础知识），也一定可以理解反向传播算法！\n首先拿一个简单的三层神经网络来举例，如下： 每个神经元由两部分组成，第一部分（e）是输入值和权重系数乘积的和，第二部分（f(e)）是一个激活函数（非线性函数）的输出， y=f(e)即为某个神经元的输出，如下： 前向传播 第一层神经网络传播\n其中$w_{x1}1$表示x1对应第一个神经元的w值，$w_{x2}1$，表示x2对应对一个神经元的w值。\n第二层神经网络传播 第三层神经网络传播 反向传播 到这里为止，神经网络的前向传播已经完成，最后输出的y就是本次前向传播神经网络计算出来的结果（预测结果），但这个预测结果不一定是正确的，要和真实的标签（z）相比较，计算预测结果和真实标签的误差（$\\delta$），如下： 下面开始计算每个神经元的误差（$\\delta$） 计算第一层误差 下面开始利用反向传播的误差，计算各个神经元（权重）的导数，开始反向传播修改权重 计算第二次的w 计算第三层 到此为止，整个网络的前向，反向传播和权重更新已经完成\n具体实例 就算上面的所有东西你都看的迷迷糊糊，通过下面的例子，相信绝大多数人也能很轻松的理解BP算法。如图是一个简单的神经网络用来举例： 下面是前向（前馈）运算（激活函数为sigmoid）： 下面是反向传播（求网络误差对各个权重参数的梯度）：\n我们先来求最简单的，求误差E对w5的导数。首先明确这是一个“链式求导”过程，要求误差E对w5的导数，需要先求误差E对out o1的导数，再求out o1对net o1的导数，最后再求net o1对w5的导数，经过这个链式法则，我们就可以求出误差E对w5的导数（偏导），如下图所示： 导数（梯度）已经计算出来了，下面就是反向传播与参数更新过程： 上面的图已经很显然了，如果还看不懂真的得去闭门思过了（开玩笑~），耐心看一下上面的几张图，一定能看懂的。\n如果要想求误差E对w1的导数，误差E对w1的求导路径不止一条，这会稍微复杂一点，但换汤不换药，计算过程如下所示： bp推导参考：https://blog.csdn.net/ft_sunshine/article/details/90221691\ntensorflow实战 加载数据集 keras.datasets.mnist 是 Keras 框架内置的一个手写数字数据集，包含了 60,000 张训练图片和 10,000 张测试图片。每张图片都是 28x28 像素的灰度图像，每个像素的取值范围为 0 到 255。该数据集常用于机器学习领域中的图像分类和数字识别任务。\nkeras.datasets.mnist 的返回值是一个元组 (x_train, y_train), (x_test, y_test)，分别表示训练集和测试集。其中 x_train 和 x_test 分别是形状为 (60000, 28, 28) 和 (10000, 28, 28) 的 numpy 数组，表示图像数据。y_train 和 y_test 则是形状为 (60000,) 和 (10000,) 的 numpy 数组，表示对应的图像标签，即每张图片所代表的数字。 记载数据集，并绘制前20张图片\n#%%\rfrom tensorflow.keras.datasets import mnist\rimport matplotlib.pyplot as plt\r# 加载数据集\r(x_train, y_train), (x_test, y_test) = mnist.load_data()\rx_test_ori=x_test # 打印数据集信息\rprint('训练集图像数据形状：', x_train.shape)\rprint('训练集标签数据形状：', y_train.shape)\rprint('测试集图像数据形状：', x_test.shape)\rprint('测试集标签数据形状：', y_test.shape)\r# 绘制前20张训练集图像\rplt.figure(figsize=(10, 10))\rfor i in range(20):\rplt.subplot(5, 5, i+1)\rplt.xticks([])\rplt.yticks([])\rplt.grid(False)\rplt.imshow(x_train[i], cmap=plt.cm.binary)\rplt.xlabel(y_train[i])\rplt.show() 输出\n训练集图像数据形状： (60000, 28, 28)\r训练集标签数据形状： (60000,)\r测试集图像数据形状： (10000, 28, 28)\r测试集标签数据形状： (10000,) 默认图片下载路径在 ~/.keras/datasets ，window下：C:\\Users\\当前用户名.keras\\datasets,大小估计10MB左右。\n数据预处理 x_train = x_train.reshape(x_train.shape[0], 784).astype('float32') / 255\rx_test = x_test.reshape(x_test.shape[0], 784).astype('float32') / 255\ry_train = keras.utils.to_categorical(y_train, 10)\ry_test = keras.utils.to_categorical(y_test, 10) 在上面的代码中，我们将输入数据的维度从 28x28 转换为 784，因为我们处理的数据一般都是一个矩阵，一行代表一个数据样本，需要转换成784*1的数据，并将像素值的范围从 0-255 缩放到 0-1 之间。同时，我们将标签数据进行 one-hot 编码，将其转换为一个 10 维的向量，每个维度代表一个数字。\none-host编码 One-hot编码是一种将离散型变量转换为连续型变量的技术，在机器学习和深度学习中广泛应用。它将每个离散型变量的取值都编码为一个二进制位，其中只有一个二进制位为1，其余二进制位为0。举例说明如下：\n假设有一个离散型变量“颜色”，它的可能取值为“红色”、“黄色”和“蓝色”。我们可以将这三个取值编码为长度为3的二进制向量，如下所示：\n红色：[1, 0, 0]\n黄色：[0, 1, 0]\n蓝色：[0, 0, 1]\n这个编码方式就是one-hot编码。在机器学习中，我们可以使用这个编码方式来处理离散型变量，使其成为连续型变量，方便模型的学习和使用。\nkeras.utils.to_categorical() keras.utils.to_categorical()函数将整数型的类别标签转换成了独热编码（one-hot encoding）的形式。在独热编码中，每个类别标签被表示为一个长度等于类别总数的向量，其中该类别标签所对应的位置值为1，其余位置为0。\n对于手写数字识别任务，共有10个类别，即数字0到9，因此需要将标签向量转换为10维的独热编码。\n例如，如果原始标签为5，则转换后的独热编码为[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]，其中第6个位置（从0开始）的值为1，表示原始标签为5。\n这样做的目的是为了让神经网络更好地理解类别之间的差异和相似性，以便更准确地进行分类预测。\n构造多层感知器模型 我们使用 keras.Sequential 构建模型，该模型包含一个输入层、一个隐藏层和一个输出层。输入层的维度为 784（即每个图片的像素数），隐藏层包含 512 个神经元，激活函数为 ReLU，输出层包含 10 个神经元，激活函数为 softmax。同时，我们使用 Dropout 防止过拟合。\n# 构建模型\rmodel = tf.keras.Sequential([\rtf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\rtf.keras.layers.Dropout(0.2),\rtf.keras.layers.Dense(10, activation='softmax')\r]) tf.keras.Sequential keras.Sequential是Keras中的一个类，用于快速搭建神经网络模型。它提供了一个简单的方法来创建顺序模型，即一系列层按照顺序堆叠在一起的模型。在keras.Sequential中，可以通过添加层的方式来搭建神经网络。\nkeras.Sequential的定义如下：\nkeras.Sequential(\rlayers=None, name=None\r) 其中，layers是一个列表，包含了按照顺序堆叠在一起的层；name是模型的名称。 上面使用keras.Sequential创建简单神经网络的例子：\nmodel = tf.keras.Sequential([\rtf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\rtf.keras.layers.Dropout(0.2),\rtf.keras.layers.Dense(10, activation='softmax')\r]) 我们创建了一个包含三层的神经网络模型。第一层是一个全连接层，包含512个神经元，使用ReLU激活函数，输入形状为(784,)。第二层是一个Dropout层，第三层是一个全连接层包含10个神经元，使用Softmax激活函数。\nkeras.layers.Dense keras.layers.Dense是Keras中的一个类，用于创建全连接层。全连接层是神经网络中最基本的一种层，它的每一个神经元都与上一层的每一个神经元相连。keras.layers.Dense可以用于创建输入层、输出层和隐藏层。\nkeras.layers.Dense的定义如下：\nkeras.layers.Dense(\runits, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs\r) 其中，units表示该层的神经元数量；activation表示该层的激活函数；use_bias表示是否使用偏置；kernel_initializer表示权重矩阵的初始化方法；bias_initializer表示偏置向量的初始化方法；kernel_regularizer、bias_regularizer、activity_regularizer表示正则化项；kernel_constraint、bias_constraint表示约束项。\n下面是一个使用keras.layers.Dense创建全连接层的例子：\nimport tensorflow.keras as keras\rlayer = tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)), 在这个例子中，我们创建了一个包含512个神经元的全连接层。激活函数为ReLU，输入形状为(784,)，表示该层的输入数据是一个长度为784的向量。\nkeras.layers.Dense的一些常用参数和方法：\nunits：该层的神经元数量； activation：该层的激活函数； use_bias：是否使用偏置； kernel_initializer：权重矩阵的初始化方法； bias_initializer：偏置向量的初始化方法； kernel_regularizer、bias_regularizer、activity_regularizer：正则化项； kernel_constraint、bias_constraint：约束项； layer.get_weights()：获取该层的权重矩阵和偏置向量； layer.set_weights(weights)：设置该层的权重矩阵和偏置向量。 以上就是keras.layers.Dense的一些基本信息和使用方法。 keras.layers.Dropout tf.keras.layers.Dropout是一种在神经网络中应用的正则化方法，用于减少过拟合的影响。在训练期间，Dropout层会随机地将输入张量的一部分元素设置为0，从而强制网络学习更健壮的特征表示，防止过拟合。具体来说，Dropout层以一定的概率（通常为0.5）随机地将输入张量的一部分神经元输出设为0，这些被屏蔽的神经元将不会参与前向传播和反向传播。这样做可以强制网络在训练过程中学习到更多的特征，并且使得网络对于输入的微小变化更加稳健。\n在tf.keras.layers.Dropout中，可以设置一个rate参数，来控制屏蔽神经元的比例，即随机将输入张量的多少个元素置为0。具体来说，如果rate=0.5，则代表在训练过程中随机选取50%的神经元输出为0，而在测试过程中不会进行任何操作。同时，可以将tf.keras.layers.Dropout层放在神经网络的任何位置，通常放在全连接层之后，以减少过拟合的影响。\nDropout层的主要作用是减少过拟合的影响，从而提高模型的泛化能力。通过随机屏蔽部分神经元，Dropout层可以强制网络学习到更健壮的特征表示，并且使得网络对于输入的微小变化更加稳健。这样可以增加模型的鲁棒性，提高模型的泛化能力，从而使得模型在测试集上表现更好。\n需要注意的是，在测试过程中不应该使用Dropout层，因为测试过程需要对整个模型进行前向传播，而不是将部分神经元置为0。因此，在测试过程中，需要将所有的神经元都参与前向传播，以获得更准确的预测结果。为了解决这个问题，可以在训练过程中使用Dropout层，并在测试过程中关闭Dropout层，或者根据Dropout的特性对输出进行调整。\nkeras.layers.其他 keras.layers 模块提供了许多常见的神经网络层类，其中一些常用的层包括：\nDense：全连接层，每个输入节点都连接到输出节点 Conv2D：二维卷积层，对图像或其他二维输入进行卷积运算 MaxPooling2D：二维最大池化层，对输入进行下采样 Dropout：随机丢弃一部分输入节点，以减少过拟合 Flatten：将输入展平为一维张量 Activation：激活函数层，如ReLU、Sigmoid、Softmax等 BatchNormalization：批量归一化层，用于加速收敛和减少过拟合 Embedding：词嵌入层，将离散的词转换为连续的向量表示 LSTM：长短时记忆循环层，用于处理时间序列数据 GRU：门控循环单元层，用于处理时间序列数据 这些层可以通过组合或堆叠来构建复杂的神经网络模型。除了这些常用的层外，keras.layers 还提供了许多其他层，如 Conv1D、Conv3D、UpSampling2D、SeparableConv2D、GlobalMaxPooling2D、GlobalAveragePooling2D等等。可以根据具体的任务需求选择合适的层。\n模型编译 model.compile(optimizer='adam',\rloss='categorical_crossentropy',\rmetrics=['accuracy']) 这段代码是用来编译模型的，其中包含了三个参数：\noptimizer：优化器，用来控制模型的学习速率。在这里，我们使用了Adam优化器，这是目前被广泛使用的一种优化器，它可以自适应地调整学习速率。\nloss：损失函数，用来衡量模型在训练过程中的误差。在这里，我们使用了交叉熵损失函数，它适用于多分类问题，能够有效地衡量模型预测结果与真实标签之间的差异。\nmetrics：评价指标，用来评价模型的性能。在这里，我们使用了准确率作为评价指标，它可以衡量模型在测试集上的分类精度。\n总的来说，这段代码的作用是为模型指定优化器、损失函数和评价指标，以便在训练过程中使用。\n优化器 常用的优化器有以下几种：\n随机梯度下降（Stochastic Gradient Descent，SGD）：是最基础、最简单的优化器，通过不断迭代来寻找最优解，对应字符串为：‘sgd’。\nAdam：是目前最广泛使用的优化器之一，结合了Adagrad和RMSprop的优点，对应字符串为：‘adam。\nAdagrad：自适应地调整每个参数的学习率，适用于稀疏数据集，对应字符串为：‘adagrad。\nRMSprop：与Adagrad类似，但是对梯度的平方进行指数加权移动平均，能够更好地适应非平稳目标函数，对应字符串为：‘rmsprop。\nAdadelta：结合了Adagrad和RMSprop的优点，同时解决了Adagrad学习率下降快的问题，对应字符串为：‘adadelta。\nAdamax：是Adam的一种变体，使用了L∞范数来代替L2范数，对应字符串为：‘adamax。\nNadam：是Adam和Nesterov动量的结合体，能够更好地适应凸函数和非凸函数，对应字符串为：’nadam。\n以上是常用的优化器，每种优化器都有其优点和缺点，选择合适的优化器需要根据具体的场景和任务来进行选择。\n损失函数 常见的损失函数包括：\n均方误差（Mean Squared Error，MSE）：该损失函数常用于回归问题，计算预测值与真实值之间的差平方的平均值。 在Keras中的字符串表示为：‘mse’\n交叉熵损失函数（Cross Entropy Loss，CE）：该损失函数常用于分类问题，通过计算预测值和真实值之间的交叉熵来衡量模型的拟合能力。 在Keras中的字符串表示为：‘categorical_crossentropy’（用于多分类问题）或’binary_crossentropy’（用于二分类问题）。\n对数损失函数（Logarithmic Loss，LogLoss）：该损失函数常用于二分类问题，通过计算预测值和真实值之间的对数损失来衡量模型的拟合能力。 在Keras中的字符串表示为：‘binary_crossentropy’\nHinge损失函数：该损失函数常用于支持向量机（SVM）模型中，通过计算预测值和真实值之间的Hinge损失来衡量模型的拟合能力。 在Keras中的字符串表示为：‘hinge’\nHuber损失函数：该损失函数常用于回归问题，通过计算预测值和真实值之间的平滑L1损失来衡量模型的拟合能力。 在Keras中的字符串表示为：‘huber_loss’\n当然，这些只是常见的损失函数，还有其他的损失函数，比如Focal Loss等。\n评价指标 常见的评价指标包括：\n准确率（Accuracy）：该指标用于分类问题，表示模型正确分类的样本数占总样本数的比例。 在Keras中的字符串表示为：‘accuracy’\n精确率（Precision）：该指标用于分类问题，表示模型正确预测为正类的样本数占预测为正类的样本总数的比例。 在Keras中的字符串表示为：‘precision’\n召回率（Recall）：该指标用于分类问题，表示模型正确预测为正类的样本数占真实为正类的样本总数的比例。 在Keras中的字符串表示为：‘recall’\nF1分数（F1 Score）：该指标综合了精确率和召回率，是二者的调和平均数，可以更全面地评估模型的性能。 在Keras中的字符串表示为：‘f1_score’\n均方误差（Mean Squared Error，MSE）：该指标用于回归问题，表示模型预测值与真实值之间的平均平方误差。 在Keras中的字符串表示为：‘mse’\n平均绝对误差（Mean Absolute Error，MAE）：该指标用于回归问题，表示模型预测值与真实值之间的平均绝对误差。 在Keras中的字符串表示为：‘mae’\n当然，这些只是常见的评价指标，还有其他的评价指标，比如AUC等。\n模型训练 history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test)) 在上面的代码中，我们使用 model.fit 进行模型训练，设置了 10 个 epochs 和 128 个批次大小。同时，我们使用测试集进行模型验证。\n在每个epoch中，模型需要对整个训练数据集进行训练，而不是仅仅针对一个样本或一个batch进行训练。因此，在每个epoch中，模型需要对所有训练样本进行前向传播和反向传播，以计算出每个样本对应的误差和梯度，并使用这些梯度更新模型的权重参数。\n为了加快模型训练的速度，通常会将训练数据集分成多个batch，每个batch包含若干个样本。在每个epoch中，模型会将整个训练数据集分成多个batch，然后对每个batch进行前向传播和反向传播，以更新权重参数。因此，在每个epoch中，模型需要进行多次前向传播和反向传播，才能完成对整个训练数据集的训练。\n如果batch_size=128，在一次epochs中数据被拆成了128份，每一份都和512个神经元进行正向和反向传播进行梯度下降修正w和b，所以一次epochs，实际上进行了128次的梯度下降算法 如果设置成10个epoch，可以理解为128*10次梯度下降算法。 像手写数字识别的，数据进行两次epoch,进行256次梯度下降，准确率就达到97%了\n注意别6w个样本一批次处理，内存罩不住啊，可能执行1个epochs都要几个小时。\n模型评估 # 评估模型\rtest_loss, test_acc = model.evaluate(x_test, y_test)\rprint('Test accuracy:', test_acc)\r# 预测结果\rpredictions = model.predict(x_test) 在上面的代码中，我们使用 model.evaluate 对模型进行评估，并使用 model.predict 进行预测。\n如果运行过程中报错\nInternalError: Attempting to perform BLAS operation using StreamExecutor without BLAS support [[node sequential/dense/MatMul (defined at C:\\Users\\liaomin\\AppData\\Local\\Temp\\ipykernel_28392\\2909523142.py:25) ]] [Op:__inference_test_function_361]\n直接安装即可,gpu版本tensorflow需要blas库，cpu版本不需要\nconda install blas 模型预测 选择某个测试元素然后将图片显示出来，并使用模型预测\n# 预测第10个测试数据的结果\rpredictions = model.predict(np.array([x_test[9]]))\rprint(\"预测结果：\", np.argmax(predictions))\r# 绘制第10个测试数据的图形\rplt.imshow(x_test_ori[9], cmap=plt.cm.binary)\rplt.show() 完整运行 #%%\r#%%\rimport tensorflow as tf\rfrom tensorflow.keras.datasets import mnist\rimport numpy as np\rimport matplotlib.pyplot as plt\r# 加载数据集\r(x_train, y_train), (x_test, y_test) = mnist.load_data()\rx_test_ori=x_test\r# 数据预处理\rx_train = x_train.reshape(x_train.shape[0], 784).astype('float32') / 255\rx_test = x_test.reshape(x_test.shape[0], 784).astype('float32') / 255\ry_train = tf.keras.utils.to_categorical(y_train, 10)\ry_test = tf.keras.utils.to_categorical(y_test, 10)\r# 构建模型\rmodel = tf.keras.Sequential([\rtf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\rtf.keras.layers.Dropout(0.2),\rtf.keras.layers.Dense(10, activation='softmax')\r])\r# 编译模型\rmodel.compile(optimizer='adam',\rloss='categorical_crossentropy',\rmetrics=['accuracy'])\r# 训练模型\rhistory = model.fit(x_train, y_train, epochs=2, #这里为了节约时间，就两轮就差不多了97%正确率了，训练十次差不多0.98左右\rbatch_size=128, validation_data=(x_test, y_test))\r# 评估模型\rtest_loss, test_acc = model.evaluate(x_test, y_test)\rprint('Test accuracy:', test_acc)\r# 预测第10个测试数据的结果\rpredictions = model.predict(np.array([x_test[9]]))\rprint(\"预测结果：\", np.argmax(predictions))\r# 绘制第10个测试数据的图形\rplt.imshow(x_test_ori[9], cmap=plt.cm.binary)\rplt.show() 输出结果\nEpoch 1/2\r1/469 [..............................] - ETA: 2:17 - loss: 2.2535 - accuracy: 0.1562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r13/469 [..............................] - ETA: 1s - loss: 1.3687 - accuracy: 0.6358 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r25/469 [\u003e.............................] - ETA: 1s - loss: 0.9935 - accuracy: 0.7331\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r38/469 [=\u003e............................] - ETA: 1s - loss: 0.8118 - accuracy: 0.7775\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r52/469 [==\u003e...........................] - ETA: 1s - loss: 0.7005 - accuracy: 0.8057\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r59/469 [==\u003e...........................] - ETA: 1s - loss: 0.6644 - accuracy: 0.8137\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r70/469 [===\u003e..........................] - ETA: 1s - loss: 0.6148 - accuracy: 0.8256\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r83/469 [====\u003e.........................] - ETA: 1s - loss: 0.5690 - accuracy: 0.8376\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r97/469 [=====\u003e........................] - ETA: 1s - loss: 0.5308 - accuracy: 0.8479\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r111/469 [======\u003e.......................] - ETA: 1s - loss: 0.5008 - accuracy: 0.8562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r124/469 [======\u003e.......................] - ETA: 1s - loss: 0.4797 - accuracy: 0.8621\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r136/469 [=======\u003e......................] - ETA: 1s - loss: 0.4648 - accuracy: 0.8659\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r145/469 [========\u003e.....................] - ETA: 1s - loss: 0.4548 - accuracy: 0.8683\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r158/469 [=========\u003e....................] - ETA: 1s - loss: 0.4398 - accuracy: 0.8727\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r172/469 [==========\u003e...................] - ETA: 1s - loss: 0.4253 - accuracy: 0.8765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r185/469 [==========\u003e...................] - ETA: 1s - loss: 0.4145 - accuracy: 0.8802\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r199/469 [===========\u003e..................] - ETA: 1s - loss: 0.3999 - accuracy: 0.8843\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r212/469 [============\u003e.................] - ETA: 1s - loss: 0.3899 - accuracy: 0.8872\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r225/469 [=============\u003e................] - ETA: 1s - loss: 0.3804 - accuracy: 0.8902\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r238/469 [==============\u003e...............] - ETA: 0s - loss: 0.3700 - accuracy: 0.8933\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r252/469 [===============\u003e..............] - ETA: 0s - loss: 0.3619 - accuracy: 0.8951\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r266/469 [================\u003e.............] - ETA: 0s - loss: 0.3537 - accuracy: 0.8979\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r280/469 [================\u003e.............] - ETA: 0s - loss: 0.3460 - accuracy: 0.9001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r291/469 [=================\u003e............] - ETA: 0s - loss: 0.3401 - accuracy: 0.9018\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r303/469 [==================\u003e...........] - ETA: 0s - loss: 0.3350 - accuracy: 0.9032\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r316/469 [===================\u003e..........] - ETA: 0s - loss: 0.3298 - accuracy: 0.9046\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r330/469 [====================\u003e.........] - ETA: 0s - loss: 0.3250 - accuracy: 0.9061\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r343/469 [====================\u003e.........] - ETA: 0s - loss: 0.3198 - accuracy: 0.9073\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r357/469 [=====================\u003e........] - ETA: 0s - loss: 0.3132 - accuracy: 0.9090\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r368/469 [======================\u003e.......] - ETA: 0s - loss: 0.3088 - accuracy: 0.9103\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/469 [=======================\u003e......] - ETA: 0s - loss: 0.3051 - accuracy: 0.9115\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r395/469 [========================\u003e.....] - ETA: 0s - loss: 0.3017 - accuracy: 0.9125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r409/469 [=========================\u003e....] - ETA: 0s - loss: 0.2970 - accuracy: 0.9139\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r423/469 [==========================\u003e...] - ETA: 0s - loss: 0.2927 - accuracy: 0.9152\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r437/469 [==========================\u003e...] - ETA: 0s - loss: 0.2889 - accuracy: 0.9163\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r448/469 [===========================\u003e..] - ETA: 0s - loss: 0.2855 - accuracy: 0.9173\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r458/469 [============================\u003e.] - ETA: 0s - loss: 0.2827 - accuracy: 0.9181\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r469/469 [==============================] - 3s 5ms/step - loss: 0.2800 - accuracy: 0.9189 - val_loss: 0.1337 - val_accuracy: 0.9610\rEpoch 2/2\r1/469 [..............................] - ETA: 1s - loss: 0.1849 - accuracy: 0.9375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r13/469 [..............................] - ETA: 1s - loss: 0.1216 - accuracy: 0.9597\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r25/469 [\u003e.............................] - ETA: 1s - loss: 0.1333 - accuracy: 0.9584\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r37/469 [=\u003e............................] - ETA: 1s - loss: 0.1389 - accuracy: 0.9573\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r49/469 [==\u003e...........................] - ETA: 1s - loss: 0.1370 - accuracy: 0.9597\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r63/469 [===\u003e..........................] - ETA: 1s - loss: 0.1326 - accuracy: 0.9606\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r75/469 [===\u003e..........................] - ETA: 1s - loss: 0.1349 - accuracy: 0.9596\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r86/469 [====\u003e.........................] - ETA: 1s - loss: 0.1339 - accuracy: 0.9597\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r98/469 [=====\u003e........................] - ETA: 1s - loss: 0.1360 - accuracy: 0.9594\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r110/469 [======\u003e.......................] - ETA: 1s - loss: 0.1385 - accuracy: 0.9587\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r123/469 [======\u003e.......................] - ETA: 1s - loss: 0.1385 - accuracy: 0.9582\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r136/469 [=======\u003e......................] - ETA: 1s - loss: 0.1371 - accuracy: 0.9591\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r147/469 [========\u003e.....................] - ETA: 1s - loss: 0.1369 - accuracy: 0.9593\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r158/469 [=========\u003e....................] - ETA: 1s - loss: 0.1361 - accuracy: 0.9595\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r170/469 [=========\u003e....................] - ETA: 1s - loss: 0.1355 - accuracy: 0.9600\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r182/469 [==========\u003e...................] - ETA: 1s - loss: 0.1360 - accuracy: 0.9601\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r194/469 [===========\u003e..................] - ETA: 1s - loss: 0.1348 - accuracy: 0.9603\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r206/469 [============\u003e.................] - ETA: 1s - loss: 0.1330 - accuracy: 0.9610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r217/469 [============\u003e.................] - ETA: 1s - loss: 0.1322 - accuracy: 0.9610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r228/469 [=============\u003e................] - ETA: 1s - loss: 0.1304 - accuracy: 0.9617\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r240/469 [==============\u003e...............] - ETA: 1s - loss: 0.1301 - accuracy: 0.9616\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r252/469 [===============\u003e..............] - ETA: 0s - loss: 0.1290 - accuracy: 0.9618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r265/469 [===============\u003e..............] - ETA: 0s - loss: 0.1276 - accuracy: 0.9623\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r278/469 [================\u003e.............] - ETA: 0s - loss: 0.1264 - accuracy: 0.9625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r291/469 [=================\u003e............] - ETA: 0s - loss: 0.1253 - accuracy: 0.9629\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r304/469 [==================\u003e...........] - ETA: 0s - loss: 0.1246 - accuracy: 0.9632\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r318/469 [===================\u003e..........] - ETA: 0s - loss: 0.1235 - accuracy: 0.9635\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r332/469 [====================\u003e.........] - ETA: 0s - loss: 0.1230 - accuracy: 0.9636\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r345/469 [=====================\u003e........] - ETA: 0s - loss: 0.1220 - accuracy: 0.9641\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r358/469 [=====================\u003e........] - ETA: 0s - loss: 0.1214 - accuracy: 0.9643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r368/469 [======================\u003e.......] - ETA: 0s - loss: 0.1210 - accuracy: 0.9644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r380/469 [=======================\u003e......] - ETA: 0s - loss: 0.1209 - accuracy: 0.9643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r392/469 [========================\u003e.....] - ETA: 0s - loss: 0.1206 - accuracy: 0.9645\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r404/469 [========================\u003e.....] - ETA: 0s - loss: 0.1205 - accuracy: 0.9644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r415/469 [=========================\u003e....] - ETA: 0s - loss: 0.1206 - accuracy: 0.9643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r427/469 [==========================\u003e...] - ETA: 0s - loss: 0.1205 - accuracy: 0.9642\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r440/469 [===========================\u003e..] - ETA: 0s - loss: 0.1201 - accuracy: 0.9644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r452/469 [===========================\u003e..] - ETA: 0s - loss: 0.1201 - accuracy: 0.9643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r464/469 [============================\u003e.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r469/469 [==============================] - 2s 5ms/step - loss: 0.1198 - accuracy: 0.9644 - val_loss: 0.0944 - val_accuracy: 0.9716\r1/313 [..............................] - ETA: 8s - loss: 0.0785 - accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r19/313 [\u003e.............................] - ETA: 0s - loss: 0.0754 - accuracy: 0.9852\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r36/313 [==\u003e...........................] - ETA: 0s - loss: 0.0956 - accuracy: 0.9705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r54/313 [====\u003e.........................] - ETA: 0s - loss: 0.1193 - accuracy: 0.9641\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r73/313 [=====\u003e........................] - ETA: 0s - loss: 0.1268 - accuracy: 0.9606\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r93/313 [=======\u003e......................] - ETA: 0s - loss: 0.1233 - accuracy: 0.9610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r112/313 [=========\u003e....................] - ETA: 0s - loss: 0.1196 - accuracy: 0.9637\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r130/313 [===========\u003e..................] - ETA: 0s - loss: 0.1234 - accuracy: 0.9618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r151/313 [=============\u003e................] - ETA: 0s - loss: 0.1204 - accuracy: 0.9617\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r171/313 [===============\u003e..............] - ETA: 0s - loss: 0.1137 - accuracy: 0.9642\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r191/313 [=================\u003e............] - ETA: 0s - loss: 0.1163 - accuracy: 0.9638\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r210/313 [===================\u003e..........] - ETA: 0s - loss: 0.1119 - accuracy: 0.9658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r230/313 [=====================\u003e........] - ETA: 0s - loss: 0.1048 - accuracy: 0.9681\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r245/313 [======================\u003e.......] - ETA: 0s - loss: 0.1003 - accuracy: 0.9695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r263/313 [========================\u003e.....] - ETA: 0s - loss: 0.0979 - accuracy: 0.9699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r282/313 [==========================\u003e...] - ETA: 0s - loss: 0.0943 - accuracy: 0.9712\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r303/313 [============================\u003e.] - ETA: 0s - loss: 0.0911 - accuracy: 0.9723\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r313/313 [==============================] - 1s 3ms/step - loss: 0.0944 - accuracy: 0.9716\rTest accuracy: 0.9715999960899353 模型保存和加载 TensorFlow 提供了两种方式来保存和加载模型： 1.使用 tf.train.Checkpoint：\nimport tensorflow as tf\r# 定义模型\rmodel = tf.keras.models.Sequential([\rtf.keras.layers.Dense(10, activation='relu', input_shape=(784,)),\rtf.keras.layers.Dense(10, activation='softmax')\r])\r# 定义优化器和损失函数\rmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r# 训练模型\rmodel.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\r# 创建 Checkpoint 对象\rcheckpoint = tf.train.Checkpoint(model=model)\r# 保存模型\rcheckpoint.save('./model.ckpt')\r# 加载模型\rcheckpoint.restore('./model.ckpt') 2.使用 tf.keras.callbacks.ModelCheckpoint：\nimport tensorflow as tf\r# 定义模型\rmodel = tf.keras.models.Sequential([\rtf.keras.layers.Dense(10, activation='relu', input_shape=(784,)),\rtf.keras.layers.Dense(10, activation='softmax')\r])\r# 定义优化器和损失函数\rmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r# 定义 ModelCheckpoint 回调函数\rcheckpoint = tf.keras.callbacks.ModelCheckpoint('./model.h5', save_best_only=True, save_weights_only=False, monitor='val_loss')\r# 训练模型\rmodel.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), callbacks=[checkpoint])\r# 加载模型\rmodel = tf.keras.models.load_model('./model.h5') 第一种方式使用 tf.train.Checkpoint 对象保存和加载模型，可以保存模型的权重和优化器状态，还支持在训练过程中保存模型和恢复模型。第二种方式使用 tf.keras.callbacks.ModelCheckpoint 回调函数保存模型，可以指定保存最佳模型，同时可以选择保存模型的权重或整个模型。\n绘制ui手写数字识别 首先改写之前的tensorflow代码保存模型\n# 定义 ModelCheckpoint 回调函数\rcheckpoint = tf.keras.callbacks.ModelCheckpoint('./model.h5', save_best_only=True, save_weights_only=False, monitor='val_loss')\r# 训练模型\rhistory = model.fit(x_train, y_train, epochs=8, batch_size=128, validation_data=(x_test, y_test),\rcallbacks=[checkpoint]\r) 绘制ui，加载模型并预测显示在ui上\nimport tkinter as tk\rfrom PIL import Image, ImageDraw\rimport numpy as np;\rimport tensorflow as tf\rimport matplotlib.pyplot as plt\rfrom tensorflow.keras.datasets import mnist\r# 加载模型并进行数字识别\rdef recognize_digit():\r# 将画板图像转换成灰度图像，并将其大小调整为 28x28,注意要用convert,因为彩色图像是rgb是三维的，resize只是改变了rg，需要convert转换成灰度的二维\rimage_resized = np.array(image.resize((28, 28)).convert('L'))\r# 反转图像，因为灰度图像是黑底白字，但是我们训练的图片都是白底黑字，所以取反\rimage_resized = np.invert(image_resized)\r# plt.imshow(image_resized, cmap=plt.cm.binary)\r# plt.show()\r# 将图像转换为数字数组\rdata = image_resized.reshape(1, 784).astype('float32') / 255.0\r# 在这里添加您的识别代码\rmodel = tf.keras.models.load_model('./model.h5')\rpredictions = model.predict(np.array([data]))\rresult_label.configure(text=\"识别结果为：\" + str(np.argmax(predictions)))\r# 清空画板\rdef clear_canvas():\rdraw.rectangle((0, 0, 280, 280), fill=\"white\")\rcanvas.delete(\"all\")\r# 创建窗口\rwindow = tk.Tk()\rwindow.title(\"手写数字识别\")\rwindow.geometry(\"400x400\")\r# 创建画布\rcanvas = tk.Canvas(window, width=280, height=280, bg=\"white\")\rcanvas.grid(row=0, column=0, columnspan=2)\r# 创建清空画布按钮\rclear_button = tk.Button(window, text=\"清空画板\", command=clear_canvas)\rclear_button.grid(row=1, column=0)\r# 创建识别按钮\rrecognize_button = tk.Button(window, text=\"识别数字\", command=recognize_digit)\rrecognize_button.grid(row=1, column=1)\r# 创建识别结果标签\rresult_label = tk.Label(window, text=\"\")\rresult_label.grid(row=2, column=0, columnspan=2)\r# 创建画板图像\rimage = Image.new(\"RGB\", (280, 280), (255, 255, 255))\rdraw = ImageDraw.Draw(image)\r# 绑定画板事件\rdef on_mouse_down(event):\rglobal prev_x, prev_y\rprev_x, prev_y = event.x, event.y\rdef on_mouse_move(event):\rglobal prev_x, prev_y\rcanvas.create_line(prev_x, prev_y, event.x, event.y, width=20)\rdraw.line((prev_x, prev_y, event.x, event.y), fill=\"black\", width=20)\rprev_x, prev_y = event.x, event.y\rcanvas.bind(\"\u003cButton-1\u003e\", on_mouse_down)\rcanvas.bind(\"\u003cB1-Motion\u003e\", on_mouse_move)\r# 显示窗口\rwindow.mainloop() 程序效果 对于手写数字识别，正确率的高低不仅取决于模型的性能，还与数据的质量和多样性有关。在训练模型时，使用的数据集可能与实际应用场景中的数据存在差异，导致模型无法很好地泛化到新的、未知的数据上。\n此外，手写数字的识别难度还受到许多因素的影响，如书写的风格、字母大小、笔画粗细、书写方向等等。如果训练集中没有涵盖到这些因素，那么模型就可能无法准确地识别新的手写数字。因此，为了提高手写数字识别的准确率，需要使用更多、更丰富的数据集，并对模型进行调参和优化以提高其泛化能力。\n来看下下面这个例子，我们的mlp多层感知器貌似基本无能为力了",
    "description": "神经网络 简介 神经网络是一种基于生物神经系统结构和功能特点而设计的人工神经网络模型，具有很强的自适应性和非线性映射能力。神经网络由多个神经元（或称节点）组成，这些神经元通过连接权重相互连接，构成多层的网络结构。每个神经元接收到来自其它神经元的信号，并将这些信号加权线性组合后通过激活函数进行非线性转换，最终输出给下一层神经元或输出层。\n学习机器学习后，学习神经网络可以帮助你更深入地理解模式识别和人工智能领域的基础知识。神经网络在很多领域都有广泛的应用，例如计算机视觉、自然语言处理、语音识别等。学习神经网络可以让你掌握这些领域中最前沿的技术，并且能够应用这些技术来解决具体的问题。同时，神经网络的学习方法和算法也是机器学习的重要组成部分，学习神经网络可以帮助你更好地理解机器学习的原理和技术，从而更好地应用机器学习来解决实际问题。\n学习路径 如果你已经学过机器学习，那么开始学习神经网络，可以从多层感知器（Multilayer Perceptron，简称 MLP）神经网络入手。 MLP 是最基本的神经网络模型之一，它的结构比较简单，易于理解和实现，同时又有很好的可扩展性和通用性，可以应用于分类、回归等多种任务。学习 MLP 之后，你可以进一步学习卷积神经网络（Convolutional Neural Networks，简称 CNN）和循环神经网络（Recurrent Neural Networks，简称 RNN），它们分别用于计算机视觉和自然语言处理等特定领域的问题。总之，建议先从 MLP 入手，逐渐深入学习其他类型的神经网络。\n分类 神经网络可以分为多种不同的类型，下面列举一些常见的神经网络类型：\n前馈神经网络（Feedforward Neural Network）：前馈神经网络是最基本的神经网络类型，也是深度学习中最常见的神经网络类型。它由若干个神经元按照一定的层次结构组成，每个神经元接收上一层的输出，产生本层的输出，从而实现信息的传递和处理。\n卷积神经网络（Convolutional Neural Network）：卷积神经网络是一种专门用于图像处理和计算机视觉任务的神经网络类型。它通过卷积和池化等操作，可以提取图像中的特征，从而实现图像分类、目标检测、图像分割等任务。\n循环神经网络（Recurrent Neural Network）：循环神经网络是一种能够处理序列数据的神经网络类型。它通过记忆单元和门控机制等方式，可以处理任意长度的序列数据，从而实现自然语言处理、语音识别等任务。\n自编码器（Autoencoder）：自编码器是一种无监督学习的神经网络类型，它的目标是将输入数据进行压缩和解压缩，从而实现特征提取和降维等任务。\n深度置信网络（Deep Belief Network）：深度置信网络是一种由多个受限玻尔兹曼机组成的神经网络类型。它可以通过逐层贪心预训练和微调等方式，实现高效的特征学习和分类任务。\n除了以上列举的几种神经网络类型，还有众多其他的神经网络类型，如反向传播神经网络、Hopfield网络、Boltzmann机等。不同的神经网络类型适用于不同的任务和数据类型，需要根据具体的问题选择合适的神经网络类型。\n多层感知器（MLP） MLP神经网络属于前馈神经网络（Feedforward Neural Network）的一种。在网络训练过程中，需要通过反向传播算法计算梯度，将误差从输出层反向传播回输入层，用于更新网络参数。这个过程中需要使用反向传播算法来计算梯度，并且在某些类型的神经网络中，例如循环神经网络（RNN），也存在反馈回路。除了MLP，其他常见的前馈神经网络包括卷积神经网络（CNN）和循环神经网络（RNN）等。\n神经网络认识 我们以一个简单的例子来认识神经网络，只是为了理解其中的一些概念。 我们已知四个数据点(1,1)(-1,1)(-1,-1)(1,-1)，这四个点分别对应I~IV象限（也就是数据属于的类别），如果这时候给我们一个新的坐标点（比如(2,2)），那么它应该属于哪个象限呢？（没错，当然是第I象限，但我们的任务是要让机器知道，机器不知道有象限这个东西啊，他只能根据历史数据的经验推断），如果机器只是知道一堆数据 比如(-2,3)属于2象限，机器就需要通过这些数据总结出一个特征，这个特征可能就是根据x和y坐标的正负来判断象限了。 两层神经网络 这里我们构建一个两层神经网络，理论上两层神经网络已经可以拟合任意函数。这个神经网络的结构如下图： 首先我们去掉途中难懂的东西",
    "tags": [],
    "title": "深度学习02-神经网络(MLP多层感知器)",
    "uri": "/docs/programming/ai/deep_learning/basic/dl_02_mlp/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 卷积神经网络",
    "content": "简介 CNN，即卷积神经网络（Convolutional Neural Network），是一种常用于图像和视频处理的深度学习模型。与传统神经网络相比，CNN 有着更好的处理图像和序列数据的能力，因为它能够自动学习图像中的特征，并提取出最有用的信息。\nCNN 的一个核心特点是卷积操作，它可以在图像上进行滑动窗口的计算，通过滤波器（又称卷积核）和池化层（Max Pooling）来提取出图像的特征。卷积操作可以有效地减少权重数量，降低计算量，同时也能够保留图像的空间结构信息。池化层则可以在不改变特征图维度的前提下，减少计算量，提高模型的鲁棒性。\nCNN 的典型结构包括卷积层、池化层、全连接层等。同时，为了防止过拟合，CNN 还会加入一些正则化的技术，如 Dropout 和 L2 正则等。\nCNN 在图像分类、目标检测、语音识别等领域都有着广泛的应用。在图像分类任务中，CNN 的经典模型包括 LeNet-5、AlexNet、VGG 和 GoogleNet/Inception 等，这些模型的设计思想和网络结构都有所不同，但都对卷积神经网络的发展做出了重要贡献。\n发展历程 卷积神经网络（CNN）是一种深度学习模型，广泛应用于图像识别、计算机视觉等领域。在CNN的发展历程中，涌现出了许多经典的模型，下面简要介绍几个著名的模型。\nLeNet-5 LeNet-5是Yann LeCun等人于1998年提出的，是第一个被广泛应用的卷积神经网络模型。它主要用于手写数字识别，包含卷积层、池化层和全连接层。LeNet-5的设计使得它在MNIST手写数字识别任务上获得了很好的表现。它的特点是卷积核数量较少（6和16）以及参数量较少，第一层卷积层使用了6个大小为5×5的卷积核，第二层卷积层使用了16个大小为5×5的卷积核。这种设计可以有效地减少模型的参数量，但它是卷积神经网络的开山鼻祖，为后续模型奠定了基础。\nAlexNet AlexNet由Alex Krizhevsky等人于2012年提出，是第一个在ImageNet图像分类比赛中取得优异成绩的卷积神经网络模型。它采用了多个卷积层和池化层，使用了ReLU激活函数和Dropout正则化技术。AlexNet的设计使得它在ImageNet图像分类比赛中大幅领先于其他模型，从而引领了卷积神经网络的新一轮发展。它的特点是使用了大量卷积核（近6000个）、参数量较大，但在准确率和效率上都有很好的表现。\nVGG VGG由Karen Simonyan和Andrew Zisserman于2014年提出，其主要贡献是提出了使用更小的卷积核（3x3）来代替较大的卷积核。这种设计使得网络更深，而且参数量更少，从而提高了效率和准确率。VGG包含了16个或19个卷积层和池化层，这些层都采用了相同的卷积核大小和步长。VGG在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ResNet等模型提供了启示。\nGoogleNet/Inception GoogleNet由Google团队于2014年提出，其主要贡献是提出了Inception模块，可以在不增加参数量的情况下增加网络的深度和宽度。Inception模块采用了多个不同大小的卷积核和池化层来进行特征提取，然后将它们串联在一起，形成了一个模块。GoogleNet还使用了全局平均池化层来代替全连接层，从而进一步减少了参数量。GoogleNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ResNet、DenseNet等模型提供了启示。\nResNet ResNet由Microsoft Research Asia团队于2015年提出，其主要贡献是提出了残差学习，可以解决深度卷积神经网络的退化问题。退化问题指的是随着网络深度的增加，准确率反而下降的现象。残差学习通过引入跨层连接来将输入直接传递到输出，从而避免了信息的损失。ResNet包含了较深的网络结构（152层），但却获得了更好的准确率。ResNet的设计思想被后续的DenseNet、MobileNet等模型所继承。\nDenseNet DenseNet由Gao Huang等人于2017年提出，其主要贡献是提出了密集连接，可以增加网络的深度和宽度，从而提高了效率和准确率。密集连接指的是将每个层的输出都与后面所有层的输入相连，形成了一个密集的连接结构。这种设计使得网络更加紧凑，参数量更少，同时也可以提高特征的复用性。DenseNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ShuffleNet、EfficientNet等模型提供了启示。\nMobileNet MobileNet由Google团队于2017年提出，其主要贡献是提出了深度可分离卷积，可以在减少参数量的同时保持较好的准确率。深度可分离卷积指的是将卷积操作分为深度卷积和逐点卷积两步，从而减少了计算量和参数量。MobileNet采用了多个深度可分离卷积层和池化层，可以在移动设备等资源受限的环境下实现高效的图像分类和目标检测。MobileNet的设计思想被后续的ShuffleNet、EfficientNet等模型所继承。\nShuffleNet ShuffleNet由Microsoft Research Asia团队于2018年提出，其主要贡献是提出了通道重组和组卷积，可以在保持准确率的前提下大幅减少参数量和计算量。通道重组指的是将输入的通道分组并重新组合，从而让不同的组之间进行信息的交流。组卷积指的是将卷积操作分为组内卷积和组间卷积两步，从而减少了计算量和参数量。ShuffleNet采用了多个通道重组和组卷积层，可以在资源受限的环境下实现高效的图像分类和目标检测。\nEfficientNet EfficientNet由Google团队于2019年提出，其主要贡献是提出了网络缩放和复合系数，可以在保持准确率的前提下大幅减少参数量和计算量。网络缩放指的是同时缩放网络的深度、宽度和分辨率，从而在不改变模型结构的情况下进行优化。复合系数指的是将深度、宽度和分辨率的缩放系数进行组合，从而得到一个更加高效的模型。EfficientNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的模型优化提供了启示。\nRegNet RegNet由Facebook AI Research团队于2020年提出，其主要贡献是提出了网络结构的自适应规则，可以在保持准确率的前提下大幅减少参数量和计算量。自适应规则指的是通过搜索和优化来自动调整网络结构的超参数，从而得到一个更加高效的模型。RegNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的模型优化提供了启示。\n以上是几个著名的卷积神经网络模型，它们的设计思想和网络结构都有所不同，但都对卷积神经网络的发展做出了重要贡献。\n图解原理 卷积神经网络在图像识别中大放异彩，达到了前所未有的准确度，有着广泛的应用。接下来将以图像识别为例子，来介绍卷积神经网络的原理。\n案例 假设给定一张图（可能是字母X或者字母O），通过CNN即可识别出是X还是O，如下图所示，那怎么做到的呢 图像输入 如果采用经典的神经网络模型，则需要读取整幅图像作为神经网络模型的输入（即全连接的方式），当图像的尺寸越大时，其连接的参数将变得很多，从而导致计算量非常大。 而我们人类对外界的认知一般是从局部到全局，先对局部有感知的认识，再逐步对全体有认知，这是人类的认识模式。在图像中的空间联系也是类似，局部范围内的像素之间联系较为紧密，而距离较远的像素则相关性较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。这种模式就是卷积神经网络中降低参数数目的重要神器：局部感受野。 特征提取 如果字母X、字母O是固定不变的，那么最简单的方式就是图像之间的像素一一比对就行，但在现实生活中，字体都有着各个形态上的变化（例如手写文字识别），例如平移、缩放、旋转、微变形等等，如下图所示： 我们的目标是对于各种形态变化的X和O，都能通过CNN准确地识别出来，这就涉及到应该如何有效地提取特征，作为识别的关键因子。 回想前面讲到的“局部感受野”模式，对于CNN来说，它是一小块一小块地来进行比对，在两幅图像中大致相同的位置找到一些粗糙的特征（小块图像）进行匹配，相比起传统的整幅图逐一比对的方式，CNN的这种小块匹配方式能够更好的比较两幅图像之间的相似性。如下图： 以字母X为例，可以提取出三个重要特征（两个交叉线、一个对角线），如下图所示： 假如以像素值\"1\"代表白色，像素值\"-1\"代表黑色，则字母X的三个重要特征如下： 上面的特征提取是个假设，实际当有多张图作为输入时，卷积神经网络会对每张图进行特征提取，具体过程如下：输入图片经过第一个卷积层，卷积核会在图像上滑动，提取出一些低层次的特征，例如边缘、角点等。\n在卷积神经网络中，如果使用了多个不同的卷积核，那么每个卷积核的局部感受野大小是相同的，但是不同卷积核的权重是不同的，这样可以使得每个卷积核学习到不同的特征。\n举个例子，假设我们在卷积层中使用三个不同的卷积核，其中第一个卷积核的权重用于检测边缘，第二个卷积核的权重用于检测纹理特征，第三个卷积核的权重用于检测目标的形状。这三个卷积核的局部感受野大小都相同，但是由于它们的权重不同，因此每个卷积核可以学习到不同的特征。\n需要注意的是，卷积核的大小和步长也会影响到每个卷积核的局部感受野大小。如果卷积核的大小较大，那么它的局部感受野也会相应地变大；如果步长较大，那么卷积核每次滑动的距离也会相应地变大，从而影响到卷积核的局部感受野大小。\n比如卷积核 [-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]\n这个矩阵实际上是一个卷积核，也被称为Sobel滤波器。它可以用来检测图像中的垂直边缘。 在计算机视觉中，边缘是指图像中灰度值变化较大的区域。垂直边缘是指从图像的上部到下部或从下部到上部的灰度值变化。 卷积核的工作原理是将它与图像的像素进行卷积操作，从而提取图像的特征。在这个例子中，卷积核的中心元素是0，表示它与图像的中心像素无关。而卷积核的上面一行元素[-1, 0, 1]表示它与图像的上方像素进行卷积操作。同理，卷积核的下面一行元素[-1, 0, 1]表示它与图像的下方像素进行卷积操作。\n当卷积核与图像中的像素进行卷积操作时，如果图像中存在垂直边缘，那么卷积结果会显示出明显的变化。具体来说，在垂直边缘的一侧，卷积结果会得到较大的正值，而在垂直边缘的另一侧，卷积结果会得到较大的负值。这样，我们就可以通过阈值化卷积结果来识别图像中的垂直边缘，是负数的部分直接就归0了。\n举个例子，假设我们有一张图像，其中一部分是垂直边缘。我们将卷积核应用于这个图像的垂直边缘部分，卷积结果会显示出正值和负值，这样我们就可以通过阈值化卷积结果来提取垂直边缘的位置。\n希望这个例子可以帮助你理解为什么[-1, 0, 1]这个矩阵可以用来检测垂直边缘。 再比如 [[-0.1111, -0.1111, -0.1111], [-0.1111, 1.0000, -0.1111], [-0.1111, -0.1111, -0.1111]] 被称为拉普拉斯滤波器或者锐化滤波器。它可以用来增强图像中的边缘。 在这个矩阵中，中心元素1表示它与图像的中心像素有关。而周围的元素-0.1111表示它们与图像的周围像素有关。\n当卷积核与图像进行卷积操作时，中心像素的值会被放大，而周围像素的值会被抑制。这样，在图像的边缘部分，由于像素值的变化较大，卷积结果会显示出较大的正值和负值，从而增强了边缘的对比度。\n举个例子，假设我们有一张图像，其中包含一些边缘。我们将这个卷积核应用于图像，卷积结果会增强边缘的对比度，使得边缘更加清晰。\n因此，这个卷积核能够检测边缘，通过增强边缘的对比度，使得边缘更加明显。\n边缘 边缘是图像中像素灰度值变化明显的地方，通常表示图像中物体的边缘、轮廓或者纹理等信息。在图像处理和计算机视觉中，边缘检测是一种常用的技术，可以用来分割图像、提取特征等。 比如如下图 提取边缘的效果 角点 角点是图像中局部区域的特殊点，具有明显的角度变化。角点通常是由不同方向的边缘交汇处形成的，具有高斯曲率，是图像中的重要特征之一。在图像配准、物体跟踪、图像匹配等方面，角点检测也是一种常用的技术。常用的角点检测算法包括Harris角点检测、Shi-Tomasi角点检测等。 如下图 opencv OpenCV（Open Source Computer Vision Library）是一个开源的计算机视觉和机器学习软件库。它可以帮助开发人员快速构建计算机视觉应用程序，如图像处理、物体检测、人脸识别、视频分析等。\nOpenCV最初是由英特尔公司发起的，现已成为一个跨平台的开源项目，支持多种编程语言，包括C++、Python、Java等，可以在Windows、Linux、macOS等操作系统上运行。\n这里使用opencv来讲某张图片的边缘和角点提取出来，比如图片是 具体这里不细讲opencv，以后在开文讲解。 代码\n#%%\rimport cv2 #注意安装open-cv conda install open-cv\rimport numpy as np\rimport matplotlib.pyplot as plt\r# 读入lena图像\rimg = cv2.imread('d:/9.png')\r# 将BGR图像转换为RGB图像，便于matplotlib显示\rimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r# 将图像转换为灰度图像\rgray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\rgray_ori=gray\r# 使用Canny边缘检测函数检测图像的边缘\redges = cv2.Canny(gray, 100, 200)\r# 创建SIFT对象\rsift = cv2.xfeatures2d.SIFT_create()\r# 检测图像的特征点\rkeypoints = sift.detect(gray, None)\r# 在图像上绘制特征点\rimg_sift = cv2.drawKeypoints(img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\r# 检测图像的角点\rdst = cv2.cornerHarris(gray, 2, 3, 0.04)\r# 将角点标记为红色\rimg_corner = img.copy()\rimg_corner[dst \u003e 0.01 * dst.max()] = [255, 0, 0]\r# 创建一个Matplotlib窗口并显示图像及其各种特征\rplt.rcParams['font.family'] = 'SimHei'\rfig, axs = plt.subplots(2, 2, figsize=(10, 10))\raxs[0, 0].imshow(img)\raxs[0, 0].set_title('原始图像')\raxs[0, 1].imshow(edges, cmap='gray')\raxs[0, 1].set_title('边缘')\raxs[1, 0].imshow(img_sift)\r#SIFT的全称是Scale Invariant Feature Transform，尺度不变特征变换。具有旋转不变性、尺度不变性、亮度变化保持不变性，是一种非常稳定的局部特征。\raxs[1, 0].set_title('SIFT特征')\raxs[1, 1].imshow(img_corner)\raxs[1, 1].set_title('角点特征')\rplt.show() 输出效果 特征提取原理 请看完【卷积】章节后再来看这一段 常用的卷积核有以下几种：\n高斯滤波器：用于图像平滑处理，可以减少图像噪声。 高通滤波器：用于突出图像中的高频信息，例如边缘、角等。 低通滤波器：用于突出图像中的低频信息，例如模糊、平滑等。 Sobel滤波器：用于检测图像中的边缘信息。 Laplacian滤波器：用于增强图像的高频信息，例如边缘、细节等。 Scharr滤波器：与Sobel滤波器类似，但对边缘的响应更强。 Prewitt滤波器：与Sobel滤波器类似，但对边缘的响应更平滑。 这些卷积核可用于图像处理中的不同任务，例如边缘检测、图像平滑、图像增强等。您可以根据任务的不同选择适合的卷积核来处理图像。\n下面定义卷积核可以被看作是一个高通滤波器，因为它的中心像素被赋予了一个较大的权重，而周围像素的权重较小。这种权重分配使得卷积核能够检测出图像中的高频信息，例如边缘、角等。在卷积操作中，卷积核和图像中的每个像素点都进行相乘，并将结果加起来，这样可以得到一个新的像素值。如果卷积核中心像素周围的像素值与中心像素值之间的差异较大，那么卷积操作的结果将会比较大，这表明这个像素点可能是边缘点。因此，这个卷积核能够突出图像中的边缘信息。\nkernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) 有如下图片 使用opencv加载他，并用卷积核进行卷积\nimport cv2\rimport numpy as np\rfrom myutils.common import show,fillColor\r# 读取图片\rimg = cv2.imread('./images/z.png')\r# 将图像转换为灰度图像\rgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r# 定义卷积核\rkernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\r# kernel = np.array([[-1,-1,-1,-1,-1],[-1,-1,-1,-1,-1], [-1,-1,20,-1,-1],[-1,-1,-1,-1,-1], [-1,-1,-1,-1,-1]])\r# kernel = cv2.getGaussianKernel(5, 1)\r# 对灰度图像进行卷积操作，#注意如果-1 \u003c0的值会被归一化为0\redges = cv2.filter2D(gray, cv2.CV_32F, kernel)\rprint(edges[:][edges\u003c0])\r# 对卷积结果进行ReLU处理\redges_relu = np.maximum(0, edges)\rshow(img,'Original Image',cmap=\"gray\",debug=True) show(edges, 'Edges Image',cmap=\"gray\",debug=True)\rshow(edges_relu, 'Edges ReLU Image',cmap=\"gray\",debug=True)\rdef show(dilate, title, cmap=None, debug=False):\rif debug:\rplt.title(title)\rplt.imshow(dilate, cmap=cmap)\rplt.show() 为什么说卷积操作提取的是线性特征，而使用relu了 让我们以一个简单的例子来说明卷积操作本身并不能提取非线性特征。\n假设我们有一个输入矩阵X，它包含以下值：\nX = [[1, 2, 3],\r[4, 5, 6],\r[7, 8, 9]]\n现在，我们使用一个大小为2x2的卷积核K来对X进行卷积，卷积核的值如下：\nK = [[1, 1],\r[1, 1]]\n我们可以使用矩阵乘法来执行卷积操作。具体来说，我们将K矩阵翻转后，与X矩阵做点积操作，得到一个输出矩阵Y：\nY = K*X = [[12, 16],\r[24, 28]]\n可以看到，输出矩阵Y是输入矩阵X的线性组合，因此卷积操作本身只能提取输入矩阵X的线性特征，例如边缘和纹理等。\n但是，当我们使用非线性激活函数，例如ReLU激活函数，对输出矩阵Y进行处理时，就可以将线性特征转换为非线性特征。例如，当我们对Y应用ReLU函数时，得到的非线性特征是：\nReLU(Y) = [[12, 16],\r[24, 28]]\n因此，卷积操作本身只能提取输入矩阵的线性特征，但当与非线性激活函数结合使用时，可以提取非线性特征。\n卷积 那么这些特征又是怎么进行匹配计算呢？（不要跟我说是像素进行一一匹配的，汗！） 这时就要请出今天的重要嘉宾：卷积。那什么是卷积呢，不急，下面慢慢道来。 当给定一张新图时，CNN并不能准确地知道这些特征到底要匹配原图的哪些部分，所以它会在原图中把每一个可能的位置都进行尝试，相当于把这个feature（特征）变成了一个过滤器。这个用来匹配的过程就被称为卷积操作，这也是卷积神经网络名字的由来。 卷积的操作如下图所示： 黄色的部分就是一个卷积核，也就是上一张提取的特征 [[1,0,1] [0,1,0] [1,0,1]] 同图像中的每个可能的3*3图像进行计算(卷积相同位置相乘后相加/当前聚集矩阵个个数9)，计算的结果得到一个数放在当前被卷积的中心位置，最终会得到一个去掉最外层的新的矩阵，具体计算逻辑参考下文。\n在本案例中，要计算一个feature（特征）和其在原图上对应的某一小块的结果，只需将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可（注：也可不除以总个数的）。 如果两个像素点都是白色（值均为1），那么11 = 1，如果均为黑色，那么(-1)(-1) = 1，也就是说，每一对能够匹配上的像素，其相乘结果为1。类似地，任何不匹配的像素相乘结果为-1。具体过程如下（第一个、第二个……、最后一个像素的匹配结果）： 先将我们之前提取的三个特征中的一个拿来进行卷积 比如拿第一个特征和绿色框框圈起来的部分比较，完全一样 根据卷积的计算方式，第一块特征匹配后的卷积计算如下，结果为1 对于其它位置的匹配，也是类似（例如中间部分的匹配） 以此类推，对三个特征图像不断地重复着上述过程，通过每一个feature（特征）的卷积操作，会得到一个新的二维数组，称之为feature map（特征图）。其中的值，越接近1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。如下图所示： 可以看出，当图像尺寸增大时，其内部的加法、乘法和除法操作的次数会增加得很快，每一个filter的大小和filter的数目呈线性增长。由于有这么多因素的影响，很容易使得计算量变得相当庞大。\n池化(Pooling) 为了有效地减少计算量，CNN使用的另一个有效的工具被称为“池化(Pooling)”。池化就是将输入图像进行缩小，减少像素信息，只保留重要信息。 池化的操作也很简单，通常情况下，池化区域是22大小，然后按一定规则转换成相应的值，例如取这个池化区域内的最大值（max-pooling）、平均值（mean-pooling）等，以这个值作为结果的像素值。 下图显示了左上角22池化区域的max-pooling结果，取该区域的最大 max(0.77,-0.11,-0.11,1.00) ，作为池化后的结果，如下图： 池化区域往左，第二小块取大值max(0.11,0.33,-0.11,0.33)，作为池化后的结果，如下图： 其它区域也是类似，取区域内的最大值作为池化后的结果，最后经过池化后，结果如下： 对所有的feature map执行同样的操作，结果如下： 最大池化（max-pooling）保留了每一小块内的最大值，也就是相当于保留了这一块最佳的匹配结果（因为值越接近1表示匹配越好）。也就是说，它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。 通过加入池化层，图像缩小了，能很大程度上减少计算量，降低机器负载。\n激活函数ReLU (Rectified Linear Units) 常用的激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者ReLU常见于卷积层。 回顾一下前面讲的感知机，感知机在接收到各个输入，然后进行求和，再经过激活函数后输出。激活函数的作用是用来加入非线性因素，把卷积层输出结果做非线性映射。 在卷积神经网络中，激活函数一般使用ReLU(The Rectified Linear Unit，修正线性单元)，它的特点是收敛快，求梯度简单。计算公式也很简单，max(0,T)，即对于输入的负值，输出全为0，对于正值，则原样输出。 下面看一下本案例的ReLU激活函数操作过程： 第一个值，取max(0,0.77)，结果为0.77，如下图 第二个值，取max(0,-0.11)，结果为0，如下图 以此类推，经过ReLU激活函数后，结果如下： 对所有的feature map执行ReLU激活函数操作，结果如下： 深度神经网络 通过将上面所提到的卷积、激活函数、池化组合在一起，就变成下图： 通过加大网络的深度，增加更多的层，就得到了深度神经网络，如下图： 全连接层(Fully connected layers) 全连接层在整个卷积神经网络中起到“分类器”的作用，即通过卷积、激活函数、池化等深度网络后，再经过全连接层对结果进行识别分类。 首先将经过卷积、激活函数、池化的深度网络后的结果串起来，如下图所示： 由于神经网络是属于监督学习，在模型训练时，根据训练样本对模型进行训练，从而得到全连接层的权重（如预测字母X的所有连接的权重） 在利用该模型进行结果识别时，根据刚才提到的模型训练得出来的权重，以及经过前面的卷积、激活函数、池化等深度网络计算出来的结果，进行加权求和，得到各个结果的预测值，然后取值最大的作为识别的结果（如下图，最后计算出来字母X的识别值为0.92，字母O的识别值为0.51，则结果判定为X） 上述这个过程定义的操作为”全连接层“(Fully connected layers)，全连接层也可以有多个，如下图： 卷积神经网络（Convolutional Neural Networks） 将以上所有结果串起来后，就形成了一个“卷积神经网络”（CNN）结构，如下图所示： 最后，再回顾总结一下，卷积神经网络主要由两部分组成，一部分是特征提取（卷积、激活函数、池化），另一部分是分类识别（全连接层），下图便是著名的手写文字识别卷积神经网络结构图： 本章节内容参考：https://my.oschina.net/u/876354/blog/1620906\n卷积api Conv2D Conv2D是卷积神经网络中最核心的层之一，它是用于图像或其他二维数据的卷积处理的层。Conv2D的作用是将输入的二维图像或数据，通过卷积核进行一系列的卷积操作，从而提取出图像或数据中的特征。\nConv2D层的输入为一个tensor，该tensor的形状通常为(batch_size, height, width, channel)，其中batch_size表示输入数据的数量，height和width表示输入数据的高度和宽度，channel表示输入数据的通道数（如RGB图像的通道数为3）。\nConv2D层的输出也是一个tensor，表示经过卷积操作后得到的特征图。输出tensor的形状通常为(batch_size, conv_height, conv_width, filters)，其中conv_height和conv_width表示卷积核作用后得到的特征图的高度和宽度，filters表示卷积核的数量，即输出特征图的通道数。\n在卷积过程中，Conv2D层将卷积核作用于输入数据，通过逐个计算每个卷积核与输入数据的卷积操作，得到卷积后的输出特征图。在卷积过程中，卷积核的大小、步长、填充方式等参数都可以自由设置，以适应不同的应用场景。 在TensorFlow 2.0和Keras中，可以通过以下代码来创建一个Conv2D层：\nfrom tensorflow.keras.layers import Conv2D\rconv_layer = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(height, width, channel)) filters：卷积核的数量，也就是输出的特征图的个数。 kernel_size：卷积核的大小，可以是一个整数，表示正方形卷积核的边长，也可以是一个元组，表示长和宽不同的卷积核。 strides：步长，也就是卷积核在输入特征图上移动的距离。可以是一个整数，表示在两个相邻的卷积核之间的距离，也可以是一个元组，表示在长和宽方向上的步长不同。 padding：填充方式，可以是’same’或’valid’。‘same’表示输出特征图的大小和输入特征图的大小相同，需要在输入特征图的周围填充一些值；‘valid’表示不需要填充，输出特征图的大小会根据输入特征图和卷积核的大小而变化。 activation：激活函数，用于给特征图添加非线性变换。常见的激活函数有’relu’、‘sigmoid’、’tanh’等。 input_shape：输入特征图的形状，可以是一个三元组，表示高、宽和通道数。在第一层卷积层中需要指定该参数。 kernel_regularizer:在深度学习中，为了防止模型过拟合，通常会使用正则化技术对模型进行约束，其中一个常用的正则化方法是L2正则化。L2正则化是指在模型的损失函数中增加一个L2范数惩罚项，以限制模型权重的大小。 在Keras中，使用regularizers.l2(0.001)可以添加L2正则化惩罚项。其中，0.001是正则化参数，控制正则化强度的大小。正则化参数越大，惩罚项对权重的影响就越大，模型的复杂度就会降低，从而有效地防止过拟合。 具体来说，regularizers.l2(0.001)可以应用于神经网络中的任何权重矩阵，例如全连接层、卷积层等。在网络的定义中，我们可以在相应的层中使用kernel_regularizer参数来添加L2正则化。例如，在Keras中添加一个带有L2正则化的全连接层的代码如下所示： layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(28, 28, 1)), 卷积实例 取minist10张图，并且使用10个卷积核进行卷积，输出特征图，并显示图像， 因为每张图会生成10个卷积核，所以总共生成100张特征图。\n#%%\rimport tensorflow as tf\rimport matplotlib.pyplot as plt\rimport numpy as np\r# 加载mnist数据集\rmnist = tf.keras.datasets.mnist\r(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r# 取1张训练集图片\rimages = train_images[:10]\r# 将图片转换为float类型\rimages = images.astype('float32') / 255.0\r# 将图片reshape成4D张量，大小为(10, 28, 28, 1)，也就是第一个维度表示有10张图像，每张图像由28行、28列和1个# 通道(灰度)组成\rimages = np.expand_dims(images, axis=3)\r# 定义卷积核数量\rnum_filters = 10\r# 定义卷积层\rmodel = tf.keras.models.Sequential([\rtf.keras.layers.Conv2D(num_filters, (3, 3), activation='relu', input_shape=(28, 28, 1)),\r])\r# 计算卷积后的特征图\rfeatures = model.predict(images)\r# 绘制卷积后的特征图\rfig, axs = plt.subplots(nrows=num_filters, ncols=10, figsize=(10, num_filters))\rfor i in range(num_filters):\rfor j in range(10):\raxs[i][j].imshow(features[j, :, :, i], cmap='gray')\raxs[i][j].axis('off')\rplt.show() 输出 np.expand_dims函数用于在数组的指定轴上扩展维度。在这个例子中，images是一个形状为(10, 28, 28)的数组，表示10张28x28的灰度图像。但是，机器学习模型通常需要输入4维的数组，即(样本数, 图像高度, 图像宽度, 通道数)。因此，我们需要将images数组的最后一个维度(通道数)扩展一维，变成形状为(10, 28, 28, 1)的数组。 具体来说，axis=3表示在数组的第3个轴(从0开始计数)上扩展维度，它会在每张图像的最后一个维度上增加一个维度，从而将每张图像变成形状为(28, 28, 1)的三维数组。最终，images数组的形状变成了(10, 28, 28, 1)，表示有10张28x28的灰度图像，每张图像由一个通道组成。这样，images就可以作为输入传递给机器学习模型了。\n从上面的输出图片可以看出，有些卷积核的输出偏向于边缘，有些角点，有些纹理。\nMaxPooling2D keras.layers.MaxPooling2D((2, 2))是Keras中的一个层，它用于进行最大池化操作。 最大池化是一种常用的卷积神经网络操作，它可以在不改变图像尺寸的前提下，减少图像中的参数数量，从而减少计算量和内存消耗。最大池化操作将输入图像划分为不重叠的块，对每个块取最大值作为输出。在卷积神经网络中，最大池化通常跟卷积层交替使用，以提取图像的空间特征。\nMaxPooling2D层的参数是一个元组(2, 2)，表示池化窗口的大小为2x2。这意味着，输入图像会被划分为多个大小为2x2的块，对每个块取最大值作为输出。如果将池化窗口大小设置为(3, 3)，那么输入图像会被划分为多个大小为3x3的块，对每个块取最大值作为输出。\n总之，MaxPooling2D层可以帮助卷积神经网络提取图像的空间特征，同时减少计算量和内存消耗。\nFlatten keras.layers.Flatten()是Keras中的一个层，它用于将输入“平铺”成一维向量。\n在卷积神经网络中，通常会使用卷积层和池化层提取图像的特征，然后使用全连接层进行分类。全连接层的输入是一个一维向量，因此需要将之前的特征图“展平”为一维向量。这就是Flatten层的作用。\nFlatten层没有任何参数，它只是将输入张量按照顺序展开成一维向量。例如，如果输入张量的shape为(batch_size, 7, 7, 64)，则Flatten层的输出shape为(batch_size, 7764)。\n在搭建卷积神经网络时，通常会在卷积层和池化层之后添加一个Flatten层，将特征图展平成一维向量，然后再连接到全连接层进行分类。\nDense|Dropout 参考多层感知器\n手写数字识别 卷积mnist数据集 我们将加载MNIST数据集并进行预处理，将像素值缩放到0到1之间，并将数据集分为训练集和测试集。 这里数据处理详解参考多层感知器\nimport tensorflow as tf\rimport tensorflow.keras\rfrom tensorflow.keras import layers\rfrom tensorflow.keras.datasets import mnist\rfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\rfrom tensorflow.keras import regularizers\r(x_train, y_train), (x_test, y_test) = mnist.load_data()\rx_train = x_train.astype('float32') / 255.\rx_test = x_test.astype('float32') / 255.\rx_train = x_train[..., tf.newaxis]\rx_test = x_test[..., tf.newaxis]\rnum_classes = 10\ry_train = tf.keras.utils.to_categorical(y_train, num_classes)\ry_test = tf.keras.utils.to_categorical(y_test, num_classes) 接下来，我们将定义一个卷积神经网络模型。我们将使用两个卷积层和两个池化层，然后是两个全连接层和一个输出层。我们还将使用dropout和L2正则化来防止过拟合。\nmodel = tf.keras.Sequential([\rlayers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(28, 28, 1)),\rlayers.MaxPooling2D((2, 2)),\rlayers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\rlayers.MaxPooling2D((2, 2)),\rlayers.Flatten(),\rlayers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\rlayers.Dropout(0.5),\rlayers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\rlayers.Dropout(0.5),\rlayers.Dense(num_classes, activation='softmax')\r]) model.summary()是Keras中模型对象的一个方法，用于打印出模型的结构信息，包括每一层的名称、输出形状、参数数量等。这对于调试、优化模型以及理解模型结构都非常有用。\nmodel.summary() 然后，我们将对模型进行编译，并使用数据增强技术来进一步防止过拟合。数据增强技术将应用一系列随机变换，例如旋转、平移、缩放等，来生成新的训练样本。这样可以使模型更加鲁棒，并防止过拟合。\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\rdatagen = ImageDataGenerator(\rrotation_range=10,\rwidth_shift_range=0.1,\rheight_shift_range=0.1,\rzoom_range=0.1\r) 接下来，我们将使用训练集来训练模型，并使用测试集来评估模型的性能。\ndatagen.fit(x_train)\rbatch_size = 1024\repochs = 10\rcheckpoint = tf.keras.callbacks.ModelCheckpoint('./model.h5', save_best_only=True, save_weights_only=False, monitor='val_loss')\rhistory = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\repochs=epochs,\rvalidation_data=(x_test, y_test),\rsteps_per_epoch=len(x_train) // batch_size,callbacks=[checkpoint])\rscore = model.evaluate(x_test, y_test, verbose=0)\rprint('Test loss:', score[0])\rprint('Test accuracy:', score[1]) steps_per_epoch和batch_size 两个参数区别 batch_size 是指每个训练批次（batch）中包含的样本数。在深度学习中，通常会将训练集分成多个批次，每个批次中包含若干个样本。这样做的好处是可以利用矩阵运算加速计算，同时也可以在训练过程中随机打乱样本顺序以避免过拟合。\nsteps_per_epoch 是指在一个 epoch 中，模型需要训练的批次数。由于每个 epoch 中包含多个批次，因此需要设置 steps_per_epoch 来指定一个 epoch 中需要经过多少个批次。通常，steps_per_epoch 的值可以通过训练集大小和 batch_size 计算得到。例如，如果训练集大小为 1000，batch_size 为 32，那么一个 epoch 中就需要训练 1000 / 32 = 31 个批次，因此 steps_per_epoch 就应该设置为 31。\n需要注意的是，steps_per_epoch 不一定等于训练集大小除以 batch_size 的结果。如果训练集大小不能被 batch_size 整除，那么最后一个批次中可能会包含少于 batch_size 个样本。为了避免这种情况，可以使用向下取整操作 // 来计算 steps_per_epoch，确保每个 epoch 中都能够处理完整个训练集。\nfine-tuning Fine-tuning是指在已经训练好的模型上，针对特定任务或特定数据集进行微调，以达到更好的性能表现的方法。通常，我们会使用一个在大规模数据集上预训练好的模型，例如ImageNet等数据集，这个模型在训练过程中已经学到了很多通用的特征和模式。我们可以通过在这个模型的基础上进行微调，调整一些参数或者增加一些新的层，使得这个模型更适合新的任务或新的数据集。这种方法通常比从头开始训练一个模型更加高效，因为预训练模型已经具有很好的初始权重和特征提取能力。\nmnist-c数据集 MNIST-C是MNIST数据集的一个变体，它是加入了人工噪声的MNIST数据集。MNIST数据集是一个手写数字识别数据集，包含60,000个训练样本和10,000个测试样本，每个样本都是一个28 x 28像素的灰度图像。MNIST-C数据集是通过在MNIST数据集的图像上添加随机噪声来创建的，这些噪声包括模糊、扭曲、亮度变化等，从而使模型更有鲁棒性。\nMNIST-C数据集对于测试机器学习模型的稳健性非常有用，因为它可以测试模型对于不同类型的噪声的鲁棒性。MNIST-C数据集中的每个图像都包含一个标签，表示它所代表的数字。这些标签与MNIST数据集中的相应标签相同，因此您可以使用相同的训练和测试流程来训练和测试您的模型。\n下载该数据集，https://github.com/google-research/mnist-c/ ，这个github地址是源码地址，实际下载地址在readme中提及：https://zenodo.org/record/3239543#.ZF2rzXZByUl，下载后解压 这些文件夹里都是npy格式的numpy数组导出。 读取每个文件夹的前10张图片显示\n# 数据集的开源地址：https://github.com/google-research/mnist-c/\rimport os\rimport numpy as np\rimport matplotlib.pyplot as plt\r#加载数据集并打印每个子文件夹前10个数据集\rdata_root = './mnist_c'\rdirlist=os.listdir(data_root)\rfig, axs = plt.subplots(len(dirlist), 10, figsize=(10, 10))\rfor i, folder_name in enumerate(dirlist):\rfolder_path = os.path.join(data_root, folder_name)\rif os.path.isdir(folder_path):\rfile_path = os.path.join(folder_path, 'train_images.npy')\rdata = np.load(file_path)\rfor j in range(0,10):\raxs[i, j].imshow(data[j].reshape(28,28), cmap='gray')\raxs[i, j].axis('off')\rplt.tight_layout()\rplt.show() 输出 fine-tuning方法训练 假设我们开始试用试用minist的训练的模型位于./model.h5,我们需要加载该模型，然后试用该模型继续训练minist-c的数据。\n#%%\rimport os\rimport numpy as np\rimport tensorflow.keras as layers\rimport tensorflow as tf\rimport datetime\rTARGET_MODEL_DIR=\"./\"\rMODEL_NAME=\"model.h5\"\repochs_count=5\r\"\"\"\rjupyter打印的日志太大导致ipynb打开很慢，这里写个一模一样代码的py运行\r\"\"\"\rdef againTrain(x_train, y_train, x_test, y_test):\rtargetModel=os.path.join(TARGET_MODEL_DIR,MODEL_NAME)\r#记载CNN模型\rmodel=tf.keras.models.load_model(targetModel)\r\"\"\"\r在使用Fine-tuning方法微调预训练模型时，通常会冻结模型的前几层，只调整模型的后面几层，这是因为：\r1.预训练模型的前几层通常是针对原始数据集的通用特征提取器，这些特征对于不同的任务和数据集都是有用的，因此我们可以直接保留这些特征提取器，不需要进行微调。\r2.预训练模型的后几层通常是针对特定任务进行的微调，这些层的参数需要根据具体任务和数据集进行调整，以使模型更好地适应特定的任务和数据集。\r3.如果我们将整个模型的所有层都进行微调，会导致训练时间较长，而且可能会出现过拟合等问题。因此，冻结前几层可以有效地减少训练时间，并提高模型的泛化能力。\r总之，冻结模型的前几层可以节省计算资源和训练时间，同时还可以提高模型的泛化能力，使其更好地适应新的任务和数据集。\r\"\"\"\rmodel.layers[0].trainable = False\rmodel.layers[1].trainable = False\r# 对输入图像进行预处理\rx_train = x_train.reshape(-1, 28, 28, 1)\rx_train = x_train.astype('float32') / 255.0\rx_test = x_test.reshape(-1, 28, 28, 1)\rx_test = x_test.astype('float32') / 255.0\ry_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\ry_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\rnow = datetime.datetime.now() # 获取当前时间\rformat_time = now.strftime(\"%Y-%m-%d%H-%M-%S\") # 转换为指定格式\rcheckpoint = tf.keras.callbacks.ModelCheckpoint(targetModel, save_best_only=True, save_weights_only=False, monitor='val_loss')\r# 继续训练模型\rhistory = model.fit(x_train, y_train, batch_size=128, epochs=epochs_count, validation_data=(x_test, y_test),\rcallbacks=[checkpoint])\rtest_loss, test_acc = model.evaluate(x_test, y_test)\rprint('Test accuracy:', test_acc)\r\"\"\"\r传入mnist-c，数据会非常大加载数据很慢，这里每加载一份子目录就训练一次，节省内存开销。\r\"\"\"\rdef loadDataMnistC(data_root,func):\rdirlist=os.listdir(data_root)\rfor i, folder_name in enumerate(dirlist):\rfolder_path = os.path.join(data_root, folder_name)\rif os.path.isdir(folder_path):\rprint(\"开始读取：\"+folder_path)\rtrain_images = np.load(os.path.join(folder_path, 'train_images.npy'))\rtrain_labels = np.load(os.path.join(folder_path, 'train_labels.npy'))\rtest_images = np.load(os.path.join(folder_path, 'test_images.npy'))\rtest_labels = np.load(os.path.join(folder_path, 'test_labels.npy'))\rprint(\"开始训练：\"+folder_path)\rfunc(train_images,train_labels,test_images,test_labels)\rprint(\"训练完成：\"+folder_path)\r# 加载 MNIST-C 数据集\rdata_root = './mnist_c'\rmodel=None;\rloadDataMnistC(data_root,againTrain)\rprint(\"全部训练完成\") 这里每次读取一次某型，然后试用子文件夹训练又会写回到该模型，知道训练完成获取到最终的模型",
    "description": "简介 CNN，即卷积神经网络（Convolutional Neural Network），是一种常用于图像和视频处理的深度学习模型。与传统神经网络相比，CNN 有着更好的处理图像和序列数据的能力，因为它能够自动学习图像中的特征，并提取出最有用的信息。\nCNN 的一个核心特点是卷积操作，它可以在图像上进行滑动窗口的计算，通过滤波器（又称卷积核）和池化层（Max Pooling）来提取出图像的特征。卷积操作可以有效地减少权重数量，降低计算量，同时也能够保留图像的空间结构信息。池化层则可以在不改变特征图维度的前提下，减少计算量，提高模型的鲁棒性。\nCNN 的典型结构包括卷积层、池化层、全连接层等。同时，为了防止过拟合，CNN 还会加入一些正则化的技术，如 Dropout 和 L2 正则等。\nCNN 在图像分类、目标检测、语音识别等领域都有着广泛的应用。在图像分类任务中，CNN 的经典模型包括 LeNet-5、AlexNet、VGG 和 GoogleNet/Inception 等，这些模型的设计思想和网络结构都有所不同，但都对卷积神经网络的发展做出了重要贡献。\n发展历程 卷积神经网络（CNN）是一种深度学习模型，广泛应用于图像识别、计算机视觉等领域。在CNN的发展历程中，涌现出了许多经典的模型，下面简要介绍几个著名的模型。\nLeNet-5 LeNet-5是Yann LeCun等人于1998年提出的，是第一个被广泛应用的卷积神经网络模型。它主要用于手写数字识别，包含卷积层、池化层和全连接层。LeNet-5的设计使得它在MNIST手写数字识别任务上获得了很好的表现。它的特点是卷积核数量较少（6和16）以及参数量较少，第一层卷积层使用了6个大小为5×5的卷积核，第二层卷积层使用了16个大小为5×5的卷积核。这种设计可以有效地减少模型的参数量，但它是卷积神经网络的开山鼻祖，为后续模型奠定了基础。\nAlexNet AlexNet由Alex Krizhevsky等人于2012年提出，是第一个在ImageNet图像分类比赛中取得优异成绩的卷积神经网络模型。它采用了多个卷积层和池化层，使用了ReLU激活函数和Dropout正则化技术。AlexNet的设计使得它在ImageNet图像分类比赛中大幅领先于其他模型，从而引领了卷积神经网络的新一轮发展。它的特点是使用了大量卷积核（近6000个）、参数量较大，但在准确率和效率上都有很好的表现。\nVGG VGG由Karen Simonyan和Andrew Zisserman于2014年提出，其主要贡献是提出了使用更小的卷积核（3x3）来代替较大的卷积核。这种设计使得网络更深，而且参数量更少，从而提高了效率和准确率。VGG包含了16个或19个卷积层和池化层，这些层都采用了相同的卷积核大小和步长。VGG在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ResNet等模型提供了启示。\nGoogleNet/Inception GoogleNet由Google团队于2014年提出，其主要贡献是提出了Inception模块，可以在不增加参数量的情况下增加网络的深度和宽度。Inception模块采用了多个不同大小的卷积核和池化层来进行特征提取，然后将它们串联在一起，形成了一个模块。GoogleNet还使用了全局平均池化层来代替全连接层，从而进一步减少了参数量。GoogleNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ResNet、DenseNet等模型提供了启示。\nResNet ResNet由Microsoft Research Asia团队于2015年提出，其主要贡献是提出了残差学习，可以解决深度卷积神经网络的退化问题。退化问题指的是随着网络深度的增加，准确率反而下降的现象。残差学习通过引入跨层连接来将输入直接传递到输出，从而避免了信息的损失。ResNet包含了较深的网络结构（152层），但却获得了更好的准确率。ResNet的设计思想被后续的DenseNet、MobileNet等模型所继承。\nDenseNet DenseNet由Gao Huang等人于2017年提出，其主要贡献是提出了密集连接，可以增加网络的深度和宽度，从而提高了效率和准确率。密集连接指的是将每个层的输出都与后面所有层的输入相连，形成了一个密集的连接结构。这种设计使得网络更加紧凑，参数量更少，同时也可以提高特征的复用性。DenseNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ShuffleNet、EfficientNet等模型提供了启示。\nMobileNet MobileNet由Google团队于2017年提出，其主要贡献是提出了深度可分离卷积，可以在减少参数量的同时保持较好的准确率。深度可分离卷积指的是将卷积操作分为深度卷积和逐点卷积两步，从而减少了计算量和参数量。MobileNet采用了多个深度可分离卷积层和池化层，可以在移动设备等资源受限的环境下实现高效的图像分类和目标检测。MobileNet的设计思想被后续的ShuffleNet、EfficientNet等模型所继承。\nShuffleNet ShuffleNet由Microsoft Research Asia团队于2018年提出，其主要贡献是提出了通道重组和组卷积，可以在保持准确率的前提下大幅减少参数量和计算量。通道重组指的是将输入的通道分组并重新组合，从而让不同的组之间进行信息的交流。组卷积指的是将卷积操作分为组内卷积和组间卷积两步，从而减少了计算量和参数量。ShuffleNet采用了多个通道重组和组卷积层，可以在资源受限的环境下实现高效的图像分类和目标检测。\nEfficientNet EfficientNet由Google团队于2019年提出，其主要贡献是提出了网络缩放和复合系数，可以在保持准确率的前提下大幅减少参数量和计算量。网络缩放指的是同时缩放网络的深度、宽度和分辨率，从而在不改变模型结构的情况下进行优化。复合系数指的是将深度、宽度和分辨率的缩放系数进行组合，从而得到一个更加高效的模型。EfficientNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的模型优化提供了启示。\nRegNet RegNet由Facebook AI Research团队于2020年提出，其主要贡献是提出了网络结构的自适应规则，可以在保持准确率的前提下大幅减少参数量和计算量。自适应规则指的是通过搜索和优化来自动调整网络结构的超参数，从而得到一个更加高效的模型。RegNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的模型优化提供了启示。\n以上是几个著名的卷积神经网络模型，它们的设计思想和网络结构都有所不同，但都对卷积神经网络的发展做出了重要贡献。\n图解原理 卷积神经网络在图像识别中大放异彩，达到了前所未有的准确度，有着广泛的应用。接下来将以图像识别为例子，来介绍卷积神经网络的原理。\n案例 假设给定一张图（可能是字母X或者字母O），通过CNN即可识别出是X还是O，如下图所示，那怎么做到的呢 图像输入 如果采用经典的神经网络模型，则需要读取整幅图像作为神经网络模型的输入（即全连接的方式），当图像的尺寸越大时，其连接的参数将变得很多，从而导致计算量非常大。 而我们人类对外界的认知一般是从局部到全局，先对局部有感知的认识，再逐步对全体有认知，这是人类的认识模式。在图像中的空间联系也是类似，局部范围内的像素之间联系较为紧密，而距离较远的像素则相关性较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。这种模式就是卷积神经网络中降低参数数目的重要神器：局部感受野。",
    "tags": [],
    "title": "深度学习03-卷积神经网络(CNN)",
    "uri": "/docs/programming/ai/deep_learning/cnn/dl_03_cnn/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 卷积神经网络",
    "content": "简介 卷积神经网络（CNN）是深度学习中非常重要的一种网络结构，它可以处理图像、文本、语音等各种类型的数据。以下是CNN的前4个经典模型\nLeNet-5 LeNet-5是由Yann LeCun等人于1998年提出的，是第一个成功应用于手写数字识别的卷积神经网络。它由7层神经网络组成，包括2层卷积层、2层池化层和3层全连接层。其中，卷积层提取图像特征，池化层降低特征图的维度，全连接层将特征映射到对应的类别上。\nLeNet-5的主要特点是使用Sigmoid激活函数、平均池化和卷积层后没有使用零填充。它在手写数字识别、人脸识别等领域都有着广泛的应用。\nAlexNet AlexNet是由Alex Krizhevsky等人于2012年提出的，是第一个在大规模图像识别任务中取得显著成果的卷积神经网络。它由5层卷积层、3层全连接层和1层Softmax输出层组成，其中使用了ReLU激活函数、最大池化和Dropout技术。\nAlexNet的主要特点是使用了GPU加速训练、数据增强和随机化Dropout等技术，使得模型的泛化能力和鲁棒性得到了大幅提升。它在ImageNet大规模图像识别比赛中取得了远超其他模型的优异成绩。\nVGGNet VGGNet是由Karen Simonyan和Andrew Zisserman于2014年提出的，它是一个非常深的卷积神经网络，有16层或19层。VGGNet的每个卷积层都使用了3x3的卷积核和ReLU激活函数，使得它的网络结构非常清晰、易于理解。\nVGGNet的主要特点是使用了更深的网络结构、小卷积核和少量的参数，使得模型的特征提取能力得到了进一步提升。它在ImageNet比赛中也获得了非常好的成绩。\nGoogLeNet GoogLeNet是由Google团队于2014年提出的，它是一个非常深的卷积神经网络，有22层。它使用了一种称为Inception模块的结构，可以在保持网络深度的同时减少参数量。\nGoogLeNet的主要特点是使用了Inception模块、1x1卷积核和全局平均池化等技术，使得模型的计算复杂度得到了大幅降低。它在ImageNet比赛中获得了非常好的成绩，并且被广泛应用于其他领域。\nCNN回顾 回顾一下 CNN 的几个特点：局部感知、参数共享、池化。\n局部感知 人类对外界的认知一般是从局部到全局、从片面到全面，类似的，在机器识别图像时也没有必要把整张图像按像素全部都连接到神经网络中，在图像中也是局部周边的像素联系比较紧密，而距离较远的像素则相关性较弱，因此可以采用局部连接的模式（将图像分块连接，这样能大大减少模型的参数），如下图所示： 参数（权值）共享 每张自然图像（人物、山水、建筑等）都有其固有特性，也就是说，图像其中一部分的统计特性与其它部分是接近的。这也意味着这一部分学习的特征也能用在另一部分上，能使用同样的学习特征。因此，在局部连接中隐藏层的每一个神经元连接的局部图像的权值参数（例如 5×5），将这些权值参数共享给其它剩下的神经元使用，那么此时不管隐藏层有多少个神经元，需要训练的参数就是这个局部图像的权限参数（例如 5×5），也就是卷积核的大小，这样大大减少了训练参数。如下图 卷积核的权值是指每个卷积核中的参数，用于对输入数据进行卷积操作时，对每个位置的像素进行加权求和。在卷积神经网络中，同一层中的所有卷积核的权值是共享的，这意味着每个卷积核在不同位置上的权值是相同的。共享权值可以减少模型中需要学习的参数数量，从而降低了模型的复杂度，同时可以提高模型的泛化能力，因为共享权值可以使模型更加稳定，避免过度拟合。共享权值的实现方式是通过使用相同的卷积核对输入数据进行卷积操作。 池化 随着模型网络不断加深，卷积核越来越多，要训练的参数还是很多，而且直接拿卷积核提取的特征直接训练也容易出现过拟合的现象。回想一下，之所以对图像使用卷积提取特征是因为图像具有一种 “静态性” 的属性，因此，一个很自然的想法就是对不同位置区域提取出有代表性的特征（进行聚合统计，例如最大值、平均值等），这种聚合的操作就叫做池化，池化的过程通常也被称为特征映射的过程（特征降维），如下图：\nLeNet-5 概述 LeNet5 诞生于 1994 年，是最早的卷积神经网络之一， 由 Yann LeCun 完成，推动了深度学习领域的发展。在那时候，没有 GPU 帮助训练模型，甚至 CPU 的速度也很慢，因此，LeNet5 通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络也是最近大量神经网络架构的起点，给这个领域带来了许多灵感。 LeNet5 的网络结构示意图如下所示： LeNet5 由 7 层 CNN（不包含输入层）组成，上图中输入的原始图像大小是 32×32 像素，卷积层用 Ci 表示，子采样层（pooling，池化）用 Si 表示，全连接层用 Fi 表示。下面逐层介绍其作用和示意图上方的数字含义。\nC1 层（卷积层）：6@28×28 该层使用了 6 个卷积核，每个卷积核的大小为 5×5，这样就得到了 6 个 feature map（特征图）。 （1）特征图大小 每个卷积核（5×5）与原始的输入图像（32×32）进行卷积，这样得到的 feature map（特征图）大小为（32-5+1）×（32-5+1）= 28×28 卷积过程如下图所示（下图是4*4只是用于演示）： 卷积核与输入图像按卷积核大小逐个区域进行匹配计算，匹配后原始输入图像的尺寸将变小，因为边缘部分卷积核无法越出界，只能匹配一次，如上图，匹配计算后的尺寸变为 Cr×Cc=（Ir-Kr+1）×（Ic-Kc+1），其中 Cr、Cc，Ir、Ic，Kr、Kc 分别表示卷积后结果图像、输入图像、卷积核的行列大小。 其中Cr表示结果行row，Cc表示结果列column （2）参数个数 由于参数（权值）共享的原因，对于同个卷积核每个神经元均使用相同的参数，因此，参数个数为（5×5+1）×6= 156，其中 5×5 为卷积核参数，1 为偏置参数 （3）连接数 卷积后的图像大小为 28×28，因此每个特征图有 28×28 个神经元，每个卷积核参数为（5×5+1）×6，因此，该层的连接数为（5×5+1）×6×28×28=122304\nS2 层（下采样层，也称池化层）：6@14×14 （1）特征图大小 这一层主要是做池化或者特征映射（特征降维），池化单元为 2×2，因此，6 个特征图的大小经池化后即变为 14×14。回顾本文刚开始讲到的池化操作，池化单元之间没有重叠，在池化区域内进行聚合统计后得到新的特征值，因此经 2×2 池化后，每两行两列重新算出一个特征值出来，相当于图像大小减半，因此卷积后的 28×28 图像经 2×2 池化后就变为 14×14。 这一层的计算过程是：2×2 单元里的值相加，然后再乘以训练参数 w，再加上一个偏置参数 b（每一个特征图共享相同的 w 和 b)，然后取 sigmoid 值（S 函数：0-1 区间），作为对应的该单元的值。卷积操作与池化的示意图如下： （2）参数个数 S2 层由于每个特征图都共享相同的 w 和 b 这两个参数，因此需要 2×6=12 个参数 （3）连接数 下采样之后的图像大小为 14×14，因此 S2 层的每个特征图有 14×14 个神经元，每个池化单元连接数为 2×2+1（1 为偏置量），因此，该层的连接数为（2×2+1）×14×14×6 = 5880\nC3 层（卷积层）：16@10×10 C3 层有 16 个卷积核，卷积模板大小为 5×5。 （1）特征图大小 与 C1 层的分析类似，C3 层的特征图大小为（14-5+1）×（14-5+1）= 10×10 （2）参数个数 需要注意的是，C3 与 S2 并不是全连接而是部分连接，有些是 C3 连接到 S2 三层、有些四层、甚至达到 6 层，通过这种方式提取更多特征，连接的规则如下表所示： 例如第一列表示 C3 层的第 0 个特征图（feature map）只跟 S2 层的第 0、1 和 2 这三个 feature maps 相连接，计算过程为：用 3 个卷积模板分别与 S2 层的 3 个 feature maps 进行卷积，然后将卷积的结果相加求和，再加上一个偏置，再取 sigmoid 得出卷积后对应的 feature map 了。其它列也是类似（有些是 3 个卷积模板，有些是 4 个，有些是 6 个）。因此，C3 层的参数数目为（5×5×3+1）×6 +（5×5×4+1）×9 +5×5×6+1 = 1516\n（3）连接数 卷积后的特征图大小为 10×10，参数数量为 1516，因此连接数为 1516×10×10= 151600\nS4（下采样层，也称池化层）：16@5×5 （1）特征图大小 与 S2 的分析类似，池化单元大小为 2×2，因此，该层与 C3 一样共有 16 个特征图，每个特征图的大小为 5×5。 （2）参数数量 与 S2 的计算类似，所需要参数个数为 16×2 = 32 （3）连接数 连接数为（2×2+1）×5×5×16 = 2000\nC5 层（卷积层）：120 （1）特征图大小 该层有 120 个卷积核，每个卷积核的大小仍为 5×5，因此有 120 个特征图。由于 S4 层的大小为 5×5，而该层的卷积核大小也是 5×5，因此特征图大小为（5-5+1）×（5-5+1）= 1×1。这样该层就刚好变成了全连接，这只是巧合，如果原始输入的图像比较大，则该层就不是全连接了。 （2）参数个数 与前面的分析类似，本层的参数数目为 120×（5×5×16+1） = 48120 （3）连接数 由于该层的特征图大小刚好为 1×1，因此连接数为 48120×1×1=48120\nF6 层（全连接层）：84 1）特征图大小 F6 层有 84 个单元，之所以选这个数字的原因是来自于输出层的设计，对应于一个 7×12 的比特图，如下图所示，-1 表示白色，1 表示黑色，这样每个符号的比特图的黑白色就对应于一个编码。 该层有 84 个特征图，特征图大小与 C5 一样都是 1×1，与 C5 层全连接。 （2）参数个数 由于是全连接，参数数量为（120+1）×84=10164。跟经典神经网络一样，F6 层计算输入向量和权重向量之间的点积，再加上一个偏置，然后将其传递给 sigmoid 函数得出结果。 （3）连接数 由于是全连接，连接数与参数数量一样，也是 10164。\nOUTPUT 层（输出层）：10 Output 层也是全连接层，共有 10 个节点，分别代表数字 0 到 9。如果第 i 个节点的值为 0，则表示网络识别的结果是数字 i。 （1）特征图大小 该层采用径向基函数（RBF）的网络连接方式，假设 x 是上一层的输入，y 是 RBF 的输出，则 RBF 输出的计算方式是： 上式中的 Wij 的值由 i 的比特图编码确定，i 从 0 到 9，j 取值从 0 到 7×12-1。RBF 输出的值越接近于 0，表示当前网络输入的识别结果与字符 i 越接近。\n（2）参数个数 由于是全连接，参数个数为 84×10=840 （3）连接数 由于是全连接，连接数与参数个数一样，也是 840\n通过以上介绍，已经了解了 LeNet 各层网络的结构、特征图大小、参数数量、连接数量等信息，下图是识别数字 3 的过程，可对照上面介绍各个层的功能进行一一回顾： 编程实现 import tensorflow as tf\rfrom tensorflow.keras import layers, models\rfrom tensorflow.keras.datasets import mnist\rimport matplotlib.pyplot as plt\rimport numpy as np\r#开启tensorflow支持numpy函数，astype是numpy的函数\rfrom tensorflow.python.ops.numpy_ops import np_config\rnp_config.enable_numpy_behavior()\r# 加载MNIST数据集\r(x_train, y_train), (x_test, y_test) = mnist.load_data()\rori_x_test1=x_test\r# 将图像从28*28转换成32*32\rx_train = tf.pad(x_train, [[0,0], [2,2], [2,2]], mode='constant')\rx_test = tf.pad(x_test, [[0,0], [2,2], [2,2]], mode='constant')\r# 将像素值缩放到0-1之间\rx_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\r# 定义Lenet-5模型\rmodel = models.Sequential([\r# 第一层卷积层，6个卷积核，大小为5*5，使用sigmoid激活函数\rlayers.Conv2D(6, (5, 5), activation='relu', input_shape=(32, 32, 1)),\r# 第一层池化层，大小为2*2\rlayers.MaxPooling2D((2, 2)),\r# 第二层卷积层，16个卷积核，大小为5*5，使用sigmoid激活函数\rlayers.Conv2D(16, (5, 5), activation='relu'),\r# 第二层池化层，大小为2*2\rlayers.MaxPooling2D((2, 2)),\r# 第三层卷积层，120个卷积核，大小为5*5，使用sigmoid激活函数\rlayers.Conv2D(120, (5, 5), activation='relu'),\r# 将卷积层的输出拉平\rlayers.Flatten(),\r# 第一层全连接层，84个节点，使用sigmoid激活函数\rlayers.Dense(84, activation='relu'),\r# 输出层，共10个节点，对应0-9十个数字，使用softmax激活函数\rlayers.Dense(10, activation='softmax')\r])\r# 编译模型\rmodel.compile(optimizer='adam',\rloss='sparse_categorical_crossentropy',\rmetrics=['accuracy'])\r# 训练模型\rmodel.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\rscore = model.evaluate(x_test, y_test, verbose=0)\rprint('Test loss:', score[0])\rprint('Test accuracy:', score[1])\r#取出其中一个测试数据进行测试\rtestdata = ori_x_test1[100]\rtestdata = testdata.reshape(-1,28,28)\rtestdata = tf.pad(testdata, [[0,0], [2,2], [2,2]], mode='constant')\rtestdata=testdata.reshape(-1, 32, 32, 1)\r# 将像素值缩放到0-1之间\rtestdata = testdata.astype('float32') / 255.0\rpredictions = model.predict(testdata)\rprint(\"预测结果：\", np.argmax(predictions))\r# 绘制第10个测试数据的图形\rplt.imshow(ori_x_test1[100], cmap=plt.cm.binary)\rplt.show() 输出： Test loss: 0.03826029598712921 Test accuracy: 0.9879999756813049 预测结果： 6 参考:https://my.oschina.net/u/876354/blog/1632862\nAlexNet 2012 年，Alex Krizhevsky、Ilya Sutskever 在多伦多大学 Geoff Hinton 的实验室设计出了一个深层的卷积神经网络 AlexNet，夺得了 2012 年 ImageNet LSVRC 的冠军，且准确率远超第二名（top5 错误率为 15.3%，第二名为 26.2%），引起了很大的轰动。AlexNet 可以说是具有历史意义的一个网络结构，在此之前，深度学习已经沉寂了很长时间，自 2012 年 AlexNet 诞生之后，后面的 ImageNet 冠军都是用卷积神经网络（CNN）来做的，并且层次越来越深，使得 CNN 成为在图像识别分类的核心算法模型，带来了深度学习的大爆发。 在本博客之前的文章中已经介绍过了卷积神经网络（CNN）的技术原理（大话卷积神经网络），也回顾过卷积神经网络（CNN）的三个重要特点（大话 CNN 经典模型：LeNet），有兴趣的同学可以打开链接重新回顾一下，在此就不再重复 CNN 基础知识的介绍了。下面将先介绍 AlexNet 的特点，然后再逐层分解解析 AlexNet 网络结构。\nAlexNet 模型的特点 AlexNet 之所以能够成功，跟这个模型设计的特点有关，主要有：\n使用了非线性激活函数：ReLU 防止过拟合的方法：Dropout，数据扩充（Data augmentation） 其他：多 GPU 实现，LRN 归一化层的使用 1、使用 ReLU 激活函数 传统的神经网络普遍使用 Sigmoid 或者 tanh 等非线性函数作为激励函数，然而它们容易出现梯度弥散或梯度饱和的情况。以 Sigmoid 函数为例，当输入的值非常大或者非常小的时候，这些神经元的梯度接近于 0（梯度饱和现象），如果输入的初始值很大的话，梯度在反向传播时因为需要乘上一个 Sigmoid 导数，会造成梯度越来越小，导致网络变的很难学习。（详见本公博客的文章：深度学习中常用的激励函数）。 在 AlexNet 中，使用了 ReLU （Rectified Linear Units）激励函数，该函数的公式为：f (x)=max (0,x)，当输入信号 \u003c 0 时，输出都是 0，当输入信号 \u003e 0 时，输出等于输入，如下图所示：\n使用 ReLU 替代 Sigmoid/tanh，由于 ReLU 是线性的，且导数始终为 1，计算量大大减少，收敛速度会比 Sigmoid/tanh 快很多，如下图所示： 2、数据扩充（Data augmentation）\n有一种观点认为神经网络是靠数据喂出来的，如果能够增加训练数据，提供海量数据进行训练，则能够有效提升算法的准确率，因为这样可以避免过拟合，从而可以进一步增大、加深网络结构。而当训练数据有限时，可以通过一些变换从已有的训练数据集中生成一些新的数据，以快速地扩充训练数据。 其中，最简单、通用的图像数据变形的方式：水平翻转图像，从原始图像中随机裁剪、平移变换，颜色、光照变换，如下图所示： AlexNet 在训练时，在数据扩充（data augmentation）这样处理： （1）随机裁剪，对 256×256 的图片进行随机裁剪到 224×224，然后进行水平翻转，相当于将样本数量增加了（（256-224）^2）×2=2048 倍； （2）测试的时候，对左上、右上、左下、右下、中间分别做了 5 次裁剪，然后翻转，共 10 个裁剪，之后对结果求平均。作者说，如果不做随机裁剪，大网络基本上都过拟合； （3）对 RGB 空间做 PCA（主成分分析），然后对主成分做一个（0, 0.1）的高斯扰动，也就是对颜色、光照作变换，结果使错误率又下降了 1%。\n3、重叠池化 (Overlapping Pooling) 一般的池化（Pooling）是不重叠的，池化区域的窗口大小与步长相同，如下图所示： 在 AlexNet 中使用的池化（Pooling）却是可重叠的，也就是说，在池化的时候，每次移动的步长小于池化的窗口长度。AlexNet 池化的大小为 3×3 的正方形，每次池化移动步长为 2，这样就会出现重叠。重叠池化可以避免过拟合，这个策略贡献了 0.3% 的 Top-5 错误率。 4、局部归一化（Local Response Normalization，简称 LRN） 在神经生物学有一个概念叫做 “侧抑制”（lateral inhibitio），指的是被激活的神经元抑制相邻神经元。归一化（normalization）的目的是 “抑制”，局部归一化就是借鉴了 “侧抑制” 的思想来实现局部抑制，尤其当使用 ReLU 时这种 “侧抑制” 很管用，因为 ReLU 的响应结果是无界的（可以非常大），所以需要归一化。使用局部归一化的方案有助于增加泛化能力。 LRN 的公式如下，核心思想就是利用临近的数据做归一化，这个策略贡献了 1.2% 的 Top-5 错误率。 5、Dropout 引入 Dropout 主要是为了防止过拟合。在神经网络中 Dropout 通过修改神经网络本身结构来实现，对于某一层的神经元，通过定义的概率将神经元置为 0，这个神经元就不参与前向和后向传播，就如同在网络中被删除了一样，同时保持输入层与输出层神经元的个数不变，然后按照神经网络的学习方法进行参数更新。在下一次迭代中，又重新随机删除一些神经元（置为 0），直至训练结束。 Dropout 应该算是 AlexNet 中一个很大的创新，以至于 “神经网络之父” Hinton 在后来很长一段时间里的演讲中都拿 Dropout 说事。Dropout 也可以看成是一种模型组合，每次生成的网络结构都不一样，通过组合多个模型的方式能够有效地减少过拟合，Dropout 只需要两倍的训练时间即可实现模型组合（类似取平均）的效果，非常高效。 如下图所示： 6、多 GPU 训练 AlexNet 当时使用了 GTX580 的 GPU 进行训练，由于单个 GTX 580 GPU 只有 3GB 内存，这限制了在其上训练的网络的最大规模，因此他们在每个 GPU 中放置一半核（或神经元），将网络分布在两个 GPU 上进行并行计算，大大加快了 AlexNet 的训练速度。\nAlexNet 网络结构的逐层解析 下图是 AlexNet 的网络结构图： AlexNet 网络结构共有 8 层，前面 5 层是卷积层，后面 3 层是全连接层，最后一个全连接层的输出传递给一个 1000 路的 softmax 层，对应 1000 个类标签的分布。 由于 AlexNet 采用了两个 GPU 进行训练，因此，该网络结构图由上下两部分组成，一个 GPU 运行图上方的层，另一个运行图下方的层，两个 GPU 只在特定的层通信。例如第二、四、五层卷积层的核只和同一个 GPU 上的前一层的核特征图相连，第三层卷积层和第二层所有的核特征图相连接，全连接层中的神经元和前一层中的所有神经元相连接。\n下面逐层解析 AlexNet 结构：\n第一层（卷积层） 该层的处理流程为：卷积 –\u003eReLU–\u003e 池化 –\u003e 归一化，流程图如下： （1）卷积 输入的原始图像大小为 224×224×3（RGB 图像），在训练时会经过预处理变为 227×227×3。在本层使用 96 个 11×11×3 的卷积核进行卷积计算，生成新的像素。由于采用了两个 GPU 并行运算，因此，网络结构图中上下两部分分别承担了 48 个卷积核的运算。 卷积核沿图像按一定的步长往 x 轴方向、y 轴方向移动计算卷积，然后生成新的特征图，其大小为：floor ((img_size - filter_size)/stride) +1 = new_feture_size，其中 floor 表示向下取整，img_size 为图像大小，filter_size 为核大小，stride 为步长，new_feture_size 为卷积后的特征图大小，这个公式表示图像尺寸减去卷积核尺寸除以步长，再加上被减去的核大小像素对应生成的一个像素，结果就是卷积后特征图的大小。 AlexNet 中本层的卷积移动步长是 4 个像素，卷积核经移动计算后生成的特征图大小为 (227-11)/4+1=55，即 55×55。 （2）ReLU 卷积后的 55×55 像素层经过 ReLU 单元的处理，生成激活像素层，尺寸仍为 2 组 55×55×48 的像素层数据。 （3）池化 RuLU 后的像素层再经过池化运算，池化运算的尺寸为 3×3，步长为 2，则池化后图像的尺寸为 (55-3)/2+1=27，即池化后像素的规模为 27×27×96 （4）归一化 池化后的像素层再进行归一化处理，归一化运算的尺寸为 5×5，归一化后的像素规模不变，仍为 27×27×96，这 96 层像素层被分为两组，每组 48 个像素层，分别在一个独立的 GPU 上进行运算。\n第二层（卷积层） 该层与第一层类似，处理流程为：卷积 –\u003eReLU–\u003e 池化 –\u003e 归一化，流程图如下： （1）卷积 第二层的输入数据为第一层输出的 27×27×96 的像素层（被分成两组 27×27×48 的像素层放在两个不同 GPU 中进行运算），为方便后续处理，在这里每幅像素层的上下左右边缘都被填充了 2 个像素（填充 0），即图像的大小变为 (27+2+2) ×(27+2+2)。第二层的卷积核大小为 5×5，移动步长为 1 个像素，跟第一层第（1）点的计算公式一样，经卷积核计算后的像素层大小变为 (27+2+2-5)/1+1=27，即卷积后大小为 27×27。 本层使用了 256 个 5×5×48 的卷积核，同样也是被分成两组，每组为 128 个，分给两个 GPU 进行卷积运算，结果生成两组 27×27×128 个卷积后的像素层。 （2）ReLU 这些像素层经过 ReLU 单元的处理，生成激活像素层，尺寸仍为两组 27×27×128 的像素层。 （3）池化 再经过池化运算的处理，池化运算的尺寸为 3×3，步长为 2，池化后图像的尺寸为 (57-3)/2+1=13，即池化后像素的规模为 2 组 13×13×128 的像素层 （4）归一化 然后再经归一化处理，归一化运算的尺度为 5×5，归一化后的像素层的规模为 2 组 13×13×128 的像素层，分别由 2 个 GPU 进行运算。\n第三层（卷积层） 第三层的处理流程为：卷积 –\u003eReLU （1）卷积 第三层输入数据为第二层输出的 2 组 13×13×128 的像素层，为便于后续处理，每幅像素层的上下左右边缘都填充 1 个像素，填充后变为 (13+1+1)×(13+1+1)×128，分布在两个 GPU 中进行运算。 这一层中每个 GPU 都有 192 个卷积核，每个卷积核的尺寸是 3×3×256。因此，每个 GPU 中的卷积核都能对 2 组 13×13×128 的像素层的所有数据进行卷积运算。如该层的结构图所示，两个 GPU 有通过交叉的虚线连接，也就是说每个 GPU 要处理来自前一层的所有 GPU 的输入。 本层卷积的步长是 1 个像素，经过卷积运算后的尺寸为 (13+1+1-3)/1+1=13，即每个 GPU 中共 13×13×192 个卷积核，2 个 GPU 中共有 13×13×384 个卷积后的像素层。 （2）ReLU 卷积后的像素层经过 ReLU 单元的处理，生成激活像素层，尺寸仍为 2 组 13×13×192 的像素层，分配给两组 GPU 处理。\n第四层（卷积层） 与第三层类似，第四层的处理流程为：卷积 –\u003eReLU 1）卷积 第四层输入数据为第三层输出的 2 组 13×13×192 的像素层，类似于第三层，为便于后续处理，每幅像素层的上下左右边缘都填充 1 个像素，填充后的尺寸变为 (13+1+1)×(13+1+1)×192，分布在两个 GPU 中进行运算。 这一层中每个 GPU 都有 192 个卷积核，每个卷积核的尺寸是 3×3×192（与第三层不同，第四层的 GPU 之间没有虚线连接，也即 GPU 之间没有通信）。卷积的移动步长是 1 个像素，经卷积运算后的尺寸为 (13+1+1-3)/1+1=13，每个 GPU 中有 13×13×192 个卷积核，2 个 GPU 卷积后生成 13×13×384 的像素层。 （2）ReLU 卷积后的像素层经过 ReLU 单元处理，生成激活像素层，尺寸仍为 2 组 13×13×192 像素层，分配给两个 GPU 处理。\n第五层（卷积层） 第五层的处理流程为：卷积 –\u003eReLU–\u003e 池化 （1）卷积 第五层输入数据为第四层输出的 2 组 13×13×192 的像素层，为便于后续处理，每幅像素层的上下左右边缘都填充 1 个像素，填充后的尺寸变为 (13+1+1)×(13+1+1) ，2 组像素层数据被送至 2 个不同的 GPU 中进行运算。 这一层中每个 GPU 都有 128 个卷积核，每个卷积核的尺寸是 3×3×192，卷积的步长是 1 个像素，经卷积后的尺寸为 (13+1+1-3)/1+1=13，每个 GPU 中有 13×13×128 个卷积核，2 个 GPU 卷积后生成 13×13×256 的像素层。 （2）ReLU 卷积后的像素层经过 ReLU 单元处理，生成激活像素层，尺寸仍为 2 组 13×13×128 像素层，由两个 GPU 分别处理。 （3）池化 2 组 13×13×128 像素层分别在 2 个不同 GPU 中进行池化运算处理，池化运算的尺寸为 3×3，步长为 2，池化后图像的尺寸为 (13-3)/2+1=6，即池化后像素的规模为两组 6×6×128 的像素层数据，共有 6×6×256 的像素层数据。\n第六层（全连接层） 第六层的处理流程为：卷积（全连接）–\u003eReLU–\u003eDropout （1）卷积（全连接） 第六层输入数据是第五层的输出，尺寸为 6×6×256。本层共有 4096 个卷积核，每个卷积核的尺寸为 6×6×256，由于卷积核的尺寸刚好与待处理特征图（输入）的尺寸相同，即卷积核中的每个系数只与特征图（输入）尺寸的一个像素值相乘，一一对应，因此，该层被称为全连接层。由于卷积核与特征图的尺寸相同，卷积运算后只有一个值，因此，卷积后的像素层尺寸为 4096×1×1，即有 4096 个神经元。 （2）ReLU 这 4096 个运算结果通过 ReLU 激活函数生成 4096 个值。 （3）Dropout 然后再通过 Dropout 运算，输出 4096 个结果值。\n第七层（全连接层） 第七层的处理流程为：全连接 –\u003eReLU–\u003eDropout 第六层输出的 4096 个数据与第七层的 4096 个神经元进行全连接，然后经 ReLU 进行处理后生成 4096 个数据，再经过 Dropout 处理后输出 4096 个数据。\n第八层（全连接层） 第八层的处理流程为：全连接 第七层输出的 4096 个数据与第八层的 1000 个神经元进行全连接，经过训练后输出 1000 个 float 型的值，这就是预测结果。\n以上就是关于 AlexNet 网络结构图的逐层解析了，看起来挺复杂的，下面是一个简图，看起来就清爽很多啊 通过前面的介绍，可以看出 AlexNet 的特点和创新之处，主要如下： 编程实现 下载imagenet数据集， Keras提供的keras.datasets模块可以用来直接加载ImageNet数据集。不过需要注意的是，ImageNet数据集非常大，包含数百万张高分辨率图像，因此通常需要使用分布式计算或者在GPU上进行训练。\nCIFAR-10数据集是一个常用的图像分类数据集，包含10个类别的图像，每个类别包含6000张32x32像素的彩色图像，总共60000张，其中50000张是用于训练，10000张是用于测试。这10个类别分别是：\n飞机（airplane） 汽车（automobile） 鸟类（bird） 猫（cat） 鹿（deer） 狗（dog） 青蛙（frog） 马（horse） 船（ship） 卡车（truck） 每个图像的标签是一个0到9之间的整数，对应上述10个类别中的一个。因此，我们可以使用这些标签来训练和测试图像分类模型。 你可以使用以下代码来加载这个小样本数据集：\nfrom tensorflow.keras.datasets import cifar10\r(x_train, y_train), (x_test, y_test) = cifar10.load_data()\rprint(x_train.shape) 执行后，日志里有一直在下载的过程，下载很慢，路径 https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 我们可以手动下载下来，重命名为：cifar-10-batches-py.tar.gz，然后上传到 ~/.keras/datasets目录即可（不用解压），程序会离线解压该文件，window下是：C:\\Users\\你的用户.keras\\datasets 再次运行输出 (50000, 32, 32, 3) 随机加载100张，看看效果\n# 随机选择100张图片进行显示\rindices = np.random.choice(len(x_train), size=100, replace=False)\rimages = x_train[indices]\rlabels = y_train[indices]\r# 绘制图片\rfig = plt.figure(figsize=(10, 10))\rfor i in range(10):\rfor j in range(10):\rindex = i * 10 + j\rax = fig.add_subplot(10, 10, index + 1)\rax.imshow(images[index])\rax.set_xticks([])\rax.set_yticks([])\rax.set_title(labels[index][0])\rplt.show() 显示 因为数据集总共有6万张，格式3232，使用alexnet模型进行计算，图像需要转换224224，rgb通道数3，每个像素都需要转换成float32，这样导致数gpu显存占用过大导致内存溢出， 需要占用显存=6000022422434＞＝３０ＧＢ， 所以需增量式进行训练\nimport tensorflow as tf\rfrom tensorflow.keras.datasets import cifar10\rfrom tensorflow.python.ops.numpy_ops import np_config\rnp_config.enable_numpy_behavior()\r(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\"\"\"\r在Python中，我们可以使用TensorFlow或Keras等深度学习框架来加载CIFAR-10数据集。为了有效地处理大量图像数据，我们可以使用生成器函数和yield语句来逐批加载数据。\r生成器函数是一个Python函数，它使用yield语句来产生一个序列的值。当函数执行到yield语句时，它会将当前的值返回给调用者，并暂停函数的执行。当函数再次被调用时，它会从上一次暂停的位置继续执行，并返回下一个值。\r\"\"\"\rdef cifar10_generator(x, y, batch_size):\r\"\"\"\rCIFAR-10 data generator.\r\"\"\"\rwhile True:\rfor i in range(0, len(x), batch_size):\rx_batch = x[i:i+batch_size]\ry_batch = y[i:i+batch_size]\rx_batch = tf.image.resize_with_pad(x_batch, target_height=224, target_width=224)\rx_batch = x_batch.astype('float32') / 255.0\ryield x_batch, y_batch\rfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\rdef alexnet(input_shape, num_classes):\rmodel = tf.keras.Sequential([\rConv2D(96, (11,11), strides=(4,4), activation='relu', input_shape=input_shape),\rMaxPooling2D(pool_size=(3,3), strides=(2,2)),\rConv2D(256, (5,5), strides=(1,1), padding='same', activation='relu'),\rMaxPooling2D(pool_size=(3,3), strides=(2,2)),\rConv2D(384, (3,3), strides=(1,1), padding='same', activation='relu'),\rConv2D(384, (3,3), strides=(1,1), padding='same', activation='relu'),\rConv2D(256, (3,3), strides=(1,1), padding='same', activation='relu'),\rMaxPooling2D(pool_size=(3,3), strides=(2,2)),\rFlatten(),\rDense(4096, activation='relu'),\rDropout(0.5),\rDense(4096, activation='relu'),\rDropout(0.5),\rDense(num_classes, activation='softmax')\r])\rreturn model\r# 定义一些超参数\rbatch_size = 256\repochs = 5\rlearning_rate = 0.001\r# 定义生成器\rtrain_generator = cifar10_generator(x_train, y_train, batch_size)\rtest_generator = cifar10_generator(x_test, y_test, batch_size)\r# 定义模型\rinput_shape = (224,224,3)\rnum_classes = 10\rmodel = alexnet(input_shape, num_classes)\r# 编译模型\roptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\rmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r# 定义 ModelCheckpoint 回调函数\rcheckpoint = tf.keras.callbacks.ModelCheckpoint('./AlexNet.h5', save_best_only=True, save_weights_only=False, monitor='val_loss')\r# 训练模型\rmodel.fit(train_generator,\repochs=epochs,\rsteps_per_epoch=len(x_train)//batch_size,\rvalidation_data=test_generator,\rvalidation_steps=len(x_test)//batch_size,\rcallbacks=[checkpoint]\r)\rtest_loss, test_acc = model.evaluate(test_generator, y_test)\rprint('Test accuracy:', test_acc) 预测结果和显示图像\n# 在这里添加您的识别代码\rmodel = tf.keras.models.load_model('./AlexNet.h5')\rsrcImage=x_test[105]\rp_test=np.array([srcImage])\rp_test = tf.image.resize_with_pad(p_test, target_height=224, target_width=224)\rp_test = p_test.astype('float32') / 255.0\rpredictions = model.predict(p_test)\rprint(\"识别结果为：\" + str(np.argmax(predictions)))\r# 绘制第10个测试数据的图形\rplt.imshow(srcImage, cmap=plt.cm.binary)\rplt.show() 输出：1 参考:https://my.oschina.net/u/876354/blog/1633143\nVGGNet 2014 年，牛津大学计算机视觉组（Visual Geometry Group）和 Google DeepMind 公司的研究员一起研发出了新的深度卷积神经网络：VGGNet，并取得了 ILSVRC2014 比赛分类项目的第二名（第一名是 GoogLeNet，也是同年提出的）和定位项目的第一名。 VGGNet 探索了卷积神经网络的深度与其性能之间的关系，成功地构筑了 16~19 层深的卷积神经网络，证明了增加网络的深度能够在一定程度上影响网络最终的性能，使错误率大幅下降，同时拓展性又很强，迁移到其它图片数据上的泛化性也非常好。到目前为止，VGG 仍然被用来提取图像特征。 VGGNet 可以看成是加深版本的 AlexNet，都是由卷积层、全连接层两大部分构成。\nVGG 的特点 先看一下 VGG 的结构图 1、结构简洁 VGG 由 5 层卷积层、3 层全连接层、softmax 输出层构成，层与层之间使用 max-pooling（最大化池）分开，所有隐层的激活单元都采用 ReLU 函数。 2、小卷积核和多卷积子层 VGG 使用多个较小卷积核（3x3）的卷积层代替一个卷积核较大的卷积层，一方面可以减少参数，另一方面相当于进行了更多的非线性映射，可以增加网络的拟合 / 表达能力。 小卷积核是 VGG 的一个重要特点，虽然 VGG 是在模仿 AlexNet 的网络结构，但没有采用 AlexNet 中比较大的卷积核尺寸（如 7x7），而是通过降低卷积核的大小（3x3），增加卷积子层数来达到同样的性能（VGG：从 1 到 4 卷积子层，AlexNet：1 子层）。 VGG 的作者认为两个 3x3 的卷积堆叠获得的感受野大小，相当一个 5x5 的卷积；而 3 个 3x3 卷积的堆叠获取到的感受野相当于一个 7x7 的卷积。这样可以增加非线性映射，也能很好地减少参数（例如 7x7 的参数为 49 个，而 3 个 3x3 的参数为 27），如下图所示： 3、小池化核 相比 AlexNet 的 3x3 的池化核，VGG 全部采用 2x2 的池化核。 4、通道数多 VGG 网络第一层的通道数为 64，后面每层都进行了翻倍，最多到 512 个通道，通道数的增加，使得更多的信息可以被提取出来。 5、层数更深、特征图更宽 由于卷积核专注于扩大通道数、池化专注于缩小宽和高，使得模型架构上更深更宽的同时，控制了计算量的增加规模。 6、全连接转卷积（测试阶段） 这也是 VGG 的一个特点，在网络测试阶段将训练阶段的三个全连接替换为三个卷积，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高为的输入，这在测试阶段很重要。 如本节第一个图所示，输入图像是 224x224x3，如果后面三个层都是全连接，那么在测试阶段就只能将测试的图像全部都要缩放大小到 224x224x3，才能符合后面全连接层的输入数量要求，这样就不便于测试工作的开展。 而 “全连接转卷积”，替换过程如下： 例如 7x7x512 的层要跟 4096 个神经元的层做全连接，则替换为对 7x7x512 的层作通道数为 4096、卷积核为 1x1 的卷积。 这个 “全连接转卷积” 的思路是 VGG 作者参考了 OverFeat 的工作思路，例如下图是 OverFeat 将全连接换成卷积后，则可以来处理任意分辨率（在整张图）上计算卷积，这就是无需对原图做重新缩放处理的优势。 VGG 的网络结构 下图是来自论文《Very Deep Convolutional Networks for Large-Scale Image Recognition》（基于甚深层卷积网络的大规模图像识别）的 VGG 网络结构，正是在这篇论文中提出了 VGG，如下图： 在这篇论文中分别使用了 A、A-LRN、B、C、D、E 这 6 种网络结构进行测试，这 6 种网络结构相似，都是由 5 层卷积层、3 层全连接层组成，其中区别在于每个卷积层的子层数量不同，从 A 至 E 依次增加（子层数量从 1 到 4），总的网络深度从 11 层到 19 层（添加的层以粗体显示），表格中的卷积层参数表示为 “conv⟨感受野大小⟩- 通道数⟩”，例如 con3-128，表示使用 3x3 的卷积核，通道数为 128。为了简洁起见，在表格中不显示 ReLU 激活功能。 其中，网络结构 D 就是著名的 VGG16，网络结构 E 就是著名的 VGG19。\n以网络结构 D（VGG16）为例，介绍其处理过程如下，请对比上面的表格和下方这张图，留意图中的数字变化，有助于理解 VGG16 的处理过程： 1、输入 224x224x3 的图片，经 64 个 3x3 的卷积核作两次卷积 + ReLU，卷积后的尺寸变为 224x224x64 2、作 max pooling（最大化池化），池化单元尺寸为 2x2（效果为图像尺寸减半），池化后的尺寸变为 112x112x64 3、经 128 个 3x3 的卷积核作两次卷积 + ReLU，尺寸变为 112x112x128 4、作 2x2 的 max pooling 池化，尺寸变为 56x56x128 5、经 256 个 3x3 的卷积核作三次卷积 + ReLU，尺寸变为 56x56x256 6、作 2x2 的 max pooling 池化，尺寸变为 28x28x256 7、经 512 个 3x3 的卷积核作三次卷积 + ReLU，尺寸变为 28x28x512 8、作 2x2 的 max pooling 池化，尺寸变为 14x14x512 9、经 512 个 3x3 的卷积核作三次卷积 + ReLU，尺寸变为 14x14x512 10、作 2x2 的 max pooling 池化，尺寸变为 7x7x512 11、与两层 1x1x4096，一层 1x1x1000 进行全连接 + ReLU（共三层） 12、通过 softmax 输出 1000 个预测结果\n以上就是 VGG16（网络结构 D）各层的处理过程，A、A-LRN、B、C、E 其它网络结构的处理过程也是类似，执行过程如下（以 VGG16 为例）： 从上面的过程可以看出 VGG 网络结构还是挺简洁的，都是由小卷积核、小池化核、ReLU 组合而成。其简化图如下（以 VGG16 为例）： A、A-LRN、B、C、D、E 这 6 种网络结构的深度虽然从 11 层增加至 19 层，但参数量变化不大，这是由于基本上都是采用了小卷积核（3x3，只有 9 个参数），这 6 种结构的参数数量（百万级）并未发生太大变化，这是因为在网络中，参数主要集中在全连接层。 经作者对 A、A-LRN、B、C、D、E 这 6 种网络结构进行单尺度的评估，错误率结果如下： 从上表可以看出：\n1、LRN 层无性能增益（A-LRN）\nVGG 作者通过网络 A-LRN 发现，AlexNet 曾经用到的 LRN 层（local response normalization，局部响应归一化）并没有带来性能的提升，因此在其它组的网络中均没再出现 LRN 层。\n2、随着深度增加，分类性能逐渐提高（A、B、C、D、E）\n从 11 层的 A 到 19 层的 E，网络深度增加对 top1 和 top5 的错误率下降很明显。\n3、多个小卷积核比单个大卷积核性能好（B）\nVGG 作者做了实验用 B 和自己一个不在实验组里的较浅网络比较，较浅网络用 conv5x5 来代替 B 的两个 conv3x3，结果显示多个小卷积核比单个大卷积核效果要好。\n最后进行个小结：\n1、通过增加深度能有效地提升性能；\n2、最佳模型：VGG16，从头到尾只有 3x3 卷积与 2x2 池化，简洁优美；\n3、卷积可代替全连接，可适应各种尺寸的图片\n编程实现 ILSVRC2014 数据集在image-net下载目前需要注册，并且需要审批比较麻烦，可以在阿里云天池数据集上下载ILSVRC2017版本（可以使用钉钉或者支付宝实名认证登录下，很多大型数据集都可以登录后直接下载），地址：https://tianchi.aliyun.com/dataset/92252，下载imagenet_object_localization_patched2019 (1).tar.gz，数据集大小155GB 由于数据集过大，我这里依然使用cifar10\nVGGNet和AlexNet都是深度神经网络模型，VGGNet比AlexNet更深，因此它需要更多的计算资源和时间来训练。具体来说，VGGNet有16层或19层，而AlexNet只有8层。这意味着VGGNet需要处理更多的参数和数据，需要更长的训练时间。此外，VGGNet使用了更小的卷积核，这也导致了更多的计算量。所以，VGGNet训练比AlexNet慢很多是很正常的。\nimport tensorflow as tf\rfrom tensorflow.keras.datasets import cifar10\rfrom tensorflow.python.ops.numpy_ops import np_config\rfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\rfrom tensorflow.keras.models import Sequential\rnp_config.enable_numpy_behavior()\r(x_train, y_train), (x_test, y_test) = cifar10.load_data()\rdef cifar10_generator(x, y, batch_size):\rwhile True:\rfor i in range(0, len(x), batch_size):\rx_batch = x[i:i+batch_size]\ry_batch = y[i:i+batch_size]\rx_batch = tf.image.resize_with_pad(x_batch, target_height=224, target_width=224)\rx_batch = x_batch.astype('float32') / 255.0\ryield x_batch, y_batch\rdef vggnet(input_shape, num_classes):\r# 定义VGGNet\rmodel = Sequential([\r# 第一层卷积和池化\rConv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape),\rConv2D(64, (3, 3), activation='relu', padding='same'),\rMaxPooling2D((2, 2)),\r# 第二层卷积和池化\rConv2D(128, (3, 3), activation='relu', padding='same'),\rConv2D(128, (3, 3), activation='relu', padding='same'),\rMaxPooling2D((2, 2)),\r# 第三层卷积和池化\rConv2D(256, (3, 3), activation='relu', padding='same'),\rConv2D(256, (3, 3), activation='relu', padding='same'),\rConv2D(256, (3, 3), activation='relu', padding='same'),\rMaxPooling2D((2, 2)),\r# 第四层卷积和池化\rConv2D(512, (3, 3), activation='relu', padding='same'),\rConv2D(512, (3, 3), activation='relu', padding='same'),\rConv2D(512, (3, 3), activation='relu', padding='same'),\rMaxPooling2D((2, 2)),\r# 第五层卷积和池化\rConv2D(512, (3, 3), activation='relu', padding='same'),\rConv2D(512, (3, 3), activation='relu', padding='same'),\rConv2D(512, (3, 3), activation='relu', padding='same'),\rMaxPooling2D((2, 2)),\r# 将输出的特征图展平，并连接全连接层\rFlatten(),\rDense(4096, activation='relu'),\rDense(4096, activation='relu'),\rDense(10, activation='softmax')\r])\rreturn model\r# 定义一些超参数\rbatch_size = 128\repochs = 5\rlearning_rate = 0.001\r# 定义生成器\rtrain_generator = cifar10_generator(x_train, y_train, batch_size)\rtest_generator = cifar10_generator(x_test, y_test, batch_size)\r# 定义模型\rinput_shape = (224,224,3)\rnum_classes = 10\rmodel = vggnet(input_shape, num_classes)\rmodel.summary()\r# 编译模型\roptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\rmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r# 定义 ModelCheckpoint 回调函数\rcheckpoint = tf.keras.callbacks.ModelCheckpoint('./VGGNet.h5', save_best_only=True, save_weights_only=False, monitor='val_loss')\r# 训练模型\rmodel.fit(train_generator,\repochs=epochs,\rsteps_per_epoch=len(x_train)//batch_size,\rvalidation_data=test_generator,\rvalidation_steps=len(x_test)//batch_size,\rcallbacks=[checkpoint]\r)\rtest_loss, test_acc = model.evaluate(test_generator, y_test)\rprint('Test accuracy:', test_acc) 参考:https://my.oschina.net/u/876354/blog/1634322\nGoogLeNet 2014 年，GoogLeNet 和 VGG 是当年 ImageNet 挑战赛 (ILSVRC14) 的双雄，GoogLeNet 获得了第一名、VGG 获得了第二名，这两类模型结构的共同特点是层次更深了。VGG 继承了 LeNet 以及 AlexNet 的一些框架结构，而 GoogLeNet 则做了更加大胆的网络结构尝试，虽然深度只有 22 层，但大小却比 AlexNet 和 VGG 小很多，GoogleNet 参数为 500 万个，AlexNet 参数个数是 GoogleNet 的 12 倍，VGGNet 参数又是 AlexNet 的 3 倍，因此在内存或计算资源有限时，GoogleNet 是比较好的选择；从模型结果来看，GoogLeNet 的性能却更加优越。\n小知识：GoogLeNet 是谷歌（Google）研究出来的深度网络结构，为什么不叫 “GoogleNet”，而叫 “GoogLeNet”，据说是为了向 “LeNet” 致敬，因此取名为 “GoogLeNet”\n那么，GoogLeNet 是如何进一步提升性能的呢？\n一般来说，提升网络性能最直接的办法就是增加网络深度和宽度，深度指网络层次数量、宽度指神经元数量。但这种方式存在以下问题：\n（1）参数太多，如果训练数据集有限，很容易产生过拟合；\n（2）网络越大、参数越多，计算复杂度越大，难以应用；\n（3）网络越深，容易出现梯度弥散问题（梯度越往后穿越容易消失），难以优化模型。\n所以，有人调侃 “深度学习” 其实是 “深度调参”。\n解决这些问题的方法当然就是在增加网络深度和宽度的同时减少参数，为了减少参数，自然就想到将全连接变成稀疏连接。但是在实现上，全连接变成稀疏连接后实际计算量并不会有质的提升，因为大部分硬件是针对密集矩阵计算优化的，稀疏矩阵虽然数据量少，但是计算所消耗的时间却很难减少。\n那么，有没有一种方法既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能。大量的文献表明可以将稀疏矩阵聚类为较为密集的子矩阵来提高计算性能，就如人类的大脑是可以看做是神经元的重复堆积，因此，GoogLeNet 团队提出了 Inception 网络结构，就是构造一种 “基础神经元” 结构，来搭建一个稀疏性、高计算性能的网络结构。 【问题来了】什么是 Inception 呢？ Inception 历经了 V1、V2、V3、V4 等多个版本的发展，不断趋于完善，下面一一进行介绍\nInception V1 通过设计一个稀疏网络结构，但是能够产生稠密的数据，既能增加神经网络表现，又能保证计算资源的使用效率。谷歌提出了最原始 Inception 的基本结构： 该结构将 CNN 中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积、池化后的尺寸相同，将通道相加），一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。 网络卷积层中的网络能够提取输入的每一个细节信息，同时 5x5 的滤波器也能够覆盖大部分接受层的的输入。还可以进行一个池化操作，以减少空间大小，降低过度拟合。在这些层之上，在每一个卷积层后都要做一个 ReLU 操作，以增加网络的非线性特征。 然而这个 Inception 原始版本，所有的卷积核都在上一层的所有输出上来做，而那个 5x5 的卷积核所需的计算量就太大了，造成了特征图的厚度很大，为了避免这种情况，在 3x3 前、5x5 前、max pooling 后分别加上了 1x1 的卷积核，以起到了降低特征图厚度的作用，这也就形成了 Inception v1 的网络结构，如下图所示： 1x1 的卷积核有什么用呢？ 1x1 卷积的主要目的是为了减少维度，还用于修正线性激活（ReLU）。比如，上一层的输出为 100x100x128，经过具有 256 个通道的 5x5 卷积层之后 (stride=1，pad=2)，输出数据为 100x100x256，其中，卷积层的参数为 128x5x5x256= 819200。而假如上一层输出先经过具有 32 个通道的 1x1 卷积层，再经过具有 256 个输出的 5x5 卷积层，那么输出数据仍为为 100x100x256，但卷积参数量已经减少为 128x1x1x32 + 32x5x5x256= 204800，大约减少了 4 倍。\n基于 Inception 构建了 GoogLeNet 的网络结构如下（共 22 层）： 对上图说明如下：\n（1）GoogLeNet 采用了模块化的结构（Inception 结构），方便增添和修改；\n（2）网络最后采用了 average pooling（平均池化）来代替全连接层，该想法来自 NIN（Network in Network），事实证明这样可以将准确率提高 0.6%。但是，实际在最后还是加了一个全连接层，主要是为了方便对输出进行灵活调整；\n（3）虽然移除了全连接，但是网络中依然使用了 Dropout ;\n（4）为了避免梯度消失，网络额外增加了 2 个辅助的 softmax 用于向前传导梯度（辅助分类器）。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的 softmax 会被去掉。\nGoogLeNet 的网络结构图细节如下： 注：上表中的 “#3x3 reduce”，“#5x5 reduce” 表示在 3x3，5x5 卷积操作之前使用了 1x1 卷积的数量。\nGoogLeNet 网络结构明细表解析如下：\n0、输入\n原始输入图像为 224x224x3，且都进行了零均值化的预处理操作（图像每个像素减去均值）。\n1、第一层（卷积层）\n使用 7x7 的卷积核（滑动步长 2，padding 为 3），64 通道，输出为 112x112x64，卷积后进行 ReLU 操作\n经过 3x3 的 max pooling（步长为 2），输出为 ((112 - 3+1)/2)+1=56，即 56x56x64，再进行 ReLU 操作\n2、第二层（卷积层）\n使用 3x3 的卷积核（滑动步长为 1，padding 为 1），192 通道，输出为 56x56x192，卷积后进行 ReLU 操作\n经过 3x3 的 max pooling（步长为 2），输出为 ((56 - 3+1)/2)+1=28，即 28x28x192，再进行 ReLU 操作\n3a、第三层（Inception 3a 层）\n分为四个分支，采用不同尺度的卷积核来进行处理\n（1）64 个 1x1 的卷积核，然后 RuLU，输出 28x28x64\n（2）96 个 1x1 的卷积核，作为 3x3 卷积核之前的降维，变成 28x28x96，然后进行 ReLU 计算，再进行 128 个 3x3 的卷积（padding 为 1），输出 28x28x128\n（3）16 个 1x1 的卷积核，作为 5x5 卷积核之前的降维，变成 28x28x16，进行 ReLU 计算后，再进行 32 个 5x5 的卷积（padding 为 2），输出 28x28x32\n（4）pool 层，使用 3x3 的核（padding 为 1），输出 28x28x192，然后进行 32 个 1x1 的卷积，输出 28x28x32。\n将四个结果进行连接，对这四部分输出结果的第三维并联，即 64+128+32+32=256，最终输出 28x28x256\n3b、第三层（Inception 3b 层）\n（1）128 个 1x1 的卷积核，然后 RuLU，输出 28x28x128\n（2）128 个 1x1 的卷积核，作为 3x3 卷积核之前的降维，变成 28x28x128，进行 ReLU，再进行 192 个 3x3 的卷积（padding 为 1），输出 28x28x192\n（3）32 个 1x1 的卷积核，作为 5x5 卷积核之前的降维，变成 28x28x32，进行 ReLU 计算后，再进行 96 个 5x5 的卷积（padding 为 2），输出 28x28x96\n（4）pool 层，使用 3x3 的核（padding 为 1），输出 28x28x256，然后进行 64 个 1x1 的卷积，输出 28x28x64。\n将四个结果进行连接，对这四部分输出结果的第三维并联，即 128+192+96+64=480，最终输出输出为 28x28x480\n第四层（4a,4b,4c,4d,4e）、第五层（5a,5b）……，与 3a、3b 类似，在此就不再重复。\n从 GoogLeNet 的实验结果来看，效果很明显，差错率比 MSRA、VGG 等模型都要低，对比结果如下表所示： Inception V2 GoogLeNet 凭借其优秀的表现，得到了很多研究人员的学习和使用，因此 GoogLeNet 团队又对其进行了进一步地发掘改进，产生了升级版本的 GoogLeNet。\nGoogLeNet 设计的初衷就是要又准又快，而如果只是单纯的堆叠网络虽然可以提高准确率，但是会导致计算效率有明显的下降，所以如何在不增加过多计算量的同时提高网络的表达能力就成为了一个问题。\nInception V2 版本的解决方案就是修改 Inception 的内部计算逻辑，提出了比较特殊的 “卷积” 计算结构。\n1、卷积分解（Factorizing Convolutions）\n大尺寸的卷积核可以带来更大的感受野，但也意味着会产生更多的参数，比如 5x5 卷积核的参数有 25 个，3x3 卷积核的参数有 9 个，前者是后者的 25/9=2.78 倍。因此，GoogLeNet 团队提出可以用 2 个连续的 3x3 卷积层组成的小网络来代替单个的 5x5 卷积层，即在保持感受野范围的同时又减少了参数量，如下图： 那么这种替代方案会造成表达能力的下降吗？通过大量实验表明，并不会造成表达缺失。 可以看出，大卷积核完全可以由一系列的 3x3 卷积核来替代，那能不能再分解得更小一点呢？GoogLeNet 团队考虑了 nx1 的卷积核，如下图所示，用 3 个 3x1 取代 3x3 卷积： 因此，任意 nxn 的卷积都可以通过 1xn 卷积后接 nx1 卷积来替代。GoogLeNet 团队发现在网络的前期使用这种分解效果并不好，在中度大小的特征图（feature map）上使用效果才会更好（特征图大小建议在 12 到 20 之间）。 2、降低特征图大小\n一般情况下，如果想让图像缩小，可以有如下两种方式： 先池化再作 Inception 卷积，或者先作 Inception 卷积再作池化。但是方法一（左图）先作 pooling（池化）会导致特征表示遇到瓶颈（特征缺失），方法二（右图）是正常的缩小，但计算量很大。为了同时保持特征表示且降低计算量，将网络结构改为下图，使用两个并行化的模块来降低计算量（卷积、池化并行执行，再进行合并） 使用 Inception V2 作改进版的 GoogLeNet，网络结构图如下： 注：上表中的 Figure 5 指没有进化的 Inception，Figure 6 是指小卷积版的 Inception（用 3x3 卷积核代替 5x5 卷积核），Figure 7 是指不对称版的 Inception（用 1xn、nx1 卷积核代替 nxn 卷积核）。\n经实验，模型结果与旧的 GoogleNet 相比有较大提升，如下表所示： Inception V3 Inception V3 一个最重要的改进是分解（Factorization），将 7x7 分解成两个一维的卷积（1x7,7x1），3x3 也是一样（1x3,3x1），这样的好处，既可以加速计算，又可以将 1 个卷积拆成 2 个卷积，使得网络深度进一步增加，增加了网络的非线性（每增加一层都要进行 ReLU）。 另外，网络输入从 224x224 变为了 299x299。\nInception V4 Inception V4 研究了 Inception 模块与残差连接的结合。ResNet 结构大大地加深了网络深度，还极大地提升了训练速度，同时性能也有提升（ResNet 的技术原理介绍见本博客之前的文章：大话深度残差网络 ResNet）。 Inception V4 主要利用残差连接（Residual Connection）来改进 V3 结构，得到 Inception-ResNet-v1，Inception-ResNet-v2，Inception-v4 网络。 ResNet 的残差结构如下： 将该结构与 Inception 相结合，变成下图： 通过 20 个类似的模块组合，Inception-ResNet 构建如下： 编程实现 后续补充\n参考:https://my.oschina.net/u/876354/blog/1637819",
    "description": "简介 卷积神经网络（CNN）是深度学习中非常重要的一种网络结构，它可以处理图像、文本、语音等各种类型的数据。以下是CNN的前4个经典模型\nLeNet-5 LeNet-5是由Yann LeCun等人于1998年提出的，是第一个成功应用于手写数字识别的卷积神经网络。它由7层神经网络组成，包括2层卷积层、2层池化层和3层全连接层。其中，卷积层提取图像特征，池化层降低特征图的维度，全连接层将特征映射到对应的类别上。\nLeNet-5的主要特点是使用Sigmoid激活函数、平均池化和卷积层后没有使用零填充。它在手写数字识别、人脸识别等领域都有着广泛的应用。\nAlexNet AlexNet是由Alex Krizhevsky等人于2012年提出的，是第一个在大规模图像识别任务中取得显著成果的卷积神经网络。它由5层卷积层、3层全连接层和1层Softmax输出层组成，其中使用了ReLU激活函数、最大池化和Dropout技术。\nAlexNet的主要特点是使用了GPU加速训练、数据增强和随机化Dropout等技术，使得模型的泛化能力和鲁棒性得到了大幅提升。它在ImageNet大规模图像识别比赛中取得了远超其他模型的优异成绩。\nVGGNet VGGNet是由Karen Simonyan和Andrew Zisserman于2014年提出的，它是一个非常深的卷积神经网络，有16层或19层。VGGNet的每个卷积层都使用了3x3的卷积核和ReLU激活函数，使得它的网络结构非常清晰、易于理解。\nVGGNet的主要特点是使用了更深的网络结构、小卷积核和少量的参数，使得模型的特征提取能力得到了进一步提升。它在ImageNet比赛中也获得了非常好的成绩。\nGoogLeNet GoogLeNet是由Google团队于2014年提出的，它是一个非常深的卷积神经网络，有22层。它使用了一种称为Inception模块的结构，可以在保持网络深度的同时减少参数量。\nGoogLeNet的主要特点是使用了Inception模块、1x1卷积核和全局平均池化等技术，使得模型的计算复杂度得到了大幅降低。它在ImageNet比赛中获得了非常好的成绩，并且被广泛应用于其他领域。\nCNN回顾 回顾一下 CNN 的几个特点：局部感知、参数共享、池化。\n局部感知 人类对外界的认知一般是从局部到全局、从片面到全面，类似的，在机器识别图像时也没有必要把整张图像按像素全部都连接到神经网络中，在图像中也是局部周边的像素联系比较紧密，而距离较远的像素则相关性较弱，因此可以采用局部连接的模式（将图像分块连接，这样能大大减少模型的参数），如下图所示： 参数（权值）共享 每张自然图像（人物、山水、建筑等）都有其固有特性，也就是说，图像其中一部分的统计特性与其它部分是接近的。这也意味着这一部分学习的特征也能用在另一部分上，能使用同样的学习特征。因此，在局部连接中隐藏层的每一个神经元连接的局部图像的权值参数（例如 5×5），将这些权值参数共享给其它剩下的神经元使用，那么此时不管隐藏层有多少个神经元，需要训练的参数就是这个局部图像的权限参数（例如 5×5），也就是卷积核的大小，这样大大减少了训练参数。如下图 卷积核的权值是指每个卷积核中的参数，用于对输入数据进行卷积操作时，对每个位置的像素进行加权求和。在卷积神经网络中，同一层中的所有卷积核的权值是共享的，这意味着每个卷积核在不同位置上的权值是相同的。共享权值可以减少模型中需要学习的参数数量，从而降低了模型的复杂度，同时可以提高模型的泛化能力，因为共享权值可以使模型更加稳定，避免过度拟合。共享权值的实现方式是通过使用相同的卷积核对输入数据进行卷积操作。 池化 随着模型网络不断加深，卷积核越来越多，要训练的参数还是很多，而且直接拿卷积核提取的特征直接训练也容易出现过拟合的现象。回想一下，之所以对图像使用卷积提取特征是因为图像具有一种 “静态性” 的属性，因此，一个很自然的想法就是对不同位置区域提取出有代表性的特征（进行聚合统计，例如最大值、平均值等），这种聚合的操作就叫做池化，池化的过程通常也被称为特征映射的过程（特征降维），如下图：\nLeNet-5 概述 LeNet5 诞生于 1994 年，是最早的卷积神经网络之一， 由 Yann LeCun 完成，推动了深度学习领域的发展。在那时候，没有 GPU 帮助训练模型，甚至 CPU 的速度也很慢，因此，LeNet5 通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络也是最近大量神经网络架构的起点，给这个领域带来了许多灵感。 LeNet5 的网络结构示意图如下所示： LeNet5 由 7 层 CNN（不包含输入层）组成，上图中输入的原始图像大小是 32×32 像素，卷积层用 Ci 表示，子采样层（pooling，池化）用 Si 表示，全连接层用 Fi 表示。下面逐层介绍其作用和示意图上方的数字含义。",
    "tags": [],
    "title": "深度学习04-CNN经典模型",
    "uri": "/docs/programming/ai/deep_learning/cnn/dl_04_cnn_models/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 循环神经网络",
    "content": "@[toc]\n概述 循环神经网络（Recurrent Neural Network，RNN）是一种具有循环连接的神经网络结构，被广泛应用于自然语言处理、语音识别、时序数据分析等任务中。相较于传统神经网络，RNN的主要特点在于它可以处理序列数据，能够捕捉到序列中的时序信息。\nRNN的基本单元是一个循环单元（Recurrent Unit），它接收一个输入和一个来自上一个时间步的隐藏状态，并输出当前时间步的隐藏状态。在传统的RNN中，循环单元通常使用tanh或ReLU等激活函数。\n基本循环神经网络 原理 基本的 循环神经网络，结构由 输入层、一个隐藏层和输出层 组成。\n$x$是输入向量，$o$是输出向量，$s$表示隐藏层的值；$U$是输入层到隐藏层的权重矩阵，$V$是隐藏层到输出层的权重矩阵。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入$x$，还取决于上一次隐藏层的值$s$。权重矩阵W就是隐藏层上一次的值作为这一次的输入的权重。 将上图的基本RNN结构在时间维度展开(RNN是一个链式结构，每个时间片使用的是相同的参数,t表示t时刻)： 现在看上去就会清楚许多，这个网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。关键一点是$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t−1}$。 公式1：$s_t=f(U∗x_t+W∗s_{t−1}+B1)$ 公式2：$o_t=g(V∗s_t+B2)$\n式1是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次隐藏层值$S_{t−1}$作为这一次的输入的权重矩阵，f是激活函数。 式2是输出层的计算公式，V是输出层的权重矩阵，g是激活函数,B1,B2是偏置假设为0。 隐含层有两个输入，第一是U与$x_t$向量的乘积，第二是上一隐含层输出的状态$s_t−1$和W的乘积。等于上一个时刻计算的$s_t−1$需要缓存一下，在本次输入$x_t$一起计算，共同输出最后的$o_t$。\n如果反复把式1带入式2，我们将得到： 从上面可以看出，循环神经网络的输出值ot，是受前面历次输入值、、、、、、、、$x_t$、$x_{t−1}$、$x_{t−2}$、$x_{t−3}$、…影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。这样其实不好，因为如果太前面的值和后面的值已经没有关系了，循环神经网络还考虑前面的值的话，就会影响后面值的判断。\n上面是整个单向单层NN的前向传播过程\n为了更快理解输入x输入格式下面使用nlp中Word Embedding讲解下。\nWord Embedding 首先我们需要对输入文本x进行编码，使之成为计算机可以读懂的语言，在编码时，我们期望句子之间保持词语间的相似行，词的向量表示是进行机器学习和深度学习的基础。\nword embedding的一个基本思路就是，我们把一个词映射到语义空间的一个点，把一个词映射到低维的稠密空间，这样的映射使得语义上比较相似的词，他在语义空间的距离也比较近，如果两个词的关系不是很接近，那么在语义空间中向量也会比较远。\n如上图英语和西班牙语映射到语义空间，语义相同的数字他们在语义空间分布的位置是相同的 简单回顾一下word embedding,对于nlp来说，我们输入的是一个个离散的符号，对于神经网络来说，它处理的都是向量或者矩阵。所以第一步，我们需要把一个词编码成向量。最简单的就是one-hot的表示方法。如下图所示： python代码（one-hot），比如\nimport numpy as np\rword_array = ['apple', 'kiwi', 'mango']\rword_dict = {'apple': 0, 'banana': 1, 'orange': 2, 'grape': 3, 'melon': 4, 'peach': 5, 'pear': 6, 'kiwi': 7, 'plum': 8, 'mango': 9}\r# 创建一个全为0的矩阵\rone_hot_matrix = np.zeros((len(word_array), len(word_dict)))\r# 对每个单词进行one-hot编码\rfor i, word in enumerate(word_array):\rword_index = word_dict[word]\rone_hot_matrix[i, word_index] = 1\rprint(one_hot_matrix) 输出:\n[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] #这就是apple的one-hot编码\r[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] #这就是kiwi的one-hot编码\r[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]] #这就是mango的one-hot编码 行表示每个单词，列表示语料库，每个列对应一个语料单词，也就是特征列\n虽然one-hot编码是一种简单有效的特征表示方法，但它也存在一些缺点：\n高维度表示：使用one-hot编码时，每个特征都需要创建一个很大的稀疏向量，维度与特征的唯一值数量相等。这会导致高维度的输入数据，增加了计算和存储的开销。特别是在处理具有大量离散特征的问题时，会导致非常庞大的特征空间。\n维度独立性：one-hot编码将每个特征都表示为独立的二进制特征，没有考虑到特征之间的相关性和语义关系。这可能会导致模型难以捕捉到特征之间的相互作用和关联性，从而影响了模型的性能。\n无法处理未知特征：one-hot编码要求特征的唯一值在训练集中都出现过，否则会出现问题。如果在测试集或实际应用中遇到了未在训练集中出现的特征值，就无法进行one-hot编码，这可能导致模型无法处理这些未知特征。\n特征稀疏性：由于one-hot编码的特征向量是稀疏的，大部分元素都是0，这会导致数据稀疏性增加，对于一些算法（如线性模型）可能会带来一些问题。\n综上所述，尽管one-hot编码在某些情况下是一种简单有效的特征表示方法，但它也存在一些缺点，特别是在处理高维度离散特征、考虑特征间关系和处理未知特征值时可能会遇到问题。\n使用nn.Embedding替代one-hot编码的原因主要有两点：\n维度灵活性：使用one-hot编码时，每个特征都需要创建一个很大的稀疏向量，维度与特征的唯一值数量相等。这会导致高维度的输入，增加了计算和存储的开销。而使用嵌入（embedding）可以将离散特征映射为低维度的连续向量表示，减少了存储和计算的成本。\n语义关系和相似性：嵌入向量可以捕捉到特征之间的语义关系和相似性。例如，在自然语言处理任务中，使用嵌入向量可以将单词映射为连续的向量表示，使得具有相似语义含义的单词在嵌入空间中距离较近。这样的特性可以帮助模型更好地理解和学习特征之间的关系，提升模型的性能。\n因此，使用nn.Embedding替代one-hot编码可以提高模型的效率和性能，特别是在处理高维度的离散特征时。\n好的，我们来看一个简单的例子来手推nn.embedding的两个参数的作用。\n假设我们有一个句子分类任务，我们的输入是一个句子，每个单词都是一个特征。我们有5个不同的单词，分别是[“I”, “love”, “deep”, “learning”, “!” ]。\n我们可以使用nn.embedding来将这些单词映射为嵌入向量（在坐标系中有一个位置指向了这个单词）。假设我们将每个单词嵌入为一个3维的向量。这里，num_embeddings为5，表示我们有5个不同的单词；embedding_dim为3，表示每个单词嵌入为一个3维的向量。\n我们可以用下面的表格来表示每个单词的嵌入向量：\n单词 嵌入向量 “I” [0.1, 0.2, 0.3] “love” [0.4, 0.5, 0.6] “deep” [0.7, 0.8, 0.9] “learning” [0.2, 0.3, 0.4] “!” [0.5, 0.6, 0.7] 通过nn.embedding，我们可以将句子中的每个单词转换为对应的嵌入向量。例如，句子\"I love deep learning!“可以转换为以下嵌入向量序列：\n[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [0.2, 0.3, 0.4], [0.5, 0.6, 0.7]]\n这样，我们就可以将离散的单词特征转换为连续的嵌入向量，在深度学习模型中使用。\n以下是pytorch(python入门)的使用\n# 创建词汇表\rvocab = {\"I\": 0, \"love\": 1, \"deep\": 2, \"learning\": 3, \"!\": 4}\rstrings=[\"I\", \"love\", \"deep\", \"learning\", \"!\" ]\r# 将字符串序列转换为整数索引序列\rinput = t.LongTensor([vocab[word] for word in strings])\r#注意第一个参数是词汇表的个数，并不是输入单词的长度，你在这里就算填100也不影响最终的输出维度，这个输入值影响的是算出来的行向量值\r#nn.Embedding模块会随机初始化嵌入矩阵。在深度学习中，模型参数通常会使用随机初始化的方法来开始训练，以便模型能够在训练过程中学习到合适的参数值。\r#在nn.Embedding中，嵌入矩阵的每个元素都会被随机初始化为一个小的随机值，这些值将作为模型在训练过程中学习的可训练参数，可以使用manual_seed固定。\rt.manual_seed(1234)\rembedding=nn.Embedding(len(vocab),3)\rprint(embedding(input)) 输出结果为： tensor([[-0.1117, -0.4966, 0.1631], [-0.8817, 0.0539, 0.6684], [-0.0597, -0.4675, -0.2153], [ 0.8840, -0.7584, -0.3689], [-0.3424, -1.4020, 0.3206]], grad_fn=)\n注意Embedding第一个参数不是输入的字符的长度，而是词汇表的长度，比如有词汇表 {“I”: 0, “love”: 1, “deep”: 2, “learning”: 3, “!”: 4}，而输入input可能是：i love，此时应该传入的是5而不是2，因为预测最后隐藏层需要做个全连接用来预测当前输入单词对于整个词汇表的所有单词的概率。\nnn.Embedding 是一个简单的查找表，它的计算过程非常直接和简单。让我们来看一个具体的例子来说明它的计算过程。\n假设我们有一个词汇表包含以下单词：\n\"apple\" - 索引为0\r\"banana\" - 索引为1\r\"orange\" - 索引为2\r\"grape\" - 索引为3 我们还假设我们正在构建一个3维的词嵌入矩阵，所以每个单词会映射到一个3维向量。现在，让我们看看如何计算单词 “banana” 的向量坐标。 初始化词嵌入矩阵 初始化词嵌入矩阵的过程通常是随机的，其中每个单词的向量都会被初始化为随机的数值。这是因为在大多数情况下，我们不会有关于单词向量的先验信息，因此随机初始化是一个常见的做法。\n在实际中，词嵌入矩阵的初始化可以采用不同的方法，其中最常见的是以下两种：\n均匀分布初始化：每个单词的向量从一个均匀分布中随机抽样，通常在[-1, 1]或者[0, 1]之间。 正态分布初始化：每个单词的向量从一个正态分布（高斯分布）中随机抽样，通常具有均值为0和标准差为1的标准正态分布。 在 PyTorch 中，默认情况下，nn.Embedding 层的权重（即词嵌入矩阵）会在初始化时使用均匀分布或者正态分布进行随机初始化，具体取决于所选择的初始化方法。\n假设我们的初始化词嵌入矩阵如下所示（这里只是一个示例）：\n[\r[0.1, 0.2, 0.3] [0.4, 0.5, 0.6] [0.7, 0.8, 0.9] [1.0, 1.1, 1.2] ] 计算 “banana” 的向量坐标：由于 “banana” 的索引为1，因此我们只需找到词嵌入矩阵中的第二行，即：\n[\r[0.4, 0.5, 0.6]\r] 所以 “banana” 的向量坐标为 [0.4, 0.5, 0.6]。\n这就是 nn.Embedding 计算单词向量坐标的过程。它简单地根据单词的索引来查找词嵌入矩阵中对应的行。\nnn.Embedding 层在体现语义相关性方面的能力来自于它的训练过程和所使用的语料库。虽然 nn.Embedding 本身只是一个简单的查找表，它将每个单词映射到一个固定长度的向量，但是这些向量在训练过程中会根据模型任务的损失函数进行调整，以使得模型在语义上更加相似的单词在向量空间中更加接近。\n具体来说，当使用诸如语言模型、机器翻译、文本分类等任务进行端到端的训练时，nn.Embedding 层会根据模型的输出和损失函数的反馈进行调整，以最大程度地提高模型在任务上的性能。在这个过程中，如果两个单词在语义上相似（如 “apple” 和 “orange”），它们的词嵌入向量在向量空间中也会更加接近，以使得模型能够更好地捕捉它们之间的语义关系。\n另外，训练过程中使用的语料库也对词嵌入向量的语义相关性产生影响。如果语料库足够大且涵盖了各种语言使用情况，那么词嵌入向量往往能够更好地捕捉单词之间的语义关系。\n总的来说，nn.Embedding 层体现语义相关性的能力主要来自于两个方面：一是在训练过程中根据任务反馈调整词嵌入向量，使得语义相似的单词在向量空间中更加接近；二是在训练过程中使用的语料库的质量和规模。\npytorch rnn 以下是pytorch使用rnn最简单的一个例子，用来熟悉pytorch rnn 注意pytorch的rnn并不处理隐藏层到输出层的逻辑，他只是关注隐藏层的输出结果，如果需要将隐藏层转换为结果输出，可以在添加一个全连接层即可，这里暂不关注这部分\n#%%\rimport torch\rimport torch.nn as nn\r# 定义输入数据\rinput_size = 10 # 输入特征的维度\rsequence_length = 5 # 时间步个数\rbatch_size = 3 # 批次大小\r# 创建随机输入数据\r#输入数据的维度为(sequence_length, batch_size, input_size)，表示有sequence_length个时间步，\r#每个时间步有batch_size个样本，每个样本的特征维度为input_size。\rinput_data = torch.randn(sequence_length, batch_size, input_size)\rprint(\"输入数据\",input_data)\r# 定义RNN模型\r# 定义RNN模型时，我们指定了输入特征的维度input_size、隐藏层的维度hidden_size、隐藏层的层数num_layers等参数。\r# batch_first=False表示输入数据的维度中批次大小是否在第一个维度，我们在第二个维度上。\rrnn = nn.RNN(input_size, hidden_size=20, num_layers=1, batch_first=False)\r\"\"\"\r在前向传播过程中，我们将输入数据传递给RNN模型，并得到输出张量output和最后一个时间步的隐藏状态hidden。\r输出张量的大小为(sequence_length, batch_size, hidden_size)，表示每个时间步的隐藏层输出。\r最后一个时间步的隐藏状态的大小为(num_layers, batch_size, hidden_size)。\r\"\"\"\r# 前向传播，第二个参数h0未传递，默认为0\routput, hidden = rnn(input_data)\rprint(\"最后一个隐藏层\",hidden.shape)\rprint(\"输出所有隐藏层\",output.shape)\r# 打印每个隐藏层的权重和偏置项\r# weight_ih表示输入到隐藏层的权重，weight_hh表示隐藏层到隐藏层的权重，注意这里使出是转置的结果。\r# bias_ih表示输入到隐藏层的偏置，bias_hh表示隐藏层到隐藏层的偏置。\rfor name, param in rnn.named_parameters():\rif 'weight' in name or 'bias' in name:\rprint(name, param.data) 输出\n最后一个隐藏层 torch.Size([1, 3, 20])\r输出所有隐藏层 torch.Size([5, 3, 20]) 权重为什么是10行20列参数卷积神经网络的原理 数据最外层的行的长度决定了前向传播时间序列的个数。 这个input_size是输入数据的维度，比如一个单词转换为one-hot后列就是字典的特征长度 这个hidden_size是隐藏层神经元的个数也就是最终隐藏层输入的特征数。 num_layer中是堆叠的多层隐藏层。\n常见的结构 RNN（循环神经网络）常用的结果类型包括单输入单输出、单输入多输出、多输入多输出和多输入单输出。下面我将详细解释每种结果类型以及它们的应用场景。\n单输入单输出（Single Input Single Output，SISO）：这是最常见的RNN结果类型，输入是一个序列，输出是一个单一的预测值。例如，给定一段文本，预测下一个词语；给定一段时间序列数据，预测下一个时间步的值。这种结果类型适用于许多序列预测任务，如语言模型、时间序列预测等。 举个例子，假设我们要预测房屋价格，可能会使用多个特征，如房屋的面积、卧室数量、浴室数量等。这样，我们可以将这些特征组合成一个特征向量作为模型的输入，而模型的输出则是预测的房屋价格。因此，线性回归可以用来解决多特征到单个输出的问题，因此被称为单输入单输出模型。\n单输入多输出（Single Input Multiple Output，SIMO）：这种结果类型中，输入是一个序列，但输出是多个预测值。例如，给定一段文本，同时预测下一个词语和该词语的词性标签；给定一段音频信号，同时预测语音情感和说话者身份。这种结果类型适用于需要同时预测多个相关任务的情况。\n多输入多输出（Multiple Input Multiple Output，MIMO）：这种结果类型中，有多个输入序列和多个输出序列。例如，在机器翻译任务中，输入是源语言的句子序列，输出是目标语言的句子序列；在对话系统中，输入是用户的问题序列，输出是系统的回答序列。这种结果类型适用于需要处理多个输入和输出序列的任务，mimo有两种一种输入和输出个数相等和不相等。\n多输入单输出（Multiple Input Single Output，MISO）：这种结果类型中，有多个输入序列，但只有一个输出。例如，在图像描述生成任务中，输入是图像序列，输出是对图像的描述；在自动驾驶中，输入是多个传感器的数据序列，输出是车辆的控制命令。这种结果类型适用于需要将多个输入序列映射到单个输出序列的任务。\n线性回归是一种简单的机器学习模型，它的输入可以是多个特征，但是输出只有一个。这里的“单输入单输出”是指模型的输入是一个向量（多个特征的组合），输出是一个标量（一个预测值）。在线性回归中，我们通过对输入特征进行线性组合，得到一个预测值。因此，尽管输入可以是多个元素，但输出只有一个。\n双向循环神经网络 普通的RNN只能依据之前时刻的时序信息来预测下一时刻的输出，但在有些问题中，当前时刻的输出不仅和之前的状态有关，还可能和未来的状态有关系。\n比如预测一句话中缺失的单词不仅需要根据前文来判断，还需要考虑它后面的内容，真正做到基于上下文判断。\nBRNN有两个RNN上下叠加在一起组成的，输出由这两个RNN的状态共同决定。 先对图片和公式中的符号集中说明，需要时方便查看：\n$h_t^1$表示t 时刻，Cell1 中从左到右获得的 memory(信息); $W^1,U^1$ 表示图中 Cell1 的可学习参数,W是隐藏层的参数U是输入层参数； $f_1$ 表示 Cell1 的激活函数； $h_t^2$ 表示 t 时刻，Cell2 中从右到左获得的 memory; $W^2$,$U^2$ 表示图中 Cell2 的可学习参数； $f_2$ 表示 Cell2 的激活函数； $V$ 是输出层的参数，可以理解为 MLP; $f_3$ 是输出层的激活函数； $y_t$ 是 t 时刻的输出值； 在图1-1中，对于 t 时刻的输入$x_t$ ，可以结合从左到右的 memory $h^1_{t-1}$ , 获得当前时刻的 memory $h^1_t$: 同理也可以结合从右到左的 memory $h^2_{t−1}$ , 获得当前时刻的 memory $h^2_t$: 然后将 $h^1_t$ 和 $h^2_t$ 首尾级联在一起通过输出层网络 $V$ 得到输出 $y_t$ : 这样对于任何一个时刻 t 可以看到从不同方向获得的 memory, 使模型更容易优化，加速了模型的收敛速度。\npytorch rnn 下面是一个使用PyTorch中nn.RNN模块实现双向RNN的最简单例子：\nimport torch\rimport torch.nn as nn\r# 定义输入数据\rinput_size = 10 # 输入特征的维度\rsequence_length = 5 # 时间步个数\rbatch_size = 3 # 批次大小\r# 创建随机输入数据\rinput_data = torch.randn(sequence_length, batch_size, input_size)\r# 定义双向RNN模型\rrnn = nn.RNN(input_size, hidden_size=20, num_layers=1, batch_first=False, bidirectional=True)\r# 前向传播\routput, hidden = rnn(input_data)\r# 输出结果\rprint(\"输出张量大小：\", output.size())\rprint(\"最后一个时间步的隐藏状态大小：\", hidden.size()) 输出\n输出张量大小： torch.Size([5, 3, 40])\r最后一个时间步的隐藏状态大小： torch.Size([2, 3, 20]) 这个例子中，输入数据的维度和之前的例子相同。\n定义双向RNN模型时，我们在RNN模型的参数中设置bidirectional=True，表示我们希望构建一个双向RNN模型。\n在前向传播过程中，我们将输入数据传递给双向RNN模型，并得到输出张量output和最后一个时间步的隐藏状态hidden。输出张量的大小为(sequence_length, batch_size, hidden_sizenum_directions)，其中num_directions为2，表示正向和反向两个方向。最后一个时间步的隐藏状态的大小为(num_layersnum_directions, batch_size, hidden_size)。\n双向RNN可以同时利用过去和未来的信息，可以更好地捕捉到时间序列数据中的特征。你可以根据需要调整输入数据的大小、RNN模型的参数等进行实验。\n双向RNN的输出通常是正向和反向隐藏状态的组合，它们被存储在一个数组中。具体来说，如果使用PyTorch中的nn.RNN模块实现双向RNN，输出张量的形状将是(sequence_length,batch_size, hidden_size * 2)，其中hidden_size * 2表示正向和反向隐藏状态的大小之和。这个输出张量包含了每个时间步上正向和反向隐藏状态的信息，可以在后续的任务中使用。 双向rnn的最后的隐藏层大小是(2,, batch_size, hidden_size)\nDeep RNN(多层 RNN) 前文我们介绍的 RNN，是数据在时间维度上的变换。不论时间维度多长，只有一个 RNN 模块, 即只有一组待学习参数 (W, U)，属于单层 RNN。deep RNN 也叫做多层 RNN，顾名思义它由多个 RNN 级联组成，是输入数据在空间维度上变换。如图, 这是 L 层的 RNN 架构。每一层是一个单独的RNN，共有L个RNN。 在每一层的水平方向，只有一组可学习参数，如第 $l$ 层的参数$W^lU^l$。水平方向是数据沿着时间维度变换，变换机制与单个 RNN 的机制一致,具体参考式上一篇文章。在每个时刻 t 的垂直方向，共有 L 组可学习参数( $W^i,U^i$) i = 1, 2, …, L。在第 $l$ 层的第 t 时刻 Cell 的输入数据来自 2 个方向：一个是来自上一层的输出 $h^{l−1}t$ : 一个是来自第 $l$ 层，$t − 1$ 时刻的 memory 数据 $h^l{t−1}$ : 所以 Cell 的输出 $h^l_t$： 本质上，Deep RNN 在单个 RNN 的基础上，将当前时刻的输入修改为上层的输出。这样 RNN 便完成了空间上的数据变换。额外提一下：DeepRNN的每一层也可以是一个双向RNN。\npytorch rnn 下面是一个使用nn.RNN模块实现多层RNN的最简单例子：\nimport torch\rimport torch.nn as nn\r# 定义输入数据和参数\rinput_size = 5\rhidden_size = 10\rnum_layers = 2\rbatch_size = 3\rsequence_length = 4\r# 创建输入张量\rinput_tensor = torch.randn(sequence_length, batch_size, input_size)\r# 创建多层RNN模型\rrnn = nn.RNN(input_size, hidden_size, num_layers)\r# 前向传播\routput, hidden = rnn(input_tensor)\r# 打印输出张量和隐藏状态的大小\rprint(\"Output shape:\", output.shape)\rprint(\"Hidden state shape:\", hidden.shape) 在上面的例子中，我们首先定义了输入数据的维度、RNN模型的参数（输入大小、隐藏状态大小和层数），以及批次大小和序列长度。然后，我们创建了一个输入张量，其形状为(sequence_length, batch_size, input_size)。接下来，我们使用nn.RNN模块创建一个多层RNN模型，其中包含两层。最后，我们通过将输入张量传递给RNN模型的前向方法来进行前向传播，并打印输出张量和隐藏状态的大小。\n请注意，输出张量的形状为(sequence_length, batch_size, hidden_size)，其中sequence_length和batch_size保持不变，hidden_size是隐藏状态的大小。隐藏状态的形状为(num_layers, batch_size, hidden_size)，其中num_layers是RNN模型的层数。\nRNN缺点 梯度爆炸和消失问题 实践中前面介绍的几种RNNs并不能很好的处理较长的序列，RNN在训练中很容易发生梯度爆炸和梯度消失，这导致梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。\n通常来说，梯度爆炸更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。\n梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：\n1、合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。\n2、使用relu代替sigmoid和tanh作为激活函数。。\n3、使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法\n短期记忆 假如需要判断用户的说话意图（问天气、问时间、设置闹钟…），用户说了一句“what time is it？”我们需要先对这句话进行分词： 然后按照顺序输入 RNN ，我们先将 “what”作为 RNN 的输入，得到输出「01」 然后，我们按照顺序，将“time”输入到 RNN 网络，得到输出「02」。\n这个过程我们可以看到，输入 “time” 的时候，前面 “what” 的输出也产生了影响（隐藏层中有一半是黑色的）。 以此类推，前面所有的输入都对未来的输出产生了影响，大家可以看到圆形隐藏层中包含了前面所有的颜色。如下图所示： 当我们判断意图的时候，只需要最后一层的输出「05」，如下图所示： RNN 的缺点也比较明显 通过上面的例子，我们已经发现，短期的记忆影响较大（如橙色区域），但是长期的记忆影响就很小（如黑色和绿色区域），这就是 RNN 存在的短期记忆问题。\nRNN 有短期记忆问题，无法处理很长的输入序列 训练 RNN 需要投入极大的成本 RNN 的优化算法 LSTM – 长短期记忆网络 RNN 是一种死板的逻辑，越晚的输入影响越大，越早的输入影响越小，且无法改变这个逻辑。 LSTM 做的最大的改变就是打破了这个死板的逻辑，而改用了一套灵活了逻辑——只保留重要的信息。 简单说就是：抓重点！ 举个例子，我们先快速的阅读下面这段话： 当我们快速阅读完之后，可能只会记住下面几个重点： LSTM 类似上面的划重点，他可以保留较长序列数据中的「重要信息」，忽略不重要的信息。这样就解决了 RNN 短期记忆的问题。\n原理 原始RNN的隐藏层只有一个状态，即h，它对于短期的输入非常敏感。那么如果我们再增加一个门（gate）机制用于控制特征的流通和损失，即c，让它来保存长期的状态，这就是长短时记忆网络(Long Short Term Memory，LSTM)。 新增加的状态c，称为单元状态。我们把LSTM按照时间维度展开： 其中图像上的标识$\\sigma$标识使用sigmod激活到[0-1],$\\tanh$激活到[-1,1] ⨀ 是一个数学符号，表示逐元素乘积（element-wise product）或哈达玛积（Hadamard product）。当两个相同维度的矩阵、向量或张量进行逐元素相乘时，可以使用 ⨀ 符号来表示。\n例如，对于两个向量 [a1,a2,a3] ⨀ [b1, b2, b3]=[a1b1,a2b2,a3*b3]，它们的逐元素乘积可以表示 可以看到在t时刻，\nLSTM的输入有三个：当前时刻网络的输出值$x_t$、上一时刻LSTM的输出值$h_{t−1}$、以及上一时刻的记忆单元向量$c_{t−1}$；\nLSTM的输出有两个：当前时刻LSTM输出值$h_t$、当前时刻的隐藏状态向量$h_t$、和当前时刻的记忆单元状态向量$c_t$。\n注意：记忆单元c在LSTM 层内部结束工作，不向其他层输出。LSTM的输出仅有隐藏状态向量h。\nLSTM 的关键是单元状态，即贯穿图表顶部的水平线，有点像传送带。这一部分一般叫做单元状态（cell state）它自始至终存在于LSTM的整个链式系统中。 遗忘门 $f_t$叫做遗忘门，表示$C_{t−1}$的哪些特征被用于计算$C_t$。$f_t$是一个向量，向量的每个元素均位于(01)范围内。通常我们使用 sigmoid 作为激活函数，sigmoid 的输出是一个介于于(01)区间内的值，但是当你观察一个训练好的LSTM时，你会发现门的值绝大多数都非常接近0或者1，其余的值少之又少。 输入门 $C_t$ 表示单元状态更新值，由输入数据$x_t$和隐节点$h_{t−1}$经由一个神经网络层得到，单元状态更新值的激活函数通常使用tanh。 $i_t$叫做输入门，同 $f_t$ 一样也是一个元素介于(0~1)区间内的向量，同样由$x_t$和$h_{t−1}$经由sigmoid激活函数计算而成 输出门 最后，为了计算预测值$y^t$和生成下个时间片完整的输入，我们需要计算隐节点的输出 $h_t$。 lstm写诗 首先我们研究下pytorch中lstm的用法 单层lstm\nsequence_length =3\rbatch_size =2\rinput_size =4\r#这里如果是输入比如[张三,李四，王五]，一般实际使用需要通过embedding后生成一个[时间步是3，批量1（这里是1，但是如果是真实数据集可能有分批处理，就是实际的批次值）,3（三个值的坐标表示一个张三或者李四）]\rinput=t.randn(sequence_length,batch_size,input_size)\rlstmModel=nn.LSTM(input_size,3,1)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, (h, c) =lstmModel(input)\r#因为是3个时间步，每个时间步都有一个隐藏层，每个隐藏层都有2条数据，隐藏层的维度是3，最终(3,2,3)\rprint(\"LSTM隐藏层输出的维度\",output.shape)\r#\rprint(\"LSTM隐藏层最后一个时间步输出的维度\",h.shape)\rprint(\"LSTM隐藏层最后一个时间步细胞状态\",c.shape) 输出\nLSTM隐藏层输出的维度 torch.Size([3, 2, 3])\rLSTM隐藏层最后一个时间步输出的维度 torch.Size([1, 2, 3])\rLSTM隐藏层最后一个时间步细胞状态 torch.Size([1, 2, 3]) 双层lstm\nsequence_length =3\rbatch_size =2\rinput_size =4\rinput=t.randn(sequence_length,batch_size,input_size)\rlstmModel=nn.LSTM(input_size,3,num_layers=2)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, (h, c) =lstmModel(input)\rprint(\"2层LSTM隐藏层输出的维度\",output.shape)\rprint(\"2层LSTM隐藏层最后一个时间步输出的维度\",h.shape)\rprint(\"2层LSTM隐藏层最后一个时间步细胞状态\",c.shape) 输出： 2层LSTM隐藏层输出的维度 torch.Size([3, 2, 3]) 2层LSTM隐藏层最后一个时间步输出的维度 torch.Size([2, 2, 3]) 2层LSTM隐藏层最后一个时间步细胞状态 torch.Size([2, 2, 3])\n2层的话输出的是最后一层的隐藏层的输出，h，c是一个时间步就有两层的隐藏层和记忆细胞\n开始写诗的例子 这是项目的目录结构 加载数据 实验数据来自Github上中文爱好者收集的5万多首唐诗，作者在此基础上进行了一些数据处理，由于数据处理很耗时间，且不是pytorch学习的重点，这里省略。作者提供了一个numpy的压缩包tang.npz，下载地址 数据具体结构可参考，以下代码main部分\nfrom torch.utils.data import Dataset,DataLoader\rimport numpy as np\rclass PoetryDataset(Dataset):\rdef __init__(self,root):\rself.data=np.load(root, allow_pickle=True)\rdef __len__(self):\rreturn len(self.data[\"data\"])\rdef __getitem__(self, index):\rreturn self.data[\"data\"][index]\rdef getData(self):\rreturn self.data[\"data\"],self.data[\"ix2word\"].item(),self.data[\"word2ix\"].item()\rif __name__==\"__main__\":\rdatas=PoetryDataset(\"./tang.npz\").data\r# data是一个57580 * 125的numpy数组，即总共有57580首诗歌，每首诗歌长度为125个字符（不足125补空格，超过125的丢弃）\rprint(datas[\"data\"].shape)\r#这里都字符已经转换成了索引\rprint(datas[\"data\"][0])\r# 使用item将numpy转换为字典类型，ix2word存储这下标对应的字,比如{0: '憁', 1: '耀'}\rix2word = datas['ix2word'].item()\rprint(ix2word)\r# word2ix存储这字对应的小标，比如{'憁': 0, '耀': 1}\rword2ix = datas['word2ix'].item()\rprint(word2ix)\r# 将某一首古诗转换为索引表示,转换后：[5272, 4236, 3286, 6933, 6010, 7066, 774, 4167, 2018, 70, 3951]\rstr=\"床前明月光，疑是地上霜\"\rprint([word2ix[i] for i in str])\r#将第一首古诗打印出来\rprint([ix2word[i] for i in datas[\"data\"][0]]) 定义模型 import torch.nn as nn\rclass Net(nn.Module):\r\"\"\"\r:param vocab_size 表示输入单词的格式\r:param embedding_dim 表示将一个单词映射到embedding_dim维度空间\r:param hidden_dim 表示lstm输出隐藏层的维度\r\"\"\"\rdef __init__(self, vocab_size, embedding_dim, hidden_dim):\rsuper(Net, self).__init__()\rself.hidden_dim = hidden_dim\r#Embedding层，将单词映射成vocab_size行embedding_dim列的矩阵，一行的坐标代表第一行的词\rself.embeddings = nn.Embedding(vocab_size, embedding_dim)\r#两层lstm，输入词向量的维度和隐藏层维度\rself.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=2, batch_first=False)\r#最后将隐藏层的维度转换为词汇表的维度\rself.linear1 = nn.Linear(self.hidden_dim, vocab_size)\rdef forward(self, input, hidden=None):\r#获取输入的数据的时间步和批次数\rseq_len, batch_size = input.size()\r#如果没有传入上一个时间的隐藏值，初始一个，注意是2层\rif hidden is None:\rh_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\rc_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\relse:\rh_0, c_0 = hidden\r#将输入的数据embeddings为（input行数,embedding_dim）\rembeds = self.embeddings(input) # (seq_len, batch_size, embedding_dim), (1,1,128)\routput, hidden = self.lstm(embeds, (h_0, c_0)) #(seq_len, batch_size, hidden_dim), (1,1,256)\routput = self.linear1(output.view(seq_len*batch_size, -1)) # ((seq_len * batch_size),hidden_dim), (1,256) → (1,8293)\rreturn output, hidden 训练 下述代码：input, target = (data[:-1, :]), (data[1:, :])解释:\n在使用LSTM进行词预测时，输入和标签的设置是为了将输入序列和目标序列对齐。\n在语言模型中，我们希望根据前面的单词来预测下一个单词。因此，输入序列是前面的单词，而目标序列是下一个单词。\n考虑以下例子： 假设我们有一个句子：“I love deep learning.” 我们可以将其分解为以下形式的输入和目标序列： 输入序列：[“I”, “love”, “deep”] 目标序列：[“love”, “deep”, “learning”]\n在这个例子中，输入序列是前面的单词[“I”, “love”, “deep”]，而目标序列是相应的下一个单词[“love”, “deep”, “learning”]。\n在代码中，data是一个包含所有单词的数据集，其中每一行代表一个单词。将data切片为input和target时，我们使用data[:-1, :]作为输入序列，即除了最后一个单词。而data[1:, :]作为目标序列，即从第二个单词开始。\n这样设置输入和目标序列的目的是为了将输入和标签对齐，使得模型可以根据前面的单词来预测下一个单词。\nimport fire\rimport torch.nn as nn\rimport torch as t\rfrom data.dataset import PoetryDataset\rfrom models.model import Net\rnum_epochs=5\rdata_root=\"./data/tang.npz\"\rbatch_size=10\rdef train(**kwargs):\rdatasets=PoetryDataset(data_root)\rdata,ix2word,word2ix=datasets.getData()\rlenData=len(data)\rdata = t.from_numpy(data)\rdataloader = t.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=1)\r#总共有8293的词。模型定义：vocab_size, embedding_dim, hidden_dim = 8293 * 128 * 256\rmodel=Net(len(word2ix),128,256)\r#定义损失函数\rcriterion = nn.CrossEntropyLoss()\rmodel=model.cuda()\roptimizer = t.optim.Adam(model.parameters(), lr=1e-3)\riteri=0\rfilename = \"example.txt\"\rtotalIter=lenData*num_epochs/batch_size\rfor epoch in range(num_epochs): # 最大迭代次数为8\rfor i, data in enumerate(dataloader): # 一批次数据 128*125\rdata = data.long().transpose(0,1).contiguous() .cuda()\roptimizer.zero_grad()\rinput, target = (data[:-1, :]), (data[1:, :])\routput, _ = model(input)\rloss = criterion(output, target.view(-1)) # torch.Size([15872, 8293]), torch.Size([15872])\rloss.backward()\roptimizer.step()\riteri+=1\rif(iteri%500==0):\rprint(str(iteri+1)+\"/\"+str(totalIter)+\"epoch\")\rif (1 + i) % 1000 == 0: # 每575个batch可视化一次\rwith open(filename, \"a\") as file:\rfile.write(str(i) + ':' + generate(model, '床前明月光', ix2word, word2ix)+\"\\n\")\rt.save(model.state_dict(), './checkpoints/model_poet_2.pth')\rdef generate(model, start_words, ix2word, word2ix): # 给定几个词，根据这几个词生成一首完整的诗歌\rtxt = []\rfor word in start_words:\rtxt.append(word)\rinput = t.Tensor([word2ix['\u003cSTART\u003e']]).view(1,1).long() # tensor([8291.]) → tensor([[8291.]]) → tensor([[8291]])\rinput = input.cuda()\rhidden = None\rnum = len(txt)\rfor i in range(48): # 最大生成长度\routput, hidden = model(input, hidden)\rif i \u003c num:\rw = txt[i]\rinput = (input.data.new([word2ix[w]])).view(1, 1)\relse:\rtop_index = output.data[0].topk(1)[1][0]\rw = ix2word[top_index.item()]\rtxt.append(w)\rinput = (input.data.new([top_index])).view(1, 1)\rif w == '\u003cEOP\u003e':\rbreak\rreturn ''.join(txt)\rif __name__==\"__main__\":\rfire.Fire() 5epoch，10batch，普通pc，GTX1050,2GB显存，训练时间30分钟。 50epoch 128batch colab免费gpu 16GB显存，训练时间1小时\n测试 def test():\rdatasets = PoetryDataset(data_root)\rdata, ix2word, word2ix = datasets.getData()\rmodle = Net(len(word2ix), 128, 256) # 模型定义：vocab_size, embedding_dim, hidden_dim —— 8293 * 128 * 256\rif t.cuda.is_available() == True:\rmodle.cuda()\rmodle.load_state_dict(t.load('./checkpoints/model_poet_2.pth'))\rmodle.eval()\rname = input(\"请输入您的开头：\")\rtxt = generate(modle, name, ix2word, word2ix)\rprint(txt) 由于才训练了5epoch效果不是太好，可视化loss后可多次epoch看看效果，还有个问题，如果输入不变，生成的结果就是相同的，所以这个可能需要一个噪声干扰。 5epoch版本效果\n(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\\案例\\生成古诗\\tang\u003epython main.py test\r请输入您的开头：唧唧复唧唧\r唧唧复唧唧，不知何所如？君不见此地，不如此中生。一朝一杯酒，一日相追寻。一朝一杯酒，一醉一相逢。\r(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\\案例\\生成古诗\\tang\u003epython main.py test\r请输入您的开头：我儿小谦谦\r我儿小谦谦，不是天地间。有时有所用，不是无为名。有时有所用，不是无生源。有时有所用，不是无生源。 50epoch版本效果\n(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\\案例\\生成古诗\\tang\u003epython main.py test\r请输入您的开头：我家小谦谦\r我家小谦谦，今古何为郎。我生不相识，我心不可忘。我来不我见，我亦不得尝。君今不我见，我亦不足伤。\r(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\\案例\\生成古诗\\tang\u003epython main.py test\r请输入您的开头：床前明月光\r床前明月光，上客不可见。玉楼金阁深，玉瑟风光紧。玉指滴芭蕉，飘飘出罗幕。玉堂无尘埃，玉节凌风雷。\r(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\\案例\\生成古诗\\tang\u003epython main.py test\r请输入您的开头：唧唧复唧唧\r唧唧复唧唧，胡儿女卿侯。妾本邯郸道，相逢两不游。妾心不可再，妾意不能休。妾本不相见，妾心如有钩。 GRU Gated Recurrent Unit – GRU 是 LSTM 的一个变体。他保留了 LSTM 划重点，遗忘不重要信息的特点，在long-term 传播的时候也不会被丢失。\nLSTM 的参数太多，计算需要很长时间。因此，最近业界又提出了 GRU（Gated RecurrentUnit，门控循环单元）。GRU 保留了 LSTM使用门的理念，但是减少了参数，缩短了计算时间。\n相对于 LSTM 使用隐藏状态和记忆单元两条线，GRU只使用隐藏状态。异同点如下： GRU的计算图 GRU计算图，σ节点和tanh节点有专用的权重，节点内部进行仿射变换（“1−”节点输入x，输出1 − x） GRU 中进行的计算由上述 4 个式子表示（这里 xt和 ht−1 都是行向量），如图所示，GRU 没有记忆单元，只有一个隐藏状态h在时间方向上传播。这里使用r和z共两个门（LSTM 使用 3 个门），r称为 reset 门，z称为 update 门。\nr（reset门）**决定在多大程度上“忽略”过去的隐藏状态。根据公式2.3，如果r是 0，则新的隐藏状态h~仅取决于输入$x_t$。也就是说，此时过去的隐藏状态将完全被忽略。\nz（update门）**是更新隐藏状态的门，它扮演了 LSTM 的 forget 门和input 门两个角色。公式2.4 的(1−z)⊙$h_{t−1}$部分充当 forget 门的功能，从过去的隐藏状态中删除应该被遗忘的信息。z⊙$h^~$的部分充当 input 门的功能，对新增的信息进行加权。",
    "description": "@[toc]\n概述 循环神经网络（Recurrent Neural Network，RNN）是一种具有循环连接的神经网络结构，被广泛应用于自然语言处理、语音识别、时序数据分析等任务中。相较于传统神经网络，RNN的主要特点在于它可以处理序列数据，能够捕捉到序列中的时序信息。\nRNN的基本单元是一个循环单元（Recurrent Unit），它接收一个输入和一个来自上一个时间步的隐藏状态，并输出当前时间步的隐藏状态。在传统的RNN中，循环单元通常使用tanh或ReLU等激活函数。\n基本循环神经网络 原理 基本的 循环神经网络，结构由 输入层、一个隐藏层和输出层 组成。\n$x$是输入向量，$o$是输出向量，$s$表示隐藏层的值；$U$是输入层到隐藏层的权重矩阵，$V$是隐藏层到输出层的权重矩阵。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入$x$，还取决于上一次隐藏层的值$s$。权重矩阵W就是隐藏层上一次的值作为这一次的输入的权重。 将上图的基本RNN结构在时间维度展开(RNN是一个链式结构，每个时间片使用的是相同的参数,t表示t时刻)： 现在看上去就会清楚许多，这个网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$。关键一点是$s_t$的值不仅仅取决于$x_t$，还取决于$s_{t−1}$。 公式1：$s_t=f(U∗x_t+W∗s_{t−1}+B1)$ 公式2：$o_t=g(V∗s_t+B2)$\n式1是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次隐藏层值$S_{t−1}$作为这一次的输入的权重矩阵，f是激活函数。 式2是输出层的计算公式，V是输出层的权重矩阵，g是激活函数,B1,B2是偏置假设为0。 隐含层有两个输入，第一是U与$x_t$向量的乘积，第二是上一隐含层输出的状态$s_t−1$和W的乘积。等于上一个时刻计算的$s_t−1$需要缓存一下，在本次输入$x_t$一起计算，共同输出最后的$o_t$。\n如果反复把式1带入式2，我们将得到： 从上面可以看出，循环神经网络的输出值ot，是受前面历次输入值、、、、、、、、$x_t$、$x_{t−1}$、$x_{t−2}$、$x_{t−3}$、…影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。这样其实不好，因为如果太前面的值和后面的值已经没有关系了，循环神经网络还考虑前面的值的话，就会影响后面值的判断。\n上面是整个单向单层NN的前向传播过程\n为了更快理解输入x输入格式下面使用nlp中Word Embedding讲解下。\nWord Embedding 首先我们需要对输入文本x进行编码，使之成为计算机可以读懂的语言，在编码时，我们期望句子之间保持词语间的相似行，词的向量表示是进行机器学习和深度学习的基础。\nword embedding的一个基本思路就是，我们把一个词映射到语义空间的一个点，把一个词映射到低维的稠密空间，这样的映射使得语义上比较相似的词，他在语义空间的距离也比较近，如果两个词的关系不是很接近，那么在语义空间中向量也会比较远。\n如上图英语和西班牙语映射到语义空间，语义相同的数字他们在语义空间分布的位置是相同的 简单回顾一下word embedding,对于nlp来说，我们输入的是一个个离散的符号，对于神经网络来说，它处理的都是向量或者矩阵。所以第一步，我们需要把一个词编码成向量。最简单的就是one-hot的表示方法。如下图所示： python代码（one-hot），比如",
    "tags": [],
    "title": "深度学习05-RNN循环神经网络",
    "uri": "/docs/programming/ai/deep_learning/rnn/dl_05_rnn/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 框架学习",
    "content": "概述 PyTorch是一个基于Python的开源机器学习框架，由Facebook的人工智能研究团队开发并维护。它提供了丰富的工具和接口，用于构建和训练深度神经网络模型。\nPyTorch的主要特点和优势包括：\n动态图：PyTorch使用动态图机制，即在运行时构建计算图。这使得模型的构建和调试更加直观和灵活，能够更好地处理复杂的计算流程和动态控制流。\n简洁明了：PyTorch的API设计简洁明了，易于学习和使用。它提供了一系列高级接口，使得模型的构建、训练和评估变得更加简单和高效。\n强大的GPU加速支持：PyTorch能够利用GPU进行张量运算和模型训练，从而加快计算速度。它提供了简单易用的接口，使得在GPU上进行加速变得更加方便。\n灵活扩展：PyTorch支持自定义操作符和扩展，使得用户可以方便地实现和使用自己的模型组件和功能。\n相比之下，TensorFlow是由Google开发的另一个流行的深度学习框架。与PyTorch相比，TensorFlow的主要特点和优势包括：\n静态图：TensorFlow使用静态图机制，即在编译时构建计算图。这使得TensorFlow在模型运行时能够进行更多的优化和性能提升，适用于大规模的、计算密集型的任务。\n跨平台支持：TensorFlow可以在多种硬件和操作系统上运行，并且具有广泛的部署支持。它提供了TensorFlow Serving、TensorFlow Lite和TensorFlow.js等工具，使得模型的部署和移植更加方便。\n分布式训练支持：TensorFlow提供了分布式训练的功能，可以在多个设备和计算节点上进行模型训练，从而加快训练速度。\n生态系统和社区：TensorFlow具有庞大的生态系统和活跃的社区，提供了丰富的资源和支持，包括模型库、教程和论坛等。\n总的来说，PyTorch和TensorFlow都是优秀的深度学习框架，各有其特点和适用场景。PyTorch适合于快速原型开发、动态计算流程和小规模任务，而TensorFlow适合于大规模、计算密集型的任务和分布式训练。选择哪个框架取决于具体的需求和个人偏好。\n对于初学接触神经网络，建议先学pytorch，它提供的api接近理论概念，有动态图，方便调试，适合做研究使用，，由于最近chargpt的大火，Hugging Face的transforms是使用PyTorch的。Hugging Face是一个提供自然语言处理（NLP）模型和工具的平台，他们的Transformers库主要基于PyTorch实现，他的入门pytorch必须要有基础。这个库提供了一系列用于数据预处理和后处理的函数，可以方便地对文本数据进行转换和处理。\n环境准备 安装cuda和cudnn 一般pc电脑或者服务器都有nvida显卡，可以通过nvidia-smi命令查看。 其中python环境（3.8+版本），cuda和cudnn安装请参考：https://blog.csdn.net/liaomin416100569/article/details/130532993 安装后可以看到我的cuda version是11.2\n安装pytorch 考虑到版本向下兼容，不一定非要下载cuda=11.2对应的那个版本的torch，或许低于这个版本就可以。所以我就选择下载cuda11.1的版本。 以下是pytorch对应的稳定版的网址下载链接，可以根据需要找到对应的torch版本下载。cu版本就是gpu版本，不带cu的是cpu版本，https://download.pytorch.org/whl/torch_stable.html，搜索cu111 直接选择\npip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html 编写测试代码\nimport torch\rprint(torch.__version__)\r#cuda是否可用，如果返回True，表示正常可用gpu\rprint(torch.cuda.is_available())\rprint(torch.cuda.device_count())\rx1=torch.rand(5,3)\r#把x1转换gpu0的tensor\rx1=x1.cuda(0)\rprint(x1) 测试运行\n1.9.1+cu111\rTrue 1\rtensor([[0.5761, 0.7046, 0.2004],\r[0.6030, 0.3285, 0.5602],\r[0.6852, 0.6602, 0.0033],\r[0.4213, 0.7174, 0.0591],\r[0.5276, 0.4181, 0.8665]], device='cuda:0') 使用colab 如果自己沒有gpu的環境，可以使用cpu進行學習，但是學到模型训练还是要gpu，如果有外网环境，可以考虑使用google提供的colab，主要是免费，gpu能给到16GB，系统磁盘100gb，googledrive15gb,非常良心了，注意：colab不支持和本地pycharm远程调试，模型比较大时磁盘是个大问题，根据Colab的官方规定，每个用户每天可以使用Colab资源的总时间为12小时。这意味着，一旦你的Colab会话运行了12小时，你将无法继续使用Colab的计算资源。当然，你可以重新启动一个新的Colab会话来继续使用。\n申请colab 首先你先需要申请一个googledrive（类似百度网盘），准备一个gmail邮箱就可以申请，申请完成后默认有15GB的存储空间，如果需要更多就需要购买了。 在我的云端云盘右侧空白的地方点击邮件，关联更多应用，搜索colab安装，安装后右键就会多了 一个google colaboratory，点击进去，会弹出一个notebook的开发环境。 可以点击左侧的文件夹，自动分配一个计算资源 查看分配的机器 点击目录..可以进入到系统根目录,点击工具栏第三个图标挂载googledrive 在content notepadbook中执行命令查看资源信息，和右侧的资源坐下对比，可以确认系统是ubuntu，内存是12.7gb，磁盘107gb，没有gpu 你的notepad文件内容，默认是新建在googledrive的根目录下，你可以双击文件直接进入notebook 挂载googledrive 挂载完成后/content多了一个drive目录，MyDrive内容就和googledrive是一致的。 colab申请gpu 点击右上角的view resouce 选择change runtime type,选择免费的GPU或者TPU 再次使用nvdia-smi确认 google的另一款免费的实验免费gpu是kaggle，也可以注册使用，比colab更简单方便，同时可直接引用其他开源模型。\nnotebook语法 python语法 可直接在快中执行python语法。 使用！执行shell命令。 比如使用 !bash进入一个交互shell行，exit退出 可以使用 ！任意shell命令执行， 基础 张量 在PyTorch中，除了张量（Tensor）之外，还有很多其他的数据类型和类。以下是一些常见的PyTorch数据类型和类：\nTensor（张量）：张量是PyTorch的核心数据结构，类似于数组，可以存储和操作多维数据。\nVariable（变量）：Variable是对张量的封装，用于自动求导。\nnn.Module：nn.Module是PyTorch中用于构建神经网络模型的基类，可以包含多个层和操作。\nnn.Parameter：nn.Parameter是Variable的子类，用于定义模型中需要进行学习的参数。\nDataLoader：DataLoader是一个用于加载数据的实用类，可以方便地对数据进行批量处理和迭代。\nOptimizer（优化器）：优化器是用于更新模型参数的算法，例如SGD、Adam等。\nLoss Function（损失函数）：损失函数用于衡量模型预测结果与真实标签之间的差异，例如交叉熵损失、均方误差等。\n这些是PyTorch中常用的一些数据类型和类，它们提供了丰富的功能来支持深度学习任务的实现和训练。\n定义 import torch as t\rimport numpy as np\r#构建5*3数组,只是分配了空间未初始化\rresult=t.Tensor(5,3)\rprint(result)\r#这里产生个0-1之间的tensor张量，并且初始化\rx1=t.rand(5,3)\ry1=t.rand(5,3)\rprint(x1)\rprint(x1.size())\rresult=x1+y1\rprint(result) numpy转换 #产生5个1的一维数组tensor转换成numpy\rprint(t.ones(5).numpy())\r#将numpy数组转换为tensor\rprint(t.from_numpy(np.array([2,2,2,]))) 数学函数 随机数 下面是一些常见的PyTorch函数，可以用于生成随机数：\ntorch.randn(size, dtype=None, device=None) - 从标准正态分布中返回随机样本。返回一个具有给定大小的张量，其中每个元素独立地从标准正态分布中抽取。可以通过指定size参数来指定张量的形状。 例子：\nx = torch.randn(3, 3)\rprint(x) torch.rand(size, dtype=None, device=None) - 从均匀分布中返回随机样本。返回一个具有给定大小的张量，其中每个元素独立地从一个均匀分布上抽取。可以通过指定size参数来指定张量的形状。 例子：\nx = torch.rand(3, 3)\rprint(x)` torch.randint(low, high, size, dtype=None, device=None) - 从离散均匀分布中返回随机整数。返回一个具有给定大小的张量，其中每个元素独立地从一个均匀分布上抽取。可以通过指定low和high参数来指定取值范围。 例子：\nx = torch.randint(0, 10, (3, 3))\rprint(x) torch.normal(mean, std, size, dtype=None, device=None) - 从正态分布中返回随机样本。返回一个具有给定大小的张量，其中每个元素独立地从一个正态分布中抽取。可以通过指定mean和std参数来指定正态分布的均值和标准差。 例子：\nx = torch.normal(0, 1, (3, 3))\rprint(x) 这些函数可以帮助您在PyTorch中生成随机数。请根据您的需求选择适当的函数。\n计算函数 常用的数学计算函数 当然，下面是PyTorch中一些常用的数学函数的清单，每个都附有简短的描述和一个调用小例子：\ntorch.abs(input): 返回输入张量的绝对值。示例：torch.abs(torch.tensor([-1, 2, -3]))。 torch.sqrt(input): 返回输入张量的平方根。示例：torch.sqrt(torch.tensor([4, 9, 16]))。 torch.exp(input): 计算输入张量的指数函数。示例：torch.exp(torch.tensor([1, 2, 3]))。 torch.log(input): 计算输入张量的自然对数。示例：torch.log(torch.tensor([1, 10, 100]))。 torch.sin(input): 计算输入张量的正弦值。示例：torch.sin(torch.tensor([0, math.pi/2, math.pi]))。 torch.cos(input): 计算输入张量的余弦值。示例：torch.cos(torch.tensor([0, math.pi/2, math.pi]))。 torch.tan(input): 计算输入张量的正切值。示例：torch.tan(torch.tensor([0, math.pi/4, math.pi/2]))。 torch.sigmoid(input): 计算输入张量的Sigmoid函数。示例：torch.sigmoid(torch.tensor([0, 1, 2]))。 torch.relu(input): 应用ReLU激活函数，即max(0, input)。示例：torch.relu(torch.tensor([-1, 0, 1]))。 torch.softmax(input, dim): 计算输入张量在指定维度上的Softmax函数。示例：torch.softmax(torch.tensor([[1, 2], [3, 4]]), dim=1)。 torch.mean(input): 计算输入张量的均值。示例：torch.mean(torch.tensor([1, 2, 3]))。 torch.sum(input): 计算输入张量的总和。示例：torch.sum(torch.tensor([1, 2, 3]))。 torch.max(input): 返回输入张量中的最大值。示例：torch.max(torch.tensor([1, 2, 3]))。 torch.min(input): 返回输入张量中的最小值。示例：torch.min(torch.tensor([1, 2, 3]))。 torch.argmax(input): 返回输入张量中最大值的索引。示例：torch.argmax(torch.tensor([1, 2, 3]))。 torch.argmin(input): 返回输入张量中最小值的索引。示例：torch.argmin(torch.tensor([1, 2, 3]))。 torch.sort(input): 对输入张量进行排序。示例：torch.sort(torch.tensor([3, 1, 2]))。 torch.clamp(input, min, max): 将输入张量的值限制在指定范围内。示例：torch.clamp(torch.tensor([1, 2, 3]), min=2, max=3)。 torch.round(input): 对输入张量进行四舍五入。示例：torch.round(torch.tensor([1.1, 2.4, 3.6]))。 torch.floor(input): 向下取整，返回不大于输入张量的最大整数。示例：torch.floor(torch.tensor([1.1, 2.4, 3.6]))。 矩阵处理函数 以下是PyTorch中常用的20个矩阵处理函数的清单及其描述：\ntorch.mm(): 计算两个矩阵的乘积。 示例：torch.mm(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5], [6]]))返回tensor([[17], [39]])\ntorch.matmul(): 计算两个张量的矩阵乘积。 示例：torch.matmul(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5], [6]]))返回tensor([[17], [39]])\ntorch.transpose(): 返回输入张量的转置。 示例：torch.transpose(torch.tensor([[1, 2], [3, 4]]), 0, 1)返回tensor([[1, 3], [2, 4]])\ntorch.mm(): 计算一个矩阵和一个向量的乘积。 示例：torch.mm(torch.tensor([[1, 2], [3, 4]]), torch.tensor([5, 6]))返回tensor([17, 39])\ntorch.trace(): 返回矩阵的迹。 示例：torch.trace(torch.tensor([[1, 2], [3, 4]]))返回tensor(5)\ntorch.det(): 计算矩阵的行列式。 示例：torch.det(torch.tensor([[1, 2], [3, 4]]))返回tensor(-2)\ntorch.svd(): 对矩阵进行奇异值分解。 示例：torch.svd(torch.tensor([[1, 2], [3, 4]]))返回(tensor([[-0.4046, -0.9145], [-0.9145, 0.4046]]), tensor([5.4645, 0.3650]), tensor([[-0.5760, -0.8174], [-0.8174, 0.5760]]))\ntorch.eig(): 计算矩阵的特征值和特征向量。 示例：torch.eig(torch.tensor([[1, 2], [3, 4]]))返回(tensor([[0.3723, 0.0000], [5.6277, 0.0000]]), tensor([]))\ntorch.inverse(): 计算矩阵的逆。 示例：torch.inverse(torch.tensor([[1, 2], [3, 4]]))返回tensor([[-2.0000, 1.0000], [ 1.5000, -0.5000]])\ntorch.diag(): 返回矩阵的对角线元素。 示例：torch.diag(torch.tensor([[1, 2], [3, 4]]))返回tensor([1, 4])\ntorch.diag_embed(): 将一维张量转化为对角矩阵。 示例：torch.diag_embed(torch.tensor([1, 2, 3]))返回tensor([[[1, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 2, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, 3]]])\ntorch.einsum(): 执行爱因斯坦求和约定。 示例：torch.einsum(‘ij,jk-\u003eik’, torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))返回tensor([[19, 22], [43, 50]])\ntorch.flatten(): 对输入张量进行扁平化操作。 示例：torch.flatten(torch.tensor([[1, 2], [3, 4]]))返回tensor([1, 2, 3, 4])\ntorch.cat(): 沿指定维度拼接张量。 示例：torch.cat((torch.tensor([[1, 2]]), torch.tensor([[3, 4]])), dim=0)返回tensor([[1, 2], [3, 4]])\ntorch.stack(): 沿新维度拼接张量。 示例：torch.stack((torch.tensor([1, 2]), torch.tensor([3, 4])), dim=0)返回tensor([[1, 2], [3, 4]])\ntorch.split(): 沿指定维度分割张量。 示例：torch.split(torch.tensor([[1, 2, 3, 4]]), 2, dim=1)返回(tensor([[1, 2]]), tensor([[3, 4]]))\ntorch.chunk(): 将张量分割成指定数量的块。 示例：torch.chunk(torch.tensor([[1, 2, 3, 4]]), 2, dim=1)返回(tensor([[1, 2]]), tensor([[3, 4]]))\ntorch.reshape(): 改变张量的形状。 示例：torch.reshape(torch.tensor([[1, 2, 3, 4]]), (2, 2))返回tensor([[1, 2], [3, 4]])\ntorch.squeeze(): 压缩张量中尺寸为1的维度。 示例：torch.squeeze(torch.tensor([[[1], [2]]]))返回tensor([1, 2])\ntorch.unsqueeze(): 在指定位置插入尺寸为1的新维度。 示例：torch.unsqueeze(torch.tensor([1, 2]), dim=1)返回tensor([[1], [2]]\ntorch.view是PyTorch中的一个函数，用于改变张量的形状，即对张量进行重塑操作。它的作用类似于NumPy中的reshape函数。 x = torch.tensor([1, 2, 3, 4, 5, 6]) y = x.view(2, 3)\ntorch.permute函数是PyTorch中的一个函数，用于重新排列张量的维度顺序。它的作用是交换或重新组织张量的维度。 在下述示例中，原始张量x的维度顺序为(2, 3, 4)，通过使用permute(2, 0, 1)，将维度顺序重新排列为(4, 2, 3)，得到了新的张量 也就是维度2换成函数索引0个维度,0维度的换成1,1维度的换成2\nimport torch\rx = torch.randn(2, 3, 4) # 创建一个形状为(2, 3, 4)的张量\rx_permuted = x.permute(2, 0, 1) # 将维度顺序重新排列为(4, 2, 3)\rprint(x_permuted.shape) # 输出: torch.Size([4, 2, 3]) 自动梯度 深度学习的算法本质上是通过反向传播求导数，PyTorch的Autograd模块实现了此功能。在Tensor上的所有操作，Autograd都能为它们自动提供微分，避免手动计算导数的复杂过程。\n在PyTorch中，Tensor和Variable都可以求梯度，但是它们有一些区别。\n在旧版本的PyTorch中，Variable是一个Tensor的封装，它包含了Tensor的数据以及关于这个Tensor的梯度信息。在新版本的PyTorch中，Variable已经被弃用，官方建议直接使用Tensor。\nPyTorch中的Tensor对象有一个属性.requires_grad，默认为False。当你将其设置为True时，表示希望计算这个Tensor的梯度。在进行反向传播计算梯度时，所有具有.requires_grad=True的Tensor都会被保留梯度信息。\n当你使用Tensor进行计算时，可以调用.backward()方法来计算相对于这个Tensor的梯度。梯度信息会保存在.grad属性中。\n所以，Variable的作用可以用Tensor的.requires_grad属性来代替，而且在新版本的PyTorch中，官方建议直接使用Tensor进行梯度计算。 Variable和Tensor主要包含三个属性。\ndata：保存计算后结果对应的的Tensor。 grad：保存data对应的梯度，是Tensor，它和data的形状一样。 grad fn：指向一个Function对象，这个Function用来反向传播计算输入的梯度,requires_grad=True x=Variable(t.from_numpy(np.array([[1,2],[2,4]],dtype=float)),requires_grad=True)\rprint(\"张量x=\",x)\ry=x.sum()\rprint(\"输出y\",y)\rprint(\"输出y的梯度\",y.grad) #注意结果是y，所以y是没有梯度的，y进行反向传播，可以求导x的导数\rprint(\"y的反向梯度函数\",y.grad_fn)\rprint(\"y的数据\",y.data)\r# 因为y=x[0][0]+x[0][1]+x[1][0]++x[1][1],可以认为四个数是四个变量，比如求每个变量的导数\r# 假设是y=x1+x2+x3+x4 x1是自变量，x1的导数就是1，同理x2的导数也是1，最后就得到了4个1\r# 注意每个点都有个梯度\ry.backward() #反向传播计算梯度\rprint(x.grad) 输出\n张量x= tensor([[1., 2.],\r[2., 4.]], dtype=torch.float64, requires_grad=True)\r输出y tensor(9., dtype=torch.float64, grad_fn=\u003cSumBackward0\u003e)\r输出y的梯度 None\ry的反向梯度函数 \u003cSumBackward0 object at 0x0000025F8F2C8C10\u003e\ry的数据 tensor(9., dtype=torch.float64)\rx的梯度 tensor([[1., 1.],\r[1., 1.]], dtype=torch.float64) 案例 案例1:计算$x^2*e^x$导数\n#计算x**2*e^x导数\r#dx=2*x*e^x+x**2*e^x\r#定义fx的函数逻辑\rdef f(x):\rreturn x**2*t.exp(x)\r#我们预先知道他的梯度函数是\rdef graddx(x):\rreturn 2*x*t.exp(x)+x**2*t.exp(x)\r#生成一个3*3随机矩阵，求梯度\rx=Variable(t.rand(3,3),requires_grad=True)\rprint(graddx(x))\r#使用反向传播求梯度\ry=f(x)\ry.backward(t.ones(y.size()))\rprint(x.grad) 案例2: 使用自动梯度梯度下降拟合最佳直线\n#使用autograd计算梯度，来实现线性回归\rimport torch as t\rfrom torch.autograd import Variable as V\rimport matplotlib.pyplot as plot\rt.manual_seed(42)\r# 使用自动梯度实现线性回归\rx=t.randn(100,1)\ry=3*x+2+t.randn(100,1) #实际值上加上一些随机噪点\rplot.plot(x,y,'.')\rplot.show()\rw=V(t.randn(1,1),requires_grad=True)\rb=V(t.randn(1),requires_grad=True)\rdef fx(x):\rreturn t.mm(x,w)+b\r#损失函数 def lossf(y_pre,y):\rreturn t.mean((y_pre-y)**2)\r#训练100次，100次梯度下降，计算到最小损失时的w和b\rw_gra_last,b_gra_last=0,0\rfor epoch in range(100):\ry_pre=fx(x)\rloss=lossf(y_pre,y)\rloss.backward()\rw_gra=w.grad.data\rb_gra=b.grad.data\rw_gra_last=w_gra.clone()\rb_gra_last=b_gra.clone()\r#如果梯度小于某个值直接退出\rif t.abs(w_gra)\u003c=1e-8 and t.abs(b_gra)\u003c=1e-8:\rbreak;\rlearn_rate=0.01\r#注意w.sub_是不行的因为w是requires_grad=True，需要后面的参数都是设置为：requires_grad=True\r#所以只能是更新他的data\rw.data.sub_(w_gra*learn_rate)\rb.data.sub_(b_gra*learn_rate)\r#注意梯度清零，否则会累加\rw.grad.data.zero_()\rb.grad.data.zero_()\r# w_gra_last是张量，item输出标量\rprint(epoch,w_gra_last.item(),b_gra_last.item()) y_pre=fx(x) plot.plot(x,y,'.')\rplot.plot(x.data.numpy(),y_pre.data.numpy())\rplot.show() 计算图 PyTorch的计算图是一种用于描述计算操作的有向无环图(Directed Acyclic Graph, DAG)。在PyTorch中，计算图是动态的，它会随着代码的执行而构建。\n计算图的主要作用是记录和管理计算操作的流程，以便进行自动微分和梯度优化。通过构建计算图，PyTorch能够追踪和记录所有的计算操作，从而实现自动求导。这使得在深度学习中，我们可以方便地进行反向传播和优化模型的参数。\n使用计算图的好处有：\n自动求导：PyTorch可以根据计算图自动生成反向传播所需的梯度计算代码，简化了手动求导的过程。 动态图灵活性：计算图是动态构建的，可以根据需要进行动态修改和调整，使得模型的结构和计算过程更加灵活和可变。 可视化和调试：计算图可以可视化，帮助我们理解和调试模型的运行过程，更好地理解和解释模型的行为。 总之，PyTorch的计算图是一种强大的工具，它为我们提供了灵活、高效的自动求导功能，使得深度学习模型的训练和优化更加方便和快捷。\n#打印计算图\rimport torch\rfrom torchviz import make_dot\r# 定义一个简单的计算图\rw= torch.randn(1, requires_grad=True)\rb= torch.randn(1, requires_grad=True)\rx = torch.randn(1, requires_grad=True)\ry = w*x + b\r# 使用make_dot函数绘制计算图,图上的数字只是代表数据的维度\rdot = make_dot(y, params={'x': x, 'w': w, 'b': b}, show_attrs=True, show_saved=True)\rdot.render(filename='compute_graph', format='png') 当前运行的目录出现一个compute_graph.png 剖析下反向求导的过程 如表达式z=wx+b可分解为y=wx和z=y+b，其计算图如图3-5所示，图中的MUL和ADD都是算子，w、x、b为变量。 如上有向无环图中，X 和b是叶子节点（leaf node），这些节点通常由用户自己创建，不依赖于其他变量。z称为根节点，是计算图的最终目标。利用链式法则很容易求得各个叶子节点的梯度。 而有了计算图，上述链式求导即可利用计算图的反向传播自动完成，其过程如图所示。 torchvision模块 torchvision是PyTorch的一个扩展库，提供了许多用于计算机视觉任务的实用函数和预训练模型。它包含了常用的数据集、数据转换、模型架构和图像处理方法等功能。\ntorchvision的主要特点包括：\n数据集：torchvision提供了许多常用的计算机视觉数据集，例如MNIST、CIFAR-10、ImageNet等。这些数据集可以方便地用于训练和测试模型。\n数据转换：torchvision提供了一系列用于数据预处理和增强的转换函数，例如对图像进行裁剪、缩放、翻转、归一化等操作。这些转换函数可以灵活地应用于数据集中的样本，以满足模型训练的需求。\n预训练模型：torchvision中集成了一些经典的计算机视觉模型，例如AlexNet、VGG、ResNet等。这些预训练模型可以直接用于特定任务的迁移学习，也可以作为基准模型进行性能比较。\n图像处理：torchvision还提供了一些常用的图像处理方法，例如图像滤波、边缘检测、颜色转换等。这些方法可以用于图像处理和增强的任务。\n总之，torchvision为PyTorch提供了丰富的计算机视觉功能和工具，可以极大地简化计算机视觉任务的开发和实现过程。\nTransforms torchvision提供了一些用于数据增强的常用transforms，如随机裁剪、翻转、旋转、归一化等。这些transforms可以在数据加载时应用于图像，以提高模型的泛化能力和鲁棒性。 以下是torch中所有的transforms：\nCompose: 将多个transforms组合在一起。 ToTensor: 将PIL图像或NumPy数组转换为张量。 ToPILImage: 将张量转换为PIL图像对象。 Normalize: 标准化张量，将每个通道的值减去均值，然后除以标准差。 Resize: 调整图像的大小。 CenterCrop: 中心裁剪图像的一部分。 RandomCrop: 随机裁剪图像的一部分。 RandomResizedCrop: 随机裁剪并调整大小图像。 FiveCrop: 对图像进行五个不同位置的裁剪。 TenCrop: 对图像进行十个不同位置的裁剪。 RandomHorizontalFlip: 随机水平翻转图像。 RandomVerticalFlip: 随机垂直翻转图像。 RandomRotation: 随机旋转图像。 RandomAffine: 随机仿射变换图像。 ColorJitter: 随机调整图像的亮度、对比度、饱和度和色调。 RandomGrayscale: 随机将图像转换为灰度图像。 RandomErasing: 随机擦除图像的一部分。 RandomChoice: 随机选择一个transform进行应用。 RandomApply: 随机应用一个transform。 RandomOrder: 随机打乱transforms的顺序。 这是torch中所有的transforms，你可以根据需要选择适合的transforms来处理图像数据。 下面是一些常用的transforms功能和示例代码：\nResize：调整图像大小 from PIL import Image\r# 定义一个Resize变换，将图像调整为指定大小\rresize = transforms.Resize((256, 256))\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行Resize变换\rresized_image = resize(image) ToTensor：将图像转换为Tensor类型 from PIL import Image\r# 定义一个ToTensor变换，将图像转换为Tensor类型\rto_tensor = transforms.ToTensor()\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行ToTensor变换\rtensor_image = to_tensor(image) Normalize：对图像进行归一化 from PIL import Image\r# 定义一个Normalize变换，将图像进行归一化\rnormalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行Normalize变换\rnormalized_image = normalize(image) transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) 的作用是将输入数据标准化到均值为0，标准差为1的范围内，而不是将值标准化到-1和1之间。标准化的目的是为了使数据具有相似的尺度，以便更好地进行模型训练和优化。 对于给定的某个点(100, 150, 200)，标准化的过程如下： 1.计算每个通道的均值：(100 + 150 + 200) / 3 = 150 2.计算每个通道的标准差：sqrt(((100-150)^2 + (150-150)^2 + (200-150)^2) / 3) = sqrt((2500 + 0 + 2500) / 3) ≈ 50 3.对每个通道的值进行标准化：(100-150)/50 = -1, (150-150)/50 = 0, (200-150)/50 = 1 所以，标准化后的点为(-1, 0, 1)。 需要注意的是，这只是一个简单的例子，实际上在计算标准差时使用的是整个数据集的均值和标准差，而不是单个点的均值和标准差。\nRandomCrop：随机裁剪图像 from PIL import Image\r# 定义一个RandomCrop变换，随机裁剪图像\rrandom_crop = transforms.RandomCrop((224, 224))\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行RandomCrop变换\rcropped_image = random_crop(image) RandomHorizontalFlip：随机水平翻转图像 from PIL import Image\r# 定义一个RandomHorizontalFlip变换，随机水平翻转图像\rrandom_horizontal_flip = transforms.RandomHorizontalFlip(p=0.5)\r# 读取图像\rimage = Image.open('image.jpg')\r# 对图像进行RandomHorizontalFlip变换\rflipped_image = random_horizontal_flip(image) 通过使用transforms模块中的这些函数，我们可以方便地对图像进行预处理和增强，以便于在训练模型时使用。需要注意的是，transforms函数通常需要作为参数传递给torchvision.transforms.Compose函数，以便将多个transforms组合在一起应用到图像上，如：\ntransform = transforms.Compose([\rtransforms.ToTensor(),\rtransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\r]) 更详细的图像增强处理例子参考：https://github.com/lzeqian/deeplearn/blob/master/learn_rnn/pytorch/4.nn%E6%A8%A1%E5%9D%97/3.%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86.ipynb\nDataSet torchvision库中提供了许多常用的计算机视觉数据集。以下是torchvision库中支持的一些常见数据集的列表：\nMNIST：手写数字图片数据集。 FashionMNIST：时尚商品图片数据集。 CIFAR10：包含10个类别的彩色图片数据集。 CIFAR100：包含100个细分类别的彩色图片数据集。 SVHN：包含数字图片的街景数据集。 ImageNet：包含超过100万个物体类别的彩色图片数据集。 COCO：包含多个物体类别的彩色图片数据集，用于目标检测和图像分割任务。 除了上述数据集，torchvision还提供了一些辅助函数和类，用于加载和预处理数据集，如DataLoader、ImageFolder等。\nDataLoader 以下是对CIFAR10的加载例子\nimport torchvision.datasets as datasets\rimport torchvision.transforms as transforms\rfrom torch.utils.data import Dataset,DataLoader\rimport torch as t\rimport numpy as np\r#加载训练数据50000条\rtrain_dataset=datasets.CIFAR10(root=\"./data\",train=True,transform=transforms.ToTensor(),download=True)\r#测试数据集10000条\rtest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\r#打印数据集的维度\rprint(train_dataset.data.shape,test_dataset.data.shape)\r#打印数据集的标签\rprint(len(train_dataset.targets))\r#torchvision.datasets.cifar.CIFAR10\rprint(type(train_dataset))\r#torchvision.datasets.vision.VisionDataset\rprint(type(train_dataset).__bases__) 注意datasets.CIFAR10在root指定的目录没有数据集会自动下载，如果下载很慢，可以将控制台打印的路径下载下来丢到./data目录即可离线加载。\nDataLoader是PyTorch中用于数据加载的实用工具类。它可以将自定义的数据集包装成一个可迭代的数据加载器，方便进行批处理、洗牌和并行加载等操作。以下是DataLoader的一些常用参数的详细解释：\ndataset：要加载的数据集。可以是继承自torch.utils.data.Dataset的自定义数据集类的实例，也可以是已有的PyTorch数据集类（如torchvision.datasets.ImageFolder）的实例。\nbatch_size：每个批次中样本的数量。默认值为1。通常会根据模型和设备的内存情况选择合适的批量大小。\nshuffle：是否在每个epoch开始前对数据进行洗牌（随机打乱顺序）。默认值为False。洗牌可以提高训练的随机性，有助于模型更好地学习数据中的模式。\nsampler：用于定义数据采样策略的采样器。如果指定了sampler，则忽略shuffle参数。常用的采样器包括torch.utils.data.RandomSampler（随机采样）和torch.utils.data.SequentialSampler（顺序采样）。\nbatch_sampler：用于定义批次级别的数据采样策略的采样器。如果指定了batch_sampler，则忽略batch_size、shuffle和sampler参数。常用的批次采样器包括torch.utils.data.BatchSampler。\nnum_workers：用于数据加载的子进程数量。默认值为0，表示在主进程中加载数据。可以根据计算机的CPU核心数和数据加载的性能需求来选择合适的数值。\ncollate_fn：用于将样本列表转换为批次张量的函数。默认情况下使用默认的collate函数，它假定样本是Tensor或Numpy数组，并将它们堆叠成批次。如果数据集返回的样本具有不同的类型或形状，可以自定义collate函数来处理。\npin_memory：是否将数据加载到CUDA固定内存中。默认值为False。当使用GPU进行训练时，设置pin_memory为True可以加速数据传输，但会占用额外的内存。\ndrop_last：如果数据集的大小不能被批次大小整除，是否丢弃最后一个不完整的批次。默认值为False。在训练过程中，通常会设置为True，以避免不完整的批次导致的错误。\ntimeout：数据加载器在等待数据时的超时时间（以秒为单位）。默认值为0，表示无超时限制。如果数据加载时间较长，可以设置一个较大的超时时间。\nworker_init_fn：用于每个数据加载器子进程的初始化函数。可以用来设置每个子进程的随机种子或其他初始化操作。\n这些参数可以根据具体的需求进行调整和配置，以实现更高效、方便的数据加载 DataLoader会将加载的数据集转换为（批量，通道，高度，宽度）的形式。在PyTorch中，图像数据一般采用CHW（通道，高度，宽度）的顺序。而DataLoader则会将加载的图像数据转换为（批量，通道，高度，宽度）的形式， 其中批量表示一次加载的图像数量。这样的数据形式符合PyTorch中卷积神经网络的输入要求。 torchvision.datasets.vision.VisionDataset复杂处理这些。\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\rtest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\r# 使用数据加载器进行迭代,一批次64条，64条一个循环\rfor batch in train_loader:\rinput_data, labels = batch\rprint(input_data.shape)\rbreak; 输出：torch.Size([64, 3, 32, 32])\n自定义数据集 自己创建的数据集没有做任何维度的转换。\nclass MyDs(Dataset):\rdef __init__(self,data,label):\rself.data=data\rself.label=label\rdef __len__(self):\rreturn len(self.data)\rdef __getitem__(self, index):\rreturn self.data[index],self.label[index]\rds=MyDs([1,2,3,4],[0,1,1,1])\rdsLoader=DataLoader(ds,batch_size=2,shuffle=True)\rfor input,label in dsLoader: #四条数据分成了2批，循环两次\rprint(input,label) nn模块 nn.Module nn.Module是PyTorch中所有神经网络模块的基类。它是构建自定义神经网络模块的核心组件，提供了一些基本功能和属性。\n下面是nn.Module的一些重要属性和方法：\nparameters()：返回模块中需要训练的参数的迭代器。 named_parameters()：返回模块中需要训练的参数及其名称的迭代器。 children()：返回模块中所有子模块的迭代器。 named_children()：返回模块中所有子模块及其名称的迭代器。 to(device)：将模块移动到指定的设备（如CPU或GPU）。 train()：将模块设置为训练模式，启用BatchNorm和Dropout等层的训练行为。 eval()：将模块设置为评估模式，禁用BatchNorm和Dropout等层的训练行为。 forward(input)：定义模块的前向传播逻辑，接收输入并返回输出。 此外，nn.Module还提供了一些方法用于模块的初始化和参数管理：\n__init__()：构造函数，用于初始化模块的参数和子模块。 zero_grad()：将模块中所有参数的梯度置零。 apply(fn)：递归地对模块和子模块应用指定的函数。 state_dict()：返回模块的当前状态字典，包含所有参数和缓冲区。 load_state_dict(state_dict)：加载给定的状态字典，用于恢复模块的参数和缓冲区。 通过继承nn.Module类，可以方便地构建自定义的神经网络模块，并使用PyTorch提供的许多功能来管理模块的参数、状态和计算逻辑。\n使用module自定义一个全连接层\nimport torch as t;\rimport torch.nn as nn\rclass Linear(nn.Module):\rdef __init__(self,input_feature,out_feature):\rnn.Module.__init__(self)\r#nn.Prameter是自动算梯度的\rself.w=nn.Parameter(t.randn(input_feature,out_feature))\rself.b=nn.Parameter(t.randn(out_feature))\rdef forward(self,x):\rreturn x.mm(self.w)+self.b\rlayer=Linear(4,1)\rrtn=layer(t.randn(3,4))\rrtn.backward(t.ones(rtn.size())) # 计算梯度\rprint(layer.w.grad) # 获取w的梯度\rprint(layer.b.grad) # 获取b的梯度 CNN 在神经网络处理中，图片矩阵的通道通常是在宽高之前。这种表示方式被称为“通道优先”（channel-first）或“NCHW”表示法。在这种表示法中，矩阵的维度顺序为（批量大小，通道数，高度，宽度）。\n例如，对于一个RGB彩色图像，它的矩阵表示将具有维度顺序为（1，3，H，W），其中1是批量大小（表示一次处理的图像数量，就是行数），3是通道数（表示RGB三个通道），H是图像的高度，W是图像的宽度，pytorch使用这种方式。\n另一种表示方式是“宽高优先”（channel-last）或“NHWC”表示法，其中矩阵的维度顺序为（批量大小，高度，宽度，通道数）。但是，通道优先的表示法更常见，因为它与卷积操作的计算方式更契合，tensorflow使用这种方式。\n图像处理层 PyTorch提供了一系列用于图像处理的层和函数。以下是一些常用的图像处理层：\nnn.Conv2d：卷积层，用于提取图像中的特征。 nn.MaxPool2d：最大池化层，用于降低特征图的空间维度。 nn.AvgPool2d：均值池化层，用于降低特征图的空间维度 nn.BatchNorm2d：批量归一化层，用于加速训练并提高模型的鲁棒性。 nn.ReLU：ReLU激活函数层，用于引入非线性性。 nn.Linear：全连接层，用于将卷积层的输出映射到最终的分类或回归结果。 nn.Dropout2d：二维Dropout层，用于减少过拟合。 nn.Upsample：上采样层，用于增加特征图的空间维度。 nn.Softmax：Softmax函数层，用于多类别分类问题中的概率计算。 除了这些层，PyTorch还提供了一些用于图像处理的函数，例如卷积操作torch.nn.functional.conv2d，池化操作torch.nn.functional.max_pool2d，以及其他常用的图像处理函数如裁剪、旋转、缩放等。\n这些层和函数可以用来构建卷积神经网络（CNN）等图像处理模型,torch.nn.functional只是用于计算结果而nn包的函数可以用于计算梯度，如果在构建神经网络时必须用nn包。\n卷积神经网络的各层的概念请参考：https://blog.csdn.net/liaomin416100569/article/details/130597944?spm=1001.2014.3001.5501\nnn.Conv2d nn.Conv是PyTorch中用于定义卷积层的类，它的参数如下：\nin_channels：输入张量的通道数。 out_channels：卷积层输出的通道数，也是卷积核的数量。 kernel_size：卷积核的大小，可以是一个整数或一个元组，如(3, 3)。 stride：卷积操作的步长，默认为1。 padding：在输入张量的边缘周围填充0的层数，默认为0。 dilation：卷积核元素之间的间隔，默认为1。 groups：将输入张量分成几组进行卷积，默认为1。 bias：是否使用偏置项，默认为True。 以下是一个示例：\nimport torch.nn as nn\r# 创建一个卷积层\rconv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\r## 打印卷积层的参数\rprint(conv) 输出结果如下：\nConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n上述代码创建了一个输入通道数为3，输出通道数（神经元个数）为64的卷积层，卷积核大小为3x3，步长为1，填充层数为1。\nin_channels代表输入张量的通道数，也可以理解为输入张量的维度。在卷积神经网络中，输入张量的维度通常是指图像的通道数。例如，对于RGB图像，通道数为3，因为图像由红、绿、蓝三个通道组成。对于灰度图像，通道数为1，因为图像只有一个通道。\n在使用nn.Conv创建卷积层时，需要根据输入张量的通道数来设置in_channels参数，以确保卷积层与输入张量的维度匹配。\nConv2d的步长（stride）参数表示卷积核在进行滑动时的步幅大小。步长的作用是控制输出特征图的尺寸。具体来说，如果步长为1， 则卷积核每次滑动一个像素；如果步长为2，则卷积核每次滑动两个像素，以此类推。 步长的两个维度分别表示在图像的行方向和列方向上的步幅大小。在您提供的示例中，步长为(1, 1)，表示卷积核在图像的行和列方向上每次滑动一个像素。\nConv2d的padding参数表示在输入图像的周围添加填充（padding）的大小。填充的作用是在卷积操作中保持输出特征图的尺寸与输入特征图的尺寸相同，或者根据需要进行调整。\n#################学习卷积\rimport torchvision.datasets as datasets\rimport torchvision.transforms as transforms\rimport matplotlib.pyplot as plt\rimport torch.nn as nn\rimport torch as t\r#预处理模块\rfrom PIL import Image\rimage=Image.open(\"./images/2023_6_30.jpg\")\r# plt.imshow(image)\r# plt.show()\r\"\"\"\r这是一个用于边缘检测的卷积核。在这个卷积核中，中心元素是1，\r表示当前位置的像素值对边缘检测有贡献，而周围的元素都是-0.1111，\r表示对边缘检测没有贡献。这样的卷积核可以帮助我们提取图像中的垂直边缘特征。\r\"\"\"\rkernel=t.Tensor(\r[[-0.1111, -0.1111, -0.1111],\r[-0.1111, 1.0000, -0.1111],\r[-0.1111, -0.1111, -0.1111]],\r)\rkernel=t.ones(3,3)/-9\rkernel[1][1]=1\r#转换成灰度图，通道数变成1了\rimage=image.convert(\"L\")\r#转换成张量\rimageTensor=transforms.ToTensor()(image)\rprint(imageTensor.shape)\r#在第0个维度添加一个一维表示批次数据\rinput=imageTensor.unsqueeze(0)\rprint(\"输入形状\",input.shape)\rlayer=nn.Conv2d(1,1,(3,3),bias=False)\r# 定义输入张量shape为(batch_size, channels, height, width)\rlayer.weight.data=kernel.view(1,1,3,3)\routput=layer(input)\rplt.imshow(transforms.ToPILImage()(output.squeeze(0)),cmap=\"gray\")\rplt.show()\r#每个卷积核（3×3）与原始的输入图像（480×479）进行卷积，这样得到的 feature map（特征图）大小为（480-3+1）×（479-3+1）= 478×477\rprint(\"输出形状\",output.shape) 原始图 输出： torch.Size([1, 480, 479]) 输入形状 torch.Size([1, 1, 480, 479]) 输出形状 torch.Size([1, 1, 478, 477]) nn.AvgPool2d和nn.MaxPool2d 上面的卷积图在经过池化\nplt.rcParams['font.sans-serif'] = ['SimHei'] # 设置全局字体为SimHei\r#平均池化（AvgPool）\rpool=nn.AvgPool2d(kernel_size=2, stride=2)\r#池化层478×477经过(2,2)池化后=(478/2=239,477/2=238)\rpoolOuput=pool(output)\rprint(poolOuput.shape)\rplt.title(\"平均池化\")\rplt.imshow(transforms.ToPILImage()(poolOuput.squeeze(0)),cmap=\"gray\")\rplt.show()\r#最大化池\rpool=nn.MaxPool2d(kernel_size=2, stride=2)\r#池化层478×477经过(2,2)池化后=(478/2=239,477/2=238)\rpoolOuput=pool(output)\rprint(poolOuput.shape)\rplt.title(\"最大池化\")\rplt.imshow(transforms.ToPILImage()(poolOuput.squeeze(0)),cmap=\"gray\")\rplt.show() 输出 nn.Linear nn.Linear是PyTorch中用于定义线性变换的类。它是nn.Module的子类，用于构建神经网络的层。\nnn.Linear接受两个参数：in_features和out_features，分别表示输入特征的大小和输出特征的大小。它会自动创建一个可学习的权重矩阵，形状为( in_features，out_features)，以及一个可学习的偏置向量，形状为(out_features,)。\n#注意全连接是特征连接是是改变最后一维的特征数的，在pytorch图片批量处理后最后需要进行view操作来降低维度到二维。\rarr=t.randn((3,4)) print(arr)\rresult=nn.Linear(4,5)\r#全连接就是一个输入数据点乘(输入数据维度,输出数据维度)最后得到一个（输入数据行数，输出数据维度）的数组\rprint(result(arr)) nn.BatchNorm2d BatchNorm2d是用于对二维卷积层的输出进行批量归一化的操作。它的计算过程如下所示：\n假设输入的维度为 [batch_size, num_channels, height, width]，其中 batch_size 表示批量大小，num_channels 表示通道数，height 和 width 表示特征图的高度和宽度。\n计算每个通道的均值和方差：\n对于每个通道，计算当前批次中所有样本的特征图在该通道上的均值和方差。 均值的计算：mean = sum(x) / N，其中 x 是当前通道上的特征图值，N 是批次大小。 方差的计算：var = sum((x - mean)^2) / N。 对于每个通道，进行归一化：\n对于每个样本，在当前通道上，将特征图的值减去均值，然后除以标准差（方差的平方根），以实现归一化。 归一化后的特征图为：y = (x - mean) / sqrt(var + eps)，其中 eps 是一个很小的数，以避免除以零的情况。 对于每个通道，进行缩放和平移：\n对于每个归一化后的特征图，通过乘以一个可学习的缩放因子（scale）和加上一个可学习的平移因子（shift），对特征图进行缩放和平移。 缩放和平移后的特征图为：y = gamma * y + beta，其中 gamma 和 beta 是可学习的参数。 最后，BatchNorm2d操作的输出为归一化、缩放和平移后的特征图。\n这样做的好处是可以加快神经网络的训练速度，提高模型的收敛性和泛化能力，并减少对学习率的敏感性。\n\"\"\"\r具体来说，nn.BatchNorm2d是应用在卷积层之后、激活函数之前的操作，其目的是对每个特征通道的数据进行归一化。\r它通过对每个特征通道的数据进行标准化，使得数据的均值为0，方差为1。这样做的好处是可以防止梯度消失或爆炸的问题，\r并且有助于加速模型的收敛速度。\r除此之外，nn.BatchNorm2d还具有正则化的效果，可以减少模型的过拟合。它通过引入额外的可学习参数，实现了对每个特征通道的平移和缩放操作，以便网络可以自行学习数据的适当分布。\r\"\"\"\rarr=t.randint(0,10,(1,1,2,2)).float()#一批次一个通道，高是2，宽是2\rprint(arr)\rresult=nn.BatchNorm2d(num_features=1)\r#全连接就是一个输入数据点乘(输入数据维度,输出数据维度)最后得到一个（输入数据行数，输出数据维度）的数组\rprint(result(arr)) 输出：\ntensor([[[[9., 1.],\r[8., 9.]]]])\rtensor([[[[ 0.6727, -1.7191],\r[ 0.3737, 0.6727]]]], grad_fn=\u003cNativeBatchNormBackward\u003e) nn.Relu nn.ReLU是PyTorch中的一个激活函数，它将输入中的所有负值变为零，保持正值不变。具体来说，对于输入张量x，nn.ReLU函数的计算公式为：\nReLU(x) = max(0, x) 例子\narr=t.randint(0,10,(1,1,2,2)).float()#一批次一个通道，高是2，宽是2\r#首先进行归一化，归一化后会有负数的部分\rbatchNorm2d=nn.BatchNorm2d(num_features=1)\rresult=batchNorm2d(arr)\rprint(\"归一化\",result)\rrelu=nn.ReLU()\r#全连接就是一个输入数据点乘(输入数据维度,输出数据维度)最后得到一个（输入数据行数，输出数据维度）的数组\rprint(\"relu结果\",relu(result)) 输出：\n归一化 tensor([[[[ 0.2773, -1.3867],\r[ 1.3867, -0.2773]]]], grad_fn=\u003cNativeBatchNormBackward\u003e)\rrelu结果 tensor([[[[0.2773, 0.0000],\r[1.3867, 0.0000]]]], grad_fn=\u003cReluBackward0\u003e) nn.Dropout2d nn.Dropout2d会在训练过程中，对输入张量的每个通道的每个元素按照给定的概率进行丢弃。被丢弃的元素会被设置为零，而保留的元素则会按比例进行缩放，以保持期望值不变。 这种随机丢弃的操作有助于在训练过程中减少过拟合现象，增强模型的泛化能力。丢弃的概率可以通过nn.Dropout2d的参数进行控制。 需要注意的是，在测试过程中，所有的元素都会被保留，不会进行丢弃操作。nn.Dropout2d通常用于卷积神经网络中，可以放在卷积层或者全连接层之后，帮助网络更好地适应数据。 例子\narr=t.randint(0,10,(1,1,4,4)).float()#一批次一个通道，高是2，宽是2\rdrop=nn.Dropout2d()\rnewArr=drop(arr)\rprint(newArr) 输出\ntensor([[[[ 8., 8., 12., 12.],\r[10., 12., 6., 12.],\r[ 0., 4., 12., 16.],\r[ 4., 4., 18., 10.]]]]) nn.Softmax nn.Softmax是PyTorch中的一个函数，它用于计算softmax函数的输出。softmax函数通常用于多分类问题的神经网络中，它将原始的类别分数转化为概率分布。\n在PyTorch中，nn.Softmax可以被应用于一维或二维张量。对于一维张量，它会对张量中的每个元素进行softmax操作，并返回一个与输入张量相同形状的张量。对于二维张量，它会在指定维度上对每行进行softmax操作。\nsoftmax函数的计算公式如下：\n$softmax(x_i) = exp(x_i) / sum(exp(x_j))$\n其中，$x_i$是原始的类别分数，exp是指数函数，sum是对所有类别分数的求和。\nsoftmax函数的输出是一个概率分布，每个类别的概率值介于0和1之间，并且所有类别的概率之和为1。这样可以方便地用于多分类问题中，根据概率选择最可能的类别。\n在PyTorch中，可以使用nn.Softmax函数对网络的输出进行处理，以得到分类结果。 例子\narr=t.randint(0,10,(1, 1, 4, 4)).float()#一批次一个通道，高是2，宽是2\rprint(arr)\r#注意在哪个维度上的和等于1，比如一个4维的（维度从0开始），(1, 1, 4, 4)如果你从0维上，取出0维第一行数据/0维上所有数据行，因为只有一行所有永远都是1\r#如果是第3维上，总共有4个数据，也就是这四个数之和等于1\r#Softmax2D==nn.Softmax(dim=1)\rsoftmax2d=nn.Softmax(dim=3)\rnewArr=softmax2d(arr)\rprint(newArr)\rt.manual_seed(10)\rarr=t.randint(0,10,(1, 2, 4, 4)).float()#一批次2个通道，高是2，宽是2\rsoftmax2d=nn.Softmax2d()\rnewArr=softmax2d(arr)\rprint(arr)\rprint(newArr) 输出\ntensor([[[[7., 5., 2., 0.],\r[3., 0., 8., 1.],\r[6., 8., 8., 4.],\r[2., 6., 3., 5.]]]])\rtensor([[[[8.7490e-01, 1.1841e-01, 5.8950e-03, 7.9781e-04],\r[6.6846e-03, 3.3281e-04, 9.9208e-01, 9.0466e-04],\r[6.2840e-02, 4.6433e-01, 4.6433e-01, 8.5045e-03],\r[1.2755e-02, 6.9639e-01, 3.4671e-02, 2.5619e-01]]]])\rtensor([[[[7., 5., 2., 7.],\r[2., 5., 7., 2.],\r[1., 5., 6., 3.],\r[1., 0., 6., 3.]],\r[[4., 0., 6., 2.],\r[8., 9., 2., 0.],\r[9., 9., 4., 4.],\r[9., 4., 4., 5.]]]])\rtensor([[[[9.5257e-01, 9.9331e-01, 1.7986e-02, 9.9331e-01],\r[2.4726e-03, 1.7986e-02, 9.9331e-01, 8.8080e-01],\r[3.3535e-04, 1.7986e-02, 8.8080e-01, 2.6894e-01],\r[3.3535e-04, 1.7986e-02, 8.8080e-01, 1.1920e-01]],\r[[4.7426e-02, 6.6929e-03, 9.8201e-01, 6.6929e-03],\r[9.9753e-01, 9.8201e-01, 6.6929e-03, 1.1920e-01],\r[9.9966e-01, 9.8201e-01, 1.1920e-01, 7.3106e-01],\r[9.9966e-01, 9.8201e-01, 1.1920e-01, 8.8080e-01]]]]) nn.Sequential nn.Sequential和nn.ModuleList是PyTorch中用于组合神经网络模块的两种容器。\nnn.Sequential：\nnn.Sequential是一个按照顺序排列的容器，其中的模块按照它们被添加到容器中的顺序依次执行。 可以通过在Sequential对象的构造函数中传递模块列表来创建Sequential容器，或者通过.add_module()方法逐个添加模块。 nn.Sequential适用于简单的顺序模型，其中每个模块只有一个输入和一个输出。 nn.ModuleList：\nnn.ModuleList是一个可以包含任意数量模块的容器，模块之间没有特定的顺序。 可以通过在ModuleList对象的构造函数中传递模块列表来创建ModuleList容器，或者通过.append()方法逐个添加模块。 nn.ModuleList适用于自定义连接和复杂的模型结构，其中模块之间可能存在多个输入和输出。 总而言之，nn.Sequential适用于简单的顺序模型，而nn.ModuleList适用于自定义连接和复杂的模型结构。在实际使用中，可以根据模型的结构和需要选择合适的容器。 使用nn.Sequential自定义一个多层感知器\nimport torch as t\rimport torch.nn as nn\r#实现一个多层感知器,多层感知器（Multilayer Perceptron, MLP）的隐藏层的特征数就是神经元的个数\rclass MulPerceptron(nn.Module):\rdef __init__(self,input_feature,hidden_feature,out_feature):\rnn.Module.__init__(self)\r#Sequential会将上一层的输出作为下层的输入\rself.model=nn.Sequential(\rnn.Linear(input_feature,hidden_feature),\rnn.ReLU(),\rnn.Linear(hidden_feature,out_feature)\r)\rdef forward(self,x):\r#隐藏层进行一次全连接得到（行，hidden_feature）数据矩阵\rreturn self.model(x);\rmp=MulPerceptron(784,512,1)\rresult=mp(t.randn(200,784))\rprint(result) 最后输出(200,1)的结果。\n损失函数 PyTorch提供了一系列常用的损失函数，下面是其中一些常见的损失函数及其用法举例：\nnn.MSELoss（均方误差损失函数）：\n用于回归任务，计算预测值与真实值之间的均方误差。\nloss_fn = nn.MSELoss()\rloss = loss_fn(output, target)\nnn.CrossEntropyLoss（交叉熵损失函数）：\n用于多分类任务，计算预测类别与真实类别之间的交叉熵损失。\nloss_fn = nn.CrossEntropyLoss()\rloss = loss_fn(output, target)\nnn.BCELoss（二分类交叉熵损失函数）：\n用于二分类任务，计算预测概率与真实标签之间的二分类交叉熵损失。\nloss_fn = nn.BCELoss()\rloss = loss_fn(output, target)\nnn.BCEWithLogitsLoss（二分类交叉熵损失函数，结合了Sigmoid函数）：\n用于二分类任务，结合了Sigmoid函数的操作，可以在计算二分类交叉熵损失时避免数值稳定性问题。\nloss_fn = nn.BCEWithLogitsLoss()\rloss = loss_fn(output, target)\nnn.NLLLoss（负对数似然损失函数）：\n用于多分类任务，计算预测类别的负对数似然损失。\nloss_fn = nn.NLLLoss()\rloss = loss_fn(output, target)\n这只是一小部分PyTorch中提供的损失函数，还有其他损失函数可用于不同的任务和应用场景。你可以根据具体的需求选择合适的损失函数来进行模型训练和优化。\n均方误差 均方误差（Mean Squared Error，MSE）是一种常用的回归问题的损失函数。它衡量了预测值与真实值之间的差异的平方的平均值。\n对于给定的预测值和真实值，MSE的计算公式如下：\nMSE = (1/n) * Σ(y_pred - y_true)^2\n其中，n是样本数量，y_pred是预测值，y_true是真实值。\nMSE的值越小，表示预测值和真实值之间的差异越小，模型的性能越好。常用的优化算法，如梯度下降法，通过最小化MSE来调整模型的参数，以提高模型的准确性。\nx=t.randn(100,1)\ry=3*x+2+t.randn(100,1) #实际值上加上一些随机噪点\ry_pre=3*x+2\rplot.plot(x,y,'.')\rplot.plot(x,y_pre)\rplot.show()\r#使用均方误差计算损失值\rcriterion=nn.MSELoss()\rloss=criterion(y,y_pre)\rprint(loss) 交叉熵 下面是一个使用nn.CrossEntropyLoss的例子，并对交叉熵的计算过程进行详细解释：\nimport torch.nn as nn\r# 假设有4个样本，每个样本有3个类别的预测结果\r# 真实标签为[2, 1, 0, 2]\r# 预测结果为一个3维张量，每一维表示对应类别的预测概率\routputs = torch.tensor([[0.1, 0.2, 0.7],\r[0.6, 0.3, 0.1],\r[0.8, 0.1, 0.1],\r[0.3, 0.5, 0.2]])\rlabels = torch.tensor([2, 1, 0, 2])\r# 创建交叉熵损失函数\rloss_fn = nn.CrossEntropyLoss()\r# 计算损失\rloss = loss_fn(outputs, labels)\rprint(loss) 输出结果为：\ntensor(0.8025)\n交叉熵是一种常用的损失函数，用于衡量两个概率分布之间的相似性。在分类任务中，我们通常将模型的预测结果视为一个概率分布，其中每个类别都有一个对应的概率。\n在上面的例子中，我们有4个样本，每个样本有3个类别的预测结果。outputs是一个3维张量，每一维表示对应类别的预测概率。例如，outputs[0]表示第一个样本对三个类别的预测概率，分别为0.1、0.2和0.7。\nlabels是一个1维张量，表示每个样本的真实类别标签。例如，labels[0]表示第一个样本的真实类别标签为2。\n交叉熵损失函数的计算过程如下：\n首先，对于每个样本，我们需要根据预测概率和真实标签计算出对应类别的预测概率。\n在上面的例子中，对于第一个样本，预测概率为[0.1, 0.2, 0.7]，真实标签为2。我们只需要取出预测概率中对应真实标签的值，即0.7。\n接下来，我们对每个样本的预测概率进行对数转换，即计算每个预测概率的自然对数。\n在上面的例子中，对于第一个样本，对数转换后的预测概率为log(0.7)。\n然后，我们将对数转换后的预测概率求和，并除以样本的数量，得到平均交叉熵损失。\n在上面的例子中，我们有4个样本，因此将对数转换后的预测概率求和，并除以4，得到平均交叉熵损失。\n最后，我们将平均交叉熵损失作为模型的损失，并用于模型的训练和优化过程。在PyTorch中，我们可以使用nn.CrossEntropyLoss函数来计算交叉熵损失，它会自动进行softmax操作和对数转换的计算。\n优化器 torch.optim是PyTorch中用于优化算法的模块。它提供了各种优化算法的实现，用于更新神经网络模型的参数以最小化损失函数。\n在PyTorch中，我们通过创建一个优化器对象来使用torch.optim模块。该优化器对象将被用于更新神经网络模型的参数。\n以下是torch.optim模块中常用的优化算法：\nSGD (Stochastic Gradient Descent): 随机梯度下降算法是最基本的优化算法之一。它通过计算损失函数对参数的梯度，并根据学习率更新参数。可以通过torch.optim.SGD类来实现。\nAdam (Adaptive Moment Estimation): Adam是一种自适应的优化算法，它结合了Momentum和RMSprop的优点。它使用动量和平方梯度的指数加权移动平均来自适应地调整学习率。可以通过torch.optim.Adam类来实现。\nAdagrad (Adaptive Gradient): Adagrad是一种自适应的优化算法，它为每个参数分配一个学习率，并根据参数的历史梯度的平方和来自适应地调整学习率。可以通过torch.optim.Adagrad类来实现。\nRMSprop (Root Mean Square Propagation): RMSprop也是一种自适应的优化算法，它使用指数加权移动平均来自适应地调整学习率。它通过除以梯度的平方和的平方根来缩放学习率。可以通过torch.optim.RMSprop类来实现。\n这些优化算法都可以通过创建相应的优化器对象，并传递神经网络模型的参数和其他超参数来使用。例如，下面的代码演示了如何使用SGD优化算法(伪代码)：\nimport torch\rimport torch.optim as optim\r# 创建神经网络模型\rmodel = MyModel()\r# 创建优化器对象，学习率为0.001\roptimizer = optim.SGD(model.parameters(), lr=0.001)\r# 在每个训练迭代中，使用优化器更新模型的参数\roptimizer.zero_grad() # 清零梯度\routput = model(input) # 前向传播\rloss = criterion(output, target) # 计算损失\rloss.backward() # 反向传播\roptimizer.step() # 更新参数 在上面的代码中，model.parameters()返回神经网络模型的所有可学习参数，这些参数将被优化器更新。optimizer.zero_grad()方法用于将参数的梯度清零，loss.backward()方法用于计算梯度，optimizer.step()方法用于更新参数。\n除了上述常用的优化算法之外，torch.optim模块还提供了其他一些优化算法，如Adadelta、AdamW等。您可以根据自己的需求选择适合的优化算法来训练模型。\nLetnet5分类CIFAR10 #%%\rimport torch\rimport torch.nn as nn\rimport torch.optim as optim\rimport torchvision.transforms as transforms\rfrom torch.utils.data import DataLoader\rfrom torchvision.datasets import CIFAR10\r#定义LeNet5模型，模型计算过程参考：https://blog.csdn.net/liaomin416100569/article/details/130677530?spm=1001.2014.3001.5501\rclass LeNet5(nn.Module):\rdef __init__(self):\rsuper(LeNet5, self).__init__()\rself.features = nn.Sequential(\r#C1 层（卷积层）：6@28×28 该层使用了 6 个卷积核，每个卷积核的大小为 5×5，这样就得到了 6 个 feature map（特征图）。\rnn.Conv2d(3, 6, kernel_size=5),\rnn.ReLU(inplace=True),\r#S2 层（下采样层，也称池化层）：6@14×14,池化单元为 2×2，因此，6 个特征图的大小经池化后即变为 14×14\rnn.MaxPool2d(kernel_size=2, stride=2),\r#C3 层（卷积层）：16@10×10 C3 层有 16 个卷积核，卷积模板大小为 5×5 C3 层的特征图大小为（14-5+1）×（14-5+1）= 10×10。\rnn.Conv2d(6, 16, kernel_size=5),\rnn.ReLU(inplace=True),\r#S4（下采样层，也称池化层）：16@5×5,与 S2 的分析类似，池化单元大小为 2×2，因此，该层与 C3 一样共有 16 个特征图，每个特征图的大小为 5×5。\rnn.MaxPool2d(kernel_size=2, stride=2)\r)\rself.classifier = nn.Sequential(\r#LeNet-5模型中的C5层是一个全连接层。在LeNet-5模型中，前两个卷积层（C1和C3）之后是一个池化层（S2和S4），\r# 然后是一个全连接层（C5），最后是输出层（F6）。全连接层C5的输入是S4层的输出，它将这个输入展平为一个向量，\r# 并将其连接到输出层F6。因此，C5层是一个全连接层，而不是卷积层，这里和文中有些冲突。\r#C5 层（卷积层）：120 该层有 120 个卷积核，每个卷积核的大小仍为 5×5，因此有 120 个特征图,特征图大小为（5-5+1）×（5-5+1）= 1×1。这样该层就刚好变成了全连接\rnn.Linear(16 * 5 * 5, 120),\rnn.ReLU(inplace=True),\r#F6 层（全连接层）：84,该层有 84 个特征图，特征图大小与 C5 一样都是 1×1\rnn.Linear(120, 84),\rnn.ReLU(inplace=True),\r# OUTPUT 层（输出层）：10\rnn.Linear(84, 10)\r)\rdef forward(self, x):\rx = self.features(x)\r#张量x在第0个维度上的大小，因为第0个维度是数据批次数（行数），s4层后的维度是(批次数，16,5,5)\r#转换成2维就是(行数,16*5*5)，-1表示自动计算合并成最后一个维度\rx = x.view(x.size(0), -1)\rx = self.classifier(x)\rreturn x\rmodel = LeNet5()\r\"\"\"\rtransforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])是一种数据预处理操作，用于对图像数据进行归一化处理。这个操作会将每个像素的数值减去均值(mean)并除以标准差(std)。\r在这个例子中，mean=[0.5, 0.5, 0.5]表示将每个通道的像素值减去0.5，std=[0.5, 0.5, 0.5]表示将每个通道的像素值除以0.5。这样处理后，图像的像素值会在-1到1之间。\r归一化可以帮助提高模型的训练效果和稳定性，因为它可以使输入数据的分布更加接近标准正态分布。此外，对于不同的数据集，可能需要不同的均值和标准差进行归一化操作，以使数据的分布更加合理。\r在使用PyTorch的transforms.Normalize时，通常需要将其与其他数据预处理操作一起使用，例如transforms.ToTensor()将图像转换为张量。可以通过transforms.Compose将多个预处理操作组合在一起，形成一个数据预处理管道。\r\"\"\"\rtransform = transforms.Compose([\rtransforms.ToTensor(),\rtransforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\r])\r#下载训练集,data是数据数组，target是标签\rtrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\r#下载测试集\rtest_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\r#数据批处理和打乱，一次64条数据\rtrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\rtest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\r#使用交叉熵损失函数\rcriterion = nn.CrossEntropyLoss()\r#使用随机梯度下降法优化参数，梯度下降的学习率是0.001\roptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\r#判断是否有gpu如果有的话讲模型附加到cuda设备上\r#momentum参数通过累积之前的梯度信息，使得参数更新具有一定的惯性，从而在参数空间中更快地找到全局最优解或局部最优解。\rdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\rmodel.to(device)\r#模型对数据集进行10次epoch\rnum_epochs = 10\rfor epoch in range(num_epochs):\rmodel.train()\repoch_loss = 0.0\rfor images, labels in train_loader:\rimages = images.to(device)\rlabels = labels.to(device)\roptimizer.zero_grad()\routputs = model(images)\rloss = criterion(outputs, labels)\rloss.backward()\roptimizer.step()\repoch_loss += loss.item()\rprint(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\r\"\"\"\rmodel.eval()是PyTorch中用于将模型设置为评估模式的函数。当调用model.eval()时，模型的行为会发生变化，包括：\r1. Batch Normalization和Dropout等具有随机性的层会固定住，不再产生随机变化。\r2. 模型的参数不会被更新，即不会进行梯度计算和反向传播。\r3. 在推断阶段，模型会根据输入数据生成输出，而不会进行训练。\r通常，在测试或评估模型时，需要调用model.eval()来确保模型的行为与训练时保持一致。\r这样可以避免由于Batch Normalization和Dropout等层的随机性而导致结果不稳定。在调用model.train()之前，\r应该使用model.eval()将模型切换回训练模式,要将模型切换回训练模式，可以使用model.train()方法。\r\"\"\" model.eval()\rcorrect = 0\rtotal = 0\r#torch.no_grad()是一个上下文管理器，将其包裹的代码块中的所有操作都不会计算梯度。\r# 通常用于在不需要计算梯度的情况下进行推理或评估。\rwith torch.no_grad():\rfor images, labels in test_loader:\r#数据加载到显存\rimages = images.to(device)\rlabels = labels.to(device)\routputs = model(images)\r#获取输出数据中概率最高的那一个\r_, predicted = torch.max(outputs.data, 1)\r#总共数据行\rtotal += labels.size(0)\r#正确的数据行\rcorrect += (predicted == labels).sum().item()\raccuracy = 100 * correct / total\rprint(f\"Test Accuracy: {accuracy:.2f}%\") RNN nn.RNN nn.RNN是PyTorch中的一个循环神经网络模块，用于处理序列数据。下面是nn.RNN的常用参数和解释：\ninput_size：输入的特征维度。 hidden_size：隐藏层的特征维度。 num_layers：RNN的层数。 nonlinearity：激活函数，默认为\"tanh\"。可以是\"tanh\"、“relu\"等。 bias：是否使用偏置，默认为True。 batch_first：是否输入数据的第一个维度为batch大小，默认为False。 dropout：是否在输出层应用dropout操作，默认为0，即不使用dropout。 bidirectional：是否使用双向RNN，默认为False。 这些参数可以在创建nn.RNN时进行设置。例如：\nimport torch.nn as nn\rinput_size = 10\rhidden_size = 20\rnum_layers = 2\rrnn = nn.RNN(input_size, hidden_size, num_layers) 这样就创建了一个具有输入特征维度为10、隐藏层特征维度为20、2层的RNN模型。\n传入数据格式 nn.RNN的输入数据格式通常为三维张量，具体格式为：\n如果batch_first=False（默认值），则输入数据的形状为(sequence_length, batch_size, input_size)。 如果batch_first=True，则输入数据的形状为(batch_size, sequence_length, input_size)。 其中，\nsequence_length表示序列的长度，即时间步的数目。 batch_size表示每个batch的样本数量。 input_size表示输入特征的维度。 例如，假设我们有一个batch包含3个样本，每个样本的序列长度为4，输入特征维度为5，那么输入数据的形状可以是(4, 3, 5)或(3, 4, 5)。\n可以使用torch.randn()函数生成随机输入数据进行测试，例如：\nimport torch.nn as nn\rbatch_size = 3\rsequence_length = 4\rinput_size = 5\rinput_data = torch.randn(sequence_length, batch_size, input_size)\rrnn = nn.RNN(input_size, hidden_size, num_layers)\routput, hidden = rnn(input_data) 其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\n案例 \"\"\"\rPyTorch中实现了如今最常用的三种RNN：RNN（vanilla RNN）、LSTM和GRU。此外还有对应的三种RNNCell。\rRNN和RNNCell层的区别在于前者能够处理整个序列，而后者一次只处理序列中一个时间点的数据，\r前者封装更完备更易于使用，后者更具灵活性。RNN层可以通过组合调用RNNCell来实现。\r理论参考：https://blog.csdn.net/liaomin416100569/article/details/131380370?spm=1001.2014.3001.5501\r输入参数和RNN参数解释参考readme.md\r\"\"\"\rimport torch as t\rimport torch.nn as nn\r#注意默认（时间步，批次数，数据维度）\rsequence_length =3\rbatch_size =2\rinput_size =4\rinput=t.randn(sequence_length,batch_size,input_size)\rprint(\"输入数据\",input)\rrnnModel=nn.RNN(input_size,3,1)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, hidden=rnnModel(input)\rprint(\"RNN最后时间步隐藏层\",hidden)\rprint(\"RNN最后时间步隐藏层维度\",hidden.shape)\rprint(\"RNN所有隐藏层\",output)\rprint(\"RNN所有隐藏层维度\",output.shape) 输出：\n输入数据 tensor([[[ 0.5364, -0.5291, 0.3117, -0.0282],\r[-0.2012, 0.9933, 1.5328, -0.8234]],\r[[ 1.3270, -1.2367, 0.5925, 1.0894],\r[-1.8035, 0.3598, -0.4404, 0.4921]],\r[[-0.6487, -0.0487, -0.9728, 0.7563],\r[ 1.2929, 0.5146, 1.2296, 1.0124]]])\rRNN最后时间步隐藏层 tensor([[[0.2800, 0.8572, 0.3759],\r[0.5901, 0.4742, 0.9417]]], grad_fn=\u003cStackBackward\u003e)\rRNN最后时间步隐藏层维度 torch.Size([1, 2, 3])\rRNN所有隐藏层 tensor([[[ 0.5862, 0.7417, 0.8068],\r[ 0.9564, 0.5668, 0.6112]],\r[[-0.1729, 0.7310, 0.9879],\r[ 0.6202, 0.7824, 0.3075]],\r[[ 0.2800, 0.8572, 0.3759],\r[ 0.5901, 0.4742, 0.9417]]], grad_fn=\u003cStackBackward\u003e)\rRNN所有隐藏层维度 torch.Size([3, 2, 3]) nn.LSTM nn.LSTM是PyTorch中的一个循环神经网络模块，它基于长短期记忆（Long Short-Term Memory，LSTM）的架构。LSTM是一种特殊类型的循环神经网络，通过使用门控机制来解决传统循环神经网络中的梯度消失和梯度爆炸问题，从而能够更好地处理长期依赖关系。\nnn.LSTM的主要参数包括：\ninput_size：输入数据的特征维度。 hidden_size：隐藏层的维度，也是LSTM单元输出的维度。 num_layers：LSTM的层数，默认为1。 bias：是否使用偏置，默认为True。 batch_first：输入数据的维度顺序是否为(batch, seq, feature)，默认为False。 dropout：是否应用dropout，用于防止过拟合，默认为0，表示不使用dropout。 bidirectional：是否使用双向LSTM，默认为False。 nn.LSTM的输入数据格式通常是一个三维张量，具体格式取决于batch_first参数的设置。如果batch_first为False（默认值），输入数据的维度应为(seq_len, batch, input_size)，其中seq_len表示序列的长度，batch表示批次的大小，input_size表示输入数据的特征维度。如果batch_first为True，输入数据的维度应为(batch, seq_len, input_size)。\nnn.LSTM的前向传播过程会根据输入数据的时间步长和层数进行迭代计算，并返回最后一个时间步的输出以及最后一个时间步的隐藏状态和记忆细胞状态。这些输出可以用于下游任务，如分类或回归。\n使用nn.LSTM时，可以通过调整参数来适应不同的任务和数据。此外，还可以使用nn.LSTMCell来构建自定义的LSTM网络。\nnn.LSTM的返回值是一个元组，包含两个元素：output和(hidden_state, cell_state)。\noutput：表示LSTM模型的隐藏状态输出。它是一个元组，包含了模型在每个时间步的输出结果。具体来说，output的形状是(seq_len, batch, num_directions * hidden_size)，其中：\nseq_len表示输入序列的长度； batch表示输入数据的批次大小； num_directions表示LSTM模型的方向数，通常为1或2（双向LSTM）； hidden_size表示隐藏状态的维度。 (hidden_state, cell_state)：表示LSTM模型的最后一个时间步的隐藏状态和细胞状态。它们的形状都是(num_layers * num_directions, batch, hidden_size)，其中：\nnum_layers表示LSTM模型的层数； num_directions表示LSTM模型的方向数，通常为1或2（双向LSTM）； batch表示输入数据的批次大小； hidden_size表示隐藏状态的维度。 这两个返回值可以用于进一步的处理和分析，比如用于序列标注、语言建模等任务。 也就是hidden_state是output最后一个值，每个时间步都有一个cell_state 案例\nlstmModel=nn.LSTM(input_size,3,1)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, (h, c) =lstmModel(input)\rprint(\"LSTM隐藏层输出的维度\",output.shape)\rprint(\"LSTM隐藏层最后一个时间步输出的维度\",h.shape)\rprint(\"LSTM隐藏层最后一个时间步细胞状态\",c.shape) 输出\nLSTM隐藏层输出的维度 torch.Size([3, 2, 3])\rLSTM隐藏层最后一个时间步输出的维度 torch.Size([1, 2, 3])\rLSTM隐藏层最后一个时间步细胞状态 torch.Size([1, 2, 3]) nn.GRU nn.GRU是PyTorch中的一个循环神经网络（Recurrent Neural Network，RNN）模块，它实现了门控循环单元（Gated Recurrent Unit，GRU）的功能。GRU是一种用于处理序列数据的RNN变体，它具有比传统的循环神经网络更强大的建模能力。\nGRU通过引入两个门控机制，即更新门（Update Gate）和重置门（Reset Gate），来控制信息的流动。这些门控机制使得GRU能够学习长期依赖关系，并且在处理长序列时能够更好地捕捉到序列中的重要信息。\n在nn.GRU模块中，可以通过设置参数来定义GRU的输入维度、隐藏状态维度、层数等。以下是nn.GRU的一些常用参数：\ninput_size：输入的特征维度。 hidden_size：隐藏状态的维度。 num_layers：GRU的层数。 bias：是否使用偏置。 batch_first：如果为True，则输入数据的形状应为（batch_size，sequence_length，input_size）；如果为False，则输入数据的形状应为（sequence_length，batch_size，input_size）。 dropout：dropout比例，用于控制输入数据的随机丢弃比例。 bidirectional：是否使用双向GRU。 除了上述参数之外，nn.GRU还提供了其他一些方法和功能，如forward方法用于前向传播计算，reset_parameters方法用于重置模型的参数等。 案例\n# gru没有细胞状态\rgruModel=nn.GRU(input_size,3,1)\r#其中，output是RNN每个时间步的输出，hidden是最后一个时间步的隐藏状态。\routput, h =gruModel(input)\rprint(\"GRU隐藏层输出的维度\",output.shape)\rprint(\"GRU隐藏层最后一个时间步输出的维度\",h.shape) 输出\nGRU隐藏层输出的维度 torch.Size([3, 2, 3])\rGRU隐藏层最后一个时间步输出的维度 torch.Size([1, 2, 3]) models checkpoints 在深度学习中，checkpoints是训练期间保存模型参数的文件。它们是在每个训练周期或某个特定时间间隔保存的，以便在训练过程中出现问题时可以恢复训练。通过保存checkpoints，我们可以在训练过程中随时停止并重新开始，而无需从头开始训练。\n“.pt\"是PyTorch中用于保存模型参数的文件扩展名。当我们训练一个模型时，我们可以将模型的参数保存在.pt文件中，以便以后在其他地方使用或加载到其他模型中。这些文件包含了模型在训练期间学到的权重和偏差等参数。在PyTorch中，我们可以使用torch.save()函数将模型参数保存为.pt文件，并使用torch.load()函数加载.pt文件中的参数。\nimport torch\rimport torch.nn as nn\r#模型的保存和加载\rmodel=nn.Linear(3,1)\r#修改权重和偏置后保存模型\rnew_weight = torch.tensor([[1.0, 2.0, 3.0]])\rnew_bias = torch.tensor([4.0])\rmodel.weight = nn.Parameter(new_weight)\rmodel.bias = nn.Parameter(new_bias)\rtorch.save(model.state_dict(),\"./model.pt\")\rnewModel=nn.Linear(3,1)\rprint(\"默认参数\",newModel.weight,newModel.bias)\rnewModel.load_state_dict(torch.load(\"./model.pt\"))\rprint(\"加载后\",newModel.weight,newModel.bias) 输出\n默认参数 Parameter containing:\rtensor([[-0.4357, -0.0781, 0.0136]], requires_grad=True) Parameter containing:\rtensor([-0.2013], requires_grad=True)\r加载后 Parameter containing:\rtensor([[1., 2., 3.]], requires_grad=True) Parameter containing:\rtensor([4.], requires_grad=True) 内置models 在PyTorch的torchvision.models模块中，提供了一些已经实现好的经典神经网络模型，包括：\nAlexNet VGG ResNet SqueezeNet DenseNet Inception GoogLeNet MobileNet ShuffleNet ResNeXt Wide ResNet MNASNet 这些模型可以通过torchvision.models模块的函数进行实例化，以便在自己的项目中使用。每个模型都有预训练的权重，也可以在自定义数据集上进行微调。你可以根据自己的需求选择适合的模型进行使用。\n以下使用models.resnet18分类datasets.CIFAR10\n#%%\rimport torch\rimport torch.nn as nn\rimport torch.optim as optim\rimport torchvision.transforms as transforms\rimport torchvision.datasets as datasets\rimport torchvision.models as models\r# 定义数据预处理\rtransform = transforms.Compose([\rtransforms.RandomCrop(32, padding=4),\rtransforms.RandomHorizontalFlip(),\rtransforms.ToTensor(),\rtransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\r])\r# 加载CIFAR-10数据集\rtrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\rtest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\r# 定义模型\r#参数pretrained表示是否加载预训练的权重。如果pretrained为True，那么模型将加载在ImageNet数据集上预训练的权重。\r# 这些预训练的权重可以提供更好的初始权重，有助于模型在其他任务上进行迁移学习。如果pretrained为False，\r# 则使用随机初始化的权重进行训练。\rmodel = models.resnet18(pretrained=False)\rnum_classes = 10\rmodel.fc = nn.Linear(512, num_classes)\r# 定义损失函数和优化器\rcriterion = nn.CrossEntropyLoss()\roptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\r# 训练模型\rbatch_size = 64\rtrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\rtest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\rnum_epochs = 10\rdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\rmodel.to(device)\rfor epoch in range(num_epochs):\rmodel.train()\repoch_loss = 0.0\rfor images, labels in train_loader:\rimages = images.to(device)\rlabels = labels.to(device)\r# 前向传播和计算损失\routputs = model(images)\rloss = criterion(outputs, labels)\r# 反向传播和优化\roptimizer.zero_grad()\rloss.backward()\roptimizer.step()\repoch_loss += loss.item()\rprint(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\r# 在测试集上评估模型\rmodel.eval()\rwith torch.no_grad():\rcorrect = 0\rtotal = 0\rfor images, labels in test_loader:\rimages = images.to(device)\rlabels = labels.to(device)\routputs = model(images)\r_, predicted = torch.max(outputs.data, 1)\rtotal += labels.size(0)\rcorrect += (predicted == labels).sum().item()\raccuracy = 100 * correct / total\rprint(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy}%')\rtorch.cuda.empty_cache() torch.hub torch.hub是PyTorch中一个用于加载预训练模型的工具。它提供了一个简单的接口，可以方便地从互联网上获取训练好的模型并加载到您的代码中使用。通过使用torch.hub，您可以轻松地使用各种预训练模型，如图像分类、目标检测、语义分割等模型。\ntorch.hub的使用非常简单，您只需要提供模型的命名空间和模型名称，它将自动下载并加载预训练模型。例如，要加载一个名为\"pytorch/vision\"的模型，您可以使用以下代码：\nimport torch\rmodel = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\n上述代码将下载并加载名为\"resnet50\"的预训练模型，并将其存储在model变量中。您可以使用model变量进行推理、特征提取等操作。\ntorch.hub还支持本地模型缓存，这意味着当您多次运行相同的代码时，它将自动从本地缓存中加载模型，而不是重新下载。这样可以提高代码的运行效率。\n总之，torch.hub是一个非常方便的工具，使您能够轻松地使用各种预训练模型，并将它们集成到您的代码中，从而加速您的深度学习项目开发。 官网模型搜索地址：https://pytorch.org/hub/research-models 以下是最火的6个model yolov5目标检测 实战使用yolov5目标检测,参考官方模型文档：https://pytorch.org/hub/ultralytics_yolov5/ 注意 Python\u003e=3.8 PyTorch\u003e=1.7 安装ultralytics\npip install -U ultralytics 编写程序\n#%%\rimport torch\r# Model，加载模型中的参数\rmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\r# Images\rimgs = ['https://ultralytics.com/images/zidane.jpg'] # batch of images\r# Inference\rresults = model(imgs)\r# Results\rresults.print()\rresults.save() # or .show()\rresults.xyxy[0] # img1 predictions (tensor) 会在当前运行的目录上生成一个runs/detect/exp/zidane.jpg 生成动漫图像 github项目地址：https://github.com/bryandlee/animegan2-pytorch 可以将人物图像转换为动漫效果。\nfrom PIL import Image\rimport torch\rfrom matplotlib import pyplot\rmodel = torch.hub.load(\"bryandlee/animegan2-pytorch:main\", \"generator\",pretrained=\"celeba_distill\").eval()\rface2paint = torch.hub.load(\"bryandlee/animegan2-pytorch:main\", \"face2paint\", size=512)\rimage=Image.open(\"./images/lyf.png\")\rout = face2paint(model, image)\rpyplot.imshow(out)\rpyplot.show() 原始图像 转换后 可视化监控 在 TensorFlow 中，最常使用的可视化工具是Tensorboard ,TensorboardX 工具使得 PyTorch 也享受到 Tensorboard 的便捷功能。 pytorch1.8之后已经包含了tensorboardx工具，在torch.utils.tensorboard包中。 FaceBook 也为 PyTorch 开发了一款交互式可视化工具 Visdom，它可以对实时数据进行丰富的可视化，帮助实时监控实验过程。\ntensorboard Tensorboard 是 TensorFlow 的一个附加工具，用于记录训练过程的模型的参数、评价指标与图像等细节内容，并通过 Web 页面提供查看细节与过程的功能，用浏览器可视化的形式展现，帮助我们在实验时观察神经网络的训练过程，把握训练趋势。既然 Tensorboard 工具这么方便，TensorFlow 外的其它深度学习框架自然也想获取 Tensorboard 的便捷功能，于是，TensorboardX 应运而生。 先安装Tensorboard\npip install tensorboard 我这里tensorboard要求的setuptools版本较低，在使用过程中报错\nAttributeError: module 'distutils' has no attribute 'version' 降级版本即可\npip uninstall setuptools\rmicromamba install setuptools==59.5.0\r或者用pip install setuptools==59.5.0 工具使用规范 1、创建SummaryWriter 的实例：\nfrom torch.utils.tensorboard import SummaryWriter\r# 创建一个SummaryWriter的实例\rwriter = SummaryWriter(log_dir=None) 其中的 log_dir 表示保存日志的路径，默认会保存在“runs/ 当前时间 _ 主机名”文件夹中。\nadd_scalar 2、add_scalar方法，这个方法用来记录数字常量（比如损失函数值），它的定义如下：\nadd_scalar(tag, scalar_value, global_step=None, walltime=None) tag：字符串类型，表示对应要监控的数据名称，是任意自定义的，不同名称的数据会使用不同曲线展示； scalar_value：浮点型，表示要监控及保存的数值； global_step：整型，表示训练的 step 数，作为横坐标； walltime：浮点型，表示记录发生的时间，默认为 time.time()。 一般会使用add_scalar方法来记录训练过程的 loss、accuracy、learning rate 等数值的变化，这样就能直观地监控训练过程。每监控一个指标，就需要使用一个add_scalar方法。（如果要看x个指标就使用x次add_scalar方法） 3、add_image方法用来记录单个图像数据（需要 Pillow 库的支持），它的定义如下 add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW') tag、global_step 和 walltime 的含义跟add_scalar方法里一样 img_tensor：PyTorch 的 Tensor 类型或 NumPy 的 array 类型，表示图像数据； dataformats：字符串类型，表示图像数据的格式，默认为“CHW”，即 Channel x Height x Width，还可以是“CHW”、“HWC”或“HW”等。 这里演示一个线性回归的例子 ，演示将epoch次数作为x，损失作为y值的的scalar图 #%%\rimport os\rimport shutil\rdef delete_directory_contents(directory):\rfor filename in os.listdir(directory): # 遍历目录下的所有文件和子目录\rfile_path = os.path.join(directory, filename) # 构建文件或子目录的完整路径\rif os.path.isfile(file_path): # 如果是文件，则直接删除\ros.remove(file_path)\relif os.path.isdir(file_path): # 如果是子目录，则递归调用删除子目录中的内容\rshutil.rmtree(file_path)\r#删除runs目录下的所有文件和目录 delete_directory_contents(\"./runs/\") import torch as t\rimport matplotlib.pyplot as plot\rimport torch.nn as nn\rfrom torch.utils.tensorboard import SummaryWriter\r#########例子演示梯度下降损失（每个epoch的损失）\r#其中的 log_dir 表示保存日志的路径，默认会保存在“runs/ 当前时间 _ 主机名”文件夹中。\rwriter=SummaryWriter(log_dir=None)\rt.manual_seed(42)\r# 使用自动梯度实现线性回归\rx=t.randn(100,1)\rx_test=t.randn(20,1)\ry=3*x+2+t.randn(100,1) #实际值上加上一些随机噪点\ry_test=3*x+2+t.randn(100,1)\rclass LinearModel(nn.Module):\rdef __init__(self):\rnn.Module.__init__(self)\rself.w=nn.Parameter(t.randn(1,1))\rself.b=nn.Parameter(t.randn(1))\rdef forward(self,x):\rreturn t.mm(x,self.w)+self.b\rmodel=LinearModel()\rlossf=nn.MSELoss()\r#定义优化器,第一个参数为模型的参数，参数传入后,自动获取他的梯度并且-梯度*学习率\roptim=t.optim.SGD(model.parameters(),lr=0.01)\r#训练100次，100次梯度下降，计算到最小损失时的w和b\repochCount=100\rfor epoch in range(epochCount):\ry_pre=model(x)\r#注意梯度清零，否则会累加\roptim.zero_grad() loss=lossf(y_pre,y)\rwriter.add_scalar(\"Loss/train\",loss,epoch)\rloss.backward()\r#更新参数w和b\roptim.step()\rplot.plot(x,y,'.')\rplot.plot(x.data.numpy(),y_pre.data.numpy())\rplot.show()\rwriter.close() 运行后在runs目录下生成了日志，切换到当前安装tensorboard的环境执行命令：tensorboard –logdir=runs，\ntensorboard 是热加载的，上面的代码比如调整epoch次数，重新运行，是实时刷新的。\n(env380) D:\\code\\deeplearn\\learn_rnn\\pytorch\\4.nn模块\u003etensorboard --logdir=runs\rTensorFlow installation not found - running with reduced feature set.\rServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\rTensorBoard 2.13.0 at http://localhost:6006/ (Press CTRL+C to quit) 访问：http://localhost:6006/ 可以看到epoch到达80左右基本损失就很小了 我们把代码的epochCount调整到20 可以看到损失梯度下降还没有达到平缓，在看下拟合的图形 再把epochCount调整到10000 可以看到在100左右基本就平缓了，后面的训练是多余的了，所以我们可以观察到epoch到100是最合适的\nadd_histogram 使用 add_histogram 方法来记录一组数据的直方图。\nadd_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None) 参数\ntag (string): 数据名称 values (torch.Tensor, numpy.array, or string/blobname): 用来构建直方图的数据 global_step (int, optional): 训练的 step bins (string, optional): 取值有 ‘tensorflow’、‘auto’、‘fd’ 等, 该参数决定了分桶的方式，详见这里。 walltime (float, optional): 记录发生的时间，默认为 time.time() max_bins (int, optional): 最大分桶数 我们可以通过观察数据、训练参数、特征的直方图，了解到它们大致的分布情况，辅助神经网络的训练过程。 我们来观察下假设10次产生均值是0方差是1的1000条数据，每一次的波动 import numpy as np\rfrom torch.utils.tensorboard import SummaryWriter\rwriter = SummaryWriter()\rflag = 1\rif flag :\rfor x in range(10):\rdata_1 = np.arange(1000)\rdata_2 = np.random.normal(size=1000)\r#直方图的结构是y轴是第多少次，x轴显示value的波动\rwriter.add_histogram(\"data1\",data_1,x)\rwriter.add_histogram('data2',data_2,x)\rwriter.close() 右侧的坐标表示循环的次数，下方的坐标表示这1000个数的分布情况\n运行图 (graph) 使用 add_graph 方法来可视化一个神经网络。\nadd_graph(model, input_to_model=None, verbose=False, **kwargs) 参数\nmodel (torch.nn.Module): 待可视化的网络模型 input_to_model (torch.Tensor or list of torch.Tensor, optional): 待输入神经网络的变量或一组变量 在add_scalar线性回归的代码中我们打印线性模型输入x的计算图 model=LinearModel()\r#加入代码\rwriter.add_graph(model,model.w) 图片add_image 使用 add_image 方法来记录单个图像数据。注意，该方法需要 pillow 库的支持。\nadd_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW') 参数\ntag (string): 数据名称 img_tensor (torch.Tensor / numpy.array): 图像数据 global_step (int, optional): 训练的 step walltime (float, optional): 记录发生的时间，默认为 time.time() dataformats (string, optional): 图像数据的格式，默认为 'CHW'，即 Channel x Height x Width，还可以是 'CHW'、'HWC' 或 'HW' 等 我们一般会使用 add_image 来实时观察生成式模型的生成效果，或者可视化分割、目标检测的结果，帮助调试模型。\nVisdom 后续补",
    "description": "概述 PyTorch是一个基于Python的开源机器学习框架，由Facebook的人工智能研究团队开发并维护。它提供了丰富的工具和接口，用于构建和训练深度神经网络模型。\nPyTorch的主要特点和优势包括：\n动态图：PyTorch使用动态图机制，即在运行时构建计算图。这使得模型的构建和调试更加直观和灵活，能够更好地处理复杂的计算流程和动态控制流。\n简洁明了：PyTorch的API设计简洁明了，易于学习和使用。它提供了一系列高级接口，使得模型的构建、训练和评估变得更加简单和高效。\n强大的GPU加速支持：PyTorch能够利用GPU进行张量运算和模型训练，从而加快计算速度。它提供了简单易用的接口，使得在GPU上进行加速变得更加方便。\n灵活扩展：PyTorch支持自定义操作符和扩展，使得用户可以方便地实现和使用自己的模型组件和功能。\n相比之下，TensorFlow是由Google开发的另一个流行的深度学习框架。与PyTorch相比，TensorFlow的主要特点和优势包括：\n静态图：TensorFlow使用静态图机制，即在编译时构建计算图。这使得TensorFlow在模型运行时能够进行更多的优化和性能提升，适用于大规模的、计算密集型的任务。\n跨平台支持：TensorFlow可以在多种硬件和操作系统上运行，并且具有广泛的部署支持。它提供了TensorFlow Serving、TensorFlow Lite和TensorFlow.js等工具，使得模型的部署和移植更加方便。\n分布式训练支持：TensorFlow提供了分布式训练的功能，可以在多个设备和计算节点上进行模型训练，从而加快训练速度。\n生态系统和社区：TensorFlow具有庞大的生态系统和活跃的社区，提供了丰富的资源和支持，包括模型库、教程和论坛等。\n总的来说，PyTorch和TensorFlow都是优秀的深度学习框架，各有其特点和适用场景。PyTorch适合于快速原型开发、动态计算流程和小规模任务，而TensorFlow适合于大规模、计算密集型的任务和分布式训练。选择哪个框架取决于具体的需求和个人偏好。\n对于初学接触神经网络，建议先学pytorch，它提供的api接近理论概念，有动态图，方便调试，适合做研究使用，，由于最近chargpt的大火，Hugging Face的transforms是使用PyTorch的。Hugging Face是一个提供自然语言处理（NLP）模型和工具的平台，他们的Transformers库主要基于PyTorch实现，他的入门pytorch必须要有基础。这个库提供了一系列用于数据预处理和后处理的函数，可以方便地对文本数据进行转换和处理。\n环境准备 安装cuda和cudnn 一般pc电脑或者服务器都有nvida显卡，可以通过nvidia-smi命令查看。 其中python环境（3.8+版本），cuda和cudnn安装请参考：https://blog.csdn.net/liaomin416100569/article/details/130532993 安装后可以看到我的cuda version是11.2\n安装pytorch 考虑到版本向下兼容，不一定非要下载cuda=11.2对应的那个版本的torch，或许低于这个版本就可以。所以我就选择下载cuda11.1的版本。 以下是pytorch对应的稳定版的网址下载链接，可以根据需要找到对应的torch版本下载。cu版本就是gpu版本，不带cu的是cpu版本，https://download.pytorch.org/whl/torch_stable.html，搜索cu111 直接选择\npip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html 编写测试代码\nimport torch\rprint(torch.__version__)\r#cuda是否可用，如果返回True，表示正常可用gpu\rprint(torch.cuda.is_available())\rprint(torch.cuda.device_count())\rx1=torch.rand(5,3)\r#把x1转换gpu0的tensor\rx1=x1.cuda(0)\rprint(x1) 测试运行",
    "tags": [],
    "title": "深度学习06-pytorch从入门到精通",
    "uri": "/docs/programming/ai/deep_learning/frameworks/dl_06_pytorch/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客 \u003e 编程开发 \u003e 人工智能 \u003e 深度学习 \u003e 生成对抗网络",
    "content": "@[toc]\n概述 GAN（Generative Adversarial Network）是一种生成模型，由两个神经网络组成：生成器（Generator）和判别器（Discriminator）。GAN的基本思想是通过让生成器和判别器相互对抗来学习生成真实样本的能力。\n生成器的作用是将一个随机噪声向量作为输入，通过一系列的神经网络层逐渐将其转化为一个与真实样本相似的输出。生成器的目标是尽量使生成的样本被判别器误认为是真实样本，从而欺骗判别器。生成器的训练目标是最小化生成样本与真实样本之间的差异。\n判别器的作用是将输入的样本区分为真实样本和生成样本。判别器的目标是尽量准确地判断样本的真伪。判别器的训练目标是最大化判别真实样本和生成样本的能力。\nGAN的训练过程可以简述为以下几个步骤：\n初始化生成器和判别器的参数。 从真实样本中随机选择一批样本，作为判别器的训练集。同时，生成一批随机噪声向量，作为生成器的输入。 使用生成器生成一批样本，并将其与真实样本混合，构成判别器的训练集。 使用判别器对训练集中的样本进行判别，并计算生成样本与真实样本的损失。 更新生成器和判别器的参数，使生成样本的质量逐渐提高，同时判别器的判别能力也逐渐增强。 重复步骤2-5，直到生成器能够生成与真实样本相似的样本。 DCGAN（Deep Convolutional GAN）是GAN的一种改进版本，主要通过引入卷积神经网络（CNN）来提高生成器和判别器的性能。DCGAN在生成器和判别器中使用了卷积层和反卷积层，使其能够处理图像数据。相较于传统的GAN，DCGAN在生成图像的细节和纹理上有更好的表现。\n总的来说，GAN是一种通过生成器和判别器相互对抗来学习生成真实样本的生成模型，而DCGAN是在GAN的基础上引入了卷积神经网络，提高了对图像数据的处理能力。\n原理简介 GAN的开山之作是被称为“GAN之父”的Ian Goodfellow发表于2014年的经典论文Generative Adversarial Networks[2]，在这篇论文中他提出了生成对抗网络，并设计了第一个GAN实验——手写数字生成。\nGAN的产生来自于一个灵机一动的想法：\n“What I cannot create，I do not understand.”（那些我所不能创造的，我也没有真正地理解它。） —Richard Feynman\n类似地，如果深度学习不能创造图片，那么它也没有真正地理解图片。当时深度学习已经开始在各类计算机视觉领域中攻城略地，在几乎所有任务中都取得了突破。但是人们一直对神经网络的黑盒模型表示质疑，于是越来越多的人从可视化的角度探索卷积网络所学习的特征和特征间的组合，而GAN则从生成学习角度展示了神经网络的强大能力。GAN解决了非监督学习中的著名问题：给定一批样本，训练一个系统能够生成类似的新样本。\n生成对抗网络的网络结构如图7-2所示，主要包含以下两个子网络。 • 生成器（generator）：输入一个随机噪声，生成一张图片。 • 判别器（discriminator）：判断输入的图片是真图片还是假图片。 训练判别器时，需要利用生成器生成的假图片和来自真实世界的真图片；训练生成器时，只用噪声生成假图片。判别器用来评估生成的假图片的质量，促使生成器相应地调整参数。\n生成器的目标是尽可能地生成以假乱真的图片，让判别器以为这是真的图片；判别器的目标是将生成器生成的图片和真实世界的图片区分开。可以看出这二者的目标相反，在训练过程中互相对抗，这也是它被称为生成对抗网络的原因。 上面的描述可能有点抽象，让我们用收藏齐白石作品（齐白石作品如图7-3所示）的书画收藏家和假画贩子的例子来说明。假画贩子相当于是生成器，他们希望能够模仿大师真迹伪造出以假乱真的假画，骗过收藏家，从而卖出高价；书画收藏家则希望将赝品和真迹区分开，让真迹流传于世，销毁赝品。这里假画贩子和收藏家所交易的画，主要是齐白石画的虾。齐白石画虾可以说是画坛一绝，历来为世人所追捧。 在这个例子中，一开始假画贩子和书画收藏家都是新手，他们对真迹和赝品的概念都很模糊。假画贩子仿造出来的假画几乎都是随机涂鸦，而书画收藏家的鉴定能力很差，有不少赝品被他当成真迹，也有许多真迹被当成赝品。\n首先，书画收藏家收集了一大堆市面上的赝品和齐白石大师的真迹，仔细研究对比，初步学习了画中虾的结构，明白画中的生物形状弯曲，并且有一对类似钳子的“螯足”，对于不符合这个条件的假画全部过滤掉。当收藏家用这个标准到市场上进行鉴定时，假画基本无法骗过收藏家，假画贩子损失惨重。但是假画贩子自己仿造的赝品中，还是有一些蒙骗过关，这些蒙骗过关的赝品中都有弯曲的形状，并且有一对类似钳子的“螯足”。于是假画贩子开始修改仿造的手法，在仿造的作品中加入弯曲的形状和一对类似钳子的“螯足”。除了这些特点，其他地方例如颜色、线条都是随机画的。假画贩子制造出的第一版赝品如所示。 当假画贩子把这些画拿到市面上去卖时，很容易就骗过了收藏家，因为画中有一只弯曲的生物，生物前面有一对类似钳子的东西，符合收藏家认定的真迹的标准，所以收藏家就把它当成真迹买回来。随着时间的推移，收藏家买回越来越多的假画，损失惨重，于是他又闭门研究赝品和真迹之间的区别，经过反复比较对比，他发现齐白石画虾的真迹中除了有弯曲的形状，虾的触须蔓长，通身作半透明状，并且画的虾的细节十分丰富，虾的每一节之间均呈白色状。\n收藏家学成之后，重新出山，而假画贩子的仿造技法没有提升，所制造出来的赝品被收藏家轻松识破。于是假画贩子也开始尝试不同的画虾手法，大多都是徒劳无功，不过在众多尝试之中，还是有一些赝品骗过了收藏家的眼睛。假画贩子发现这些仿制的赝品触须蔓长，通身作半透明状，并且画的虾的细节十分丰富，如图7-5所示。于是假画贩子开始大量仿造这种画，并拿到市面上销售，许多都成功地骗过了收藏家。 收藏家再度损失惨重，被迫关门研究齐白石的真迹和赝品之间的区别，学习齐白石真迹的特点，提升自己的鉴定能力。就这样，通过收藏家和假画贩子之间的博弈，收藏家从零开始慢慢提升了自己对真迹和赝品的鉴别能力，而假画贩子也不断地提高自己仿造齐白石真迹的水平。收藏家利用假画贩子提供的赝品，作为和真迹的对比，对齐白石画虾真迹有了更好的鉴赏能力；而假画贩子也不断尝试，提升仿造水平，提升仿造假画的质量，即使最后制造出来的仍属于赝品，但是和真迹相比也很接近了。收藏家和假画贩子二者之间互相博弈对抗，同时又不断促使着对方学习进步，达到共同提升的目的。\n在这个例子中，假画贩子相当于一个生成器，收藏家相当于一个判别器。一开始生成器和判别器的水平都很差，因为二者都是随机初始化的。训练过程分为两步交替进行，第一步是训练判别器（只修改判别器的参数，固定生成器），目标是把真迹和赝品区分开；第二步是训练生成器（只修改生成器的参数，固定判别器），为的是生成的假画能够被判别器判别为真迹（被收藏家认为是真迹）。这两步交替进行，进而分类器和判别器都达到了一个很高的水平。训练到最后，生成器生成的虾的图片（如图7-6所示）和齐白石的真迹几乎没有差别。 下面我们来思考网络结构的设计。判别器的目标是判断输入的图片是真迹还是赝品，所以可以看成是一个二分类网络，我们可以设计一个简单的卷积网络。生成器的目标是从噪声中生成一张彩色图片，这里我们采用广泛使用的DCGAN（Deep Convolutional Generative Adversarial Networks）结构，即采用全卷积网络，其结构如下图所示。网络的输入是一个100维的噪声，输出是一个3×64×64的图片。这里的输入可以看成是一个100×1×1的图片，通过上卷积慢慢增大为4×4、8×8、16×16、32×32和64×64。上卷积，或称转置卷积，是一种特殊的卷积操作，类似于卷积操作的逆运算。当卷积的stride为2时，输出相比输入会下采样到一半的尺寸；而当上卷积的stride为2时，输出会上采样到输入的两倍尺寸。这种上采样的做法可以理解为图片的信息保存于100个向量之中，神经网络根据这100个向量描述的信息，前几步的上采样先勾勒出轮廓、色调等基础信息，后几步上采样慢慢完善细节。网络越深，细节越详细。 该章节引用自书籍《深度学习框架pytorch，入门到实践》\n专业术语 上采样(Upsample)在应用在计算机视觉的深度学习领域，由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算(e.g.:图像的语义分割)，这个采用扩大图像尺寸，实现图像由小分辨率到大分辨率的映射的操作，叫做上采样(Upsample)。 矩阵零填充（Zero Padding）是指向矩阵的边界添加零值的过程。在计算机视觉和深度学习中，矩阵零填充常用于图像处理和卷积神经网络（CNN）中。 在图像处理中，矩阵零填充可以用于扩展图像的尺寸，以便在进行卷积运算时保持图像的大小不变。通过在图像周围添加零值，可以确保卷积核可以完全覆盖图像的边缘像素。这样做可以避免在卷积操作中丢失图像边缘的信息。 在卷积神经网络（CNN）中，矩阵零填充常用于调整卷积层的输入尺寸和输出尺寸。通过在输入矩阵的边界上添加零值，可以确保卷积操作产生的特征图的尺寸与输入矩阵的尺寸保持一致。这对于构建深度神经网络和处理不同尺寸的输入数据非常重要。 矩阵零填充的大小通常由填充的行数和列数决定。在CNN中，填充的大小往往与卷积核的大小和步幅相关。通过合理选择填充大小，可以在保持输入输出尺寸一致的同时，控制特征图的尺寸和感受野的大小。 反卷积(Transposed Convolution)上采样有3种常见的方法：双线性插值(bilinear)，反卷积(Transposed Convolution)，反池化(Unpooling)，我们这里只讨论反卷积。这里指的反卷积，也叫转置卷积，它并不是正向卷积的完全逆过程，用一句话来解释：反卷积是一种特殊的正向卷积，先按照一定的比例通过补 000 来扩大输入图像的尺寸，接着旋转卷积核，再进行正向卷积。 零填充 对于图像处理中的一些过程，我需要对读取的numpy矩阵进行size的扩充，比如原本是（4，6）的矩阵，现在需要上下左右各扩充3行，且为了不影响数值计算，都用0填充。 比如下图，我有一个4x5大小的全1矩阵，但是现在我要在四周都加上3行的0来扩充大小，最后扩充完还要对原区域进行操作。\n如果原始矩阵的形状为 (m, n)，并且在每个边缘上填充了 p 行和 q 列的值，那么填充后的矩阵的形状将是 (m + 2p, n + 2q) 如果原始矩阵的形状为 (m, n)，并且在每个元素之间插入一个零，那么新的矩阵是（m+m-1,n,n-1）=(2m-1,2n-1) numpy已经封装了一个函数，就是pad\n#%%\rimport numpy as np\roneArry=np.ones((4,5))\rprint(oneArry)\rprint(\"周围\",np.pad(oneArry,3)) #等价于print(np.pad(oneArry,(3,3)))\r#注意元组0是左上角补充3行 ，元组1表示右小角\rprint(\"左上角\",np.pad(oneArry,(3,0)))\rprint(\"右下角\",np.pad(oneArry,(0,3))) 输出\n[[1. 1. 1. 1. 1.]\r[1. 1. 1. 1. 1.]\r[1. 1. 1. 1. 1.]\r[1. 1. 1. 1. 1.]]\r周围 [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\r[0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\r[0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\r[0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\r左上角 [[0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 1. 1. 1. 1. 1.]\r[0. 0. 0. 1. 1. 1. 1. 1.]\r[0. 0. 0. 1. 1. 1. 1. 1.]\r[0. 0. 0. 1. 1. 1. 1. 1.]]\r右下角 [[1. 1. 1. 1. 1. 0. 0. 0.]\r[1. 1. 1. 1. 1. 0. 0. 0.]\r[1. 1. 1. 1. 1. 0. 0. 0.]\r[1. 1. 1. 1. 1. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0.]\r[0. 0. 0. 0. 0. 0. 0. 0.]] 元素内填充指在元素的内部上下左右填充， 比如\n[[1, 1],\r[1, 1]] 填充为：\n[[1, 0, 1], [0, 0, 0],\r[1, 0, 1]] 代码实现\nimport numpy as np\rmatrix = [[1, 1],\r[1, 1]]\rzero_inserted_matrix = np.zeros((2*len(matrix)-1, 2*len(matrix[0])-1))\rfor i in range(len(matrix)):\rfor j in range(len(matrix[0])):\rzero_inserted_matrix[2*i][2*j] = matrix[i][j]\rprint(zero_inserted_matrix) 输出\n[[1. 0. 1.]\r[0. 0. 0.]\r[1. 0. 1.]] 转置卷积 参考：https://www.zhihu.com/question/48279880 ​ 转置卷积或微步幅卷积。但是，需要指出去卷积这个名称并不是很合适，因为转置卷积并非信号/图像处理领域定义的那种真正的去卷积。从技术上讲，信号处理中的去卷积是卷积运算的逆运算。但这里却不是这种运算。后面我们会介绍为什么将这种运算称为转置卷积更自然且更合适。\n​ 我们可以使用常见卷积实现转置卷积。这里我们用一个简单的例子来说明，输入层为2∗2(下面蓝色的部分)，先进行填充值Padding为2∗2单位步长的零填充（下面在蓝色上下左右填充2行2列），再使用步长Stride为1的3∗3卷积核进行卷积操作（卷积一次获得一个值）则实现了上采样，上采样输出的大小为4∗4 也就是（6-3+1，6-3+1）\n值得一提的是，可以通过各种填充和步长，我们可以将同样的2∗2输入映射到不同的图像尺寸。下图，转置卷积被应用在同一张2∗2的输入上（输入之间插入了一个零，并且周围加了2∗2的单位步长的零填充）上应用3∗3的卷积核，得到的结果（即上采样结果）大小为5∗5 通过观察上述例子中的转置卷积能够帮助我们构建起一些直观的认识。但为了进一步应用转置卷积，我们还需要了解计算机的矩阵乘法是如何实现的。从实现过程的角度我们可以理解为何转置卷积才是最合适的名称。\n​ 在卷积中，我们这样定义：用C代表卷积核，input为输入图像，output为输出图像。经过卷积（矩阵乘法）后，我们将input从大图像下采样为小图像output。这种矩阵乘法实现遵循C∗input=output。\n​ 下面的例子展示了这种运算在计算机内的工作方式。它将输入平展（16∗1）矩阵，并将卷积核转换为一个稀疏矩阵、（4∗16）。然后，在稀疏矩阵和平展的输入之间使用矩阵乘法。之后，再将所得到的矩阵（4∗1)转为2∗2输出。 此时，若用卷积核对应稀疏矩阵的转置$C^T$（16∗4）乘以输出的平展（4∗1）所得到的结果（16∗1）的形状和输入的形状（16∗1）相同。\n但值得注意的是，上述两次操作并不是可逆关系，对于同一个卷积核（因非其稀疏矩阵不是正交矩阵，结果转置操作之后并不能恢复到原始的数值，而仅仅保留原始的形状，所以转置卷积的名字由此而来。并回答了上面提到的疑问，相比于逆卷积而言转置卷积更加准确。\n生成动漫图像 使用DCGAN训练一个模型用于生成64*64动漫图像,并通过这个例子规划神经网络的目录结构组织，大部分的开源项目的目录结构相似，以后分析开源model更加容易。\n算力选择 由于gan训练需要的资源较大，时间较长，建议使用gpu服务器 gpt云平台上提供的GPU型号很多。我们按照GPU架构大致分为五类（推荐autodl或者inscode，可以按时计费，用完就释放）：\nNVIDIA Pascal架构的GPU，如TitanXp，GTX 10系列等。 这类GPU缺乏低精度的硬件加速能力，但却具备中等的单精度算力。由于价格便宜，适合用来练习训练小模型(如Cifar10)或调试模型代码。 NVIDIA Volta/Turing架构的GPU，如GTX 20系列, Tesla V100等。 这类GPU搭载专为低精度(int8/float16)计算加速的TensorCore, 但单精度算力相较于上代提升不大。我们建议在实例上启用深度学习框架的混合精度训练来加速模型计算。 相较于单精度训练，混合精度训练通常能够提供2倍以上的训练加速。 NVIDIA Ampere架构的GPU，如GTX 30系列，Tesla A40/A100等。 这类GPU搭载第三代TensorCore。相较于前一代，支持了TensorFloat32格式，可直接加速单精度训练 (PyTorch已默认开启)。但我们仍建议使用超高算力的float16半精度训练模型，可获得比上一代GPU更显著的性能提升。 寒武纪 MLU 200系列加速卡。 暂不支持模型训练。使用该系列加速卡进行模型推理需要量化为int8进行计算。 并且需要安装适配寒武纪MLU的深度学习框架。 华为 Ascend 系列加速卡。 支持模型训练及推理。但需安装MindSpore框架进行计算。 GPU型号的选择并不困难。对于常用的深度学习模型，根据GPU对应精度的算力可大致推算GPU训练模型的性能。AutoDL平台标注并排名了每种型号GPU的算力，方便大家选择适合自己的GPU。\nGPU的数量选择与训练任务有关。一般我们认为模型的一次训练应当在24小时内完成，这样隔天就能训练改进之后的模型。以下是选择多GPU的一些建议：\n1块GPU。适合一些数据集较小的训练任务，如Pascal VOC等。 2块GPU。同单块GPU，但是你可以一次跑两组参数或者把Batchsize扩大。 4块GPU。适合一些中等数据集的训练任务，如MS COCO等。 8块GPU。经典永流传的配置！适合各种训练任务，也非常方便复现论文结果。 我要更多！用于训练大参数模型、大规模调参或超快地完成模型训练。 我常用的gpu按照性能从高到低的顺序，这些机器的GPU算力排名如下，并附上它们的基本配置信息：\nA100： CUDA核心数：6912 Tensor核心数：432 显存容量：40 GB 内存带宽：1555 GB/s 架构：Ampere V100： CUDA核心数：5120 Tensor核心数：640 显存容量：16 GB / 32 GB / 32 GB HBM2 内存带宽：900 GB/s / 1134 GB/s / 1134 GB/s 架构：Volta P100： CUDA核心数：3584 Tensor核心数：0 显存容量：16 GB / 12 GB HBM2 内存带宽：732 GB/s / 549 GB/s 架构：Pascal Tesla T4： CUDA核心数：2560 Tensor核心数：320 显存容量：16 GB 内存带宽：320 GB/s 架构：Turing RTX A4000： CUDA核心数：6144 Tensor核心数：192 显存容量：16 GB 内存带宽：448 GB/s 架构：Ampere 使用P100训练完大概1个小时（3831.2s） 数据集 kagle上：https://www.kaggle.com/code/splcher/starter-anime-face-dataset 邮箱注册个账号即可下载，数据集下有不同的用户基于数据集的训练代码和结果。 参考代码：https://www.kaggle.com/code/splcher/starter-anime-face-dataset\n目录规划 其中各个文件的主要内容和作用如下。\n• checkpoints/：用于保存训练好的模型，可使程序在异常退出后仍能重新载入模型，恢复训练。 • data/：数据相关操作，包括数据预处理、dataset实现等。 • models/：模型定义，可以有多个模型，例如上面的AlexNet和ResNet34，一个模型对应一个文件。 • utils/：可能用到的工具函数，本次实验中主要封装了可视化工具。 • config.py：配置文件，所有可配置的变量都集中在此，并提供默认值。 • main.py：主文件，训练和测试程序的入口，可通过不同的命令来指定不同的操作和参数。 • requirements.txt：程序依赖的第三方库。 • README.md：提供程序的必要说明。\n源代码 数据源加载 将下载好的AnimeFaceDataset添加到data目录，新建dataset.py用于加载数据集\nfrom torch.utils.data import Dataset,DataLoader\rfrom torchvision import datasets, transforms\rclass AtomicDataset(Dataset):\rdef __init__(self,root,image_size):\rDataset.__init__(self)\rself.dataset=datasets.ImageFolder(root,\rtransform=transforms.Compose([\rtransforms.Resize(image_size),\rtransforms.CenterCrop(image_size),\rtransforms.ToTensor(),\rtransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r]))\rdef __getitem__(self,index):\rreturn self.dataset[index]\rdef __len__(self):\rreturn len(self.dataset)\rdef toBatchLoader(self,batch_size):\rreturn DataLoader(self,batch_size=batch_size, shuffle=False) 定义配置类 config.py定义配置类\nclass Config:\r#定义转换后图像的大小\rimg_size=64\r#训练图片所在目录，目录必须是有子目录，子目录名称就是分类名\rimg_root=\"./data/AnimeFaceDataset\"\r#每次加载的批次数\rbatch_size=64\r\"\"\"\r在卷积神经网络中，这些缩写通常表示以下含义：\rnz：表示输入噪声向量的维度。全称为\"noise dimension\"，即噪声维度。\rngf：表示生成器网络中特征图的通道数。全称为\"number of generator features\"，即生成器特征图通道数。\rnc：表示输入图像的通道数。全称为\"number of image channels\"，即图像通道数。\r\"\"\"\r#表示噪声的维度，一般是(100,1,1)\rnz=100\r#表示生成特征图的维度,64*64的图片\rngf=64\r#生成或者传入图片的通道数\rnc=3\r# 表示判别器输入特征图的维度,64*64的图片\rndf = 64\r# 优化器的学习率\rlr = 0.0002\r# Beta1 hyperparam for Adam optimizers\rbeta1 = 0.5\r# epochs的次数\rnum_epochs=50\rdef __init__(self,kv):\rfor key, value in kv.items():\rsetattr(self, key, value) 由于这些配置默认是静态的，可以使用fire将参数定义到命令行，通过\npython main.py 函数名 --参数值1=值1 --参数值2=值2的方式传入到**kwargs main.py定义train方式\ndef train(**kwargs):\rprint(kwargs)\rif __name__ == \"__main__\":\r# 将main.py中所有的函数映射成 python main.py 方法名 --参数1=参数值 --参数2=参数值的形式，这些参数以keyvalue字典的形式传入kwargs\rfire.Fire() 定义模型 在models目录下新建models.py定义G和D模型\nimport torch.nn as nn\r\"\"\"\rnn.ConvTranspose2d的参数包括：\rin_channels：输入通道数\rout_channels：输出通道数\rkernel_size：卷积核大小\rstride：步长\rpadding：填充大小\routput_padding：输出填充大小\rgroups：分组卷积数量，默认为1\rbias：是否使用偏置，默认为True\r生成器的目标是从一个随机噪声向量生成逼真的图像。在生成器中，通道数从大到小可以理解为从抽象的特征逐渐转化为具体的图像细节。通过逐层转置卷积（ConvTranspose2d）操作，\r将低维度的特征逐渐转化为高维度的图像。通道数的减少可以理解为对特征进行提取和压缩，以生成更具细节和逼真度的图像。\r\"\"\"\r#生成网络\rclass Generator(nn.Module):\rdef __init__(self, nz,ngf,nc):\rsuper(Generator, self).__init__()\rself.main = nn.Sequential(\r# nz表示噪声的维度，一般是(100,1,1)\r# ngf表示生成特征图的维度\r# nc表示输入或者输出图像的维度\r#输出尺寸 = (输入尺寸（高度） - 1) * stride - 2 * padding + kernel_size + output_padding\r#如果（卷积核,步长，填充）=(4, 1, 0)表示图像的维度是卷积核的大小（卷积核高,卷积核宽）\r#如果（卷积核,步长，填充）=(4, 2, 1)表示图像的维度是是上一个图像的2被（输入图像高度*2,输入图像宽度*2）\rnn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\rnn.BatchNorm2d(ngf * 8),\rnn.ReLU(True),\r# state size. (ngf*8) x 4 x 4\rnn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\rnn.BatchNorm2d(ngf * 4),\rnn.ReLU(True),\r# state size. (ngf*4) x 8 x 8\rnn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\rnn.BatchNorm2d(ngf * 2),\rnn.ReLU(True),\r# state size. (ngf*2) x 16 x 16\rnn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\rnn.BatchNorm2d(ngf),\rnn.ReLU(True),\r# state size. (ngf) x 32 x 32\rnn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\rnn.Tanh()\r# state size. (nc) x 64 x 64\r)\rdef forward(self, input):\rreturn self.main(input)\r\"\"\"\r和转置卷积相反的是（4,2,1）会让维度2倍降低\r卷积过程是height-kerel+1\r\"\"\"\rclass Discriminator(nn.Module):\rdef __init__(self, nc,ndf):\rsuper(Discriminator, self).__init__()\rself.main = nn.Sequential(\r# input is (nc) x 64 x 64\rnn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\rnn.LeakyReLU(0.2, inplace=True),\r# state size. (ndf) x 32 x 32\rnn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\rnn.BatchNorm2d(ndf * 2),\rnn.LeakyReLU(0.2, inplace=True),\r# state size. (ndf*2) x 16 x 16\rnn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\rnn.BatchNorm2d(ndf * 4),\rnn.LeakyReLU(0.2, inplace=True),\r# state size. (ndf*4) x 8 x 8\rnn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\rnn.BatchNorm2d(ndf * 8),\rnn.LeakyReLU(0.2, inplace=True),\r# state size. (ndf*8) x 4 x 4\rnn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\rnn.Sigmoid()\r# state size （1,1,1）\r)\rdef forward(self, input):\rreturn self.main(input) 训练 训练D模型，让输出的数据和1比较计算损失，让损失最小化，G生成的数据使用D模型预测和0比较（骗不过）计算损失，让损失最小化。 训练G模型，生成的图片，使用D模型预测，和1比较（骗过D模型）损失，让损失最小化\ndef train(**kwargs):\r# 通过传入的参数初始化Config\rdefaultConfig = Config(kwargs)\r# 通过给定的目录和图像大小转换成数据集\rdataset = AtomicDataset(defaultConfig.img_root, defaultConfig.img_size)\r# 转换为可迭代的批次为defaultConfig.batch_size的数据集\rdataloader = dataset.toBatchLoader(defaultConfig.batch_size)\r# 创建生成网络模型\rnetG = Generator(defaultConfig.nz, defaultConfig.ngf, defaultConfig.nc).to(device)\r# 创建分类器模型\rnetD = Discriminator(defaultConfig.nc, defaultConfig.ndf).to(device)\r# 使用criterion = nn.BCELoss()\rcriterion = nn.BCELoss()\r# Setup Adam optimizers for both G and D\roptimizerD = optim.Adam(netD.parameters(), lr=defaultConfig.lr, betas=(defaultConfig.beta1, 0.999))\roptimizerG = optim.Adam(netG.parameters(), lr=defaultConfig.lr, betas=(defaultConfig.beta1, 0.999))\r# 如果是真的图片label=1，伪造的图片为0\rreal_label = 1\rfake_label = 0\r# Lists to keep track of progress\rimg_list = []\rG_losses = []\rD_losses = []\riters = 0\r#生成一个64批次100*1*1的噪声\rfixed_noise = torch.randn(64, defaultConfig.nz, 1, 1, device=device)\rprint(\"Starting Training Loop...\")\r# For each epoch\rfor epoch in range(defaultConfig.num_epochs):\r# For each batch in the dataloader\rfor i, data in enumerate(dataloader, 0):\r############################\r# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\r# 对于真实传入的图片进行判断器训练，label肯定是1\r# 对于噪声传入的图片进行判断器训练，label肯定是0\r###########################\r## 通过真实图片训练D网络\rnetD.zero_grad()\r# 将64批次数据转换为gpu设备\rreal_cpu = data[0].to(device)\r# 获取批次的个数\rb_size = real_cpu.size(0)\r# 生成的是一个一维的张量，其中包含64个元素,每个元素的值为1。\rlabel = torch.full((b_size,), real_label, device=device).float()\r# 分类器捲積后最后产生一个64个批次的1*1，转换成1维数组。\routput = netD(real_cpu).view(-1)\r# 计算和真实数据的损失\rerrD_real = criterion(output, label)\r# 反向传播计算梯度\rerrD_real.backward()\r# D_x的值表示判别器对真实样本的平均预测概率\rD_x = output.mean().item()\r## 通过噪声训练生成器模型\r# 生成噪声的变量 也是64批次，噪声的通道数是100\rnoise = torch.randn(b_size, defaultConfig.nz, 1, 1, device=device)\r# 传入到生成网络中，生成一张64*3*64*64的图片\rfake = netG(noise)\r# 生成器生成的图片对应的真实的label应该是0\rlabel.fill_(fake_label)\r# detach()是PyTorch中的一个函数，它用于从计算图中分离出一个Tensor。当我们调用detach()函数时，它会返回一个新的Tensor，该Tensor与原始Tensor共享相同的底层数据，但不会有梯度信息。\r# 使用判别器网络来判断通过噪声生成的图片，转换为1维\routput = netD(fake.detach()).view(-1)\r# 进行损失函数计算\rerrD_fake = criterion(output, label)\r# 反向传播计算梯度\rerrD_fake.backward()\r# 表示判别器对虚假样本的平均预测概率\rD_G_z1 = output.mean().item()\r# 将真实图片和虚假图片的损失求和获取所有的损失\rerrD = errD_real + errD_fake\r# 更新权重参数\roptimizerD.step()\r############################\r# (2) Update G network: maximize log(D(G(z)))\r# 对于G网络来说，对于虚假传入的图片进行判断器训练，尽量让判别器认为是真1，生成的图片才够真实\r###########################\rnetG.zero_grad()\rlabel.fill_(real_label) # fake labels are real for generator cost\r# 使用之前的G网络生成的图片64*3*64*64,传入D网络\routput = netD(fake).view(-1)\r# 计算G网路的损失\rerrG = criterion(output, label)\r# 反向计算梯度\rerrG.backward()\r#表示判别器对虚假样本判断为真的的平均预测概率\rD_G_z2 = output.mean().item()\r# 更新G的权重\roptimizerG.step()\r# 输出训练统计，每1000批次\rif i % 1000 == 0:\rprint('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\r% (epoch, defaultConfig.num_epochs, i, len(dataloader),\rerrD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\r# Save Losses for plotting later\rG_losses.append(errG.item())\rD_losses.append(errD.item())\r# 即每经过一定数量的迭代（iters % 250 == 0）或者是训练的最后一个epoch的最后一个batch（(epoch == defaultConfig.num_epochs - 1) and (i == len(dataloader) - 1)），\r# 就会使用G网络通过噪声生成64批次3通道64*64的图像，并且加入到img_list去做可视化，看看效果\rif (iters % 250 == 0) or ((epoch == defaultConfig.num_epochs - 1) and (i == len(dataloader) - 1)):\rwith torch.no_grad():\rfake = netG(fixed_noise).detach().cpu()\rimg_list.append(vutils.make_grid(fake, padding=2, normalize=True))\riters += 1\r#保存生成器的网络到checkpoints目录\rtorch.save(netG.state_dict(), \"./checkpoints/optimizerG.pt\") 可视化 绘制损失 #绘制G和D的损失函数图像\rplt.figure(figsize=(10, 5))\rplt.title(\"Generator and Discriminator Loss During Training\")\r#一维数组的索引值是x坐标也就是批次索引\rplt.plot(G_losses, label=\"G\")\rplt.plot(D_losses, label=\"D\")\rplt.xlabel(\"iterations\")\rplt.ylabel(\"Loss\")\rplt.legend()\rplt.show() 绘制生成器图像变化 #创建一个8*8的画布\rfig = plt.figure(figsize=(8, 8))\rplt.axis(\"off\")\rims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in img_list]\rani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\rHTML(ani.to_jshtml()) 每250次迭代就通过G生成64图片，发现越到后面图片就越清晰。 其他项目 CycleGAN 参考地址：https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\nstargan 参考地址：https://github.com/yunjey/stargan",
    "description": "@[toc]\n概述 GAN（Generative Adversarial Network）是一种生成模型，由两个神经网络组成：生成器（Generator）和判别器（Discriminator）。GAN的基本思想是通过让生成器和判别器相互对抗来学习生成真实样本的能力。\n生成器的作用是将一个随机噪声向量作为输入，通过一系列的神经网络层逐渐将其转化为一个与真实样本相似的输出。生成器的目标是尽量使生成的样本被判别器误认为是真实样本，从而欺骗判别器。生成器的训练目标是最小化生成样本与真实样本之间的差异。\n判别器的作用是将输入的样本区分为真实样本和生成样本。判别器的目标是尽量准确地判断样本的真伪。判别器的训练目标是最大化判别真实样本和生成样本的能力。\nGAN的训练过程可以简述为以下几个步骤：\n初始化生成器和判别器的参数。 从真实样本中随机选择一批样本，作为判别器的训练集。同时，生成一批随机噪声向量，作为生成器的输入。 使用生成器生成一批样本，并将其与真实样本混合，构成判别器的训练集。 使用判别器对训练集中的样本进行判别，并计算生成样本与真实样本的损失。 更新生成器和判别器的参数，使生成样本的质量逐渐提高，同时判别器的判别能力也逐渐增强。 重复步骤2-5，直到生成器能够生成与真实样本相似的样本。 DCGAN（Deep Convolutional GAN）是GAN的一种改进版本，主要通过引入卷积神经网络（CNN）来提高生成器和判别器的性能。DCGAN在生成器和判别器中使用了卷积层和反卷积层，使其能够处理图像数据。相较于传统的GAN，DCGAN在生成图像的细节和纹理上有更好的表现。\n总的来说，GAN是一种通过生成器和判别器相互对抗来学习生成真实样本的生成模型，而DCGAN是在GAN的基础上引入了卷积神经网络，提高了对图像数据的处理能力。\n原理简介 GAN的开山之作是被称为“GAN之父”的Ian Goodfellow发表于2014年的经典论文Generative Adversarial Networks[2]，在这篇论文中他提出了生成对抗网络，并设计了第一个GAN实验——手写数字生成。\nGAN的产生来自于一个灵机一动的想法：\n“What I cannot create，I do not understand.”（那些我所不能创造的，我也没有真正地理解它。） —Richard Feynman\n类似地，如果深度学习不能创造图片，那么它也没有真正地理解图片。当时深度学习已经开始在各类计算机视觉领域中攻城略地，在几乎所有任务中都取得了突破。但是人们一直对神经网络的黑盒模型表示质疑，于是越来越多的人从可视化的角度探索卷积网络所学习的特征和特征间的组合，而GAN则从生成学习角度展示了神经网络的强大能力。GAN解决了非监督学习中的著名问题：给定一批样本，训练一个系统能够生成类似的新样本。\n生成对抗网络的网络结构如图7-2所示，主要包含以下两个子网络。 • 生成器（generator）：输入一个随机噪声，生成一张图片。 • 判别器（discriminator）：判断输入的图片是真图片还是假图片。 训练判别器时，需要利用生成器生成的假图片和来自真实世界的真图片；训练生成器时，只用噪声生成假图片。判别器用来评估生成的假图片的质量，促使生成器相应地调整参数。\n生成器的目标是尽可能地生成以假乱真的图片，让判别器以为这是真的图片；判别器的目标是将生成器生成的图片和真实世界的图片区分开。可以看出这二者的目标相反，在训练过程中互相对抗，这也是它被称为生成对抗网络的原因。 上面的描述可能有点抽象，让我们用收藏齐白石作品（齐白石作品如图7-3所示）的书画收藏家和假画贩子的例子来说明。假画贩子相当于是生成器，他们希望能够模仿大师真迹伪造出以假乱真的假画，骗过收藏家，从而卖出高价；书画收藏家则希望将赝品和真迹区分开，让真迹流传于世，销毁赝品。这里假画贩子和收藏家所交易的画，主要是齐白石画的虾。齐白石画虾可以说是画坛一绝，历来为世人所追捧。 在这个例子中，一开始假画贩子和书画收藏家都是新手，他们对真迹和赝品的概念都很模糊。假画贩子仿造出来的假画几乎都是随机涂鸦，而书画收藏家的鉴定能力很差，有不少赝品被他当成真迹，也有许多真迹被当成赝品。\n首先，书画收藏家收集了一大堆市面上的赝品和齐白石大师的真迹，仔细研究对比，初步学习了画中虾的结构，明白画中的生物形状弯曲，并且有一对类似钳子的“螯足”，对于不符合这个条件的假画全部过滤掉。当收藏家用这个标准到市场上进行鉴定时，假画基本无法骗过收藏家，假画贩子损失惨重。但是假画贩子自己仿造的赝品中，还是有一些蒙骗过关，这些蒙骗过关的赝品中都有弯曲的形状，并且有一对类似钳子的“螯足”。于是假画贩子开始修改仿造的手法，在仿造的作品中加入弯曲的形状和一对类似钳子的“螯足”。除了这些特点，其他地方例如颜色、线条都是随机画的。假画贩子制造出的第一版赝品如所示。 当假画贩子把这些画拿到市面上去卖时，很容易就骗过了收藏家，因为画中有一只弯曲的生物，生物前面有一对类似钳子的东西，符合收藏家认定的真迹的标准，所以收藏家就把它当成真迹买回来。随着时间的推移，收藏家买回越来越多的假画，损失惨重，于是他又闭门研究赝品和真迹之间的区别，经过反复比较对比，他发现齐白石画虾的真迹中除了有弯曲的形状，虾的触须蔓长，通身作半透明状，并且画的虾的细节十分丰富，虾的每一节之间均呈白色状。\n收藏家学成之后，重新出山，而假画贩子的仿造技法没有提升，所制造出来的赝品被收藏家轻松识破。于是假画贩子也开始尝试不同的画虾手法，大多都是徒劳无功，不过在众多尝试之中，还是有一些赝品骗过了收藏家的眼睛。假画贩子发现这些仿制的赝品触须蔓长，通身作半透明状，并且画的虾的细节十分丰富，如图7-5所示。于是假画贩子开始大量仿造这种画，并拿到市面上销售，许多都成功地骗过了收藏家。 收藏家再度损失惨重，被迫关门研究齐白石的真迹和赝品之间的区别，学习齐白石真迹的特点，提升自己的鉴定能力。就这样，通过收藏家和假画贩子之间的博弈，收藏家从零开始慢慢提升了自己对真迹和赝品的鉴别能力，而假画贩子也不断地提高自己仿造齐白石真迹的水平。收藏家利用假画贩子提供的赝品，作为和真迹的对比，对齐白石画虾真迹有了更好的鉴赏能力；而假画贩子也不断尝试，提升仿造水平，提升仿造假画的质量，即使最后制造出来的仍属于赝品，但是和真迹相比也很接近了。收藏家和假画贩子二者之间互相博弈对抗，同时又不断促使着对方学习进步，达到共同提升的目的。",
    "tags": [],
    "title": "深度学习07-深度卷积生成对抗网络(DCGAN)",
    "uri": "/docs/programming/ai/deep_learning/gans/dl_07_gans/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/docs/categories/index.html"
  },
  {
    "breadcrumb": "liaomin416100569博客",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/docs/tags/index.html"
  }
]
