<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>卷积神经网络 :: liaomin416100569博客</title>
    <link>https://jiaozi789.github.io/docs/programming/ai/deep_learning/cnn/index.html</link>
    <description></description>
    <generator>Hugo</generator>
    <language>zh</language>
    <lastBuildDate>Thu, 18 Sep 2025 16:55:17 +0800</lastBuildDate>
    <atom:link href="https://jiaozi789.github.io/docs/programming/ai/deep_learning/cnn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>深度学习03-卷积神经网络(CNN)</title>
      <link>https://jiaozi789.github.io/docs/programming/ai/deep_learning/cnn/dl_03_cnn/index.html</link>
      <pubDate>Thu, 18 Sep 2025 16:55:17 +0800</pubDate>
      <guid>https://jiaozi789.github.io/docs/programming/ai/deep_learning/cnn/dl_03_cnn/index.html</guid>
      <description>简介 CNN，即卷积神经网络（Convolutional Neural Network），是一种常用于图像和视频处理的深度学习模型。与传统神经网络相比，CNN 有着更好的处理图像和序列数据的能力，因为它能够自动学习图像中的特征，并提取出最有用的信息。&#xA;CNN 的一个核心特点是卷积操作，它可以在图像上进行滑动窗口的计算，通过滤波器（又称卷积核）和池化层（Max Pooling）来提取出图像的特征。卷积操作可以有效地减少权重数量，降低计算量，同时也能够保留图像的空间结构信息。池化层则可以在不改变特征图维度的前提下，减少计算量，提高模型的鲁棒性。&#xA;CNN 的典型结构包括卷积层、池化层、全连接层等。同时，为了防止过拟合，CNN 还会加入一些正则化的技术，如 Dropout 和 L2 正则等。&#xA;CNN 在图像分类、目标检测、语音识别等领域都有着广泛的应用。在图像分类任务中，CNN 的经典模型包括 LeNet-5、AlexNet、VGG 和 GoogleNet/Inception 等，这些模型的设计思想和网络结构都有所不同，但都对卷积神经网络的发展做出了重要贡献。&#xA;发展历程 卷积神经网络（CNN）是一种深度学习模型，广泛应用于图像识别、计算机视觉等领域。在CNN的发展历程中，涌现出了许多经典的模型，下面简要介绍几个著名的模型。&#xA;LeNet-5 LeNet-5是Yann LeCun等人于1998年提出的，是第一个被广泛应用的卷积神经网络模型。它主要用于手写数字识别，包含卷积层、池化层和全连接层。LeNet-5的设计使得它在MNIST手写数字识别任务上获得了很好的表现。它的特点是卷积核数量较少（6和16）以及参数量较少，第一层卷积层使用了6个大小为5×5的卷积核，第二层卷积层使用了16个大小为5×5的卷积核。这种设计可以有效地减少模型的参数量，但它是卷积神经网络的开山鼻祖，为后续模型奠定了基础。&#xA;AlexNet AlexNet由Alex Krizhevsky等人于2012年提出，是第一个在ImageNet图像分类比赛中取得优异成绩的卷积神经网络模型。它采用了多个卷积层和池化层，使用了ReLU激活函数和Dropout正则化技术。AlexNet的设计使得它在ImageNet图像分类比赛中大幅领先于其他模型，从而引领了卷积神经网络的新一轮发展。它的特点是使用了大量卷积核（近6000个）、参数量较大，但在准确率和效率上都有很好的表现。&#xA;VGG VGG由Karen Simonyan和Andrew Zisserman于2014年提出，其主要贡献是提出了使用更小的卷积核（3x3）来代替较大的卷积核。这种设计使得网络更深，而且参数量更少，从而提高了效率和准确率。VGG包含了16个或19个卷积层和池化层，这些层都采用了相同的卷积核大小和步长。VGG在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ResNet等模型提供了启示。&#xA;GoogleNet/Inception GoogleNet由Google团队于2014年提出，其主要贡献是提出了Inception模块，可以在不增加参数量的情况下增加网络的深度和宽度。Inception模块采用了多个不同大小的卷积核和池化层来进行特征提取，然后将它们串联在一起，形成了一个模块。GoogleNet还使用了全局平均池化层来代替全连接层，从而进一步减少了参数量。GoogleNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ResNet、DenseNet等模型提供了启示。&#xA;ResNet ResNet由Microsoft Research Asia团队于2015年提出，其主要贡献是提出了残差学习，可以解决深度卷积神经网络的退化问题。退化问题指的是随着网络深度的增加，准确率反而下降的现象。残差学习通过引入跨层连接来将输入直接传递到输出，从而避免了信息的损失。ResNet包含了较深的网络结构（152层），但却获得了更好的准确率。ResNet的设计思想被后续的DenseNet、MobileNet等模型所继承。&#xA;DenseNet DenseNet由Gao Huang等人于2017年提出，其主要贡献是提出了密集连接，可以增加网络的深度和宽度，从而提高了效率和准确率。密集连接指的是将每个层的输出都与后面所有层的输入相连，形成了一个密集的连接结构。这种设计使得网络更加紧凑，参数量更少，同时也可以提高特征的复用性。DenseNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的ShuffleNet、EfficientNet等模型提供了启示。&#xA;MobileNet MobileNet由Google团队于2017年提出，其主要贡献是提出了深度可分离卷积，可以在减少参数量的同时保持较好的准确率。深度可分离卷积指的是将卷积操作分为深度卷积和逐点卷积两步，从而减少了计算量和参数量。MobileNet采用了多个深度可分离卷积层和池化层，可以在移动设备等资源受限的环境下实现高效的图像分类和目标检测。MobileNet的设计思想被后续的ShuffleNet、EfficientNet等模型所继承。&#xA;ShuffleNet ShuffleNet由Microsoft Research Asia团队于2018年提出，其主要贡献是提出了通道重组和组卷积，可以在保持准确率的前提下大幅减少参数量和计算量。通道重组指的是将输入的通道分组并重新组合，从而让不同的组之间进行信息的交流。组卷积指的是将卷积操作分为组内卷积和组间卷积两步，从而减少了计算量和参数量。ShuffleNet采用了多个通道重组和组卷积层，可以在资源受限的环境下实现高效的图像分类和目标检测。&#xA;EfficientNet EfficientNet由Google团队于2019年提出，其主要贡献是提出了网络缩放和复合系数，可以在保持准确率的前提下大幅减少参数量和计算量。网络缩放指的是同时缩放网络的深度、宽度和分辨率，从而在不改变模型结构的情况下进行优化。复合系数指的是将深度、宽度和分辨率的缩放系数进行组合，从而得到一个更加高效的模型。EfficientNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的模型优化提供了启示。&#xA;RegNet RegNet由Facebook AI Research团队于2020年提出，其主要贡献是提出了网络结构的自适应规则，可以在保持准确率的前提下大幅减少参数量和计算量。自适应规则指的是通过搜索和优化来自动调整网络结构的超参数，从而得到一个更加高效的模型。RegNet在ImageNet图像分类比赛中取得了很好的成绩，同时也为后续的模型优化提供了启示。&#xA;以上是几个著名的卷积神经网络模型，它们的设计思想和网络结构都有所不同，但都对卷积神经网络的发展做出了重要贡献。&#xA;图解原理 卷积神经网络在图像识别中大放异彩，达到了前所未有的准确度，有着广泛的应用。接下来将以图像识别为例子，来介绍卷积神经网络的原理。&#xA;案例 假设给定一张图（可能是字母X或者字母O），通过CNN即可识别出是X还是O，如下图所示，那怎么做到的呢 图像输入 如果采用经典的神经网络模型，则需要读取整幅图像作为神经网络模型的输入（即全连接的方式），当图像的尺寸越大时，其连接的参数将变得很多，从而导致计算量非常大。 而我们人类对外界的认知一般是从局部到全局，先对局部有感知的认识，再逐步对全体有认知，这是人类的认识模式。在图像中的空间联系也是类似，局部范围内的像素之间联系较为紧密，而距离较远的像素则相关性较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。这种模式就是卷积神经网络中降低参数数目的重要神器：局部感受野。</description>
    </item>
    <item>
      <title>深度学习04-CNN经典模型</title>
      <link>https://jiaozi789.github.io/docs/programming/ai/deep_learning/cnn/dl_04_cnn_models/index.html</link>
      <pubDate>Thu, 18 Sep 2025 16:55:17 +0800</pubDate>
      <guid>https://jiaozi789.github.io/docs/programming/ai/deep_learning/cnn/dl_04_cnn_models/index.html</guid>
      <description>简介 卷积神经网络（CNN）是深度学习中非常重要的一种网络结构，它可以处理图像、文本、语音等各种类型的数据。以下是CNN的前4个经典模型&#xA;LeNet-5 LeNet-5是由Yann LeCun等人于1998年提出的，是第一个成功应用于手写数字识别的卷积神经网络。它由7层神经网络组成，包括2层卷积层、2层池化层和3层全连接层。其中，卷积层提取图像特征，池化层降低特征图的维度，全连接层将特征映射到对应的类别上。&#xA;LeNet-5的主要特点是使用Sigmoid激活函数、平均池化和卷积层后没有使用零填充。它在手写数字识别、人脸识别等领域都有着广泛的应用。&#xA;AlexNet AlexNet是由Alex Krizhevsky等人于2012年提出的，是第一个在大规模图像识别任务中取得显著成果的卷积神经网络。它由5层卷积层、3层全连接层和1层Softmax输出层组成，其中使用了ReLU激活函数、最大池化和Dropout技术。&#xA;AlexNet的主要特点是使用了GPU加速训练、数据增强和随机化Dropout等技术，使得模型的泛化能力和鲁棒性得到了大幅提升。它在ImageNet大规模图像识别比赛中取得了远超其他模型的优异成绩。&#xA;VGGNet VGGNet是由Karen Simonyan和Andrew Zisserman于2014年提出的，它是一个非常深的卷积神经网络，有16层或19层。VGGNet的每个卷积层都使用了3x3的卷积核和ReLU激活函数，使得它的网络结构非常清晰、易于理解。&#xA;VGGNet的主要特点是使用了更深的网络结构、小卷积核和少量的参数，使得模型的特征提取能力得到了进一步提升。它在ImageNet比赛中也获得了非常好的成绩。&#xA;GoogLeNet GoogLeNet是由Google团队于2014年提出的，它是一个非常深的卷积神经网络，有22层。它使用了一种称为Inception模块的结构，可以在保持网络深度的同时减少参数量。&#xA;GoogLeNet的主要特点是使用了Inception模块、1x1卷积核和全局平均池化等技术，使得模型的计算复杂度得到了大幅降低。它在ImageNet比赛中获得了非常好的成绩，并且被广泛应用于其他领域。&#xA;CNN回顾 回顾一下 CNN 的几个特点：局部感知、参数共享、池化。&#xA;局部感知 人类对外界的认知一般是从局部到全局、从片面到全面，类似的，在机器识别图像时也没有必要把整张图像按像素全部都连接到神经网络中，在图像中也是局部周边的像素联系比较紧密，而距离较远的像素则相关性较弱，因此可以采用局部连接的模式（将图像分块连接，这样能大大减少模型的参数），如下图所示： 参数（权值）共享 每张自然图像（人物、山水、建筑等）都有其固有特性，也就是说，图像其中一部分的统计特性与其它部分是接近的。这也意味着这一部分学习的特征也能用在另一部分上，能使用同样的学习特征。因此，在局部连接中隐藏层的每一个神经元连接的局部图像的权值参数（例如 5×5），将这些权值参数共享给其它剩下的神经元使用，那么此时不管隐藏层有多少个神经元，需要训练的参数就是这个局部图像的权限参数（例如 5×5），也就是卷积核的大小，这样大大减少了训练参数。如下图 卷积核的权值是指每个卷积核中的参数，用于对输入数据进行卷积操作时，对每个位置的像素进行加权求和。在卷积神经网络中，同一层中的所有卷积核的权值是共享的，这意味着每个卷积核在不同位置上的权值是相同的。共享权值可以减少模型中需要学习的参数数量，从而降低了模型的复杂度，同时可以提高模型的泛化能力，因为共享权值可以使模型更加稳定，避免过度拟合。共享权值的实现方式是通过使用相同的卷积核对输入数据进行卷积操作。 池化 随着模型网络不断加深，卷积核越来越多，要训练的参数还是很多，而且直接拿卷积核提取的特征直接训练也容易出现过拟合的现象。回想一下，之所以对图像使用卷积提取特征是因为图像具有一种 “静态性” 的属性，因此，一个很自然的想法就是对不同位置区域提取出有代表性的特征（进行聚合统计，例如最大值、平均值等），这种聚合的操作就叫做池化，池化的过程通常也被称为特征映射的过程（特征降维），如下图：&#xA;LeNet-5 概述 LeNet5 诞生于 1994 年，是最早的卷积神经网络之一， 由 Yann LeCun 完成，推动了深度学习领域的发展。在那时候，没有 GPU 帮助训练模型，甚至 CPU 的速度也很慢，因此，LeNet5 通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络也是最近大量神经网络架构的起点，给这个领域带来了许多灵感。 LeNet5 的网络结构示意图如下所示： LeNet5 由 7 层 CNN（不包含输入层）组成，上图中输入的原始图像大小是 32×32 像素，卷积层用 Ci 表示，子采样层（pooling，池化）用 Si 表示，全连接层用 Fi 表示。下面逐层介绍其作用和示意图上方的数字含义。</description>
    </item>
  </channel>
</rss>