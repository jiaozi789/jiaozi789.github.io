<!DOCTYPE html>
<html lang="zh" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="html">
  <head><script src="/docs/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=docs/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.150.0">
    <meta name="generator" content="Relearn 8.0.1+b23cf6629eada0c2802f34ae4012e04343497862">
    <meta name="description" content="聚类概念 聚类是一种无监督的机器学习方法，它主要是通过在数据集中找到相似的样本并将它们分组来发现数据中的模式和结构。聚类算法可以将数据分成具有相似特征的组，每个组被称为一个簇。
常见的聚类算法有以下几种：
K-means聚类算法：它是最常见的聚类算法之一，它的目标是将数据集分为K个簇，使得每个簇内的数据点相似度最高，不同簇之间的差异最大。
层次聚类算法：该算法将数据集中的样本逐渐合并到一起，直到形成一个完整的聚类结构，从而形成一颗聚类树。
密度聚类算法：它是一种基于数据点密度的聚类算法，它将数据点分为密集的区域和稀疏的区域，并将密集区域看作是一个簇。
均值漂移聚类算法：该算法使用核密度估计来找到数据点的局部最大值，以确定簇的质心。
DBSCAN聚类算法：它基于在数据集中的密度来确定簇的个数和形状，它可以识别任意形状的簇。
K-means简介 K-means是一种基于距离度量的聚类算法，其主要思想是将数据集分成K个簇，每个簇包含距离最近的K个数据点。该算法通过迭代优化簇的中心点，来不断调整簇的划分，最终得到一组最优的簇划分结果。
通俗来说，K-means算法就像是一位假设聪明的小学生在玩“猜数字”游戏。他会先猜一个数字，然后根据猜测与正确答案的距离（越接近答案距离越小），将答案所在的数字范围分成两个区域。接着，他会重复这个过程，直到将数字范围分成了K个区域为止，并记录下每个区域的中心点。最后，他会告诉你每个数字应该属于哪个区域（或者说簇），并告诉你每个区域的中心点。
在K-means算法中，我们需要指定簇的个数K，然后随机选择K个数据点作为初始中心点。接着，我们计算每个数据点距离各个中心点的距离，并将其归入距离最近的簇中。然后，重新计算每个簇的中心点，并重复上述过程（在计算每个点到新中心的举例重新归类到簇），直到簇的中心点不再发生变化为止。最终，我们将得到K个簇，每个簇包含一组距离最近的数据点，并且每个数据点只属于一个簇。
需要注意的是，由于K-means算法的初始中心点是随机选择的，因此可能会得到不同的簇划分结果。为了获得更好的结果，可以多次运行算法，并选择最优的簇划分结果。
K-means和KNN区别 K-Means和K-NN是两种不同的机器学习算法，其区别如下：
K-Means是一种聚类算法，它将数据集划分为K个簇，并将每个数据点分配到其最近的簇中心。K-NN是一种分类算法，它根据最近邻居的标签来预测新数据点的标签。
K-Means需要指定簇的数量K，而K-NN不需要。
K-Means是一种无监督学习算法，它不需要标记数据，而K-NN是一种监督学习算法，需要标记数据。
K-Means使用欧几里得距离来计算数据点之间的相似度，而K-NN可以使用不同的距离度量，如曼哈顿距离、余弦相似度等。
K-Means在处理大规模数据时可能会遇到性能问题，而K-NN可以轻松处理大规模数据。
总的来说，K-Means和K-NN是两种不同的机器学习算法，适用于不同的问题和数据集。
Kmeans的计算过程 （1）适当选择c个类的初始中心； （2）在k次迭代中，对任意一个样本，求其到c各中心的距离（欧式距离），将该样本归到距离更短的中心所在的类； （3）利用均值等方法更新该类的中心值； （4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。
假设 现在有4组数据，每组数据有2个维度，对其进行聚类分为2类，将其可视化一下。 $A=(1,1),B=(2,1),C=(4,3),D=(5,4)$ 假设选取两个星的位置为初始中心 $c_1=(1,1),c_2=(2,1)$，计算每个点到初始中心的距离，使用欧式距离得到4个点分别距离两个初始中心的距离，归于最近的类： $D^0第一行表示ABCD四个点到c1的举例，第二行表示ABCD四个点到c2的举例，举例使用欧氏距离公式计算出来，以C为例，到c1这一组的举例是3.61,到c2这一组的举例是2.83说明第一次迭代C是属于group-2$
通过比较，将其进行归类。并使用平均法更新中心位置。 由于归于group1的只有一个点，一次更新后的中心位置$c_1=(1,1)$，而 $c_{2} = (\frac{11}{3}, \frac{8}{3})$
group2的新中心点也就是$x={(x1&#43;x2&#43;x3)\over 3} ={(2&#43;4&#43;5)\over3}={11\over3}$ $y={(y1&#43;y2&#43;y3)\over 3} ={(1&#43;3&#43;4)\over3}={8\over3}$ 再次计算每个点与更新后的位置中心的距离">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="机器学习实战教程（十一）：支持向量机SVM :: liaomin416100569博客">
    <meta name="twitter:description" content="聚类概念 聚类是一种无监督的机器学习方法，它主要是通过在数据集中找到相似的样本并将它们分组来发现数据中的模式和结构。聚类算法可以将数据分成具有相似特征的组，每个组被称为一个簇。
常见的聚类算法有以下几种：
K-means聚类算法：它是最常见的聚类算法之一，它的目标是将数据集分为K个簇，使得每个簇内的数据点相似度最高，不同簇之间的差异最大。
层次聚类算法：该算法将数据集中的样本逐渐合并到一起，直到形成一个完整的聚类结构，从而形成一颗聚类树。
密度聚类算法：它是一种基于数据点密度的聚类算法，它将数据点分为密集的区域和稀疏的区域，并将密集区域看作是一个簇。
均值漂移聚类算法：该算法使用核密度估计来找到数据点的局部最大值，以确定簇的质心。
DBSCAN聚类算法：它基于在数据集中的密度来确定簇的个数和形状，它可以识别任意形状的簇。
K-means简介 K-means是一种基于距离度量的聚类算法，其主要思想是将数据集分成K个簇，每个簇包含距离最近的K个数据点。该算法通过迭代优化簇的中心点，来不断调整簇的划分，最终得到一组最优的簇划分结果。
通俗来说，K-means算法就像是一位假设聪明的小学生在玩“猜数字”游戏。他会先猜一个数字，然后根据猜测与正确答案的距离（越接近答案距离越小），将答案所在的数字范围分成两个区域。接着，他会重复这个过程，直到将数字范围分成了K个区域为止，并记录下每个区域的中心点。最后，他会告诉你每个数字应该属于哪个区域（或者说簇），并告诉你每个区域的中心点。
在K-means算法中，我们需要指定簇的个数K，然后随机选择K个数据点作为初始中心点。接着，我们计算每个数据点距离各个中心点的距离，并将其归入距离最近的簇中。然后，重新计算每个簇的中心点，并重复上述过程（在计算每个点到新中心的举例重新归类到簇），直到簇的中心点不再发生变化为止。最终，我们将得到K个簇，每个簇包含一组距离最近的数据点，并且每个数据点只属于一个簇。
需要注意的是，由于K-means算法的初始中心点是随机选择的，因此可能会得到不同的簇划分结果。为了获得更好的结果，可以多次运行算法，并选择最优的簇划分结果。
K-means和KNN区别 K-Means和K-NN是两种不同的机器学习算法，其区别如下：
K-Means是一种聚类算法，它将数据集划分为K个簇，并将每个数据点分配到其最近的簇中心。K-NN是一种分类算法，它根据最近邻居的标签来预测新数据点的标签。
K-Means需要指定簇的数量K，而K-NN不需要。
K-Means是一种无监督学习算法，它不需要标记数据，而K-NN是一种监督学习算法，需要标记数据。
K-Means使用欧几里得距离来计算数据点之间的相似度，而K-NN可以使用不同的距离度量，如曼哈顿距离、余弦相似度等。
K-Means在处理大规模数据时可能会遇到性能问题，而K-NN可以轻松处理大规模数据。
总的来说，K-Means和K-NN是两种不同的机器学习算法，适用于不同的问题和数据集。
Kmeans的计算过程 （1）适当选择c个类的初始中心； （2）在k次迭代中，对任意一个样本，求其到c各中心的距离（欧式距离），将该样本归到距离更短的中心所在的类； （3）利用均值等方法更新该类的中心值； （4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。
假设 现在有4组数据，每组数据有2个维度，对其进行聚类分为2类，将其可视化一下。 $A=(1,1),B=(2,1),C=(4,3),D=(5,4)$ 假设选取两个星的位置为初始中心 $c_1=(1,1),c_2=(2,1)$，计算每个点到初始中心的距离，使用欧式距离得到4个点分别距离两个初始中心的距离，归于最近的类： $D^0第一行表示ABCD四个点到c1的举例，第二行表示ABCD四个点到c2的举例，举例使用欧氏距离公式计算出来，以C为例，到c1这一组的举例是3.61,到c2这一组的举例是2.83说明第一次迭代C是属于group-2$
通过比较，将其进行归类。并使用平均法更新中心位置。 由于归于group1的只有一个点，一次更新后的中心位置$c_1=(1,1)$，而 $c_{2} = (\frac{11}{3}, \frac{8}{3})$
group2的新中心点也就是$x={(x1&#43;x2&#43;x3)\over 3} ={(2&#43;4&#43;5)\over3}={11\over3}$ $y={(y1&#43;y2&#43;y3)\over 3} ={(1&#43;3&#43;4)\over3}={8\over3}$ 再次计算每个点与更新后的位置中心的距离">
    <meta property="og:url" content="http://localhost:1313/docs/programming/ai/machine_learning/algorithms/action_12_cluster-copy/index.html">
    <meta property="og:site_name" content="liaomin416100569博客">
    <meta property="og:title" content="机器学习实战教程（十一）：支持向量机SVM :: liaomin416100569博客">
    <meta property="og:description" content="聚类概念 聚类是一种无监督的机器学习方法，它主要是通过在数据集中找到相似的样本并将它们分组来发现数据中的模式和结构。聚类算法可以将数据分成具有相似特征的组，每个组被称为一个簇。
常见的聚类算法有以下几种：
K-means聚类算法：它是最常见的聚类算法之一，它的目标是将数据集分为K个簇，使得每个簇内的数据点相似度最高，不同簇之间的差异最大。
层次聚类算法：该算法将数据集中的样本逐渐合并到一起，直到形成一个完整的聚类结构，从而形成一颗聚类树。
密度聚类算法：它是一种基于数据点密度的聚类算法，它将数据点分为密集的区域和稀疏的区域，并将密集区域看作是一个簇。
均值漂移聚类算法：该算法使用核密度估计来找到数据点的局部最大值，以确定簇的质心。
DBSCAN聚类算法：它基于在数据集中的密度来确定簇的个数和形状，它可以识别任意形状的簇。
K-means简介 K-means是一种基于距离度量的聚类算法，其主要思想是将数据集分成K个簇，每个簇包含距离最近的K个数据点。该算法通过迭代优化簇的中心点，来不断调整簇的划分，最终得到一组最优的簇划分结果。
通俗来说，K-means算法就像是一位假设聪明的小学生在玩“猜数字”游戏。他会先猜一个数字，然后根据猜测与正确答案的距离（越接近答案距离越小），将答案所在的数字范围分成两个区域。接着，他会重复这个过程，直到将数字范围分成了K个区域为止，并记录下每个区域的中心点。最后，他会告诉你每个数字应该属于哪个区域（或者说簇），并告诉你每个区域的中心点。
在K-means算法中，我们需要指定簇的个数K，然后随机选择K个数据点作为初始中心点。接着，我们计算每个数据点距离各个中心点的距离，并将其归入距离最近的簇中。然后，重新计算每个簇的中心点，并重复上述过程（在计算每个点到新中心的举例重新归类到簇），直到簇的中心点不再发生变化为止。最终，我们将得到K个簇，每个簇包含一组距离最近的数据点，并且每个数据点只属于一个簇。
需要注意的是，由于K-means算法的初始中心点是随机选择的，因此可能会得到不同的簇划分结果。为了获得更好的结果，可以多次运行算法，并选择最优的簇划分结果。
K-means和KNN区别 K-Means和K-NN是两种不同的机器学习算法，其区别如下：
K-Means是一种聚类算法，它将数据集划分为K个簇，并将每个数据点分配到其最近的簇中心。K-NN是一种分类算法，它根据最近邻居的标签来预测新数据点的标签。
K-Means需要指定簇的数量K，而K-NN不需要。
K-Means是一种无监督学习算法，它不需要标记数据，而K-NN是一种监督学习算法，需要标记数据。
K-Means使用欧几里得距离来计算数据点之间的相似度，而K-NN可以使用不同的距离度量，如曼哈顿距离、余弦相似度等。
K-Means在处理大规模数据时可能会遇到性能问题，而K-NN可以轻松处理大规模数据。
总的来说，K-Means和K-NN是两种不同的机器学习算法，适用于不同的问题和数据集。
Kmeans的计算过程 （1）适当选择c个类的初始中心； （2）在k次迭代中，对任意一个样本，求其到c各中心的距离（欧式距离），将该样本归到距离更短的中心所在的类； （3）利用均值等方法更新该类的中心值； （4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。
假设 现在有4组数据，每组数据有2个维度，对其进行聚类分为2类，将其可视化一下。 $A=(1,1),B=(2,1),C=(4,3),D=(5,4)$ 假设选取两个星的位置为初始中心 $c_1=(1,1),c_2=(2,1)$，计算每个点到初始中心的距离，使用欧式距离得到4个点分别距离两个初始中心的距离，归于最近的类： $D^0第一行表示ABCD四个点到c1的举例，第二行表示ABCD四个点到c2的举例，举例使用欧氏距离公式计算出来，以C为例，到c1这一组的举例是3.61,到c2这一组的举例是2.83说明第一次迭代C是属于group-2$
通过比较，将其进行归类。并使用平均法更新中心位置。 由于归于group1的只有一个点，一次更新后的中心位置$c_1=(1,1)$，而 $c_{2} = (\frac{11}{3}, \frac{8}{3})$
group2的新中心点也就是$x={(x1&#43;x2&#43;x3)\over 3} ={(2&#43;4&#43;5)\over3}={11\over3}$ $y={(y1&#43;y2&#43;y3)\over 3} ={(1&#43;3&#43;4)\over3}={8\over3}$ 再次计算每个点与更新后的位置中心的距离">
    <meta property="og:locale" content="zh">
    <meta property="og:type" content="article">
    <meta property="article:section" content="编程开发">
    <meta property="article:published_time" content="2025-09-18T16:55:17+08:00">
    <meta property="article:modified_time" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="name" content="机器学习实战教程（十一）：支持向量机SVM :: liaomin416100569博客">
    <meta itemprop="description" content="聚类概念 聚类是一种无监督的机器学习方法，它主要是通过在数据集中找到相似的样本并将它们分组来发现数据中的模式和结构。聚类算法可以将数据分成具有相似特征的组，每个组被称为一个簇。
常见的聚类算法有以下几种：
K-means聚类算法：它是最常见的聚类算法之一，它的目标是将数据集分为K个簇，使得每个簇内的数据点相似度最高，不同簇之间的差异最大。
层次聚类算法：该算法将数据集中的样本逐渐合并到一起，直到形成一个完整的聚类结构，从而形成一颗聚类树。
密度聚类算法：它是一种基于数据点密度的聚类算法，它将数据点分为密集的区域和稀疏的区域，并将密集区域看作是一个簇。
均值漂移聚类算法：该算法使用核密度估计来找到数据点的局部最大值，以确定簇的质心。
DBSCAN聚类算法：它基于在数据集中的密度来确定簇的个数和形状，它可以识别任意形状的簇。
K-means简介 K-means是一种基于距离度量的聚类算法，其主要思想是将数据集分成K个簇，每个簇包含距离最近的K个数据点。该算法通过迭代优化簇的中心点，来不断调整簇的划分，最终得到一组最优的簇划分结果。
通俗来说，K-means算法就像是一位假设聪明的小学生在玩“猜数字”游戏。他会先猜一个数字，然后根据猜测与正确答案的距离（越接近答案距离越小），将答案所在的数字范围分成两个区域。接着，他会重复这个过程，直到将数字范围分成了K个区域为止，并记录下每个区域的中心点。最后，他会告诉你每个数字应该属于哪个区域（或者说簇），并告诉你每个区域的中心点。
在K-means算法中，我们需要指定簇的个数K，然后随机选择K个数据点作为初始中心点。接着，我们计算每个数据点距离各个中心点的距离，并将其归入距离最近的簇中。然后，重新计算每个簇的中心点，并重复上述过程（在计算每个点到新中心的举例重新归类到簇），直到簇的中心点不再发生变化为止。最终，我们将得到K个簇，每个簇包含一组距离最近的数据点，并且每个数据点只属于一个簇。
需要注意的是，由于K-means算法的初始中心点是随机选择的，因此可能会得到不同的簇划分结果。为了获得更好的结果，可以多次运行算法，并选择最优的簇划分结果。
K-means和KNN区别 K-Means和K-NN是两种不同的机器学习算法，其区别如下：
K-Means是一种聚类算法，它将数据集划分为K个簇，并将每个数据点分配到其最近的簇中心。K-NN是一种分类算法，它根据最近邻居的标签来预测新数据点的标签。
K-Means需要指定簇的数量K，而K-NN不需要。
K-Means是一种无监督学习算法，它不需要标记数据，而K-NN是一种监督学习算法，需要标记数据。
K-Means使用欧几里得距离来计算数据点之间的相似度，而K-NN可以使用不同的距离度量，如曼哈顿距离、余弦相似度等。
K-Means在处理大规模数据时可能会遇到性能问题，而K-NN可以轻松处理大规模数据。
总的来说，K-Means和K-NN是两种不同的机器学习算法，适用于不同的问题和数据集。
Kmeans的计算过程 （1）适当选择c个类的初始中心； （2）在k次迭代中，对任意一个样本，求其到c各中心的距离（欧式距离），将该样本归到距离更短的中心所在的类； （3）利用均值等方法更新该类的中心值； （4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。
假设 现在有4组数据，每组数据有2个维度，对其进行聚类分为2类，将其可视化一下。 $A=(1,1),B=(2,1),C=(4,3),D=(5,4)$ 假设选取两个星的位置为初始中心 $c_1=(1,1),c_2=(2,1)$，计算每个点到初始中心的距离，使用欧式距离得到4个点分别距离两个初始中心的距离，归于最近的类： $D^0第一行表示ABCD四个点到c1的举例，第二行表示ABCD四个点到c2的举例，举例使用欧氏距离公式计算出来，以C为例，到c1这一组的举例是3.61,到c2这一组的举例是2.83说明第一次迭代C是属于group-2$
通过比较，将其进行归类。并使用平均法更新中心位置。 由于归于group1的只有一个点，一次更新后的中心位置$c_1=(1,1)$，而 $c_{2} = (\frac{11}{3}, \frac{8}{3})$
group2的新中心点也就是$x={(x1&#43;x2&#43;x3)\over 3} ={(2&#43;4&#43;5)\over3}={11\over3}$ $y={(y1&#43;y2&#43;y3)\over 3} ={(1&#43;3&#43;4)\over3}={8\over3}$ 再次计算每个点与更新后的位置中心的距离">
    <meta itemprop="datePublished" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="dateModified" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="wordCount" content="92">
    <title>机器学习实战教程（十一）：支持向量机SVM :: liaomin416100569博客</title>
    <link href="/docs/css/auto-complete/auto-complete.min.css?1758336979" rel="stylesheet">
    <script src="/docs/js/auto-complete/auto-complete.min.js?1758336979" defer></script>
    <script src="/docs/js/search-lunr.js?1758336979" defer></script>
    <script src="/docs/js/search.js?1758336979" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/docs/searchindex.en.js?1758336979";
    </script>
    <script src="/docs/js/lunr/lunr.min.js?1758336979" defer></script>
    <script src="/docs/js/lunr/lunr.stemmer.support.min.js?1758336979" defer></script>
    <script src="/docs/js/lunr/lunr.multi.min.js?1758336979" defer></script>
    <script src="/docs/js/lunr/lunr.zh.min.js?1758336979" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['zh'];
    </script>
    <link href="/docs/fonts/fontawesome/css/fontawesome-all.min.css?1758336979" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/docs/fonts/fontawesome/css/fontawesome-all.min.css?1758336979" rel="stylesheet"></noscript>
    <link href="/docs/css/perfect-scrollbar/perfect-scrollbar.min.css?1758336979" rel="stylesheet">
    <link href="/docs/css/theme.css?1758336979" rel="stylesheet">
    <link href="/docs/css/format-html.css?1758336979" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = ``;
      window.relearn.path='\/programming\/ai\/machine_learning\/algorithms\/action_12_cluster-copy\/index.html';
      window.relearn.relBasePath='..\/..\/..\/..\/..';
      window.relearn.relBaseUri='..\/..\/..\/..\/..\/..';
      window.relearn.absBaseUri='http:\/\/localhost:1313\/docs';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      // variant stuff
      window.relearn.themevariants = [ 'auto' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script>
    <link href="/docs/css/custom.css?1758336979" rel="stylesheet">
  </head>
  <body class="mobile-support html" data-url="/docs/programming/ai/machine_learning/algorithms/action_12_cluster-copy/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
<nav class="TableOfContents">
  <ul>
    <li><a href="#聚类概念">聚类概念</a></li>
    <li><a href="#k-means简介">K-means简介</a></li>
    <li><a href="#k-means和knn区别">K-means和KNN区别</a></li>
    <li><a href="#kmeans的计算过程">Kmeans的计算过程</a></li>
    <li><a href="#kmeans的编程实现">Kmeans的编程实现</a></li>
  </ul>
</nav>
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/index.html"><span itemprop="name">liaomin416100569博客</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/index.html"><span itemprop="name">编程开发</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/index.html"><span itemprop="name">人工智能</span></a><meta itemprop="position" content="3">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/machine_learning/index.html"><span itemprop="name">机器学习</span></a><meta itemprop="position" content="4">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/machine_learning/algorithms/index.html"><span itemprop="name">核心算法</span></a><meta itemprop="position" content="5">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">机器学习实战教程（十一）：支持向量机SVM</span><meta itemprop="position" content="6"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/docs/programming/ai/machine_learning/algorithms/action_11_vectormachine/index.html" title="机器学习实战教程（十一）：支持向量机SVM (🡐)"><i class="fa-fw fas fa-chevron-left"></i></a>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/docs/programming/ai/machine_learning/algorithms/action_12_cluster/index.html" title="机器学习实战教程（十一）：支持向量机SVM (🡒)"><i class="fa-fw fas fa-chevron-right"></i></a>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable programming" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
  </header>

<h1 id="机器学习实战教程十一支持向量机svm">机器学习实战教程（十一）：支持向量机SVM</h1>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<h1 id="聚类概念">聚类概念</h1>
<p>聚类是一种无监督的机器学习方法，它主要是通过在数据集中找到相似的样本并将它们分组来发现数据中的模式和结构。聚类算法可以将数据分成具有相似特征的组，每个组被称为一个簇。</p>
<p>常见的聚类算法有以下几种：</p>
<ol>
<li>
<p>K-means聚类算法：它是最常见的聚类算法之一，它的目标是将数据集分为K个簇，使得每个簇内的数据点相似度最高，不同簇之间的差异最大。</p>
</li>
<li>
<p>层次聚类算法：该算法将数据集中的样本逐渐合并到一起，直到形成一个完整的聚类结构，从而形成一颗聚类树。</p>
</li>
<li>
<p>密度聚类算法：它是一种基于数据点密度的聚类算法，它将数据点分为密集的区域和稀疏的区域，并将密集区域看作是一个簇。</p>
</li>
<li>
<p>均值漂移聚类算法：该算法使用核密度估计来找到数据点的局部最大值，以确定簇的质心。</p>
</li>
<li>
<p>DBSCAN聚类算法：它基于在数据集中的密度来确定簇的个数和形状，它可以识别任意形状的簇。</p>
</li>
</ol>
<h1 id="k-means简介">K-means简介</h1>
<p>K-means是一种基于距离度量的聚类算法，其主要思想是将数据集分成K个簇，每个簇包含距离最近的K个数据点。该算法通过迭代优化簇的中心点，来不断调整簇的划分，最终得到一组最优的簇划分结果。</p>
<p>通俗来说，K-means算法就像是一位假设聪明的小学生在玩“猜数字”游戏。他会先猜一个数字，然后根据猜测与正确答案的距离（越接近答案距离越小），将答案所在的数字范围分成两个区域。接着，他会重复这个过程，直到将数字范围分成了K个区域为止，并记录下每个区域的中心点。最后，他会告诉你每个数字应该属于哪个区域（或者说簇），并告诉你每个区域的中心点。</p>
<p>在K-means算法中，我们需要指定簇的个数K，然后随机选择K个数据点作为初始中心点。接着，我们计算每个数据点距离各个中心点的距离，并将其归入距离最近的簇中。然后，重新计算每个簇的中心点，并重复上述过程（在计算每个点到新中心的举例重新归类到簇），直到簇的中心点不再发生变化为止。最终，我们将得到K个簇，每个簇包含一组距离最近的数据点，并且每个数据点只属于一个簇。</p>
<p>需要注意的是，由于K-means算法的初始中心点是随机选择的，因此可能会得到不同的簇划分结果。为了获得更好的结果，可以多次运行算法，并选择最优的簇划分结果。</p>
<h1 id="k-means和knn区别">K-means和KNN区别</h1>
<p>K-Means和K-NN是两种不同的机器学习算法，其区别如下：</p>
<ol>
<li>
<p>K-Means是一种聚类算法，它将数据集划分为K个簇，并将每个数据点分配到其最近的簇中心。K-NN是一种分类算法，它根据最近邻居的标签来预测新数据点的标签。</p>
</li>
<li>
<p>K-Means需要指定簇的数量K，而K-NN不需要。</p>
</li>
<li>
<p>K-Means是一种无监督学习算法，它不需要标记数据，而K-NN是一种监督学习算法，需要标记数据。</p>
</li>
<li>
<p>K-Means使用欧几里得距离来计算数据点之间的相似度，而K-NN可以使用不同的距离度量，如曼哈顿距离、余弦相似度等。</p>
</li>
<li>
<p>K-Means在处理大规模数据时可能会遇到性能问题，而K-NN可以轻松处理大规模数据。</p>
</li>
</ol>
<p>总的来说，K-Means和K-NN是两种不同的机器学习算法，适用于不同的问题和数据集。</p>
<h1 id="kmeans的计算过程">Kmeans的计算过程</h1>
<p>（1）适当选择c个类的初始中心；
（2）在k次迭代中，对任意一个样本，求其到c各中心的距离（<a href="https://blog.csdn.net/liaomin416100569/article/details/84035678?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168319961316800197020567%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=168319961316800197020567&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-9-84035678-null-null.blog_rank_default&utm_term=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&spm=1018.2226.3001.4450" rel="external" target="_blank">欧式距离</a>），将该样本归到距离更短的中心所在的类；
（3）利用均值等方法更新该类的中心值；
（4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。</p>
<p>假设 现在有4组数据，每组数据有2个维度，对其进行聚类分为2类，将其可视化一下。
$A=(1,1),B=(2,1),C=(4,3),D=(5,4)$
<a href="#R-image-f8414fcaf5676b9f354f55bc5aad2790" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/5911f95f115bcb1d1fd6cf34a5c9d911.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f8414fcaf5676b9f354f55bc5aad2790"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/5911f95f115bcb1d1fd6cf34a5c9d911.png"></a>
假设选取两个星的位置为初始中心 $c_1=(1,1),c_2=(2,1)$，计算每个点到初始中心的距离，使用欧式距离得到4个点分别距离两个初始中心的距离，归于最近的类：
<a href="#R-image-587e81323a06de78b1f8cac055c4f45f" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/7d7cfd62340cd43bcf6bdf40f725a4ee.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-587e81323a06de78b1f8cac055c4f45f"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/7d7cfd62340cd43bcf6bdf40f725a4ee.png"></a></p>
<blockquote>
<p>$D^0第一行表示ABCD四个点到c1的举例，第二行表示ABCD四个点到c2的举例，举例使用欧氏距离公式计算出来，以C为例，到c1这一组的举例是3.61,到c2这一组的举例是2.83说明第一次迭代C是属于group-2$</p></blockquote>
<p>通过比较，将其进行归类。并使用平均法更新中心位置。
<a href="#R-image-8ffeee040531056237e55af2c6337cf8" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/2413bd3ffa5f49f2cdfca9c303e11274.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-8ffeee040531056237e55af2c6337cf8"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/2413bd3ffa5f49f2cdfca9c303e11274.png"></a>
由于归于group1的只有一个点，一次更新后的中心位置$c_1=(1,1)$，而 $c_{2} = (\frac{11}{3}, \frac{8}{3})$</p>
<blockquote>
<p>group2的新中心点也就是$x={(x1+x2+x3)\over 3} ={(2+4+5)\over3}={11\over3}$
$y={(y1+y2+y3)\over 3} ={(1+3+4)\over3}={8\over3}$
<a href="#R-image-1c2591628bdd75501530da40eb586756" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/c86bb1f6f448950e3147294c6094fe09.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-1c2591628bdd75501530da40eb586756"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/c86bb1f6f448950e3147294c6094fe09.png"></a>
再次计算每个点与更新后的位置中心的距离</p></blockquote>
<p><a href="#R-image-c511ab762d3aba2fc0ce3e235115a064" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/d6eec84ebdede9d0261c3bed31b1d0e5.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-c511ab762d3aba2fc0ce3e235115a064"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/d6eec84ebdede9d0261c3bed31b1d0e5.png"></a>
<a href="#R-image-7b26104a0121adf8ece1be7f0f5d33db" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/bc3a24c6ca8602ebf26159cc0ebb2b78.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7b26104a0121adf8ece1be7f0f5d33db"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/bc3a24c6ca8602ebf26159cc0ebb2b78.png"></a>
<a href="#R-image-75918180eed251d274a2ab5a8c24da7f" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/864712201ddaad36dd5708daeb09ff86.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-75918180eed251d274a2ab5a8c24da7f"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/864712201ddaad36dd5708daeb09ff86.png"></a>
继续迭代下去，
<a href="#R-image-f2fd17e1c7946ff641b9d3f27e4d39be" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/aed49ae48b4cbf872d6f1d371c23ee4c.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f2fd17e1c7946ff641b9d3f27e4d39be"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/aed49ae48b4cbf872d6f1d371c23ee4c.png"></a>
<a href="#R-image-8ffd894f6937c835526f23b345eaad9b" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/179ca7024136a25c7c60f0189abc527b.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-8ffd894f6937c835526f23b345eaad9b"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/179ca7024136a25c7c60f0189abc527b.png"></a>
<a href="#R-image-4d883bac24c88da44e99f48293ce57ae" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/a3ca8d52acc18f15b0fa6a99ba862a72.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-4d883bac24c88da44e99f48293ce57ae"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/a3ca8d52acc18f15b0fa6a99ba862a72.png"></a>
此时，与上一次的类别标记无变化，即可停止。</p>
<h1 id="kmeans的编程实现">Kmeans的编程实现</h1>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#%%

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 生成随机数据
X, y = make_blobs(n_samples=300, centers=4, random_state=42)

# 使用KMeans算法进行聚类
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit(X)

# 绘制聚类结果
plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap=&#39;viridis&#39;)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker=&#39;*&#39;, s=150, color=&#39;red&#39;)
plt.show()</code></pre></div>
<p><a href="#R-image-8d79d0dc648d2cf7c6eb10a75dec530a" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/7d9f78b9823b28f32f98bc73c07a9071.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-8d79d0dc648d2cf7c6eb10a75dec530a"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="https://i-blog.csdnimg.cn/blog_migrate/7d9f78b9823b28f32f98bc73c07a9071.png"></a></p>

  <footer class="footline">
              <i class='fa-fw fas fa-calendar'></i> Sep 18, 2025
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          <a id="R-logo" class="R-default" href="/docs/index.html">
            <div class="logo-title">liaomin416100569博客</div>
          </a>
        </div>
        <search><form action="/docs/search/index.html" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-homelinks">
          <ul class="space collapsible-menu">
            <li class="" data-nav-id="/docs/index.html"><a class="padding" href="/docs/index.html"><i class="fa-fw fas fa-home"></i> Home</a></li>
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-headercontrols">
          <ul class="">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div class="R-sidebarmenu R-shortcutmenu-main">
          <ul class="enlarge morespace collapsible-menu">
            <li class="parent " data-nav-id="/docs/programming/index.html"><a class="padding" href="/docs/programming/index.html">编程开发</a><ul id="R-subsections-e3fc01b477dbaf64a8f5013a3dab5c5b" class="collapsible-menu">
            <li class="alwaysopen " data-nav-id="/docs/programming/languages/index.html"><a class="padding" href="/docs/programming/languages/index.html">编程语言</a><ul id="R-subsections-1bbde7fb0c312ba940b425df5a4caf67" class="collapsible-menu"></ul></li>
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/index.html"><a class="padding" href="/docs/programming/ai/index.html">人工智能</a><ul id="R-subsections-9d06be7bd8c736c09a65fb0b91b71d0e" class="collapsible-menu">
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/tools_libraries/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/index.html">工具库</a><ul id="R-subsections-e43804740042696aa314af8cc1e28fa9" class="collapsible-menu"></ul></li>
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/machine_learning/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/index.html">机器学习</a><ul id="R-subsections-d3b98ca0beda96811b8c41829d886d7f" class="collapsible-menu">
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/machine_learning/basic/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/basic/index.html">基础理论</a><ul id="R-subsections-2f18a18645b7652a148815c1a6786b18" class="collapsible-menu"></ul></li>
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/machine_learning/algorithms/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/index.html">核心算法</a><ul id="R-subsections-921418d1d7190828278c689c88df6881" class="collapsible-menu">
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_01_knn/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_01_knn/index.html">机器学习实战教程（一）：K-近邻（KNN）算法</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_04_pca/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_04_pca/index.html">机器学习实战教程（四）：从特征分解到协方差矩阵：详细剖析和实现PCA算法</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_05_pcaface/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_05_pcaface/index.html">机器学习实战教程（五）：使用PCA实战人脸降维</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_06_decidetree/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_06_decidetree/index.html">机器学习实战教程（六）：决策树</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_07_bays/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_07_bays/index.html">机器学习实战教程（七）：朴素贝叶斯</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_08_multinomial/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_08_multinomial/index.html">机器学习实战教程（八）：多项式回归</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_10_logic/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_10_logic/index.html">机器学习实战教程（十）：逻辑回归</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_11_vectormachine/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_11_vectormachine/index.html">机器学习实战教程（十一）：支持向量机SVM</a></li>
            <li class="active " data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_12_cluster-copy/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_12_cluster-copy/index.html">机器学习实战教程（十一）：支持向量机SVM</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_12_cluster/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_12_cluster/index.html">机器学习实战教程（十一）：支持向量机SVM</a></li></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/machine_learning/evaluation/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/evaluation/index.html">模型评估</a><ul id="R-subsections-d83b378e097742dc59073d128d99d653" class="collapsible-menu"></ul></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/tools/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/tools/index.html">实践工具</a></li></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/deep_learning/index.html"><a class="padding" href="/docs/programming/ai/deep_learning/index.html">深度学习</a><ul id="R-subsections-8e4f2a2c63b9f66a19e3b2a7c957ccda" class="collapsible-menu"></ul></li></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/plugins/index.html"><a class="padding" href="/docs/programming/plugins/index.html">插件开发</a><ul id="R-subsections-de66f54cff99288ca68bfcb5bb0439ae" class="collapsible-menu"></ul></li></ul></li>
          </ul>
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-shortcuts">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div id="R-footer-margin"></div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-footercontrols">
          <ul class="">
          </ul>
        </div>
<div id="R-footer"><p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p></div>
      </div>
    </aside>
    <script src="/docs/js/clipboard/clipboard.min.js?1758336979" defer></script>
    <script src="/docs/js/perfect-scrollbar/perfect-scrollbar.min.js?1758336979" defer></script>
    <script src="/docs/js/theme.js?1758336979" defer></script>
  </body>
</html>
