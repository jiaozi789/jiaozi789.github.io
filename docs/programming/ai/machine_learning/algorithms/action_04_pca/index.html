<!DOCTYPE html>
<html lang="zh" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="html">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.150.0">
    <meta name="generator" content="Relearn 8.0.1+b23cf6629eada0c2802f34ae4012e04343497862">
    <meta name="description" content="1. 协方差 概念 方差和标准差的原理和实例演示，请参考
方差 方差（Variance）是度量一组数据的分散程度。方差是各个样本与样本均值的差的平方和的均值： 标准差 标准差是数值分散的测量。 标准差的符号是 σ （希腊语字母 西格马，英语 sigma） 公式很简单：方差的平方根。 协方差 通俗理解 可以通俗的理解为：两个变量在变化过程中是同方向变化？还是反方向变化？同向或反向程度如何？ 你变大，同时我也变大，说明两个变量是同向变化的，这时协方差就是正的。 你变大，同时我变小，说明两个变量是反向变化的，这时协方差就是负的。 从数值来看，协方差的数值越大，两个变量同向程度也就越大。反之亦然。 通俗易懂的理解看知乎文章 或者 gitlab转载
协方差矩阵 协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。 这个解释摘自维基百科，看起来很是抽象，不好理解。其实简单来讲，协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减）。而协方差矩阵，只是将所有变量的协方差关系用矩阵的形式表现出来而已。通过矩阵这一工具，可以更方便地进行数学运算。 数学定义 回想概率统计里面关于方差的数学定义： 协方差的数学定义异曲同工： 这里的 x和y表示两个变量空间。用机器学习的话讲，就是样本有 x和 y两种特征， 而 X 就是包含所有样本的 x特征的集合， Y就是包含所有样本的 y特征的集合。 用一个例子来解释会更加形象。 用一个矩阵表示为： 现在，我们用两个变量空间X ，Y 来表示这两个特征： 由于协方差反应的是两个变量之间的相关性，因此，协方差矩阵表示的是所有变量之间两两相关的关系，具体来讲，一个包含两个特征的矩阵，其协方差矩阵应该有 2*2 大小： 接下来，就来逐一计算 Cov(Z)的值。 首先，我们需要先计算出 X，Y 两个特征空间的平均值： AVG(X)=3.25,AVG(Y)=3 ， 。 然后，根据协方差的数学定义，计算协方差矩阵的每个元素： 所以协方差矩阵 好了，虽然这只是一个二维特征的例子，但我们已经可以从中总结出协方差矩阵 的「计算套路」： python协方差原理">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="机器学习实战教程（四）：从特征分解到协方差矩阵：详细剖析和实现PCA算法 :: liaomin416100569博客">
    <meta name="twitter:description" content="1. 协方差 概念 方差和标准差的原理和实例演示，请参考
方差 方差（Variance）是度量一组数据的分散程度。方差是各个样本与样本均值的差的平方和的均值： 标准差 标准差是数值分散的测量。 标准差的符号是 σ （希腊语字母 西格马，英语 sigma） 公式很简单：方差的平方根。 协方差 通俗理解 可以通俗的理解为：两个变量在变化过程中是同方向变化？还是反方向变化？同向或反向程度如何？ 你变大，同时我也变大，说明两个变量是同向变化的，这时协方差就是正的。 你变大，同时我变小，说明两个变量是反向变化的，这时协方差就是负的。 从数值来看，协方差的数值越大，两个变量同向程度也就越大。反之亦然。 通俗易懂的理解看知乎文章 或者 gitlab转载
协方差矩阵 协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。 这个解释摘自维基百科，看起来很是抽象，不好理解。其实简单来讲，协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减）。而协方差矩阵，只是将所有变量的协方差关系用矩阵的形式表现出来而已。通过矩阵这一工具，可以更方便地进行数学运算。 数学定义 回想概率统计里面关于方差的数学定义： 协方差的数学定义异曲同工： 这里的 x和y表示两个变量空间。用机器学习的话讲，就是样本有 x和 y两种特征， 而 X 就是包含所有样本的 x特征的集合， Y就是包含所有样本的 y特征的集合。 用一个例子来解释会更加形象。 用一个矩阵表示为： 现在，我们用两个变量空间X ，Y 来表示这两个特征： 由于协方差反应的是两个变量之间的相关性，因此，协方差矩阵表示的是所有变量之间两两相关的关系，具体来讲，一个包含两个特征的矩阵，其协方差矩阵应该有 2*2 大小： 接下来，就来逐一计算 Cov(Z)的值。 首先，我们需要先计算出 X，Y 两个特征空间的平均值： AVG(X)=3.25,AVG(Y)=3 ， 。 然后，根据协方差的数学定义，计算协方差矩阵的每个元素： 所以协方差矩阵 好了，虽然这只是一个二维特征的例子，但我们已经可以从中总结出协方差矩阵 的「计算套路」： python协方差原理">
    <meta property="og:url" content="https://jiaozi789.github.io/docs/programming/ai/machine_learning/algorithms/action_04_pca/index.html">
    <meta property="og:site_name" content="liaomin416100569博客">
    <meta property="og:title" content="机器学习实战教程（四）：从特征分解到协方差矩阵：详细剖析和实现PCA算法 :: liaomin416100569博客">
    <meta property="og:description" content="1. 协方差 概念 方差和标准差的原理和实例演示，请参考
方差 方差（Variance）是度量一组数据的分散程度。方差是各个样本与样本均值的差的平方和的均值： 标准差 标准差是数值分散的测量。 标准差的符号是 σ （希腊语字母 西格马，英语 sigma） 公式很简单：方差的平方根。 协方差 通俗理解 可以通俗的理解为：两个变量在变化过程中是同方向变化？还是反方向变化？同向或反向程度如何？ 你变大，同时我也变大，说明两个变量是同向变化的，这时协方差就是正的。 你变大，同时我变小，说明两个变量是反向变化的，这时协方差就是负的。 从数值来看，协方差的数值越大，两个变量同向程度也就越大。反之亦然。 通俗易懂的理解看知乎文章 或者 gitlab转载
协方差矩阵 协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。 这个解释摘自维基百科，看起来很是抽象，不好理解。其实简单来讲，协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减）。而协方差矩阵，只是将所有变量的协方差关系用矩阵的形式表现出来而已。通过矩阵这一工具，可以更方便地进行数学运算。 数学定义 回想概率统计里面关于方差的数学定义： 协方差的数学定义异曲同工： 这里的 x和y表示两个变量空间。用机器学习的话讲，就是样本有 x和 y两种特征， 而 X 就是包含所有样本的 x特征的集合， Y就是包含所有样本的 y特征的集合。 用一个例子来解释会更加形象。 用一个矩阵表示为： 现在，我们用两个变量空间X ，Y 来表示这两个特征： 由于协方差反应的是两个变量之间的相关性，因此，协方差矩阵表示的是所有变量之间两两相关的关系，具体来讲，一个包含两个特征的矩阵，其协方差矩阵应该有 2*2 大小： 接下来，就来逐一计算 Cov(Z)的值。 首先，我们需要先计算出 X，Y 两个特征空间的平均值： AVG(X)=3.25,AVG(Y)=3 ， 。 然后，根据协方差的数学定义，计算协方差矩阵的每个元素： 所以协方差矩阵 好了，虽然这只是一个二维特征的例子，但我们已经可以从中总结出协方差矩阵 的「计算套路」： python协方差原理">
    <meta property="og:locale" content="zh">
    <meta property="og:type" content="article">
    <meta property="article:section" content="编程开发">
    <meta property="article:published_time" content="2025-09-18T16:55:17+08:00">
    <meta property="article:modified_time" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="name" content="机器学习实战教程（四）：从特征分解到协方差矩阵：详细剖析和实现PCA算法 :: liaomin416100569博客">
    <meta itemprop="description" content="1. 协方差 概念 方差和标准差的原理和实例演示，请参考
方差 方差（Variance）是度量一组数据的分散程度。方差是各个样本与样本均值的差的平方和的均值： 标准差 标准差是数值分散的测量。 标准差的符号是 σ （希腊语字母 西格马，英语 sigma） 公式很简单：方差的平方根。 协方差 通俗理解 可以通俗的理解为：两个变量在变化过程中是同方向变化？还是反方向变化？同向或反向程度如何？ 你变大，同时我也变大，说明两个变量是同向变化的，这时协方差就是正的。 你变大，同时我变小，说明两个变量是反向变化的，这时协方差就是负的。 从数值来看，协方差的数值越大，两个变量同向程度也就越大。反之亦然。 通俗易懂的理解看知乎文章 或者 gitlab转载
协方差矩阵 协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。 这个解释摘自维基百科，看起来很是抽象，不好理解。其实简单来讲，协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减）。而协方差矩阵，只是将所有变量的协方差关系用矩阵的形式表现出来而已。通过矩阵这一工具，可以更方便地进行数学运算。 数学定义 回想概率统计里面关于方差的数学定义： 协方差的数学定义异曲同工： 这里的 x和y表示两个变量空间。用机器学习的话讲，就是样本有 x和 y两种特征， 而 X 就是包含所有样本的 x特征的集合， Y就是包含所有样本的 y特征的集合。 用一个例子来解释会更加形象。 用一个矩阵表示为： 现在，我们用两个变量空间X ，Y 来表示这两个特征： 由于协方差反应的是两个变量之间的相关性，因此，协方差矩阵表示的是所有变量之间两两相关的关系，具体来讲，一个包含两个特征的矩阵，其协方差矩阵应该有 2*2 大小： 接下来，就来逐一计算 Cov(Z)的值。 首先，我们需要先计算出 X，Y 两个特征空间的平均值： AVG(X)=3.25,AVG(Y)=3 ， 。 然后，根据协方差的数学定义，计算协方差矩阵的每个元素： 所以协方差矩阵 好了，虽然这只是一个二维特征的例子，但我们已经可以从中总结出协方差矩阵 的「计算套路」： python协方差原理">
    <meta itemprop="datePublished" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="dateModified" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="wordCount" content="529">
    <title>机器学习实战教程（四）：从特征分解到协方差矩阵：详细剖析和实现PCA算法 :: liaomin416100569博客</title>
    <link href="/docs/css/auto-complete/auto-complete.min.css?1758355652" rel="stylesheet">
    <script src="/docs/js/auto-complete/auto-complete.min.js?1758355652" defer></script>
    <script src="/docs/js/search-lunr.min.js?1758355652" defer></script>
    <script src="/docs/js/search.min.js?1758355652" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/docs/searchindex.en.js?1758355652";
    </script>
    <script src="/docs/js/lunr/lunr.min.js?1758355652" defer></script>
    <script src="/docs/js/lunr/lunr.stemmer.support.min.js?1758355652" defer></script>
    <script src="/docs/js/lunr/lunr.multi.min.js?1758355652" defer></script>
    <script src="/docs/js/lunr/lunr.zh.min.js?1758355652" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['zh'];
    </script>
    <link href="/docs/fonts/fontawesome/css/fontawesome-all.min.css?1758355652" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/docs/fonts/fontawesome/css/fontawesome-all.min.css?1758355652" rel="stylesheet"></noscript>
    <link href="/docs/css/perfect-scrollbar/perfect-scrollbar.min.css?1758355652" rel="stylesheet">
    <link href="/docs/css/theme.min.css?1758355652" rel="stylesheet">
    <link href="/docs/css/format-html.min.css?1758355652" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = `.min`;
      window.relearn.path='\/programming\/ai\/machine_learning\/algorithms\/action_04_pca\/index.html';
      window.relearn.relBasePath='..\/..\/..\/..\/..';
      window.relearn.relBaseUri='..\/..\/..\/..\/..\/..';
      window.relearn.absBaseUri='https:\/\/jiaozi789.github.io\/docs';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      // variant stuff
      window.relearn.themevariants = [ 'auto' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script>
    <link href="/docs/css/custom.css?1758355652" rel="stylesheet">
  </head>
  <body class="mobile-support html" data-url="/docs/programming/ai/machine_learning/algorithms/action_04_pca/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
<nav class="TableOfContents">
  <ul>
    <li><a href="#1-协方差">1. 协方差</a>
      <ul>
        <li><a href="#概念">概念</a>
          <ul>
            <li><a href="#方差">方差</a></li>
            <li><a href="#标准差">标准差</a></li>
            <li><a href="#协方差">协方差</a>
              <ul>
                <li><a href="#通俗理解">通俗理解</a></li>
                <li><a href="#协方差矩阵">协方差矩阵</a></li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#2-pca">2. PCA</a>
      <ul>
        <li><a href="#概念-1">概念</a></li>
        <li><a href="#理论">理论</a></li>
        <li><a href="#公式推导">公式推导</a></li>
        <li><a href="#编程实现第一主成分">编程实现（第一主成分）</a></li>
      </ul>
    </li>
  </ul>
</nav>
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/index.html"><span itemprop="name">liaomin416100569博客</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/index.html"><span itemprop="name">编程开发</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/index.html"><span itemprop="name">人工智能</span></a><meta itemprop="position" content="3">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/machine_learning/index.html"><span itemprop="name">机器学习</span></a><meta itemprop="position" content="4">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/machine_learning/algorithms/index.html"><span itemprop="name">核心算法</span></a><meta itemprop="position" content="5">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">机器学习实战教程（四）：从特征分解到协方差矩阵：详细剖析和实现PCA算法</span><meta itemprop="position" content="6"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/docs/programming/ai/machine_learning/algorithms/action_01_knn/index.html" title="机器学习实战教程（一）：K-近邻（KNN）算法 (🡐)"><i class="fa-fw fas fa-chevron-left"></i></a>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/docs/programming/ai/machine_learning/algorithms/action_05_pcaface/index.html" title="机器学习实战教程（五）：使用PCA实战人脸降维 (🡒)"><i class="fa-fw fas fa-chevron-right"></i></a>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable programming" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
  </header>

<h1 id="机器学习实战教程四从特征分解到协方差矩阵详细剖析和实现pca算法">机器学习实战教程（四）：从特征分解到协方差矩阵：详细剖析和实现PCA算法</h1>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<h1 id="1-协方差">1. 协方差</h1>
<h2 id="概念">概念</h2>
<p>方差和标准差的原理和实例演示，请<a href="https://raw.githubusercontent.com/lzeqian/machinelearn/master/learn_algorithm/%E6%A0%87%E5%87%86%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE%E7%90%86%E8%A7%A3.png" rel="external" target="_blank">参考</a></p>
<h3 id="方差">方差</h3>
<p>方差（Variance）是度量一组数据的分散程度。方差是各个样本与样本均值的差的平方和的均值：
<a href="#R-image-fdfcabb944ce6044d13db079b94632af" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/3c88381f9ecf1248f7d117726825d7ad.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-fdfcabb944ce6044d13db079b94632af"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/3c88381f9ecf1248f7d117726825d7ad.png"></a></p>
<h3 id="标准差">标准差</h3>
<p>标准差是数值分散的测量。
标准差的符号是 σ （希腊语字母 西格马，英语 sigma）
公式很简单：方差的平方根。
<a href="#R-image-46d5cf4ed288e9b39b61ad2bf2ca3f4f" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/a759f02e48a4050b9731475e3a97f921.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-46d5cf4ed288e9b39b61ad2bf2ca3f4f"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/a759f02e48a4050b9731475e3a97f921.png"></a></p>
<h3 id="协方差">协方差</h3>
<h4 id="通俗理解">通俗理解</h4>
<p>可以通俗的理解为：两个变量在变化过程中是同方向变化？还是反方向变化？同向或反向程度如何？
你变大，同时我也变大，说明两个变量是同向变化的，这时协方差就是正的。
你变大，同时我变小，说明两个变量是反向变化的，这时协方差就是负的。
从数值来看，协方差的数值越大，两个变量同向程度也就越大。反之亦然。
通俗易懂的理解看<a href="https://www.zhihu.com/question/20852004" rel="external" target="_blank">知乎文章</a> 或者 <a href="https://raw.githubusercontent.com/lzeqian/machinelearn/master/learn_algorithm/%E5%A6%82%E4%BD%95%E9%80%9A%E4%BF%97%E6%98%93%E6%87%82%E5%9C%B0%E8%A7%A3%E9%87%8A%E3%80%8C%E5%8D%8F%E6%96%B9%E5%B7%AE%E3%80%8D%E4%B8%8E%E3%80%8C%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E3%80%8D%E7%9A%84%E6%A6%82%E5%BF%B5.png" rel="external" target="_blank">gitlab转载</a></p>
<h4 id="协方差矩阵">协方差矩阵</h4>
<p>协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。 这个解释摘自维基百科，看起来很是抽象，不好理解。其实简单来讲，协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减）。而协方差矩阵，只是将所有变量的协方差关系用矩阵的形式表现出来而已。通过矩阵这一工具，可以更方便地进行数学运算。
<strong>数学定义</strong>
回想概率统计里面关于方差的数学定义：
<a href="#R-image-63a46aab781effd2fadab79b0dac0660" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/c47bb97596c055e595e5970eb82c2795.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-63a46aab781effd2fadab79b0dac0660"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/c47bb97596c055e595e5970eb82c2795.png"></a>
协方差的数学定义异曲同工：
<a href="#R-image-16ec4c8dee25ae92e354198af8707975" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/fdb8c3fd4a067249f71627a1555e9b8f.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-16ec4c8dee25ae92e354198af8707975"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/fdb8c3fd4a067249f71627a1555e9b8f.png"></a>
这里的 x和y表示两个变量空间。用机器学习的话讲，就是样本有 x和 y两种特征，
而 X 就是包含所有样本的 x特征的集合，
Y就是包含所有样本的 y特征的集合。
用一个例子来解释会更加形象。
<a href="#R-image-6259438fe933d30cbc4b015e94d1f6f0" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/6925227ee279b3b084078e39c5fc4eab.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-6259438fe933d30cbc4b015e94d1f6f0"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/6925227ee279b3b084078e39c5fc4eab.png"></a>
用一个矩阵表示为：
<a href="#R-image-d4756f64be5ea36060d1675e5b192dcf" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/d8440298fe8204f7c1ad37a7d8205402.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-d4756f64be5ea36060d1675e5b192dcf"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/d8440298fe8204f7c1ad37a7d8205402.png"></a>
现在，我们用两个变量空间X ，Y 来表示这两个特征：
<a href="#R-image-cc1361a715911f31fc37507d225859c6" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/4c19ba09ef917699ff8e226e094ea684.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-cc1361a715911f31fc37507d225859c6"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/4c19ba09ef917699ff8e226e094ea684.png"></a>
由于协方差反应的是两个变量之间的相关性，因此，协方差矩阵表示的是所有变量之间两两相关的关系，具体来讲，一个包含两个特征的矩阵，其协方差矩阵应该有 2*2 大小：
<a href="#R-image-7223df628b05f83e925f9643f92963a9" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/e7f66c48a2edbad38913c02616b3ab81.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7223df628b05f83e925f9643f92963a9"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/e7f66c48a2edbad38913c02616b3ab81.png"></a>
接下来，就来逐一计算 Cov(Z)的值。 首先，我们需要先计算出 X，Y  两个特征空间的平均值：
AVG(X)=3.25,AVG(Y)=3 ，  。 然后，根据协方差的数学定义，计算协方差矩阵的每个元素：
<a href="#R-image-caa0854e5a2c9bdf1099c4ceb40e6116" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/404f5a07cccb75f6bf102f6863040157.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-caa0854e5a2c9bdf1099c4ceb40e6116"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/404f5a07cccb75f6bf102f6863040157.png"></a>
所以协方差矩阵
<a href="#R-image-104c442ea2a015f6dabe8572e2e31981" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/254202ba81a740d449d5fc7e2e96c8e8.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-104c442ea2a015f6dabe8572e2e31981"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/254202ba81a740d449d5fc7e2e96c8e8.png"></a>
好了，虽然这只是一个二维特征的例子，但我们已经可以从中总结出协方差矩阵
的「计算套路」：
<a href="#R-image-1d5aa9b8e270d7b89a7c4bdc0ccaa343" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/7ae4fe089b617e639500052edd304d5a.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-1d5aa9b8e270d7b89a7c4bdc0ccaa343"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/7ae4fe089b617e639500052edd304d5a.png"></a>
python协方差原理</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code># 协方差主要是多个特征
pa=np.array([
     [1,2] ,
     [3,6] ,
     [4,2] ,
     [5,2] 
   ])
&#39;&#39;&#39;
 x应该是个二维矩阵表示
 [
     [1] ,
     [3] ,
     [4] ,
     [5] 
   ]
&#39;&#39;&#39;
x=np.array([pa[:,0]]).reshape((4,1))
&#39;&#39;&#39;
 y应该是个二维矩阵表示
 [
     [2] ,
     [6] ,
     [2] ,
     [2] 
   ]
&#39;&#39;&#39;
y=np.array([pa[:,1]]).reshape((4,1))
print(&#34;分别获取X和Y:&#34;,x,y)
x_mean=np.mean(x)
y_mean=np.mean(y)
print(&#34;x和y特征的均值&#34;,x_mean,y_mean)
&#39;&#39;&#39;
这里只是求第一个特征x（第一列）和第二个特征(第二列)的方差cov(x,y)，
，实际还有cov(x,x),cov(y,x),cov(y,y)
x-x_mean转置T就变成了
 [
     [1-xmean,3-xmean,4-xmean,5-xmean] 
 ]
y-ymean是
 [
     [2-ymean] ,
     [6-ymean] ,
     [2-ymean] ,
     [2-ymean] 
 ]
(x-x_mean).T.dot(y-y_mean)就变成了矩阵乘法了

（1-xmean）*（2-ymean）+（3-xmean）*（6-ymean）+（4-xmean）*（2-ymean）+（5-xmean）*（2-ymean）
然后除以n-1就是协方差了cov(x,y)
&#39;&#39;&#39;
print(&#34;cov(x,y)=&#34;,np.sum((x-x_mean).T.dot(y-y_mean))/(len(pa)-1))

#数学表示横表示列，竖表示行，默认行横表示特征
&#39;&#39;&#39;
[[1 3 4 5]
 [2 6 2 2]]
&#39;&#39;&#39;
print(pa.T)
print(&#34;conv&#34;,np.cov(pa.T))
# 使用rowvar=False，表示列是变量是特征
print(&#34;conv&#34;,np.cov(pa,rowvar=False))</code></pre></div>
<p>输出结果可查看：<a href="https://github.com/lzeqian/machinelearn/blob/master/learn_numpy/%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97.ipynb" rel="external" target="_blank">github</a></p>
<blockquote>
<p>方差是一种特殊的协方差，协方差=cov(x,x)
熟悉协方差概念方便理解矩阵构造</p></blockquote>
<h1 id="2-pca">2. PCA</h1>
<h2 id="概念-1">概念</h2>
<p>主成分分析(Principal Component Analysis):</p>
<ul>
<li>一个非监督的机器学习算法</li>
<li>主要用于数据的降维</li>
<li>通过降维，可以发现更便于人类理解的特征</li>
<li>其他应用：可视化，降噪</li>
</ul>
<p>假设存在一根直线，将所有的点都映射在该条指直线上，这样的话点的整体分布和原来的点的分布就没有很大的差异（点和点的距离比映射到x轴或者映射到y轴都要大，区分度就更加明显），与此同时所有的点都在一个轴上（理解成一个维度），虽然这个轴是斜着的。用这种方式将二维降到了一维度</p>
<p>下表1是某些学生的语文、数学、物理、化学成绩统计：</p>
<table>
  <thead>
      <tr>
          <th>学生姓名</th>
          <th>语文</th>
          <th>数学</th>
          <th>英文</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>张三</td>
          <td>90</td>
          <td>87</td>
          <td>75</td>
      </tr>
      <tr>
          <td>李四</td>
          <td>90</td>
          <td>50</td>
          <td>76</td>
      </tr>
      <tr>
          <td>王五</td>
          <td>90</td>
          <td>99</td>
          <td>70</td>
      </tr>
      <tr>
          <td>赵六</td>
          <td>90</td>
          <td>60</td>
          <td>80</td>
      </tr>
  </tbody>
</table>
<p>首先，假设这些科目成绩不相关，那么怎么区分谁的成绩好了，明显语文和英文大家都差不多，数学上就拉开了差距，数学就可以理解为主成分。</p>
<h2 id="理论">理论</h2>
<p>先看下面这幅图：
<a href="#R-image-12a177f640e5b21a106ee13a74c92bf6" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/f346783456a8e701ca310be61b1cfbfe.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-12a177f640e5b21a106ee13a74c92bf6"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/f346783456a8e701ca310be61b1cfbfe.png"></a>
先假定特征只有二维，即只有两个变量，它们由横坐标和纵坐标所代表；因此每个观测值都有相应于这两个坐标轴的两个坐标值；如果这些数据形成一个椭圆形状的点阵，那么这个椭圆有一个长轴和一个短轴。在短轴方向上，数据变化很少；在极端的情况，短轴如果退化成一点，那只有在长轴的方向才能够解释这些点的变化了；这样，由二维到一维的降维就自然完成了。</p>
<p>上图中，u1就是主成分方向，然后在二维空间中取和u1方向正交的方向，就是u2的方向。则n个数据在u1轴的离散程度最大（方差最大），数据在u1上的投影代表了原始数据的绝大部分信息，即使不考虑u2，信息损失也不多。而且，u1、u2不相关。只考虑u1时，二维降为一维。</p>
<p>椭圆的长短轴相差得越大，降维也越有道理。</p>
<h2 id="公式推导">公式推导</h2>
<p>那么如何找到这个让样本间距最大的轴？</p>
<p>如何定义样本间间距？ 使用方差(Variance)
<a href="#R-image-5a426631650bf92bf828ad9eb0bf3eab" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/b2c6ae46a68989fc6c7d25d87cca5d11.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-5a426631650bf92bf828ad9eb0bf3eab"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/b2c6ae46a68989fc6c7d25d87cca5d11.png"></a>
方差越大代表样本之间越稀疏，方差越小代表样本之间越紧密。
移动坐标轴，使得样本在每一个维度均值都为0：</p>
<p>创建一个3x+4线性附近20个随机样本，样例和结果：https://github.com/lzeqian/machinelearn/blob/master/sklean_pca/demean.ipynb</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>import numpy as np;
import matplotlib.pyplot as plot
from commons.common import setXY
#生成一个 3x+4附近的点
np.random.seed(100);
# 获取randc个随机点
randc=20
x1=np.random.rand(randc);
x2=x1*3+4+np.random.rand(randc);
plot.plot(x1,x2,&#34;o&#34;);</code></pre></div>
<p><a href="#R-image-524c6e6774ae0a3e016c36ec363074ed" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/7ae5cc00619dc88f610da94823fcd20e.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-524c6e6774ae0a3e016c36ec363074ed"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/7ae5cc00619dc88f610da94823fcd20e.png"></a>
转换为矩阵表示</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#使用矩阵数组表示,x1,x2  x1和x2是两个特征
&#39;&#39;&#39;
[   [x1,x2]
    [1,2],
    [3,4]
]
X=x1.reshape(-1,1); 等价于x1.reshape(len(x1),1)
[1,2]转换为
x1=[
  [1],
  [2]
]
x2=[
  [4],
  [5]
]
x1.hstack(x2)
[
  [1,4],
  [2,5]
]
&#39;&#39;&#39;
X=np.hstack((x1.reshape(len(x1),1),x2.reshape(len(x2),1)))
print(X)
[[0.54340494 6.06191901]
 [0.27836939 5.77513797]
 [0.42451759 6.09120215]
 [0.84477613 6.87044035]
 [0.00471886 4.18956702]
 [0.12156912 4.73753941]
 [0.67074908 6.01793576]
 [0.82585276 6.72998462]
 [0.13670659 5.20578228]
 [0.57509333 5.74053496]
 [0.89132195 7.27280924]
 [0.20920212 5.23141091]
 [0.18532822 4.66113234]
 [0.10837689 4.70707412]
 [0.21969749 4.69556853]
 [0.97862378 7.82628292]
 [0.81168315 7.4159703 ]
 [0.17194101 4.57576503]
 [0.81622475 7.33922019]
 [0.27407375 5.39912274]]</code></pre></div>
<p>demean，均值归0处理</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>np.set_printoptions(suppress=True) #不使用科学计数法
setXY()
def demean(X):
    return X-np.mean(X,axis=0) #取对应列的均值
#均值归零的算法是x1-xmean，x2-x2mean
X_demean=demean(X)
plot.plot(X_demean[:,0],X_demean[:,1],&#34;o&#34;);</code></pre></div>
<p><a href="#R-image-be0e161307024a371bc7bfaa2d842a54" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/ac681b32c52517e4e4902e7f93c72d5c.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-be0e161307024a371bc7bfaa2d842a54"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/ac681b32c52517e4e4902e7f93c72d5c.png"></a>
demean之后的方差最大其实就是求映射后每个点到(0,0)的距离最大再求和，假设降维后轴的方向是w=(w1, w2)
<a href="#R-image-7a567ebcb33942b1c90be210bb3a1daf" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/ba2ff38292cf0c3db56a18df8775997d.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7a567ebcb33942b1c90be210bb3a1daf"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/ba2ff38292cf0c3db56a18df8775997d.png"></a>
，Xi是映射前的向量，Xi(project)是映射后的向量，这里注意w向量是单位向量 |w|=1
<a href="#R-image-ee4f166e5afae43ed63ff7d32b7a1282" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/d0b980850a29292f0b6c57e2c45412ce.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-ee4f166e5afae43ed63ff7d32b7a1282"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/d0b980850a29292f0b6c57e2c45412ce.png"></a>
以上是推导公式
目标函数是：
<a href="#R-image-973321bfc04b9cef69bea5b290b5c810" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/65e00e762152889eb062264fde120fd2.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-973321bfc04b9cef69bea5b290b5c810"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/65e00e762152889eb062264fde120fd2.png"></a>
通过公司可知
<a href="#R-image-ac6c6046dd955cf744ec5276b8f327d1" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/8569b5586ccb2f512a43d5fc85bae7ab.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-ac6c6046dd955cf744ec5276b8f327d1"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/8569b5586ccb2f512a43d5fc85bae7ab.png"></a></p>
<blockquote>
<p><!-- raw HTML omitted -->注意i是样本索引，下标1，2是特征数，x1是特征1，x2是特征2  xj是特征j，每个特征xj对应一个维度wj<!-- raw HTML omitted --></p></blockquote>
<p>目标函数即：
<a href="#R-image-f414fa4701a620bece6ac57250feecc9" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/2f5c527579fdc683110708467201b75f.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f414fa4701a620bece6ac57250feecc9"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/2f5c527579fdc683110708467201b75f.png"></a>
对目标函数求梯度：
<a href="#R-image-280dd485dce2faa5f5fe5efd932d04f9" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/1520a9604a9108661085456861cca3db.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-280dd485dce2faa5f5fe5efd932d04f9"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/1520a9604a9108661085456861cca3db.png"></a>
转化为：
<a href="#R-image-3977b29d6cf2563eed088b7e11f87d76" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/ee261b01d6ea0bc17e3a9b33fbe08634.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-3977b29d6cf2563eed088b7e11f87d76"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/ee261b01d6ea0bc17e3a9b33fbe08634.png"></a>
由于最终转换的结果是一个1行m列的矩阵，而我们想要得到一个n行1列的矩阵，所以还要进行一次转置</p>
<p><a href="#R-image-0713e6b52224b50e819bd74ebdcf682c" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/3b9d2f2215be5b10b44b41b94bea85d3.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-0713e6b52224b50e819bd74ebdcf682c"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/3b9d2f2215be5b10b44b41b94bea85d3.png"></a></p>
<h2 id="编程实现第一主成分">编程实现（第一主成分）</h2>
<p>产生一个 3x+4附近的点</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>import numpy as np;
import matplotlib.pyplot as plot
from commons.common import setXY
#生成一个 3x+4附近的点
#np.random.seed(100);
# 获取randc个随机点
randc=100
# 0-1的数*多少倍，注意太小的样本点，abs(f(w=w, X=X) - f(w=last_w, X=X)) 的值增量过小可能导致循环次数后，还没有到&lt;epsilon导致拟合不准确
# 如果是1，建议循环次数加大100000
# 如果是100 可以设置为100
blow=100
x1=np.random.rand(randc)*blow;
x2=x1*3+4+np.random.rand(randc)*blow;
plot.plot(x1,x2,&#34;o&#34;);
X=np.hstack((x1.reshape(len(x1),1),x2.reshape(len(x2),1)))
np.set_printoptions(suppress=True) #不使用科学计数法</code></pre></div>
<p>定义目标函数</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code># 这是目标函数 np.sum（（X*W）**2)/M
# 注意目标函数是传入X是已知的数据样本，w是个2个特征向量 ，f(w1,w2)是个三位空间

def f(X,w):
    return np.sum((X.dot(w))**2)/len(X)</code></pre></div>
<p>获取w在各个特征的导数</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code># 获取各个维度的导数    
def df_w(X,w):
    return X.T.dot(X.dot(w))*2/len(X)</code></pre></div>
<p>也可以用这个通用的方法求导数</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>&#39;&#39;&#39;
通用计算某个点的斜率的方法
为了验证我们的这个是正确的，使用这个df_debug这个函数，
和线性下降法一样，使两个点之间连成的直线不断的靠近应得的直线，
使其斜率相当，注意的是，这里的epsilon取值比较小，是因为在PCA的梯度上升法中，
w是一个方向向量，其模为1，所以w的每一个维度其实都很小，那么为了适应，相应的epsilon也要小一些
&#39;&#39;&#39;
def df_debug(w,X,epsilon=0.0001):
      res = np.empty(len(w))
      for i in range(len(w)):
          w_1 = w.copy()
          w_1[i] += epsilon
          w_2 = w.copy()
          w_2[i] -= epsilon
          res[i] = (f(w=w_1,X=X) - f(w=w_2,X=X)) / (2*epsilon)</code></pre></div>
<p>将w向量转换为单位向量</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code># 将任意向量转换为单位向量 np.linalg.norm(w)是 x**2+x1**2开根号
# (3,4)/5=(3/5,4/5)就是单位向量，模是1
def direction(w):
    return w / np.linalg.norm(w)</code></pre></div>
<p>梯度上升测试</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>wapp=np.array([])
def gradient_ascent(df, X, initial_w, eta, n_iters=1e4, epsilon=1e-8):
    w = direction(initial_w)
    i_iter = 1
    global wapp
    wapp=np.append(wapp,w).reshape((1,len(w)))
    while i_iter &lt; n_iters:
        gradient = df(w=w, X=X)
        last_w = w
        # gradient是对每个维度求偏导得到的列表，如果偏导数为负则w的这个维度加上一个负值，降维后的方差趋于变大
        # 如果偏导数为正，则w的这个维度加上一个正值，降维后的方差趋于变大，因此w加上导数值，降维后的方差趋于变大
        # 在eta合适的情况下，随着循环进行，导数值逐渐趋近0，eta是常数，降维后的方差的变化量会越来越小
        w = w + eta * gradient
        w = direction(w)  # 注意1，每次求一个单位向量
        wapp=np.vstack((wapp,np.array([w])))
        # abs求绝对值
        if (abs(f(w=w, X=X) - f(w=last_w, X=X)) &lt; epsilon):
            print(&#34;精度：&#34;,abs(f(w=w, X=X) - f(w=last_w, X=X)),epsilon)
            print(&#34;梯度&#34;,gradient)
            
            break
        i_iter += 1
    return w


initial_w = np.random.random(X.shape[1])  # 注意2：不能用0向量开始
eta = 0.0001
# print(gradient_ascent(df_debug, X=X_demean, initial_w=initial_w, eta=eta))
w=(gradient_ascent(df_w, X=X_demean, initial_w=initial_w, eta=eta, n_iters=100))
setXY()
plot.plot(X_demean[:,0],X_demean[:,1],&#34;o&#34;);
print(w)
# 单位向量乘同一个，方向是相同的
plot.plot([0,w[0]*(blow)],[0,w[1]*blow])
#plot.plot(X_demean[:,0],w[1]/w[0]*X_demean[:,0])
#%%
fig = plot.figure()
#创建梯度上升的过程
ax = plot.axes(projection=&#39;3d&#39;)
wappy=[f(w=w_t, X=X) for w_t in wapp]
ax.plot3D(wapp[:,0],wapp[:,1],wappy, &#39;red&#39;)
print(&#34;w1值&#34;,wapp[:,0])
print(&#34;w2值&#34;,wapp[:,1])
print(&#34;方差：&#34;,wappy)

ax.set_title(&#39;3D line plot&#39;)
plot.show()</code></pre></div>
<p>拟合的直线
<a href="#R-image-7e0fdc4ff8188ecc5a9c69b5518734fb" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/f38c0937db779fc160fc4f8a234ae0cb.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7e0fdc4ff8188ecc5a9c69b5518734fb"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/f38c0937db779fc160fc4f8a234ae0cb.png"></a>
梯度上升过程
<a href="#R-image-8c98ddd0dcabaa24582133772a840ad0" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/7c36504a5cd47247dc106ac39ff6d2dc.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-8c98ddd0dcabaa24582133772a840ad0"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/machine_learning/algorithms/action_04_pca.md.images/7c36504a5cd47247dc106ac39ff6d2dc.png"></a></p>

  <footer class="footline">
              <i class='fa-fw fas fa-calendar'></i> Sep 18, 2025
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          <a id="R-logo" class="R-default" href="/docs/index.html">
            <div class="logo-title">liaomin416100569博客</div>
          </a>
        </div>
        <search><form action="/docs/search/index.html" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-homelinks">
          <ul class="space collapsible-menu">
            <li class="" data-nav-id="/docs/index.html"><a class="padding" href="/docs/index.html"><i class="fa-fw fas fa-home"></i> Home</a></li>
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-headercontrols">
          <ul class="">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div class="R-sidebarmenu R-shortcutmenu-main">
          <ul class="enlarge morespace collapsible-menu">
            <li class="parent " data-nav-id="/docs/programming/index.html"><a class="padding" href="/docs/programming/index.html">编程开发</a><ul id="R-subsections-e3fc01b477dbaf64a8f5013a3dab5c5b" class="collapsible-menu">
            <li class="alwaysopen " data-nav-id="/docs/programming/languages/index.html"><a class="padding" href="/docs/programming/languages/index.html">编程语言</a><ul id="R-subsections-1bbde7fb0c312ba940b425df5a4caf67" class="collapsible-menu"></ul></li>
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/index.html"><a class="padding" href="/docs/programming/ai/index.html">人工智能</a><ul id="R-subsections-9d06be7bd8c736c09a65fb0b91b71d0e" class="collapsible-menu">
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/tools_libraries/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/index.html">工具库</a><ul id="R-subsections-e43804740042696aa314af8cc1e28fa9" class="collapsible-menu"></ul></li>
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/machine_learning/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/index.html">机器学习</a><ul id="R-subsections-d3b98ca0beda96811b8c41829d886d7f" class="collapsible-menu">
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/machine_learning/basic/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/basic/index.html">基础理论</a><ul id="R-subsections-2f18a18645b7652a148815c1a6786b18" class="collapsible-menu"></ul></li>
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/machine_learning/algorithms/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/index.html">核心算法</a><ul id="R-subsections-921418d1d7190828278c689c88df6881" class="collapsible-menu">
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_01_knn/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_01_knn/index.html">机器学习实战教程（一）：K-近邻（KNN）算法</a></li>
            <li class="active " data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_04_pca/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_04_pca/index.html">机器学习实战教程（四）：从特征分解到协方差矩阵：详细剖析和实现PCA算法</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_05_pcaface/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_05_pcaface/index.html">机器学习实战教程（五）：使用PCA实战人脸降维</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_06_decidetree/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_06_decidetree/index.html">机器学习实战教程（六）：决策树</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_07_bays/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_07_bays/index.html">机器学习实战教程（七）：朴素贝叶斯</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_08_multinomial/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_08_multinomial/index.html">机器学习实战教程（八）：多项式回归</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_10_logic/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_10_logic/index.html">机器学习实战教程（十）：逻辑回归</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_11_vectormachine/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_11_vectormachine/index.html">机器学习实战教程（十一）：支持向量机SVM</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_12_cluster/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_12_cluster/index.html">机器学习实战教程（十二）：聚类算法Kmeans</a></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/algorithms/action_13_intelearn/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/algorithms/action_13_intelearn/index.html">机器学习实战教程（十三）：集成学习</a></li></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/machine_learning/evaluation/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/evaluation/index.html">模型评估</a><ul id="R-subsections-d83b378e097742dc59073d128d99d653" class="collapsible-menu"></ul></li>
            <li class="" data-nav-id="/docs/programming/ai/machine_learning/tools/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/tools/index.html">实践工具</a></li></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/deep_learning/index.html"><a class="padding" href="/docs/programming/ai/deep_learning/index.html">深度学习</a><ul id="R-subsections-8e4f2a2c63b9f66a19e3b2a7c957ccda" class="collapsible-menu"></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/computer_vision/index.html"><a class="padding" href="/docs/programming/ai/computer_vision/index.html">计算机视觉</a><ul id="R-subsections-ee78ef5588610a65894e6d07832cb0b2" class="collapsible-menu"></ul></li></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/plugins/index.html"><a class="padding" href="/docs/programming/plugins/index.html">插件开发</a><ul id="R-subsections-de66f54cff99288ca68bfcb5bb0439ae" class="collapsible-menu"></ul></li></ul></li>
            <li class="" data-nav-id="/docs/devops/index.html"><a class="padding" href="/docs/devops/index.html">运维一体化</a><ul id="R-subsections-389d4feb4920b919bcbc0b1e9947dace" class="collapsible-menu"></ul></li>
            <li class="" data-nav-id="/docs/security/index.html"><a class="padding" href="/docs/security/index.html">安全攻防</a><ul id="R-subsections-66815ecaaecfc1c209e5637d03b258b2" class="collapsible-menu"></ul></li>
          </ul>
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-shortcuts">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div id="R-footer-margin"></div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-footercontrols">
          <ul class="">
          </ul>
        </div>
<div id="R-footer"><p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p></div>
      </div>
    </aside>
    <script src="/docs/js/clipboard/clipboard.min.js?1758355652" defer></script>
    <script src="/docs/js/perfect-scrollbar/perfect-scrollbar.min.js?1758355652" defer></script>
    <script src="/docs/js/theme.min.js?1758355652" defer></script>
  </body>
</html>
