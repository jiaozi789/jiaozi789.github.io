<!DOCTYPE html>
<html lang="zh" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="html">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.150.0">
    <meta name="generator" content="Relearn 8.0.1+b23cf6629eada0c2802f34ae4012e04343497862">
    <meta name="description" content="yolov5 YOLOv5 是一种目标检测算法，它是 YOLO (You Only Look Once) 系列算法的最新版本。YOLOv5 采用了一种新的架构，它包括一个基于 CSPNet (Cross Stage Partial Network) 的主干网络以及一系列改进的技巧，如多尺度训练、数据增强、网络混合精度训练等，从而实现了更快的检测速度和更好的检测精度。
YOLOv5 支持多种类型的目标检测任务，如物体检测、人脸检测、车辆检测等，可以应用于各种实际场景，如智能安防、自动驾驶、机器人视觉等。同时，YOLOv5 还提供了预训练的模型和开源代码，方便开发者进行模型的训练和应用。
github地址：https://github.com/ultralytics/yolov5/blob/master/README.zh-CN.md 官网：https://ultralytics.com/
发展历程 YOLO（You Only Look Once）是一系列的目标检测模型，由Joseph Redmon等人开发。以下是YOLO系列的发展历程：
YOLOv1：于2015年首次提出，是YOLO系列的第一个版本。YOLOv1通过将目标检测任务转化为回归问题，将图像划分为网格并预测每个网格的边界框和类别概率。然而，YOLOv1存在定位不准确和对小目标敏感的问题。
YOLOv2（YOLO9000）：于2016年提出，是YOLO系列的第二个版本。YOLOv2通过引入Darknet-19网络结构、使用anchor boxes和多尺度预测来改进检测性能。同时，YOLOv2还引入了目标类别的语义分割，可以检测更多类别的目标。
YOLOv3：于2018年提出，是YOLO系列的第三个版本。YOLOv3针对YOLOv2存在的问题进行了改进，引入了多尺度预测、使用FPN结构和使用更小的anchor boxes等技术，提高了检测精度和对小目标的检测能力。
YOLOv4：于2020年提出，是YOLO系列的第四个版本。YOLOv4在YOLOv3的基础上引入了一系列改进，包括CSPDarknet53作为主干网络、使用SAM和PANet模块来提取特征、使用YOLOv3和YOLOv4的预训练权重进行初始化等，提高了检测性能和速度。
YOLOv5：于2020年提出，是YOLO系列的第五个版本。YOLOv5采用了轻量化的网络结构，提高了检测的速度，并引入了一些新功能，如YOLOv5-seg分割模型、Paddle Paddle导出功能、YOLOv5 AutoCache自动缓存功能和Comet日志记录和可视化集成功能。
总体而言，YOLO系列模型通过不断的改进和优化，提高了目标检测的性能和速度，并在计算机视觉领域取得了重要的突破。
yolov8 YOLOv8是YOLO系列模型的一个变种，它在YOLOv5的基础上进行了改进和优化。YOLOv8模型包含了检测（Detect）、分割（Segment）和姿态估计（Pose）、跟踪（Track）以及分类（Classify）等功能。下面是对这些功能的简要说明：
检测（Detect）：YOLOv8模型能够对图像或视频中的目标进行实时的物体检测。它通过预测目标的边界框和类别信息来完成检测任务。
分割（Segment）：YOLOv8模型还支持目标分割的功能，即将图像中的每个像素进行分类，将不同的目标区域进行分割。这个功能可以用于识别图像中的不同物体，并进行更精确的定位和分析。
姿态估计（Pose）：YOLOv8模型还可以对检测到的目标进行姿态估计，即推断目标在三维空间中的姿态信息。这对于一些需要了解目标的方向和位置的应用非常有用，比如人体姿态分析、机器人导航等。
跟踪（Track）：YOLOv8模型还具有目标跟踪的功能，即在视频中连续追踪相同目标的位置和轨迹。这对于视频监控、自动驾驶等应用非常重要。
分类（Classify）：除了目标检测和分割功能之外，YOLOv8模型还可以对检测到的目标进行分类，即给出目标的类别信息。这对于了解目标的属性和进行更细粒度的分析非常重要。
总而言之，YOLOv8模型综合了多种功能，包括检测、分割、姿态估计、跟踪和分类等，使其具备了更广泛的应用领域和更强大的功能。 github地址：https://github.com/ultralytics/ultralytics
v5入门示例 安装 克隆 repo，并要求在 Python&gt;=3.7.0 环境中安装 requirements.txt ，且要求 PyTorch&gt;=1.7 。
micromamba create prefix=d:/python380 python=3.8 #创建3.8的虚拟环境micromamba activate d:/python380git clone https://github.com/ultralytics/yolov5 # clonecd yolov5pip install -r requirements.txt # install 源代码目录结构">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="图像处理实战02-yolov5目标检测 :: liaomin416100569博客">
    <meta name="twitter:description" content="yolov5 YOLOv5 是一种目标检测算法，它是 YOLO (You Only Look Once) 系列算法的最新版本。YOLOv5 采用了一种新的架构，它包括一个基于 CSPNet (Cross Stage Partial Network) 的主干网络以及一系列改进的技巧，如多尺度训练、数据增强、网络混合精度训练等，从而实现了更快的检测速度和更好的检测精度。
YOLOv5 支持多种类型的目标检测任务，如物体检测、人脸检测、车辆检测等，可以应用于各种实际场景，如智能安防、自动驾驶、机器人视觉等。同时，YOLOv5 还提供了预训练的模型和开源代码，方便开发者进行模型的训练和应用。
github地址：https://github.com/ultralytics/yolov5/blob/master/README.zh-CN.md 官网：https://ultralytics.com/
发展历程 YOLO（You Only Look Once）是一系列的目标检测模型，由Joseph Redmon等人开发。以下是YOLO系列的发展历程：
YOLOv1：于2015年首次提出，是YOLO系列的第一个版本。YOLOv1通过将目标检测任务转化为回归问题，将图像划分为网格并预测每个网格的边界框和类别概率。然而，YOLOv1存在定位不准确和对小目标敏感的问题。
YOLOv2（YOLO9000）：于2016年提出，是YOLO系列的第二个版本。YOLOv2通过引入Darknet-19网络结构、使用anchor boxes和多尺度预测来改进检测性能。同时，YOLOv2还引入了目标类别的语义分割，可以检测更多类别的目标。
YOLOv3：于2018年提出，是YOLO系列的第三个版本。YOLOv3针对YOLOv2存在的问题进行了改进，引入了多尺度预测、使用FPN结构和使用更小的anchor boxes等技术，提高了检测精度和对小目标的检测能力。
YOLOv4：于2020年提出，是YOLO系列的第四个版本。YOLOv4在YOLOv3的基础上引入了一系列改进，包括CSPDarknet53作为主干网络、使用SAM和PANet模块来提取特征、使用YOLOv3和YOLOv4的预训练权重进行初始化等，提高了检测性能和速度。
YOLOv5：于2020年提出，是YOLO系列的第五个版本。YOLOv5采用了轻量化的网络结构，提高了检测的速度，并引入了一些新功能，如YOLOv5-seg分割模型、Paddle Paddle导出功能、YOLOv5 AutoCache自动缓存功能和Comet日志记录和可视化集成功能。
总体而言，YOLO系列模型通过不断的改进和优化，提高了目标检测的性能和速度，并在计算机视觉领域取得了重要的突破。
yolov8 YOLOv8是YOLO系列模型的一个变种，它在YOLOv5的基础上进行了改进和优化。YOLOv8模型包含了检测（Detect）、分割（Segment）和姿态估计（Pose）、跟踪（Track）以及分类（Classify）等功能。下面是对这些功能的简要说明：
检测（Detect）：YOLOv8模型能够对图像或视频中的目标进行实时的物体检测。它通过预测目标的边界框和类别信息来完成检测任务。
分割（Segment）：YOLOv8模型还支持目标分割的功能，即将图像中的每个像素进行分类，将不同的目标区域进行分割。这个功能可以用于识别图像中的不同物体，并进行更精确的定位和分析。
姿态估计（Pose）：YOLOv8模型还可以对检测到的目标进行姿态估计，即推断目标在三维空间中的姿态信息。这对于一些需要了解目标的方向和位置的应用非常有用，比如人体姿态分析、机器人导航等。
跟踪（Track）：YOLOv8模型还具有目标跟踪的功能，即在视频中连续追踪相同目标的位置和轨迹。这对于视频监控、自动驾驶等应用非常重要。
分类（Classify）：除了目标检测和分割功能之外，YOLOv8模型还可以对检测到的目标进行分类，即给出目标的类别信息。这对于了解目标的属性和进行更细粒度的分析非常重要。
总而言之，YOLOv8模型综合了多种功能，包括检测、分割、姿态估计、跟踪和分类等，使其具备了更广泛的应用领域和更强大的功能。 github地址：https://github.com/ultralytics/ultralytics
v5入门示例 安装 克隆 repo，并要求在 Python&gt;=3.7.0 环境中安装 requirements.txt ，且要求 PyTorch&gt;=1.7 。
micromamba create prefix=d:/python380 python=3.8 #创建3.8的虚拟环境micromamba activate d:/python380git clone https://github.com/ultralytics/yolov5 # clonecd yolov5pip install -r requirements.txt # install 源代码目录结构">
    <meta property="og:url" content="https://jiaozi789.github.io/docs/programming/ai/computer_vision/applications/action_02_yolov5/index.html">
    <meta property="og:site_name" content="liaomin416100569博客">
    <meta property="og:title" content="图像处理实战02-yolov5目标检测 :: liaomin416100569博客">
    <meta property="og:description" content="yolov5 YOLOv5 是一种目标检测算法，它是 YOLO (You Only Look Once) 系列算法的最新版本。YOLOv5 采用了一种新的架构，它包括一个基于 CSPNet (Cross Stage Partial Network) 的主干网络以及一系列改进的技巧，如多尺度训练、数据增强、网络混合精度训练等，从而实现了更快的检测速度和更好的检测精度。
YOLOv5 支持多种类型的目标检测任务，如物体检测、人脸检测、车辆检测等，可以应用于各种实际场景，如智能安防、自动驾驶、机器人视觉等。同时，YOLOv5 还提供了预训练的模型和开源代码，方便开发者进行模型的训练和应用。
github地址：https://github.com/ultralytics/yolov5/blob/master/README.zh-CN.md 官网：https://ultralytics.com/
发展历程 YOLO（You Only Look Once）是一系列的目标检测模型，由Joseph Redmon等人开发。以下是YOLO系列的发展历程：
YOLOv1：于2015年首次提出，是YOLO系列的第一个版本。YOLOv1通过将目标检测任务转化为回归问题，将图像划分为网格并预测每个网格的边界框和类别概率。然而，YOLOv1存在定位不准确和对小目标敏感的问题。
YOLOv2（YOLO9000）：于2016年提出，是YOLO系列的第二个版本。YOLOv2通过引入Darknet-19网络结构、使用anchor boxes和多尺度预测来改进检测性能。同时，YOLOv2还引入了目标类别的语义分割，可以检测更多类别的目标。
YOLOv3：于2018年提出，是YOLO系列的第三个版本。YOLOv3针对YOLOv2存在的问题进行了改进，引入了多尺度预测、使用FPN结构和使用更小的anchor boxes等技术，提高了检测精度和对小目标的检测能力。
YOLOv4：于2020年提出，是YOLO系列的第四个版本。YOLOv4在YOLOv3的基础上引入了一系列改进，包括CSPDarknet53作为主干网络、使用SAM和PANet模块来提取特征、使用YOLOv3和YOLOv4的预训练权重进行初始化等，提高了检测性能和速度。
YOLOv5：于2020年提出，是YOLO系列的第五个版本。YOLOv5采用了轻量化的网络结构，提高了检测的速度，并引入了一些新功能，如YOLOv5-seg分割模型、Paddle Paddle导出功能、YOLOv5 AutoCache自动缓存功能和Comet日志记录和可视化集成功能。
总体而言，YOLO系列模型通过不断的改进和优化，提高了目标检测的性能和速度，并在计算机视觉领域取得了重要的突破。
yolov8 YOLOv8是YOLO系列模型的一个变种，它在YOLOv5的基础上进行了改进和优化。YOLOv8模型包含了检测（Detect）、分割（Segment）和姿态估计（Pose）、跟踪（Track）以及分类（Classify）等功能。下面是对这些功能的简要说明：
检测（Detect）：YOLOv8模型能够对图像或视频中的目标进行实时的物体检测。它通过预测目标的边界框和类别信息来完成检测任务。
分割（Segment）：YOLOv8模型还支持目标分割的功能，即将图像中的每个像素进行分类，将不同的目标区域进行分割。这个功能可以用于识别图像中的不同物体，并进行更精确的定位和分析。
姿态估计（Pose）：YOLOv8模型还可以对检测到的目标进行姿态估计，即推断目标在三维空间中的姿态信息。这对于一些需要了解目标的方向和位置的应用非常有用，比如人体姿态分析、机器人导航等。
跟踪（Track）：YOLOv8模型还具有目标跟踪的功能，即在视频中连续追踪相同目标的位置和轨迹。这对于视频监控、自动驾驶等应用非常重要。
分类（Classify）：除了目标检测和分割功能之外，YOLOv8模型还可以对检测到的目标进行分类，即给出目标的类别信息。这对于了解目标的属性和进行更细粒度的分析非常重要。
总而言之，YOLOv8模型综合了多种功能，包括检测、分割、姿态估计、跟踪和分类等，使其具备了更广泛的应用领域和更强大的功能。 github地址：https://github.com/ultralytics/ultralytics
v5入门示例 安装 克隆 repo，并要求在 Python&gt;=3.7.0 环境中安装 requirements.txt ，且要求 PyTorch&gt;=1.7 。
micromamba create prefix=d:/python380 python=3.8 #创建3.8的虚拟环境micromamba activate d:/python380git clone https://github.com/ultralytics/yolov5 # clonecd yolov5pip install -r requirements.txt # install 源代码目录结构">
    <meta property="og:locale" content="zh">
    <meta property="og:type" content="article">
    <meta property="article:section" content="编程开发">
    <meta property="article:published_time" content="2025-09-18T16:55:17+08:00">
    <meta property="article:modified_time" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="name" content="图像处理实战02-yolov5目标检测 :: liaomin416100569博客">
    <meta itemprop="description" content="yolov5 YOLOv5 是一种目标检测算法，它是 YOLO (You Only Look Once) 系列算法的最新版本。YOLOv5 采用了一种新的架构，它包括一个基于 CSPNet (Cross Stage Partial Network) 的主干网络以及一系列改进的技巧，如多尺度训练、数据增强、网络混合精度训练等，从而实现了更快的检测速度和更好的检测精度。
YOLOv5 支持多种类型的目标检测任务，如物体检测、人脸检测、车辆检测等，可以应用于各种实际场景，如智能安防、自动驾驶、机器人视觉等。同时，YOLOv5 还提供了预训练的模型和开源代码，方便开发者进行模型的训练和应用。
github地址：https://github.com/ultralytics/yolov5/blob/master/README.zh-CN.md 官网：https://ultralytics.com/
发展历程 YOLO（You Only Look Once）是一系列的目标检测模型，由Joseph Redmon等人开发。以下是YOLO系列的发展历程：
YOLOv1：于2015年首次提出，是YOLO系列的第一个版本。YOLOv1通过将目标检测任务转化为回归问题，将图像划分为网格并预测每个网格的边界框和类别概率。然而，YOLOv1存在定位不准确和对小目标敏感的问题。
YOLOv2（YOLO9000）：于2016年提出，是YOLO系列的第二个版本。YOLOv2通过引入Darknet-19网络结构、使用anchor boxes和多尺度预测来改进检测性能。同时，YOLOv2还引入了目标类别的语义分割，可以检测更多类别的目标。
YOLOv3：于2018年提出，是YOLO系列的第三个版本。YOLOv3针对YOLOv2存在的问题进行了改进，引入了多尺度预测、使用FPN结构和使用更小的anchor boxes等技术，提高了检测精度和对小目标的检测能力。
YOLOv4：于2020年提出，是YOLO系列的第四个版本。YOLOv4在YOLOv3的基础上引入了一系列改进，包括CSPDarknet53作为主干网络、使用SAM和PANet模块来提取特征、使用YOLOv3和YOLOv4的预训练权重进行初始化等，提高了检测性能和速度。
YOLOv5：于2020年提出，是YOLO系列的第五个版本。YOLOv5采用了轻量化的网络结构，提高了检测的速度，并引入了一些新功能，如YOLOv5-seg分割模型、Paddle Paddle导出功能、YOLOv5 AutoCache自动缓存功能和Comet日志记录和可视化集成功能。
总体而言，YOLO系列模型通过不断的改进和优化，提高了目标检测的性能和速度，并在计算机视觉领域取得了重要的突破。
yolov8 YOLOv8是YOLO系列模型的一个变种，它在YOLOv5的基础上进行了改进和优化。YOLOv8模型包含了检测（Detect）、分割（Segment）和姿态估计（Pose）、跟踪（Track）以及分类（Classify）等功能。下面是对这些功能的简要说明：
检测（Detect）：YOLOv8模型能够对图像或视频中的目标进行实时的物体检测。它通过预测目标的边界框和类别信息来完成检测任务。
分割（Segment）：YOLOv8模型还支持目标分割的功能，即将图像中的每个像素进行分类，将不同的目标区域进行分割。这个功能可以用于识别图像中的不同物体，并进行更精确的定位和分析。
姿态估计（Pose）：YOLOv8模型还可以对检测到的目标进行姿态估计，即推断目标在三维空间中的姿态信息。这对于一些需要了解目标的方向和位置的应用非常有用，比如人体姿态分析、机器人导航等。
跟踪（Track）：YOLOv8模型还具有目标跟踪的功能，即在视频中连续追踪相同目标的位置和轨迹。这对于视频监控、自动驾驶等应用非常重要。
分类（Classify）：除了目标检测和分割功能之外，YOLOv8模型还可以对检测到的目标进行分类，即给出目标的类别信息。这对于了解目标的属性和进行更细粒度的分析非常重要。
总而言之，YOLOv8模型综合了多种功能，包括检测、分割、姿态估计、跟踪和分类等，使其具备了更广泛的应用领域和更强大的功能。 github地址：https://github.com/ultralytics/ultralytics
v5入门示例 安装 克隆 repo，并要求在 Python&gt;=3.7.0 环境中安装 requirements.txt ，且要求 PyTorch&gt;=1.7 。
micromamba create prefix=d:/python380 python=3.8 #创建3.8的虚拟环境micromamba activate d:/python380git clone https://github.com/ultralytics/yolov5 # clonecd yolov5pip install -r requirements.txt # install 源代码目录结构">
    <meta itemprop="datePublished" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="dateModified" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="wordCount" content="912">
    <title>图像处理实战02-yolov5目标检测 :: liaomin416100569博客</title>
    <link href="/docs/css/auto-complete/auto-complete.min.css?1758355652" rel="stylesheet">
    <script src="/docs/js/auto-complete/auto-complete.min.js?1758355652" defer></script>
    <script src="/docs/js/search-lunr.min.js?1758355652" defer></script>
    <script src="/docs/js/search.min.js?1758355652" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/docs/searchindex.en.js?1758355652";
    </script>
    <script src="/docs/js/lunr/lunr.min.js?1758355652" defer></script>
    <script src="/docs/js/lunr/lunr.stemmer.support.min.js?1758355652" defer></script>
    <script src="/docs/js/lunr/lunr.multi.min.js?1758355652" defer></script>
    <script src="/docs/js/lunr/lunr.zh.min.js?1758355652" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['zh'];
    </script>
    <link href="/docs/fonts/fontawesome/css/fontawesome-all.min.css?1758355652" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/docs/fonts/fontawesome/css/fontawesome-all.min.css?1758355652" rel="stylesheet"></noscript>
    <link href="/docs/css/perfect-scrollbar/perfect-scrollbar.min.css?1758355652" rel="stylesheet">
    <link href="/docs/css/theme.min.css?1758355652" rel="stylesheet">
    <link href="/docs/css/format-html.min.css?1758355652" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = `.min`;
      window.relearn.path='\/programming\/ai\/computer_vision\/applications\/action_02_yolov5\/index.html';
      window.relearn.relBasePath='..\/..\/..\/..\/..';
      window.relearn.relBaseUri='..\/..\/..\/..\/..\/..';
      window.relearn.absBaseUri='https:\/\/jiaozi789.github.io\/docs';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      // variant stuff
      window.relearn.themevariants = [ 'auto' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script>
    <link href="/docs/css/custom.css?1758355652" rel="stylesheet">
  </head>
  <body class="mobile-support html" data-url="/docs/programming/ai/computer_vision/applications/action_02_yolov5/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
<nav class="TableOfContents">
  <ul>
    <li><a href="#yolov5">yolov5</a>
      <ul>
        <li><a href="#发展历程">发展历程</a></li>
        <li><a href="#yolov8">yolov8</a></li>
        <li><a href="#v5入门示例">v5入门示例</a>
          <ul>
            <li><a href="#安装">安装</a></li>
            <li><a href="#模型下载">模型下载</a>
              <ul>
                <li><a href="#v61">v6.1</a></li>
                <li><a href="#v70">v7.0</a></li>
              </ul>
            </li>
            <li><a href="#预测">预测</a></li>
          </ul>
        </li>
        <li><a href="#训练模型">训练模型</a>
          <ul>
            <li><a href="#准备数据集">准备数据集</a>
              <ul>
                <li><a href="#创建数据集yaml">创建数据集yaml</a></li>
                <li><a href="#创建labels">创建labels</a></li>
                <li><a href="#训练">训练</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#模型应用">模型应用</a></li>
      </ul>
    </li>
  </ul>
</nav>
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/index.html"><span itemprop="name">liaomin416100569博客</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/index.html"><span itemprop="name">编程开发</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/index.html"><span itemprop="name">人工智能</span></a><meta itemprop="position" content="3">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/computer_vision/index.html"><span itemprop="name">计算机视觉</span></a><meta itemprop="position" content="4">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/computer_vision/applications/index.html"><span itemprop="name">应用案例</span></a><meta itemprop="position" content="5">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">图像处理实战02-yolov5目标检测</span><meta itemprop="position" content="6"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/docs/programming/ai/computer_vision/applications/index.html" title="应用案例 (🡐)"><i class="fa-fw fas fa-chevron-left"></i></a>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/docs/programming/plugins/index.html" title="插件开发 (🡒)"><i class="fa-fw fas fa-chevron-right"></i></a>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable programming" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
  </header>

<h1 id="图像处理实战02-yolov5目标检测">图像处理实战02-yolov5目标检测</h1>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']]
  }
};
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<h1 id="yolov5">yolov5</h1>
<p>YOLOv5 是一种目标检测算法，它是 YOLO (You Only Look Once) 系列算法的最新版本。YOLOv5 采用了一种新的架构，它包括一个基于 CSPNet (Cross Stage Partial Network) 的主干网络以及一系列改进的技巧，如多尺度训练、数据增强、网络混合精度训练等，从而实现了更快的检测速度和更好的检测精度。</p>
<p>YOLOv5 支持多种类型的目标检测任务，如物体检测、人脸检测、车辆检测等，可以应用于各种实际场景，如智能安防、自动驾驶、机器人视觉等。同时，YOLOv5 还提供了预训练的模型和开源代码，方便开发者进行模型的训练和应用。</p>
<p>github地址：https://github.com/ultralytics/yolov5/blob/master/README.zh-CN.md
官网：https://ultralytics.com/</p>
<h2 id="发展历程">发展历程</h2>
<p>YOLO（You Only Look Once）是一系列的目标检测模型，由Joseph Redmon等人开发。以下是YOLO系列的发展历程：</p>
<ol>
<li>
<p>YOLOv1：于2015年首次提出，是YOLO系列的第一个版本。YOLOv1通过将目标检测任务转化为回归问题，将图像划分为网格并预测每个网格的边界框和类别概率。然而，YOLOv1存在定位不准确和对小目标敏感的问题。</p>
</li>
<li>
<p>YOLOv2（YOLO9000）：于2016年提出，是YOLO系列的第二个版本。YOLOv2通过引入Darknet-19网络结构、使用anchor boxes和多尺度预测来改进检测性能。同时，YOLOv2还引入了目标类别的语义分割，可以检测更多类别的目标。</p>
</li>
<li>
<p>YOLOv3：于2018年提出，是YOLO系列的第三个版本。YOLOv3针对YOLOv2存在的问题进行了改进，引入了多尺度预测、使用FPN结构和使用更小的anchor boxes等技术，提高了检测精度和对小目标的检测能力。</p>
</li>
<li>
<p>YOLOv4：于2020年提出，是YOLO系列的第四个版本。YOLOv4在YOLOv3的基础上引入了一系列改进，包括CSPDarknet53作为主干网络、使用SAM和PANet模块来提取特征、使用YOLOv3和YOLOv4的预训练权重进行初始化等，提高了检测性能和速度。</p>
</li>
<li>
<p>YOLOv5：于2020年提出，是YOLO系列的第五个版本。YOLOv5采用了轻量化的网络结构，提高了检测的速度，并引入了一些新功能，如YOLOv5-seg分割模型、Paddle Paddle导出功能、YOLOv5 AutoCache自动缓存功能和Comet日志记录和可视化集成功能。</p>
</li>
</ol>
<p>总体而言，YOLO系列模型通过不断的改进和优化，提高了目标检测的性能和速度，并在计算机视觉领域取得了重要的突破。</p>
<h2 id="yolov8">yolov8</h2>
<p>YOLOv8是YOLO系列模型的一个变种，它在YOLOv5的基础上进行了改进和优化。YOLOv8模型包含了检测（Detect）、分割（Segment）和姿态估计（Pose）、跟踪（Track）以及分类（Classify）等功能。下面是对这些功能的简要说明：</p>
<ol>
<li>
<p>检测（Detect）：YOLOv8模型能够对图像或视频中的目标进行实时的物体检测。它通过预测目标的边界框和类别信息来完成检测任务。</p>
</li>
<li>
<p>分割（Segment）：YOLOv8模型还支持目标分割的功能，即将图像中的每个像素进行分类，将不同的目标区域进行分割。这个功能可以用于识别图像中的不同物体，并进行更精确的定位和分析。</p>
</li>
<li>
<p>姿态估计（Pose）：YOLOv8模型还可以对检测到的目标进行姿态估计，即推断目标在三维空间中的姿态信息。这对于一些需要了解目标的方向和位置的应用非常有用，比如人体姿态分析、机器人导航等。</p>
</li>
<li>
<p>跟踪（Track）：YOLOv8模型还具有目标跟踪的功能，即在视频中连续追踪相同目标的位置和轨迹。这对于视频监控、自动驾驶等应用非常重要。</p>
</li>
<li>
<p>分类（Classify）：除了目标检测和分割功能之外，YOLOv8模型还可以对检测到的目标进行分类，即给出目标的类别信息。这对于了解目标的属性和进行更细粒度的分析非常重要。</p>
</li>
</ol>
<p>总而言之，YOLOv8模型综合了多种功能，包括检测、分割、姿态估计、跟踪和分类等，使其具备了更广泛的应用领域和更强大的功能。
<a href="#R-image-910cf1193dd9a7fd3dd27b692f009c3e" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/94d5a026522a53938354c72fd4d655e4.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-910cf1193dd9a7fd3dd27b692f009c3e"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/94d5a026522a53938354c72fd4d655e4.png"></a>
github地址：https://github.com/ultralytics/ultralytics</p>
<h2 id="v5入门示例">v5入门示例</h2>
<h3 id="安装">安装</h3>
<p>克隆 repo，并要求在 Python&gt;=3.7.0 环境中安装 requirements.txt ，且要求 PyTorch&gt;=1.7 。</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>micromamba create prefix=d:/python380 python=3.8  #创建3.8的虚拟环境
micromamba activate d:/python380
git clone https://github.com/ultralytics/yolov5  # clone
cd yolov5
pip install -r requirements.txt  # install</code></pre></div>
<p>源代码目录结构</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>yolov5/
├── data/                  # 数据集配置目录
│   ├── coco.yaml            # COCO数据集配置文件，里面有数据集的下载地址和加载的python脚本
│   ├──ImageNet.yaml           # ImageNet数据集
│   ├── custom.yaml          # 自定义数据集配置文件
│   └── ...                  # 其他数据集配置文件
├── models/                # 模型定义目录
│   ├── common.py            # 通用函数和类定义
│   ├── experimental.py      # 实验性模型定义
│   ├── export.py            # 导出模型为ONNX的脚本
│   ├── models.py            # YOLOv5模型定义
│   ├── yolo.py              # YOLO类定义
│   └── ...                  # 其他模型定义文件
├── utils/                 # 实用工具目录
│   ├── autoanchor.py        # 自动锚框生成工具
│   ├── datasets.py          # 数据集处理工具
│   ├── general.py           # 通用实用函数
│   ├── google_utils.py      # Google云平台工具
│   ├── loss.py              # 损失函数定义
│   ├── metrics.py           # 评估指标定义
│   ├── torch_utils.py       # PyTorch工具
│   ├── wandb_logging.py     # WandB日志记录工具
│   └── ...                  # 其他实用工具文件
├── runs/                 # 训练和预测的结果输出目录
│   ├── detect        # 使用detect.py训练后输出目录，输出的目录是[ex自增数字]
│   ├── train        # 使用detect.py训练后输出目录，输出的目录是[ex自增数字],包含了训练好的模型和测试集效果
├── weights/               # 预训练模型权重目录
├── .gitignore             # Git忽略文件配置
├── Dockerfile             # Docker容器构建文件
├── LICENSE                # 许可证文件
├── README.md              # 项目说明文档
├── requirements.txt       # 项目依赖包列表
├── train.py               # 训练脚本
├── detect.py               # 预测脚本
├── export.py               # 导出YOLOv5 PyTorch model to 其他格式
├── hubconf.py               # hubconf.py文件是用于定义模型和数据集的Python模块
└── ...                    # 其他源代码文件</code></pre></div>
<blockquote>
<p>这里通过yolov5可以下载到很多常用的训练数据集，而且很轻松的找到下载地址,如ImageNet,
coco128等，不用自己辛苦的找了</p></blockquote>
<h3 id="模型下载">模型下载</h3>
<p>下载地址：https://github.com/ultralytics/yolov5/releases</p>
<h4 id="v61">v6.1</h4>
<p>这里的版本是v6.1是yolov5的子版本号</p>
<h5 id="pretrained-checkpoints">Pretrained Checkpoints</h5>
<p>Pretrained Checkpoints 是预训练权重文件的一种称呼。在深度学习中，预训练权重是指在大规模数据集上通过无监督学习或有监督学习得到的模型参数。这些参数通常可以被用来初始化一个新的模型，从而加速模型训练并提高模型的性能。</p>
<p>Pretrained Checkpoints 是指已经训练好的预训练权重文件，可以用来初始化一个新的模型，并继续训练这个模型以适应新的任务或数据集。这种方法被称为迁移学习，可以大大提高模型的训练效率和泛化能力。在计算机视觉领域，常见的预训练网络包括 VGG、ResNet、Inception、MobileNet 等。</p>
<h5 id="模型概述">模型概述</h5>
<p>以下模型列的解释</p>
<table>
  <thead>
      <tr>
          <th>列名</th>
          <th>解释</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Model</td>
          <td>模型的名称</td>
      </tr>
      <tr>
          <td>size(pixels)</td>
          <td>输入图像的大小（以像素为单位）</td>
      </tr>
      <tr>
          <td>mAPval0.5:0.95</td>
          <td>在验证集上的平均精确度（mean Average Precision），考虑所有IOU阈值从0.5到0.95的情况，准确率是%</td>
      </tr>
      <tr>
          <td>mAPval0.5</td>
          <td>在验证集上的平均精确度，只考虑IOU阈值为0.5的情况</td>
      </tr>
      <tr>
          <td>Speed CPU b1(ms)</td>
          <td>在CPU上使用batch size为1时的推理速度（以毫秒为单位）</td>
      </tr>
      <tr>
          <td>Speed V100 b1(ms)</td>
          <td>在NVIDIA V100 GPU上使用batch size为1时的推理速度（以毫秒为单位）</td>
      </tr>
      <tr>
          <td>Speed V100 b32(ms)</td>
          <td>在NVIDIA V100 GPU上使用batch size为32时的推理速度（以毫秒为单位）</td>
      </tr>
      <tr>
          <td>params (M)</td>
          <td>模型的参数量（以百万为单位）</td>
      </tr>
      <tr>
          <td>FLOPs @640 (B)</td>
          <td>在输入图像大小为640时，模型的浮点运算次数（以十亿为单位）</td>
      </tr>
  </tbody>
</table>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>size(pixels)</th>
          <th>mAPval0.5:0.95</th>
          <th>mAPval0.5</th>
          <th>Speed  CPU b1(ms)</th>
          <th>Speed  V100 b1(ms)</th>
          <th>Speed  V100 b32(ms)</th>
          <th>params  (M)</th>
          <th>FLOPs  @640 (B)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases" rel="external" target="_blank">YOLOv5n</a></td>
          <td>640</td>
          <td>28.0</td>
          <td>45.7</td>
          <td><strong>45</strong></td>
          <td><strong>6.3</strong></td>
          <td><strong>0.6</strong></td>
          <td><strong>1.9</strong></td>
          <td><strong>4.5</strong></td>
      </tr>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases" rel="external" target="_blank">YOLOv5s</a></td>
          <td>640</td>
          <td>37.4</td>
          <td>56.8</td>
          <td>98</td>
          <td>6.4</td>
          <td>0.9</td>
          <td>7.2</td>
          <td>16.5</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases" rel="external" target="_blank">YOLOv5m</a></td>
          <td>640</td>
          <td>45.4</td>
          <td>64.1</td>
          <td>224</td>
          <td>8.2</td>
          <td>1.7</td>
          <td>21.2</td>
          <td>49.0</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases" rel="external" target="_blank">YOLOv5l</a></td>
          <td>640</td>
          <td>49.0</td>
          <td>67.3</td>
          <td>430</td>
          <td>10.1</td>
          <td>2.7</td>
          <td>46.5</td>
          <td>109.1</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases" rel="external" target="_blank">YOLOv5x</a></td>
          <td>640</td>
          <td>50.7</td>
          <td>68.9</td>
          <td>766</td>
          <td>12.1</td>
          <td>4.8</td>
          <td>86.7</td>
          <td>205.7</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases" rel="external" target="_blank">YOLOv5n6</a></td>
          <td>1280</td>
          <td>36.0</td>
          <td>54.4</td>
          <td>153</td>
          <td>8.1</td>
          <td>2.1</td>
          <td>3.2</td>
          <td>4.6</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases" rel="external" target="_blank">YOLOv5s6</a></td>
          <td>1280</td>
          <td>44.8</td>
          <td>63.7</td>
          <td>385</td>
          <td>8.2</td>
          <td>3.6</td>
          <td>12.6</td>
          <td>16.8</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases" rel="external" target="_blank">YOLOv5m6</a></td>
          <td>1280</td>
          <td>51.3</td>
          <td>69.3</td>
          <td>887</td>
          <td>11.1</td>
          <td>6.8</td>
          <td>35.7</td>
          <td>50.0</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases" rel="external" target="_blank">YOLOv5l6</a></td>
          <td>1280</td>
          <td>53.7</td>
          <td>71.3</td>
          <td>1784</td>
          <td>15.8</td>
          <td>10.5</td>
          <td>76.8</td>
          <td>111.4</td>
      </tr>
  </tbody>
</table>
<h4 id="v70">v7.0</h4>
<p>新的YOLOv5 v7.0实例分割模型是世界上最快、最准确的，超过了所有当前的SOTA基准。我们使它们非常简单易用，可以轻松进行训练、验证和部署。
这个版本中的主要目标是引入与我们现有的目标检测模型类似的超级简单的YOLOv5分割工作流程。
重要更新</p>
<ul>
<li>分割模型 ⭐ 新增：第一次提供了SOTA YOLOv5-seg COCO预训练的分割模型（由@glenn-jocher、@AyushExel和@Laughing-q开发的#9052）</li>
<li>Paddle Paddle导出：使用python export.py &ndash;include paddle 可以将任何YOLOv5模型（cls、seg、det）导出为Paddle格式（由@glenn-jocher开发的#9459）</li>
<li>YOLOv5 AutoCache：使用python train.py &ndash;cache ram 现在会扫描可用内存并与预测的数据集RAM使用量进行比较。这降低了缓存风险，并应该有助于提高数据集缓存功能的使用率，从而显著加快训练速度。（由@glenn-jocher开发的#10027）</li>
<li>Comet日志记录和可视化集成：永久免费，Comet可以保存YOLOv5模型，恢复训练，并进行交互式可视化和调试预测。（由@DN6开发的#9232）</li>
</ul>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>size  (pixels)</th>
          <th>mAPbox<!-- raw HTML omitted -->50-95</th>
          <th>mAPmask<!-- raw HTML omitted -->50-95</th>
          <th>Train time  300 epochs<!-- raw HTML omitted -->A100 (hours)</th>
          <th>Speed  ONNX CPU<!-- raw HTML omitted -->(ms)</th>
          <th>Speed  TRT A100<!-- raw HTML omitted -->(ms)</th>
          <th>params  (M)</th>
          <th>FLOPs  @640(B)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-seg.pt" rel="external" target="_blank">YOLOv5n-seg</a></td>
          <td>640</td>
          <td>27.6</td>
          <td>23.4</td>
          <td>80:17</td>
          <td><strong>62.7</strong></td>
          <td><strong>1.2</strong></td>
          <td><strong>2.0</strong></td>
          <td><strong>7.1</strong></td>
      </tr>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-seg.pt" rel="external" target="_blank">YOLOv5s-seg</a></td>
          <td>640</td>
          <td>37.6</td>
          <td>31.7</td>
          <td>88:16</td>
          <td>173.3</td>
          <td>1.4</td>
          <td>7.6</td>
          <td>26.4</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-seg.pt" rel="external" target="_blank">YOLOv5m-seg</a></td>
          <td>640</td>
          <td>45.0</td>
          <td>37.1</td>
          <td>108:36</td>
          <td>427.0</td>
          <td>2.2</td>
          <td>22.0</td>
          <td>70.8</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-seg.pt" rel="external" target="_blank">YOLOv5l-seg</a></td>
          <td>640</td>
          <td>49.0</td>
          <td>39.9</td>
          <td>66:43 (2x)</td>
          <td>857.4</td>
          <td>2.9</td>
          <td>47.9</td>
          <td>147.7</td>
      </tr>
      <tr>
          <td><a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-seg.pt" rel="external" target="_blank">YOLOv5x-seg</a></td>
          <td>640</td>
          <td><strong>50.7</strong></td>
          <td><strong>41.4</strong></td>
          <td>62:56 (3x)</td>
          <td>1579.2</td>
          <td>4.5</td>
          <td>88.8</td>
          <td>265.7</td>
      </tr>
  </tbody>
</table>
<p>我这里选择一个V6.1模型<a href="https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n6.pt" rel="external" target="_blank">yolov5n6.pt</a>
将模型丢到yolov5项目根目录即可
<a href="#R-image-7dc51edcd64d56b2a74d1baebf4ca28f" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/26d4a9db0f18946d754745af26462a0a.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7dc51edcd64d56b2a74d1baebf4ca28f"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/26d4a9db0f18946d754745af26462a0a.png"></a></p>
<h3 id="预测">预测</h3>
<p>因为预训练模型，已经有检测某些类别能力，我们可以看下data/coco.yml中names可以看到总共有80个类别
<a href="#R-image-f2627c373c68e55420fbaeccd8e3abc6" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/e38098f8a42a27b31ba895b442480798.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-f2627c373c68e55420fbaeccd8e3abc6"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/e38098f8a42a27b31ba895b442480798.png"></a>
在yolov5中可以使用./detect.py脚本来进行目标物品检测。
以下是对&quot;./detect.py&quot;脚本中常见参数的详细解释：</p>
<ol>
<li>
<p><code>--source</code>：指定输入源，可以是图像路径、视频文件路径或摄像头索引（默认为当前目录data/images，里面就两张图片）。</p>
</li>
<li>
<p><code>--weights</code>：指定模型权重文件的路径。可以是本地路径或PaddleHub模型中心的模型名称，默认是当前目录的yolov5s.pt。</p>
</li>
<li>
<p><code>--data</code>：指定要使用的数据集的配置文件。数据集的配置文件包含了数据集的路径、类别标签、训练集、验证集和测试集的划分等信息,默认data/coco128.yaml，选填。</p>
</li>
<li>
<p><code>--img-size</code>：指定输入图像的尺寸，格式为&quot;<!-- raw HTML omitted -->,<!-- raw HTML omitted -->&quot;，例如&quot;640,480&quot;。默认为640x640。</p>
</li>
<li>
<p><code>--conf-thres</code>：目标置信度阈值，范围为0到1。超过该阈值的目标将被保留，默认为0.25。</p>
</li>
<li>
<p><code>--iou-thres</code>：NMS（非极大值抑制）的IoU（交并比）阈值，范围为0到1。重叠度大于该阈值的目标将被合并，默认为0.45。</p>
</li>
<li>
<p><code>--max-det</code>：每个图像中最多检测的目标数，默认为100。</p>
</li>
<li>
<p><code>--device</code>：指定使用的设备，可以是&quot;cpu&quot;或&quot;cuda&quot;。默认为&quot;cpu&quot;。</p>
</li>
<li>
<p><code>--view-img</code>：在检测过程中显示图像窗口。</p>
</li>
<li>
<p><code>--save-txt</code>：保存检测结果的txt文件。</p>
</li>
<li>
<p><code>--save-conf</code>：保存检测结果的置信度。</p>
</li>
<li>
<p><code>--save-crop</code>：保存检测结果的裁剪图像。</p>
</li>
<li>
<p><code>--half</code>：使用半精度浮点数进行推理。</p>
</li>
</ol>
<p>这些参数可以根据您的需求进行调整，以获得最佳的检测结果。您可以在运行脚本时使用<code>--help</code>参数查看更多参数选项和说明。</p>
<p>执行命令预测</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>python ./detect.py --source ./data/images --weight ./yolov5n6.pt</code></pre></div>
<p>执行结果</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>(D:\condaenv\yolov5) D:\code1\yolov5-master\yolov5-master&gt;python ./detect.py --source ./data/images --weight ./yolov5n6.pt
detect: weights=[&#39;./yolov5n6.pt&#39;], source=./data/images, data=data\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=Fal
se, augment=False, visualize=False, update=False, project=runs\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1
YOLOv5  2023-5-30 Python-3.8.16 torch-2.0.1+cpu CPU

Fusing layers...
YOLOv5n6 summary: 280 layers, 3239884 parameters, 0 gradients
image 1/2 D:\code1\yolov5-master\yolov5-master\data\images\bus.jpg: 640x512 4 persons, 1 bus, 211.9ms
image 2/2 D:\code1\yolov5-master\yolov5-master\data\images\zidane.jpg: 384x640 3 persons, 1 tie, 152.9ms
Speed: 1.0ms pre-process, 182.4ms inference, 3.0ms NMS per image at shape (1, 3, 640, 640)
Results saved to runs\detect\exp8</code></pre></div>
<p>找到runs\detect\exp8 打开目录查看分类图片
<a href="#R-image-b0ac3da4c5c653d133bf94998b6bed38" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/012c627b4168acb1e0c5a84f9c433058.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-b0ac3da4c5c653d133bf94998b6bed38"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/012c627b4168acb1e0c5a84f9c433058.png"></a>
<a href="#R-image-06417ce64b7738a25d9b50e42af25f53" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/7b04d44fef4a4683b90a8f1dc73d2cb6.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-06417ce64b7738a25d9b50e42af25f53"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/7b04d44fef4a4683b90a8f1dc73d2cb6.png"></a></p>
<h2 id="训练模型">训练模型</h2>
<p>参考自官网：https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/#before-you-start</p>
<h3 id="准备数据集">准备数据集</h3>
<h4 id="创建数据集yaml">创建数据集yaml</h4>
<p>COCO128是一个小型教程数据集的例子，由COCO train2017中的前128张图像组成。这128张图像同时用于训练和验证，以验证我们的训练流程能够过拟合。data/coco128.yaml是数据集配置文件，定义了以下内容：
1）数据集根目录路径以及训练/验证/测试图像目录的相对路径（或包含图像路径的*.txt文件）；
2）类别名称字典。</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ../datasets/coco128  # dataset root dir
train: images/train2017  # train images (relative to &#39;path&#39;) 128 images
val: images/train2017  # val images (relative to &#39;path&#39;) 128 images
test:  # test images (optional)

# Classes (80 COCO classes)
names:
  0: person
  1: bicycle
  2: car
  ...
  77: teddy bear
  78: hair drier
  79: toothbrush
  # Download script/URL (optional)
download: https://ultralytics.com/assets/coco128.zip</code></pre></div>
<p><a href="https://ultralytics.com/assets/coco128.zip" rel="external" target="_blank">https://ultralytics.com/assets/coco128.zip</a>下载后，目录结构如下
<a href="#R-image-5ae386e06369bf61a529c9361d515ec8" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/f97f0509219e731ac380833f75b912e2.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-5ae386e06369bf61a529c9361d515ec8"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/f97f0509219e731ac380833f75b912e2.png"></a>
我这里用来训练判断一个身份证的正反面，我在项目根目录新建一个idcard目录，下面在建一个mul目录，这个目录只是用来训练不同的身份证信息用来区分的，我们的所有数据集都在mul目录</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ./idcard/mul  # dataset root dir
train: images  # train images 
val: images  # val images
test: images   # test images 

# Classes
names:
  0: idcard_z  #表示身份证正面
  1: idcard_f   #表示身份证反面</code></pre></div>
<p>注意这里yolov5回自动找path下的train目录在加上你的images作为图片的目录
比如真正的训练目录是：./idcard/mul/train/images，images的同级目录下会有个labels目录是标注
验证集的目录是：./idcard/mul/val/images
测试集的目录是：./idcard/test/val/images
<a href="#R-image-548a3bb31f6d5b95fffb9a16a5608003" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/c286af9bfde29439ab3fe13bb6269d00.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-548a3bb31f6d5b95fffb9a16a5608003"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/c286af9bfde29439ab3fe13bb6269d00.png"></a></p>
<blockquote>
<p>一般来说，常见的做法是将数据集划分为训练集、验证集和测试集，比如将数据划分为70%的训练集、15%的验证集和15%的测试集。这种比例通常适用于较小的数据集。对于较大的数据集，可以考虑增加验证集和测试集的比例。</p></blockquote>
<h4 id="创建labels">创建labels</h4>
<p>在使用注释工具（labelme,lableimg）为图像标注后，将标签导出为YOLO格式，每个图像对应一个*.txt文件（如果图像中没有对象，则不需要*.txt文件）。*.txt文件的规范如下：</p>
<ul>
<li>每个对象占据一行</li>
<li>每行的格式为：类别 x中心点 y中心点 宽度 高度。
框的坐标必须使用归一化的xywh格式（范围在0-1之间）。如果您的框的坐标是以像素为单位的，则需要将x中心点和宽度除以图像宽度，并将y中心点和高度除以图像高度。</li>
<li>类别编号从零开始（索引为0），和数据集yaml的names索引对应。</li>
</ul>
<p>这里建议使用labelimg标注</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>pip install labelimg -i https://pypi.tuna.tsinghua.edu.cn/simple</code></pre></div>
<p>切换到当前环境输入labelimg ，输入labelimage命令打开
<a href="#R-image-e10e0be0d1907361318ee902ef2ca283" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/02f18addff99771ced8ef234733a4420.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-e10e0be0d1907361318ee902ef2ca283"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/02f18addff99771ced8ef234733a4420.png"></a></p>
<p>选择open dir选择你的需要标记的图片目录(idcard/mul/train/images目录)，Change Save Dir选择你的idcard/mul/train/labels目录,选择YOLO格式
打开了图片后，需要一张一张图片的标记，常用的操作步骤是：</p>
<ol>
<li>按w唤起一个矩形框，选择你要选择的目标，选择后，弹出label，注意要先标注一个data.yaml中索引为0的，然后是1的，后面在弹出是可以选择的。
<a href="#R-image-4a45fee1bdec7418732e0fe15305aa2b" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/32ea57950b1389032193d17e93d98909.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-4a45fee1bdec7418732e0fe15305aa2b"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/32ea57950b1389032193d17e93d98909.png"></a></li>
<li>标准完成后ctrl+s保存。</li>
<li>按键盘d键切换到下一张图片，继续按w矩形框标注，知道所有图片完成。</li>
</ol>
<p>在你的labels目录下会有个classes.txt，看下他的顺序是否和data.yaml一致，如果不一致，不要调整classes.txt,调整data.yaml保持一致就行。</p>
<h4 id="训练">训练</h4>
<p>我这里准备了差不多350个标注好的图片，训练后识别率98%。
使用train.py执行</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#  --weight是指定初始的权重，可以用它来fine tuning调整训练你自己的模型。
python train.py --batch-size 4 --epochs 10 --data .\idcard\mul\idcard.yaml --weight .\yolov5n6.pt</code></pre></div>
<p>执行完成后，runs\trains\expn\weights\best.pt就是训练好的模型，可以使用之前的detect.py指定这个模型来预测下</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>python ./detect.py --source .\idcard\mul\test\images --weight .\runs\train\exp3\weights\best.pt</code></pre></div>
<p>查看runs\detect\expn\下的预测图片
<a href="#R-image-fb6ad3b83af19927bc7e0fcd78d8b71a" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/5d5dd54558786e99a2e4f01c74a92d36.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-fb6ad3b83af19927bc7e0fcd78d8b71a"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/5d5dd54558786e99a2e4f01c74a92d36.png"></a>
<a href="#R-image-3518bbac1cdce5128eff127aa3126d47" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/eefd1388498b6a07ae23c8e104f5cc29.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-3518bbac1cdce5128eff127aa3126d47"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/eefd1388498b6a07ae23c8e104f5cc29.png"></a>
<a href="#R-image-e48ce94e3e505f391ad5ff111a3daa82" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/f93da1b19de229d70ef803d9659541e0.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-e48ce94e3e505f391ad5ff111a3daa82"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/f93da1b19de229d70ef803d9659541e0.png"></a></p>
<h2 id="模型应用">模型应用</h2>
<p>我们需要在我们的应用使用生成好的best.pt模型可以使用torch.hub</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#使用我们本地之前用于训练的yolov5-master，我有把best.pt拷贝到当前目录
model = torch.hub.load(&#39;D:\\code1\\yolov5-master\\yolov5-master&#39;, &#39;custom&#39;, path=&#39;./best.pt&#39;, source=&#39;local&#39;)  # local repo
#print(model)
# 读取图像
img = cv2.imread(&#39;../images/zm.jpg&#39;)
# 进行预测
results = model(img)
resultLabel=[]
# 解析预测结果
for result in results.xyxy[0]:
    x1, y1, x2, y2, conf, cls = result.tolist()
    if conf &gt; 0.5:
        # 绘制边框和标签
        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
        cv2.putText(img, f&#34;{model.names[int(cls)]} {conf:.2f}&#34;, (int(x1), int(y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        resultLabel.append(model.names[int(cls)])
# 显示图像
print(&#34;预测的结果是&#34;,resultLabel)
plt.imshow(img)
plt.show()</code></pre></div>
<p><a href="#R-image-218d7e0729b420ca91a3139b1cfd8600" class="lightbox-link"><img alt="在这里插入图片描述" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/193ca1afd25ae3b96553d2a39d597949.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-218d7e0729b420ca91a3139b1cfd8600"><img alt="在这里插入图片描述" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/computer_vision/applications/action_02_yolov5.md.images/193ca1afd25ae3b96553d2a39d597949.png"></a></p>
<p>这是官方提供在线的版本调用，但是程序会自动去下载ultralytics/yolov5包和yolov5s模型，速度很慢</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>import torch
# Model
model = torch.hub.load(&#34;ultralytics/yolov5&#34;, &#34;yolov5s&#34;)  # or yolov5n - yolov5x6, custom
# Images
img = &#34;https://ultralytics.com/images/zidane.jpg&#34;  # or file, Path, PIL, OpenCV, numpy, list
# Inference
results = model(img)
# Results
results.print()  # or .show(), .save(), .crop(), .pandas(), etc.</code></pre></div>

  <footer class="footline">
              <i class='fa-fw fas fa-calendar'></i> Sep 18, 2025
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          <a id="R-logo" class="R-default" href="/docs/index.html">
            <div class="logo-title">liaomin416100569博客</div>
          </a>
        </div>
        <search><form action="/docs/search/index.html" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-homelinks">
          <ul class="space collapsible-menu">
            <li class="" data-nav-id="/docs/index.html"><a class="padding" href="/docs/index.html"><i class="fa-fw fas fa-home"></i> Home</a></li>
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-headercontrols">
          <ul class="">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div class="R-sidebarmenu R-shortcutmenu-main">
          <ul class="enlarge morespace collapsible-menu">
            <li class="parent " data-nav-id="/docs/programming/index.html"><a class="padding" href="/docs/programming/index.html">编程开发</a><ul id="R-subsections-e3fc01b477dbaf64a8f5013a3dab5c5b" class="collapsible-menu">
            <li class="alwaysopen " data-nav-id="/docs/programming/languages/index.html"><a class="padding" href="/docs/programming/languages/index.html">编程语言</a><ul id="R-subsections-1bbde7fb0c312ba940b425df5a4caf67" class="collapsible-menu"></ul></li>
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/index.html"><a class="padding" href="/docs/programming/ai/index.html">人工智能</a><ul id="R-subsections-9d06be7bd8c736c09a65fb0b91b71d0e" class="collapsible-menu">
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/tools_libraries/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/index.html">工具库</a><ul id="R-subsections-e43804740042696aa314af8cc1e28fa9" class="collapsible-menu"></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/machine_learning/index.html"><a class="padding" href="/docs/programming/ai/machine_learning/index.html">机器学习</a><ul id="R-subsections-d3b98ca0beda96811b8c41829d886d7f" class="collapsible-menu"></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/deep_learning/index.html"><a class="padding" href="/docs/programming/ai/deep_learning/index.html">深度学习</a><ul id="R-subsections-8e4f2a2c63b9f66a19e3b2a7c957ccda" class="collapsible-menu"></ul></li>
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/computer_vision/index.html"><a class="padding" href="/docs/programming/ai/computer_vision/index.html">计算机视觉</a><ul id="R-subsections-ee78ef5588610a65894e6d07832cb0b2" class="collapsible-menu">
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/computer_vision/tools/index.html"><a class="padding" href="/docs/programming/ai/computer_vision/tools/index.html">工具与框架</a><ul id="R-subsections-26f296805a693b12bbc91b3fd7032db3" class="collapsible-menu"></ul></li>
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/computer_vision/applications/index.html"><a class="padding" href="/docs/programming/ai/computer_vision/applications/index.html">应用案例</a><ul id="R-subsections-7fdacab544ae033be313c9cbf3aa39ac" class="collapsible-menu">
            <li class="active " data-nav-id="/docs/programming/ai/computer_vision/applications/action_02_yolov5/index.html"><a class="padding" href="/docs/programming/ai/computer_vision/applications/action_02_yolov5/index.html">图像处理实战02-yolov5目标检测</a></li></ul></li></ul></li></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/plugins/index.html"><a class="padding" href="/docs/programming/plugins/index.html">插件开发</a><ul id="R-subsections-de66f54cff99288ca68bfcb5bb0439ae" class="collapsible-menu"></ul></li></ul></li>
            <li class="" data-nav-id="/docs/devops/index.html"><a class="padding" href="/docs/devops/index.html">运维一体化</a><ul id="R-subsections-389d4feb4920b919bcbc0b1e9947dace" class="collapsible-menu"></ul></li>
            <li class="" data-nav-id="/docs/security/index.html"><a class="padding" href="/docs/security/index.html">安全攻防</a><ul id="R-subsections-66815ecaaecfc1c209e5637d03b258b2" class="collapsible-menu"></ul></li>
          </ul>
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-shortcuts">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div id="R-footer-margin"></div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-footercontrols">
          <ul class="">
          </ul>
        </div>
<div id="R-footer"><p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p></div>
      </div>
    </aside>
    <script src="/docs/js/clipboard/clipboard.min.js?1758355652" defer></script>
    <script src="/docs/js/perfect-scrollbar/perfect-scrollbar.min.js?1758355652" defer></script>
    <script src="/docs/js/theme.min.js?1758355652" defer></script>
  </body>
</html>
