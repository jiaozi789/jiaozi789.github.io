<!DOCTYPE html>
<html lang="en-us" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="html">
  <head><script src="/docs/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=docs/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.150.0">
    <meta name="generator" content="Relearn 8.0.1+b23cf6629eada0c2802f34ae4012e04343497862">
    <meta name="description" content="BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯ä¸€ç§åŸºäº Transformer æ¨¡å‹çš„é¢„è®­ç»ƒè¯­è¨€è¡¨ç¤ºæ–¹æ³•ï¼Œç”±Googleç ”ç©¶å›¢é˜Ÿäº2018å¹´æå‡ºã€‚BERT é€šè¿‡åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šè¿›è¡Œæ— ç›‘ç£çš„é¢„è®­ç»ƒï¼Œå­¦ä¹ äº†é€šç”¨çš„è¯­è¨€è¡¨ç¤ºï¼Œå¹¶ä¸”åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚
BERTä»…ä½¿ç”¨äº†Transformeræ¶æ„çš„Encoderéƒ¨åˆ†ã€‚BERTè‡ª2018å¹´ç”±è°·æ­Œå‘å¸ƒåï¼Œåœ¨å¤šç§NLPä»»åŠ¡ä¸­ï¼ˆä¾‹å¦‚QAã€æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æç­‰ç­‰ï¼‰éƒ½å®ç°äº†æ›´å¥½çš„ç»“æœã€‚
â€œWord2vecä¸GloVeéƒ½æœ‰ä¸€ä¸ªç‰¹ç‚¹ï¼Œå°±æ˜¯å®ƒä»¬æ˜¯ä¸Šä¸‹æ–‡æ— å…³ï¼ˆcontext-freeï¼‰çš„è¯åµŒå…¥ã€‚æ‰€ä»¥å®ƒä»¬æ²¡æœ‰è§£å†³ï¼šä¸€ä¸ªå•è¯åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­ä»£è¡¨ä¸åŒçš„å«ä¹‰çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå•è¯bankï¼Œå®ƒåœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæœ‰é“¶è¡Œã€æ²³ç•”è¿™ç§å·®åˆ«éå¸¸å¤§çš„å«ä¹‰ã€‚BERTçš„å‡ºç°ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚
BERT çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
åŒå‘æ€§ï¼šBERT ä½¿ç”¨åŒå‘ Transformer æ¨¡å‹æ¥å¤„ç†è¾“å…¥åºåˆ—ï¼Œä»è€Œèƒ½å¤ŸåŒæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œè€Œä¸ä»…ä»…æ˜¯å•å‘çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§åŒå‘æ€§ä½¿å¾— BERT èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¥å­ä¸­çš„è¯­ä¹‰å’Œè¯­å¢ƒã€‚
é¢„è®­ç»ƒ-å¾®è°ƒæ¡†æ¶ï¼šBERT ä½¿ç”¨äº†é¢„è®­ç»ƒ-å¾®è°ƒçš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œåœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šè¿›è¡Œæ— ç›‘ç£çš„é¢„è®­ç»ƒï¼Œé€šè¿‡ Masked Language Modelï¼ˆMLMï¼‰å’Œ Next Sentence Predictionï¼ˆNSPï¼‰ä»»åŠ¡å­¦ä¹ è¯­è¨€è¡¨ç¤ºï¼›ç„¶åï¼Œåœ¨ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹å‚æ•°ï¼Œä½¿å…¶é€‚åº”äºç‰¹å®šçš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚
Transformer æ¨¡å‹ï¼šBERT åŸºäº Transformer æ¨¡å‹ç»“æ„ï¼Œå…¶ä¸­åŒ…æ‹¬å¤šå±‚çš„ç¼–ç å™¨ï¼Œæ¯ä¸ªç¼–ç å™¨ç”±è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œç»„æˆã€‚è¿™ç§ç»“æ„èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·è¾“å…¥åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚
å¤šå±‚è¡¨ç¤ºï¼šBERT æä¾›äº†å¤šå±‚çš„è¯­è¨€è¡¨ç¤ºï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©ä¸åŒå±‚çš„è¡¨ç¤ºè¿›è¡Œåº”ç”¨ã€‚è¾ƒåº•å±‚çš„è¡¨ç¤ºé€šå¸¸æ›´åŠ æ¥è¿‘åŸå§‹è¾“å…¥ï¼Œè€Œè¾ƒé«˜å±‚çš„è¡¨ç¤ºåˆ™æ›´åŠ æŠ½è±¡ï¼ŒåŒ…å«äº†æ›´å¤šçš„è¯­ä¹‰ä¿¡æ¯ã€‚
å¼€æ”¾æºä»£ç ï¼šBERT çš„æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²ç»åœ¨ GitHub ä¸Šå¼€æ”¾ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥åŸºäº BERT è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåº”ç”¨å¼€å‘ã€‚
BERT é€šè¿‡é¢„è®­ç»ƒå¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šçš„é€šç”¨è¯­è¨€è¡¨ç¤ºï¼Œä»¥åŠåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¾®è°ƒï¼Œæœ‰æ•ˆåœ°æé«˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶ä¸”æˆä¸ºäº†å½“å‰é¢†åŸŸå†…æœ€å…·å½±å“åŠ›çš„é¢„è®­ç»ƒæ¨¡å‹ä¹‹ä¸€ã€‚
transformeræä¾›äº†ä¸åŒé¢†åŸŸä¸­å¸¸è§çš„æœºå™¨å­¦ä¹ æ¨¡å‹ç±»å‹ï¼š
TEXT MODELSï¼ˆæ–‡æœ¬æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†ææ–‡æœ¬æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„BERTã€GPTç­‰ã€‚
VISION MODELSï¼ˆè§†è§‰æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æå›¾åƒæ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­çš„ResNetã€VGG,Vision Transformer (ViT)ç­‰ã€‚
AUDIO MODELSï¼ˆéŸ³é¢‘æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æéŸ³é¢‘æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å£°å­¦æ¨¡å‹ã€è¯­éŸ³è¯†åˆ«æ¨¡å‹ç­‰ã€‚
VIDEO MODELSï¼ˆè§†é¢‘æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æè§†é¢‘æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚è§†é¢‘åˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«ç­‰ã€‚
MULTIMODAL MODELSï¼ˆå¤šæ¨¡æ€æ¨¡å‹ï¼‰ï¼šç»“åˆå¤šç§æ•°æ®ç±»å‹ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰è¿›è¡Œåˆ†æå’Œé¢„æµ‹çš„æ¨¡å‹ï¼Œå¦‚OpenAIçš„CLIPã€‚
REINFORCEMENT LEARNING MODELSï¼ˆå¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼‰ï¼šç”¨äºè§£å†³å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„æ¨¡å‹ï¼Œå¦‚Deep Q-Networksï¼ˆDQNï¼‰ã€Actor-Criticç­‰ã€‚
TIME SERIES MODELSï¼ˆæ—¶é—´åºåˆ—æ¨¡å‹ï¼‰ï¼šç”¨äºåˆ†æå’Œé¢„æµ‹æ—¶é—´åºåˆ—æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ç­‰ã€‚
GRAPH MODELSï¼ˆå›¾æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æå›¾æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ã€å›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰ç­‰ã€‚
BERTçš„åŸºæœ¬åŸç† BERTåŸºäºçš„æ˜¯Transformeræ¨¡å‹ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨Transformeræ¨¡å‹çš„Encoderéƒ¨åˆ†ã€‚åœ¨Transformeræ¨¡å‹ä¸­ï¼ŒEncoderçš„è¾“å…¥æ˜¯ä¸€ä¸²åºåˆ—ï¼Œè¾“å‡ºçš„æ˜¯å¯¹åºåˆ—ä¸­æ¯ä¸ªå­—ç¬¦çš„è¡¨ç¤ºã€‚åŒæ ·ï¼Œåœ¨BERTä¸­ï¼Œè¾“å…¥çš„æ˜¯ä¸€ä¸²åºåˆ—ï¼Œè¾“å‡ºçš„æ˜¯ä¹Ÿæ˜¯å¯¹åº”åºåˆ—ä¸­æ¯ä¸ªå•è¯çš„ç¼–ç ã€‚ ä»¥â€œHe got bit by Pythonâ€ä¸ºä¾‹ï¼ŒBERTçš„è¾“å…¥è¾“å‡ºå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š å…¶ä¸­è¾“å…¥ä¸ºåºåˆ—â€œHe got bit by Pythonâ€ï¼Œè¾“å‡ºçš„æ˜¯å¯¹æ¯ä¸ªå•è¯çš„ç¼–ç $R_{word}$ã€‚è¿™æ ·åœ¨ç»è¿‡äº†BERTå¤„ç†åï¼Œå³å¾—åˆ°äº†å¯¹æ¯ä¸ªå•è¯åŒ…å«çš„ä¸Šä¸‹æ–‡è¡¨ç¤º$R_{word}$ã€‚">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Transformerså®æˆ˜02-BERTé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ :: liaomin416100569åšå®¢">
    <meta name="twitter:description" content="BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯ä¸€ç§åŸºäº Transformer æ¨¡å‹çš„é¢„è®­ç»ƒè¯­è¨€è¡¨ç¤ºæ–¹æ³•ï¼Œç”±Googleç ”ç©¶å›¢é˜Ÿäº2018å¹´æå‡ºã€‚BERT é€šè¿‡åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šè¿›è¡Œæ— ç›‘ç£çš„é¢„è®­ç»ƒï¼Œå­¦ä¹ äº†é€šç”¨çš„è¯­è¨€è¡¨ç¤ºï¼Œå¹¶ä¸”åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚
BERTä»…ä½¿ç”¨äº†Transformeræ¶æ„çš„Encoderéƒ¨åˆ†ã€‚BERTè‡ª2018å¹´ç”±è°·æ­Œå‘å¸ƒåï¼Œåœ¨å¤šç§NLPä»»åŠ¡ä¸­ï¼ˆä¾‹å¦‚QAã€æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æç­‰ç­‰ï¼‰éƒ½å®ç°äº†æ›´å¥½çš„ç»“æœã€‚
â€œWord2vecä¸GloVeéƒ½æœ‰ä¸€ä¸ªç‰¹ç‚¹ï¼Œå°±æ˜¯å®ƒä»¬æ˜¯ä¸Šä¸‹æ–‡æ— å…³ï¼ˆcontext-freeï¼‰çš„è¯åµŒå…¥ã€‚æ‰€ä»¥å®ƒä»¬æ²¡æœ‰è§£å†³ï¼šä¸€ä¸ªå•è¯åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­ä»£è¡¨ä¸åŒçš„å«ä¹‰çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå•è¯bankï¼Œå®ƒåœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæœ‰é“¶è¡Œã€æ²³ç•”è¿™ç§å·®åˆ«éå¸¸å¤§çš„å«ä¹‰ã€‚BERTçš„å‡ºç°ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚
BERT çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
åŒå‘æ€§ï¼šBERT ä½¿ç”¨åŒå‘ Transformer æ¨¡å‹æ¥å¤„ç†è¾“å…¥åºåˆ—ï¼Œä»è€Œèƒ½å¤ŸåŒæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œè€Œä¸ä»…ä»…æ˜¯å•å‘çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§åŒå‘æ€§ä½¿å¾— BERT èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¥å­ä¸­çš„è¯­ä¹‰å’Œè¯­å¢ƒã€‚
é¢„è®­ç»ƒ-å¾®è°ƒæ¡†æ¶ï¼šBERT ä½¿ç”¨äº†é¢„è®­ç»ƒ-å¾®è°ƒçš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œåœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šè¿›è¡Œæ— ç›‘ç£çš„é¢„è®­ç»ƒï¼Œé€šè¿‡ Masked Language Modelï¼ˆMLMï¼‰å’Œ Next Sentence Predictionï¼ˆNSPï¼‰ä»»åŠ¡å­¦ä¹ è¯­è¨€è¡¨ç¤ºï¼›ç„¶åï¼Œåœ¨ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹å‚æ•°ï¼Œä½¿å…¶é€‚åº”äºç‰¹å®šçš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚
Transformer æ¨¡å‹ï¼šBERT åŸºäº Transformer æ¨¡å‹ç»“æ„ï¼Œå…¶ä¸­åŒ…æ‹¬å¤šå±‚çš„ç¼–ç å™¨ï¼Œæ¯ä¸ªç¼–ç å™¨ç”±è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œç»„æˆã€‚è¿™ç§ç»“æ„èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·è¾“å…¥åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚
å¤šå±‚è¡¨ç¤ºï¼šBERT æä¾›äº†å¤šå±‚çš„è¯­è¨€è¡¨ç¤ºï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©ä¸åŒå±‚çš„è¡¨ç¤ºè¿›è¡Œåº”ç”¨ã€‚è¾ƒåº•å±‚çš„è¡¨ç¤ºé€šå¸¸æ›´åŠ æ¥è¿‘åŸå§‹è¾“å…¥ï¼Œè€Œè¾ƒé«˜å±‚çš„è¡¨ç¤ºåˆ™æ›´åŠ æŠ½è±¡ï¼ŒåŒ…å«äº†æ›´å¤šçš„è¯­ä¹‰ä¿¡æ¯ã€‚
å¼€æ”¾æºä»£ç ï¼šBERT çš„æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²ç»åœ¨ GitHub ä¸Šå¼€æ”¾ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥åŸºäº BERT è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåº”ç”¨å¼€å‘ã€‚
BERT é€šè¿‡é¢„è®­ç»ƒå¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šçš„é€šç”¨è¯­è¨€è¡¨ç¤ºï¼Œä»¥åŠåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¾®è°ƒï¼Œæœ‰æ•ˆåœ°æé«˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶ä¸”æˆä¸ºäº†å½“å‰é¢†åŸŸå†…æœ€å…·å½±å“åŠ›çš„é¢„è®­ç»ƒæ¨¡å‹ä¹‹ä¸€ã€‚
transformeræä¾›äº†ä¸åŒé¢†åŸŸä¸­å¸¸è§çš„æœºå™¨å­¦ä¹ æ¨¡å‹ç±»å‹ï¼š
TEXT MODELSï¼ˆæ–‡æœ¬æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†ææ–‡æœ¬æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„BERTã€GPTç­‰ã€‚
VISION MODELSï¼ˆè§†è§‰æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æå›¾åƒæ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­çš„ResNetã€VGG,Vision Transformer (ViT)ç­‰ã€‚
AUDIO MODELSï¼ˆéŸ³é¢‘æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æéŸ³é¢‘æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å£°å­¦æ¨¡å‹ã€è¯­éŸ³è¯†åˆ«æ¨¡å‹ç­‰ã€‚
VIDEO MODELSï¼ˆè§†é¢‘æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æè§†é¢‘æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚è§†é¢‘åˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«ç­‰ã€‚
MULTIMODAL MODELSï¼ˆå¤šæ¨¡æ€æ¨¡å‹ï¼‰ï¼šç»“åˆå¤šç§æ•°æ®ç±»å‹ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰è¿›è¡Œåˆ†æå’Œé¢„æµ‹çš„æ¨¡å‹ï¼Œå¦‚OpenAIçš„CLIPã€‚
REINFORCEMENT LEARNING MODELSï¼ˆå¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼‰ï¼šç”¨äºè§£å†³å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„æ¨¡å‹ï¼Œå¦‚Deep Q-Networksï¼ˆDQNï¼‰ã€Actor-Criticç­‰ã€‚
TIME SERIES MODELSï¼ˆæ—¶é—´åºåˆ—æ¨¡å‹ï¼‰ï¼šç”¨äºåˆ†æå’Œé¢„æµ‹æ—¶é—´åºåˆ—æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ç­‰ã€‚
GRAPH MODELSï¼ˆå›¾æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æå›¾æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ã€å›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰ç­‰ã€‚
BERTçš„åŸºæœ¬åŸç† BERTåŸºäºçš„æ˜¯Transformeræ¨¡å‹ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨Transformeræ¨¡å‹çš„Encoderéƒ¨åˆ†ã€‚åœ¨Transformeræ¨¡å‹ä¸­ï¼ŒEncoderçš„è¾“å…¥æ˜¯ä¸€ä¸²åºåˆ—ï¼Œè¾“å‡ºçš„æ˜¯å¯¹åºåˆ—ä¸­æ¯ä¸ªå­—ç¬¦çš„è¡¨ç¤ºã€‚åŒæ ·ï¼Œåœ¨BERTä¸­ï¼Œè¾“å…¥çš„æ˜¯ä¸€ä¸²åºåˆ—ï¼Œè¾“å‡ºçš„æ˜¯ä¹Ÿæ˜¯å¯¹åº”åºåˆ—ä¸­æ¯ä¸ªå•è¯çš„ç¼–ç ã€‚ ä»¥â€œHe got bit by Pythonâ€ä¸ºä¾‹ï¼ŒBERTçš„è¾“å…¥è¾“å‡ºå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š å…¶ä¸­è¾“å…¥ä¸ºåºåˆ—â€œHe got bit by Pythonâ€ï¼Œè¾“å‡ºçš„æ˜¯å¯¹æ¯ä¸ªå•è¯çš„ç¼–ç $R_{word}$ã€‚è¿™æ ·åœ¨ç»è¿‡äº†BERTå¤„ç†åï¼Œå³å¾—åˆ°äº†å¯¹æ¯ä¸ªå•è¯åŒ…å«çš„ä¸Šä¸‹æ–‡è¡¨ç¤º$R_{word}$ã€‚">
    <meta property="og:url" content="http://localhost:1313/docs/programming/ai/tools_libraries/transformers/transformers_actions_02-copy/index.html">
    <meta property="og:site_name" content="liaomin416100569åšå®¢">
    <meta property="og:title" content="Transformerså®æˆ˜02-BERTé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ :: liaomin416100569åšå®¢">
    <meta property="og:description" content="BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯ä¸€ç§åŸºäº Transformer æ¨¡å‹çš„é¢„è®­ç»ƒè¯­è¨€è¡¨ç¤ºæ–¹æ³•ï¼Œç”±Googleç ”ç©¶å›¢é˜Ÿäº2018å¹´æå‡ºã€‚BERT é€šè¿‡åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šè¿›è¡Œæ— ç›‘ç£çš„é¢„è®­ç»ƒï¼Œå­¦ä¹ äº†é€šç”¨çš„è¯­è¨€è¡¨ç¤ºï¼Œå¹¶ä¸”åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚
BERTä»…ä½¿ç”¨äº†Transformeræ¶æ„çš„Encoderéƒ¨åˆ†ã€‚BERTè‡ª2018å¹´ç”±è°·æ­Œå‘å¸ƒåï¼Œåœ¨å¤šç§NLPä»»åŠ¡ä¸­ï¼ˆä¾‹å¦‚QAã€æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æç­‰ç­‰ï¼‰éƒ½å®ç°äº†æ›´å¥½çš„ç»“æœã€‚
â€œWord2vecä¸GloVeéƒ½æœ‰ä¸€ä¸ªç‰¹ç‚¹ï¼Œå°±æ˜¯å®ƒä»¬æ˜¯ä¸Šä¸‹æ–‡æ— å…³ï¼ˆcontext-freeï¼‰çš„è¯åµŒå…¥ã€‚æ‰€ä»¥å®ƒä»¬æ²¡æœ‰è§£å†³ï¼šä¸€ä¸ªå•è¯åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­ä»£è¡¨ä¸åŒçš„å«ä¹‰çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå•è¯bankï¼Œå®ƒåœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæœ‰é“¶è¡Œã€æ²³ç•”è¿™ç§å·®åˆ«éå¸¸å¤§çš„å«ä¹‰ã€‚BERTçš„å‡ºç°ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚
BERT çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
åŒå‘æ€§ï¼šBERT ä½¿ç”¨åŒå‘ Transformer æ¨¡å‹æ¥å¤„ç†è¾“å…¥åºåˆ—ï¼Œä»è€Œèƒ½å¤ŸåŒæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œè€Œä¸ä»…ä»…æ˜¯å•å‘çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§åŒå‘æ€§ä½¿å¾— BERT èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¥å­ä¸­çš„è¯­ä¹‰å’Œè¯­å¢ƒã€‚
é¢„è®­ç»ƒ-å¾®è°ƒæ¡†æ¶ï¼šBERT ä½¿ç”¨äº†é¢„è®­ç»ƒ-å¾®è°ƒçš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œåœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šè¿›è¡Œæ— ç›‘ç£çš„é¢„è®­ç»ƒï¼Œé€šè¿‡ Masked Language Modelï¼ˆMLMï¼‰å’Œ Next Sentence Predictionï¼ˆNSPï¼‰ä»»åŠ¡å­¦ä¹ è¯­è¨€è¡¨ç¤ºï¼›ç„¶åï¼Œåœ¨ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹å‚æ•°ï¼Œä½¿å…¶é€‚åº”äºç‰¹å®šçš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚
Transformer æ¨¡å‹ï¼šBERT åŸºäº Transformer æ¨¡å‹ç»“æ„ï¼Œå…¶ä¸­åŒ…æ‹¬å¤šå±‚çš„ç¼–ç å™¨ï¼Œæ¯ä¸ªç¼–ç å™¨ç”±è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œç»„æˆã€‚è¿™ç§ç»“æ„èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·è¾“å…¥åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚
å¤šå±‚è¡¨ç¤ºï¼šBERT æä¾›äº†å¤šå±‚çš„è¯­è¨€è¡¨ç¤ºï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©ä¸åŒå±‚çš„è¡¨ç¤ºè¿›è¡Œåº”ç”¨ã€‚è¾ƒåº•å±‚çš„è¡¨ç¤ºé€šå¸¸æ›´åŠ æ¥è¿‘åŸå§‹è¾“å…¥ï¼Œè€Œè¾ƒé«˜å±‚çš„è¡¨ç¤ºåˆ™æ›´åŠ æŠ½è±¡ï¼ŒåŒ…å«äº†æ›´å¤šçš„è¯­ä¹‰ä¿¡æ¯ã€‚
å¼€æ”¾æºä»£ç ï¼šBERT çš„æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²ç»åœ¨ GitHub ä¸Šå¼€æ”¾ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥åŸºäº BERT è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåº”ç”¨å¼€å‘ã€‚
BERT é€šè¿‡é¢„è®­ç»ƒå¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šçš„é€šç”¨è¯­è¨€è¡¨ç¤ºï¼Œä»¥åŠåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¾®è°ƒï¼Œæœ‰æ•ˆåœ°æé«˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶ä¸”æˆä¸ºäº†å½“å‰é¢†åŸŸå†…æœ€å…·å½±å“åŠ›çš„é¢„è®­ç»ƒæ¨¡å‹ä¹‹ä¸€ã€‚
transformeræä¾›äº†ä¸åŒé¢†åŸŸä¸­å¸¸è§çš„æœºå™¨å­¦ä¹ æ¨¡å‹ç±»å‹ï¼š
TEXT MODELSï¼ˆæ–‡æœ¬æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†ææ–‡æœ¬æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„BERTã€GPTç­‰ã€‚
VISION MODELSï¼ˆè§†è§‰æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æå›¾åƒæ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­çš„ResNetã€VGG,Vision Transformer (ViT)ç­‰ã€‚
AUDIO MODELSï¼ˆéŸ³é¢‘æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æéŸ³é¢‘æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å£°å­¦æ¨¡å‹ã€è¯­éŸ³è¯†åˆ«æ¨¡å‹ç­‰ã€‚
VIDEO MODELSï¼ˆè§†é¢‘æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æè§†é¢‘æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚è§†é¢‘åˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«ç­‰ã€‚
MULTIMODAL MODELSï¼ˆå¤šæ¨¡æ€æ¨¡å‹ï¼‰ï¼šç»“åˆå¤šç§æ•°æ®ç±»å‹ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰è¿›è¡Œåˆ†æå’Œé¢„æµ‹çš„æ¨¡å‹ï¼Œå¦‚OpenAIçš„CLIPã€‚
REINFORCEMENT LEARNING MODELSï¼ˆå¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼‰ï¼šç”¨äºè§£å†³å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„æ¨¡å‹ï¼Œå¦‚Deep Q-Networksï¼ˆDQNï¼‰ã€Actor-Criticç­‰ã€‚
TIME SERIES MODELSï¼ˆæ—¶é—´åºåˆ—æ¨¡å‹ï¼‰ï¼šç”¨äºåˆ†æå’Œé¢„æµ‹æ—¶é—´åºåˆ—æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ç­‰ã€‚
GRAPH MODELSï¼ˆå›¾æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æå›¾æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ã€å›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰ç­‰ã€‚
BERTçš„åŸºæœ¬åŸç† BERTåŸºäºçš„æ˜¯Transformeræ¨¡å‹ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨Transformeræ¨¡å‹çš„Encoderéƒ¨åˆ†ã€‚åœ¨Transformeræ¨¡å‹ä¸­ï¼ŒEncoderçš„è¾“å…¥æ˜¯ä¸€ä¸²åºåˆ—ï¼Œè¾“å‡ºçš„æ˜¯å¯¹åºåˆ—ä¸­æ¯ä¸ªå­—ç¬¦çš„è¡¨ç¤ºã€‚åŒæ ·ï¼Œåœ¨BERTä¸­ï¼Œè¾“å…¥çš„æ˜¯ä¸€ä¸²åºåˆ—ï¼Œè¾“å‡ºçš„æ˜¯ä¹Ÿæ˜¯å¯¹åº”åºåˆ—ä¸­æ¯ä¸ªå•è¯çš„ç¼–ç ã€‚ ä»¥â€œHe got bit by Pythonâ€ä¸ºä¾‹ï¼ŒBERTçš„è¾“å…¥è¾“å‡ºå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š å…¶ä¸­è¾“å…¥ä¸ºåºåˆ—â€œHe got bit by Pythonâ€ï¼Œè¾“å‡ºçš„æ˜¯å¯¹æ¯ä¸ªå•è¯çš„ç¼–ç $R_{word}$ã€‚è¿™æ ·åœ¨ç»è¿‡äº†BERTå¤„ç†åï¼Œå³å¾—åˆ°äº†å¯¹æ¯ä¸ªå•è¯åŒ…å«çš„ä¸Šä¸‹æ–‡è¡¨ç¤º$R_{word}$ã€‚">
    <meta property="og:locale" content="en_us">
    <meta property="og:type" content="article">
    <meta property="article:section" content="ç¼–ç¨‹å¼€å‘">
    <meta property="article:published_time" content="2025-09-18T16:55:17+08:00">
    <meta property="article:modified_time" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="name" content="Transformerså®æˆ˜02-BERTé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ :: liaomin416100569åšå®¢">
    <meta itemprop="description" content="BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯ä¸€ç§åŸºäº Transformer æ¨¡å‹çš„é¢„è®­ç»ƒè¯­è¨€è¡¨ç¤ºæ–¹æ³•ï¼Œç”±Googleç ”ç©¶å›¢é˜Ÿäº2018å¹´æå‡ºã€‚BERT é€šè¿‡åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šè¿›è¡Œæ— ç›‘ç£çš„é¢„è®­ç»ƒï¼Œå­¦ä¹ äº†é€šç”¨çš„è¯­è¨€è¡¨ç¤ºï¼Œå¹¶ä¸”åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚
BERTä»…ä½¿ç”¨äº†Transformeræ¶æ„çš„Encoderéƒ¨åˆ†ã€‚BERTè‡ª2018å¹´ç”±è°·æ­Œå‘å¸ƒåï¼Œåœ¨å¤šç§NLPä»»åŠ¡ä¸­ï¼ˆä¾‹å¦‚QAã€æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æç­‰ç­‰ï¼‰éƒ½å®ç°äº†æ›´å¥½çš„ç»“æœã€‚
â€œWord2vecä¸GloVeéƒ½æœ‰ä¸€ä¸ªç‰¹ç‚¹ï¼Œå°±æ˜¯å®ƒä»¬æ˜¯ä¸Šä¸‹æ–‡æ— å…³ï¼ˆcontext-freeï¼‰çš„è¯åµŒå…¥ã€‚æ‰€ä»¥å®ƒä»¬æ²¡æœ‰è§£å†³ï¼šä¸€ä¸ªå•è¯åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­ä»£è¡¨ä¸åŒçš„å«ä¹‰çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå•è¯bankï¼Œå®ƒåœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæœ‰é“¶è¡Œã€æ²³ç•”è¿™ç§å·®åˆ«éå¸¸å¤§çš„å«ä¹‰ã€‚BERTçš„å‡ºç°ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚
BERT çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
åŒå‘æ€§ï¼šBERT ä½¿ç”¨åŒå‘ Transformer æ¨¡å‹æ¥å¤„ç†è¾“å…¥åºåˆ—ï¼Œä»è€Œèƒ½å¤ŸåŒæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œè€Œä¸ä»…ä»…æ˜¯å•å‘çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§åŒå‘æ€§ä½¿å¾— BERT èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¥å­ä¸­çš„è¯­ä¹‰å’Œè¯­å¢ƒã€‚
é¢„è®­ç»ƒ-å¾®è°ƒæ¡†æ¶ï¼šBERT ä½¿ç”¨äº†é¢„è®­ç»ƒ-å¾®è°ƒçš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œåœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šè¿›è¡Œæ— ç›‘ç£çš„é¢„è®­ç»ƒï¼Œé€šè¿‡ Masked Language Modelï¼ˆMLMï¼‰å’Œ Next Sentence Predictionï¼ˆNSPï¼‰ä»»åŠ¡å­¦ä¹ è¯­è¨€è¡¨ç¤ºï¼›ç„¶åï¼Œåœ¨ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹å‚æ•°ï¼Œä½¿å…¶é€‚åº”äºç‰¹å®šçš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚
Transformer æ¨¡å‹ï¼šBERT åŸºäº Transformer æ¨¡å‹ç»“æ„ï¼Œå…¶ä¸­åŒ…æ‹¬å¤šå±‚çš„ç¼–ç å™¨ï¼Œæ¯ä¸ªç¼–ç å™¨ç”±è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œç»„æˆã€‚è¿™ç§ç»“æ„èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·è¾“å…¥åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚
å¤šå±‚è¡¨ç¤ºï¼šBERT æä¾›äº†å¤šå±‚çš„è¯­è¨€è¡¨ç¤ºï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©ä¸åŒå±‚çš„è¡¨ç¤ºè¿›è¡Œåº”ç”¨ã€‚è¾ƒåº•å±‚çš„è¡¨ç¤ºé€šå¸¸æ›´åŠ æ¥è¿‘åŸå§‹è¾“å…¥ï¼Œè€Œè¾ƒé«˜å±‚çš„è¡¨ç¤ºåˆ™æ›´åŠ æŠ½è±¡ï¼ŒåŒ…å«äº†æ›´å¤šçš„è¯­ä¹‰ä¿¡æ¯ã€‚
å¼€æ”¾æºä»£ç ï¼šBERT çš„æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²ç»åœ¨ GitHub ä¸Šå¼€æ”¾ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥åŸºäº BERT è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåº”ç”¨å¼€å‘ã€‚
BERT é€šè¿‡é¢„è®­ç»ƒå¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šçš„é€šç”¨è¯­è¨€è¡¨ç¤ºï¼Œä»¥åŠåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¾®è°ƒï¼Œæœ‰æ•ˆåœ°æé«˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶ä¸”æˆä¸ºäº†å½“å‰é¢†åŸŸå†…æœ€å…·å½±å“åŠ›çš„é¢„è®­ç»ƒæ¨¡å‹ä¹‹ä¸€ã€‚
transformeræä¾›äº†ä¸åŒé¢†åŸŸä¸­å¸¸è§çš„æœºå™¨å­¦ä¹ æ¨¡å‹ç±»å‹ï¼š
TEXT MODELSï¼ˆæ–‡æœ¬æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†ææ–‡æœ¬æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„BERTã€GPTç­‰ã€‚
VISION MODELSï¼ˆè§†è§‰æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æå›¾åƒæ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­çš„ResNetã€VGG,Vision Transformer (ViT)ç­‰ã€‚
AUDIO MODELSï¼ˆéŸ³é¢‘æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æéŸ³é¢‘æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å£°å­¦æ¨¡å‹ã€è¯­éŸ³è¯†åˆ«æ¨¡å‹ç­‰ã€‚
VIDEO MODELSï¼ˆè§†é¢‘æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æè§†é¢‘æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚è§†é¢‘åˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«ç­‰ã€‚
MULTIMODAL MODELSï¼ˆå¤šæ¨¡æ€æ¨¡å‹ï¼‰ï¼šç»“åˆå¤šç§æ•°æ®ç±»å‹ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰è¿›è¡Œåˆ†æå’Œé¢„æµ‹çš„æ¨¡å‹ï¼Œå¦‚OpenAIçš„CLIPã€‚
REINFORCEMENT LEARNING MODELSï¼ˆå¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼‰ï¼šç”¨äºè§£å†³å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„æ¨¡å‹ï¼Œå¦‚Deep Q-Networksï¼ˆDQNï¼‰ã€Actor-Criticç­‰ã€‚
TIME SERIES MODELSï¼ˆæ—¶é—´åºåˆ—æ¨¡å‹ï¼‰ï¼šç”¨äºåˆ†æå’Œé¢„æµ‹æ—¶é—´åºåˆ—æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ç­‰ã€‚
GRAPH MODELSï¼ˆå›¾æ¨¡å‹ï¼‰ï¼šç”¨äºå¤„ç†å’Œåˆ†æå›¾æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ã€å›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰ç­‰ã€‚
BERTçš„åŸºæœ¬åŸç† BERTåŸºäºçš„æ˜¯Transformeræ¨¡å‹ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨Transformeræ¨¡å‹çš„Encoderéƒ¨åˆ†ã€‚åœ¨Transformeræ¨¡å‹ä¸­ï¼ŒEncoderçš„è¾“å…¥æ˜¯ä¸€ä¸²åºåˆ—ï¼Œè¾“å‡ºçš„æ˜¯å¯¹åºåˆ—ä¸­æ¯ä¸ªå­—ç¬¦çš„è¡¨ç¤ºã€‚åŒæ ·ï¼Œåœ¨BERTä¸­ï¼Œè¾“å…¥çš„æ˜¯ä¸€ä¸²åºåˆ—ï¼Œè¾“å‡ºçš„æ˜¯ä¹Ÿæ˜¯å¯¹åº”åºåˆ—ä¸­æ¯ä¸ªå•è¯çš„ç¼–ç ã€‚ ä»¥â€œHe got bit by Pythonâ€ä¸ºä¾‹ï¼ŒBERTçš„è¾“å…¥è¾“å‡ºå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š å…¶ä¸­è¾“å…¥ä¸ºåºåˆ—â€œHe got bit by Pythonâ€ï¼Œè¾“å‡ºçš„æ˜¯å¯¹æ¯ä¸ªå•è¯çš„ç¼–ç $R_{word}$ã€‚è¿™æ ·åœ¨ç»è¿‡äº†BERTå¤„ç†åï¼Œå³å¾—åˆ°äº†å¯¹æ¯ä¸ªå•è¯åŒ…å«çš„ä¸Šä¸‹æ–‡è¡¨ç¤º$R_{word}$ã€‚">
    <meta itemprop="datePublished" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="dateModified" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="wordCount" content="1657">
    <title>Transformerså®æˆ˜02-BERTé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ :: liaomin416100569åšå®¢</title>
    <link href="/docs/css/auto-complete/auto-complete.min.css?1758263149" rel="stylesheet">
    <script src="/docs/js/auto-complete/auto-complete.min.js?1758263149" defer></script>
    <script src="/docs/js/search-lunr.js?1758263149" defer></script>
    <script src="/docs/js/search.js?1758263149" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/docs/searchindex.en.js?1758263149";
    </script>
    <script src="/docs/js/lunr/lunr.min.js?1758263149" defer></script>
    <script src="/docs/js/lunr/lunr.stemmer.support.min.js?1758263149" defer></script>
    <script src="/docs/js/lunr/lunr.multi.min.js?1758263149" defer></script>
    <script src="/docs/js/lunr/lunr.en.min.js?1758263149" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['en'];
    </script>
    <link href="/docs/fonts/fontawesome/css/fontawesome-all.min.css?1758263149" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/docs/fonts/fontawesome/css/fontawesome-all.min.css?1758263149" rel="stylesheet"></noscript>
    <link href="/docs/css/perfect-scrollbar/perfect-scrollbar.min.css?1758263149" rel="stylesheet">
    <link href="/docs/css/theme.css?1758263149" rel="stylesheet">
    <link href="/docs/css/format-html.css?1758263149" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = ``;
      window.relearn.path='\/programming\/ai\/tools_libraries\/transformers\/transformers_actions_02-copy\/index.html';
      window.relearn.relBasePath='..\/..\/..\/..\/..';
      window.relearn.relBaseUri='..\/..\/..\/..\/..\/..';
      window.relearn.absBaseUri='http:\/\/localhost:1313\/docs';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      // variant stuff
      window.relearn.themevariants = [ 'auto' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script>
    <link href="/docs/css/custom.css?1758263149" rel="stylesheet">
  </head>
  <body class="mobile-support html" data-url="/docs/programming/ai/tools_libraries/transformers/transformers_actions_02-copy/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
<nav class="TableOfContents">
  <ul>
    <li><a href="#åˆ†è¯">åˆ†è¯</a></li>
    <li><a href="#æ¨¡å‹è¾“å‡º">æ¨¡å‹è¾“å‡º</a></li>
  </ul>

  <ul>
    <li><a href="#åŠ è½½æ•°æ®é›†">åŠ è½½æ•°æ®é›†</a>
      <ul>
        <li><a href="#dataset">Dataset</a></li>
        <li><a href="#dataloader">DataLoader</a></li>
      </ul>
    </li>
    <li><a href="#è®­ç»ƒæ¨¡å‹">è®­ç»ƒæ¨¡å‹</a>
      <ul>
        <li><a href="#æ„å»ºæ¨¡å‹">æ„å»ºæ¨¡å‹</a></li>
        <li><a href="#tqdmä½¿ç”¨">tqdmä½¿ç”¨</a></li>
        <li><a href="#è®­ç»ƒæ¨¡å‹-1">è®­ç»ƒæ¨¡å‹</a></li>
        <li><a href="#æ¨¡å‹é¢„æµ‹">æ¨¡å‹é¢„æµ‹</a></li>
      </ul>
    </li>
  </ul>
</nav>
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/index.html"><span itemprop="name">liaomin416100569åšå®¢</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/index.html"><span itemprop="name">ç¼–ç¨‹å¼€å‘</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/tools_libraries/index.html"><span itemprop="name">å·¥å…·åº“</span></a><meta itemprop="position" content="3">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/tools_libraries/transformers/index.html"><span itemprop="name">transformers</span></a><meta itemprop="position" content="4">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">Transformerså®æˆ˜02-BERTé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ</span><meta itemprop="position" content="5"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/docs/programming/ai/tools_libraries/transformers/transformers_actions_01/index.html" title="Transformerså®æˆ˜01-å¼€ç®±å³ç”¨çš„ pipelines (ğŸ¡)"><i class="fa-fw fas fa-chevron-left"></i></a>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/docs/programming/ai/tools_libraries/transformers/transformers_actions_02/index.html" title="Transformerså®æˆ˜02-BERTé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ (ğŸ¡’)"><i class="fa-fw fas fa-chevron-right"></i></a>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable programming" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
  </header>

<h1 id="transformerså®æˆ˜02-berté¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ">Transformerså®æˆ˜02-BERTé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ</h1>

<p>BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯ä¸€ç§åŸºäº Transformer æ¨¡å‹çš„é¢„è®­ç»ƒè¯­è¨€è¡¨ç¤ºæ–¹æ³•ï¼Œç”±Googleç ”ç©¶å›¢é˜Ÿäº2018å¹´æå‡ºã€‚BERT é€šè¿‡åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šè¿›è¡Œæ— ç›‘ç£çš„é¢„è®­ç»ƒï¼Œå­¦ä¹ äº†é€šç”¨çš„è¯­è¨€è¡¨ç¤ºï¼Œå¹¶ä¸”åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p>BERTä»…ä½¿ç”¨äº†Transformeræ¶æ„çš„Encoderéƒ¨åˆ†ã€‚BERTè‡ª2018å¹´ç”±è°·æ­Œå‘å¸ƒåï¼Œåœ¨å¤šç§NLPä»»åŠ¡ä¸­ï¼ˆä¾‹å¦‚QAã€æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æç­‰ç­‰ï¼‰éƒ½å®ç°äº†æ›´å¥½çš„ç»“æœã€‚</p>
<p>â€œWord2vecä¸GloVeéƒ½æœ‰ä¸€ä¸ªç‰¹ç‚¹ï¼Œå°±æ˜¯å®ƒä»¬æ˜¯ä¸Šä¸‹æ–‡æ— å…³ï¼ˆcontext-freeï¼‰çš„è¯åµŒå…¥ã€‚æ‰€ä»¥å®ƒä»¬æ²¡æœ‰è§£å†³ï¼šä¸€ä¸ªå•è¯åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­ä»£è¡¨ä¸åŒçš„å«ä¹‰çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå•è¯bankï¼Œå®ƒåœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæœ‰é“¶è¡Œã€æ²³ç•”è¿™ç§å·®åˆ«éå¸¸å¤§çš„å«ä¹‰ã€‚BERTçš„å‡ºç°ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚</p>
<p>BERT çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š</p>
<ol>
<li>
<p><strong>åŒå‘æ€§</strong>ï¼šBERT ä½¿ç”¨åŒå‘ Transformer æ¨¡å‹æ¥å¤„ç†è¾“å…¥åºåˆ—ï¼Œä»è€Œèƒ½å¤ŸåŒæ—¶è€ƒè™‘ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œè€Œä¸ä»…ä»…æ˜¯å•å‘çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§åŒå‘æ€§ä½¿å¾— BERT èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å¥å­ä¸­çš„è¯­ä¹‰å’Œè¯­å¢ƒã€‚</p>
</li>
<li>
<p><strong>é¢„è®­ç»ƒ-å¾®è°ƒæ¡†æ¶</strong>ï¼šBERT ä½¿ç”¨äº†é¢„è®­ç»ƒ-å¾®è°ƒçš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œåœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šè¿›è¡Œæ— ç›‘ç£çš„é¢„è®­ç»ƒï¼Œé€šè¿‡ Masked Language Modelï¼ˆMLMï¼‰å’Œ Next Sentence Predictionï¼ˆNSPï¼‰ä»»åŠ¡å­¦ä¹ è¯­è¨€è¡¨ç¤ºï¼›ç„¶åï¼Œåœ¨ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹å‚æ•°ï¼Œä½¿å…¶é€‚åº”äºç‰¹å®šçš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚</p>
</li>
<li>
<p><strong>Transformer æ¨¡å‹</strong>ï¼šBERT åŸºäº Transformer æ¨¡å‹ç»“æ„ï¼Œå…¶ä¸­åŒ…æ‹¬å¤šå±‚çš„ç¼–ç å™¨ï¼Œæ¯ä¸ªç¼–ç å™¨ç”±è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œç»„æˆã€‚è¿™ç§ç»“æ„èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·è¾“å…¥åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</p>
</li>
<li>
<p><strong>å¤šå±‚è¡¨ç¤º</strong>ï¼šBERT æä¾›äº†å¤šå±‚çš„è¯­è¨€è¡¨ç¤ºï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥æ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©ä¸åŒå±‚çš„è¡¨ç¤ºè¿›è¡Œåº”ç”¨ã€‚è¾ƒåº•å±‚çš„è¡¨ç¤ºé€šå¸¸æ›´åŠ æ¥è¿‘åŸå§‹è¾“å…¥ï¼Œè€Œè¾ƒé«˜å±‚çš„è¡¨ç¤ºåˆ™æ›´åŠ æŠ½è±¡ï¼ŒåŒ…å«äº†æ›´å¤šçš„è¯­ä¹‰ä¿¡æ¯ã€‚</p>
</li>
<li>
<p><strong>å¼€æ”¾æºä»£ç </strong>ï¼šBERT çš„æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²ç»åœ¨ GitHub ä¸Šå¼€æ”¾ï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥åŸºäº BERT è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåº”ç”¨å¼€å‘ã€‚</p>
</li>
</ol>
<p>BERT é€šè¿‡é¢„è®­ç»ƒå¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ä¸Šçš„é€šç”¨è¯­è¨€è¡¨ç¤ºï¼Œä»¥åŠåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¾®è°ƒï¼Œæœ‰æ•ˆåœ°æé«˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶ä¸”æˆä¸ºäº†å½“å‰é¢†åŸŸå†…æœ€å…·å½±å“åŠ›çš„é¢„è®­ç»ƒæ¨¡å‹ä¹‹ä¸€ã€‚</p>
<p>transformeræä¾›äº†ä¸åŒé¢†åŸŸä¸­å¸¸è§çš„æœºå™¨å­¦ä¹ æ¨¡å‹ç±»å‹ï¼š</p>
<ol>
<li>
<p><strong>TEXT MODELSï¼ˆæ–‡æœ¬æ¨¡å‹ï¼‰</strong>ï¼šç”¨äºå¤„ç†å’Œåˆ†ææ–‡æœ¬æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„<a href="https://huggingface.co/docs/transformers/model_doc/bert" rel="external" target="_blank">BERT</a>ã€<a href="https://huggingface.co/docs/transformers/model_doc/gpt2" rel="external" target="_blank">GPT</a>ç­‰ã€‚</p>
</li>
<li>
<p><strong>VISION MODELSï¼ˆè§†è§‰æ¨¡å‹ï¼‰</strong>ï¼šç”¨äºå¤„ç†å’Œåˆ†æå›¾åƒæ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­çš„ResNetã€VGG,<a href="https://huggingface.co/docs/transformers/model_doc/vit" rel="external" target="_blank">Vision Transformer (ViT)</a>ç­‰ã€‚</p>
</li>
<li>
<p><strong>AUDIO MODELSï¼ˆéŸ³é¢‘æ¨¡å‹ï¼‰</strong>ï¼šç”¨äºå¤„ç†å’Œåˆ†æéŸ³é¢‘æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å£°å­¦æ¨¡å‹ã€è¯­éŸ³è¯†åˆ«æ¨¡å‹ç­‰ã€‚</p>
</li>
<li>
<p><strong>VIDEO MODELSï¼ˆè§†é¢‘æ¨¡å‹ï¼‰</strong>ï¼šç”¨äºå¤„ç†å’Œåˆ†æè§†é¢‘æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚è§†é¢‘åˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¡Œä¸ºè¯†åˆ«ç­‰ã€‚</p>
</li>
<li>
<p><strong>MULTIMODAL MODELSï¼ˆå¤šæ¨¡æ€æ¨¡å‹ï¼‰</strong>ï¼šç»“åˆå¤šç§æ•°æ®ç±»å‹ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰ï¼‰è¿›è¡Œåˆ†æå’Œé¢„æµ‹çš„æ¨¡å‹ï¼Œå¦‚OpenAIçš„CLIPã€‚</p>
</li>
<li>
<p><strong>REINFORCEMENT LEARNING MODELSï¼ˆå¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼‰</strong>ï¼šç”¨äºè§£å†³å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„æ¨¡å‹ï¼Œå¦‚Deep Q-Networksï¼ˆDQNï¼‰ã€Actor-Criticç­‰ã€‚</p>
</li>
<li>
<p><strong>TIME SERIES MODELSï¼ˆæ—¶é—´åºåˆ—æ¨¡å‹ï¼‰</strong>ï¼šç”¨äºåˆ†æå’Œé¢„æµ‹æ—¶é—´åºåˆ—æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ç­‰ã€‚</p>
</li>
<li>
<p><strong>GRAPH MODELSï¼ˆå›¾æ¨¡å‹ï¼‰</strong>ï¼šç”¨äºå¤„ç†å’Œåˆ†æå›¾æ•°æ®çš„æ¨¡å‹ï¼Œå¦‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ã€å›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰ç­‰ã€‚</p>
</li>
</ol>
<h1 id="bertçš„åŸºæœ¬åŸç†">BERTçš„åŸºæœ¬åŸç†</h1>
<p>BERTåŸºäºçš„æ˜¯Transformeræ¨¡å‹ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨Transformeræ¨¡å‹çš„Encoderéƒ¨åˆ†ã€‚åœ¨Transformeræ¨¡å‹ä¸­ï¼ŒEncoderçš„è¾“å…¥æ˜¯ä¸€ä¸²åºåˆ—ï¼Œè¾“å‡ºçš„æ˜¯å¯¹åºåˆ—ä¸­æ¯ä¸ªå­—ç¬¦çš„è¡¨ç¤ºã€‚åŒæ ·ï¼Œåœ¨BERTä¸­ï¼Œè¾“å…¥çš„æ˜¯ä¸€ä¸²åºåˆ—ï¼Œè¾“å‡ºçš„æ˜¯ä¹Ÿæ˜¯å¯¹åº”åºåˆ—ä¸­æ¯ä¸ªå•è¯çš„ç¼–ç ã€‚
ä»¥â€œHe got bit by Pythonâ€ä¸ºä¾‹ï¼ŒBERTçš„è¾“å…¥è¾“å‡ºå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
<a href="#R-image-ac5c2b64d1da0988c1b419fb4cdac3fc" class="lightbox-link"><img alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/tools_libraries/transformers/transformers_actions_02.md.images/77d14abf55fea6c76269fcdeb93bb7c0.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-ac5c2b64d1da0988c1b419fb4cdac3fc"><img alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/tools_libraries/transformers/transformers_actions_02.md.images/77d14abf55fea6c76269fcdeb93bb7c0.png"></a>
å…¶ä¸­è¾“å…¥ä¸ºåºåˆ—â€œHe got bit by Pythonâ€ï¼Œè¾“å‡ºçš„æ˜¯å¯¹æ¯ä¸ªå•è¯çš„ç¼–ç $R_{word}$ã€‚è¿™æ ·åœ¨ç»è¿‡äº†BERTå¤„ç†åï¼Œå³å¾—åˆ°äº†å¯¹æ¯ä¸ªå•è¯åŒ…å«çš„ä¸Šä¸‹æ–‡è¡¨ç¤º$R_{word}$ã€‚</p>
<h2 id="åˆ†è¯">åˆ†è¯</h2>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>from transformers import AutoModel, BertTokenizer
model_name=&#34;bert-base-chinese&#34; #bert-base-uncased
model=AutoModel.from_pretrained(model_name)
tokenizer=BertTokenizer.from_pretrained(model_name)
print(type(model),type(tokenizer))
sequence = [&#34;æˆ‘å‡ºç”Ÿåœ¨æ¹–å—Aé˜³,æˆ‘å¾—å®¶åœ¨æ·±åœ³.&#34;,&#34;æˆ‘å¾—å„¿å­æ˜¯å»–Xè°¦&#34;]
#è¾“å‡ºä¸­åŒ…å«ä¸¤ä¸ªé”® input_ids å’Œ attention_maskï¼Œå…¶ä¸­ input_ids å¯¹åº”åˆ†è¯ä¹‹åçš„ tokens æ˜ å°„åˆ°çš„æ•°å­—ç¼–å·åˆ—è¡¨ï¼Œè€Œ attention_mask åˆ™æ˜¯ç”¨æ¥æ ‡è®°å“ªäº› tokens #æ˜¯è¢«å¡«å……çš„ï¼ˆè¿™é‡Œâ€œ1â€è¡¨ç¤ºæ˜¯åŸæ–‡ï¼Œâ€œ0â€è¡¨ç¤ºæ˜¯å¡«å……å­—ç¬¦ï¼‰ã€‚
print(tokenizer(sequence, padding=True, truncation=True, return_tensors=&#34;pt&#34;,pair=True))
#å°†è¾“å…¥åˆ‡åˆ†ä¸ºè¯è¯­ã€å­è¯æˆ–è€…ç¬¦å·ï¼ˆä¾‹å¦‚æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œç»Ÿç§°ä¸º tokensï¼›
print(tokenizer.tokenize(sequence[0]),len(tokenizer.tokenize(sequence[0])))
#æˆ‘ä»¬é€šè¿‡ convert_tokens_to_ids() å°†åˆ‡åˆ†å‡ºçš„ tokens è½¬æ¢ä¸ºå¯¹åº”çš„ token IDsï¼š
print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence[0])))
#å¯ä»¥é€šè¿‡ encode() å‡½æ•°å°†è¿™ä¸¤ä¸ªæ­¥éª¤åˆå¹¶ï¼Œå¹¶ä¸” encode() ä¼šè‡ªåŠ¨æ·»åŠ æ¨¡å‹éœ€è¦çš„ç‰¹æ®Š tokenï¼Œä¾‹å¦‚ BERT åˆ†è¯å™¨ä¼šåˆ†åˆ«åœ¨åºåˆ—çš„é¦–å°¾æ·»åŠ [CLS] å’Œ [SEP]
print(tokenizer.encode(sequence[0]))
#è§£ç è¿˜åŸæ–‡å­—ï¼Œå¯ä»¥çœ‹åˆ°encodeå‰ååŠ äº†[CLS] å’Œ [SEP]
print(tokenizer.decode(tokenizer.encode(sequence[1])))</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>&lt;class &#39;transformers.models.bert.modeling_bert.BertModel&#39;&gt; &lt;class &#39;transformers.models.bert.tokenization_bert.BertTokenizer&#39;&gt;
{&#39;input_ids&#39;: tensor([[ 101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345,  117, 2769, 2533,
         2157, 1762, 3918, 1766,  119,  102],
        [ 101, 2769, 2533, 1036, 2094, 3221, 2445, 3813, 6472,  102,    0,    0,
            0,    0,    0,    0,    0,    0]]), &#39;token_type_ids&#39;: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}
[&#39;æˆ‘&#39;, &#39;å‡º&#39;, &#39;ç”Ÿ&#39;, &#39;åœ¨&#39;, &#39;æ¹–&#39;, &#39;å—&#39;, &#39;A&#39;, &#39;é˜³&#39;, &#39;,&#39;, &#39;æˆ‘&#39;, &#39;å¾—&#39;, &#39;å®¶&#39;, &#39;åœ¨&#39;, &#39;æ·±&#39;, &#39;åœ³&#39;, &#39;.&#39;] 16
[2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 2533, 2157, 1762, 3918, 1766, 119]
[101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 2533, 2157, 1762, 3918, 1766, 119, 102]
[CLS] æˆ‘ å¾— å„¿ å­ æ˜¯ å»– X è°¦ [SEP]</code></pre></div>
<h2 id="æ¨¡å‹è¾“å‡º">æ¨¡å‹è¾“å‡º</h2>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#è¿™é‡Œæ¼”ç¤ºæœ€ç»ˆè¾“å‡ºéšè—çŠ¶æ€å¾—è¾“å‡º
from transformers import AutoModel,AutoTokenizer
model_name=&#34;bert-base-chinese&#34; #bert-base-uncased
model=AutoModel.from_pretrained(model_name)
tokenizer=BertTokenizer.from_pretrained(model_name)
raw_inputs = [
    &#34;I&#39;ve been waiting for a HuggingFace course my whole life.&#34;,
    &#34;I hate this so much!&#34;,
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=&#34;pt&#34;)
outputs = model(**inputs)
print(&#34;è¯ä¸ªæ•°&#34;,len(tokenizer.encode(raw_inputs[0])))
&#34;&#34;&#34;
åœ¨BERTæ¨¡å‹ä¸­ï¼Œlast_hidden_state çš„å½¢çŠ¶æ˜¯ [batch_size, sequence_length, hidden_size]ï¼Œå…¶ä¸­ï¼š
 batch_size è¡¨ç¤ºæ‰¹é‡å¤§å°ï¼Œå³è¾“å…¥çš„æ ·æœ¬æ•°é‡ã€‚åœ¨ä½ çš„ä¾‹å­ä¸­ï¼Œbatch_size æ˜¯ 2ï¼Œè¡¨ç¤ºä½ æœ‰ä¸¤ä¸ªå¥å­ã€‚
 sequence_length è¡¨ç¤ºåºåˆ—é•¿åº¦ï¼Œå³è¾“å…¥æ–‡æœ¬ä¸­è¯å…ƒçš„æ•°é‡ã€‚åœ¨ä½ çš„ä¾‹å­ä¸­ï¼Œsequence_length æ˜¯ 19ï¼Œè¡¨ç¤ºæ¯ä¸ªå¥å­åŒ…å« 19 ä¸ªè¯å…ƒ,æˆ‘çˆ±ä¸­å›½ï¼Œæˆ‘å°±æ˜¯ä¸€ä¸ªè¯å…ƒï¼Œçˆ±ä¹Ÿæ˜¯ä¸€ä¸ªè¯å…ƒã€‚
 hidden_size è¡¨ç¤ºéšè—çŠ¶æ€çš„ç»´åº¦ï¼Œé€šå¸¸æ˜¯æ¨¡å‹çš„éšè—å±‚çš„å¤§å°ã€‚åœ¨BERT-baseæ¨¡å‹ä¸­ï¼Œhidden_size æ˜¯ 768ï¼Œè¡¨ç¤ºæ¯ä¸ªè¯å…ƒçš„éšè—çŠ¶æ€æ˜¯ä¸€ä¸ªåŒ…å« 768 ä¸ªå€¼çš„å‘é‡ã€‚
&#34;&#34;&#34;
print(outputs.last_hidden_state.shape)</code></pre></div>
<p>è¾“å‡ºï¼štorch.Size([2, 19, 768])</p>
<h1 id="berté¢„è®­ç»ƒçš„æ–¹æ³•">BERTé¢„è®­ç»ƒçš„æ–¹æ³•</h1>
<p>BERTçš„é¢„è®­ç»ƒè¯­æ–™åº“ä½¿ç”¨çš„æ˜¯Toronto BookCorpuså’ŒWikipediaæ•°æ®é›†ã€‚åœ¨å‡†å¤‡è®­ç»ƒæ•°æ®æ—¶ï¼Œé¦–å…ˆä»è¯­æ–™åº“ä¸­é‡‡æ ·2æ¡å¥å­ï¼Œä¾‹å¦‚Sentence-Aä¸Sentence-Bã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼š2æ¡å¥å­çš„å•è¯ä¹‹å’Œä¸èƒ½è¶…è¿‡512ä¸ªã€‚å¯¹äºé‡‡é›†çš„è¿™äº›å¥å­ï¼Œ50%ä¸ºä¸¤ä¸ªå¥å­æ˜¯ç›¸é‚»å¥å­ï¼Œå¦50%ä¸ºä¸¤ä¸ªå¥å­æ¯«æ— å…³ç³»ã€‚</p>
<p>å‡è®¾é‡‡é›†äº†ä»¥ä¸‹2æ¡å¥å­ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>Beijing is a beautiful city
I love Beijing</code></pre></div>
<p>å¯¹è¿™2æ¡å¥å­å…ˆåšåˆ†è¯ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>Tokens = [ [CLS], Beijing, is, a, beautiful, city, [SEP], I, love, Beijing, [SEP] ]</code></pre></div>
<p>ç„¶åï¼Œä»¥15%çš„æ¦‚ç‡é®æŒ¡å•è¯ï¼Œå¹¶éµå¾ª80%-10%-10%çš„è§„åˆ™ã€‚å‡è®¾é®æŒ¡çš„å•è¯ä¸ºcityï¼Œåˆ™ï¼š</p>
<p>Tokens = [ [CLS], Beijing, is, a, beautiful, [MASK], [SEP], I, love, Beijing, [SEP] ]</p>
<p>æ¥ä¸‹æ¥å°†Tokensé€å…¥åˆ°BERTä¸­ï¼Œå¹¶è®­ç»ƒBERTé¢„æµ‹è¢«é®æŒ¡çš„å•è¯ï¼ŒåŒæ—¶ä¹Ÿè¦é¢„æµ‹è¿™2æ¡å¥å­æ˜¯å¦ä¸ºç›¸é‚»ï¼ˆå¥å­2æ˜¯å¥å­1çš„ä¸‹ä¸€æ¡å¥å­ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒBERTæ˜¯åŒæ—¶è®­ç»ƒMasked Language Modelingå’ŒNSPä»»åŠ¡ã€‚</p>
<p>BERTçš„è®­ç»ƒå‚æ•°æ˜¯ï¼š1000000ä¸ªstepï¼Œæ¯ä¸ªbatchåŒ…å«256æ¡åºåˆ—ï¼ˆ256 * 512ä¸ªå•è¯ = 128000å•è¯/batchï¼‰ã€‚ä½¿ç”¨çš„æ˜¯Adamï¼Œlearning rateä¸º1e-4ã€Î²1 = 0.9ã€Î²2 = 0.999ã€‚L2æ­£åˆ™æƒé‡çš„è¡°å‡å‚æ•°ä¸º0.01ã€‚å¯¹äºlearning reteï¼Œå‰10000ä¸ªstepsä½¿ç”¨äº†rate warmupï¼Œä¹‹åå¼€å§‹çº¿æ€§è¡°å‡learning rateï¼ˆç®€å•åœ°è¯´ï¼Œå°±æ˜¯å‰æœŸè®­ç»ƒä½¿ç”¨ä¸€ä¸ªè¾ƒå¤§çš„learning rateï¼ŒåæœŸå¼€å§‹çº¿æ€§å‡å°‘ï¼‰ã€‚å¯¹æ‰€æœ‰layerä½¿ç”¨0.1æ¦‚ç‡çš„dropoutã€‚ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ä¸ºgeluï¼Œè€Œéreluã€‚
éªŒè¯ä½¿ç”¨ä¸¤æ¡å¥å­ã€‚</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>checkpoint = &#34;bert-base-chinese&#34;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
raw_inputs = [
    &#34;æ‹¼å¤šå¤šå¾—è´§ç‰©çœŸæ˜¯å·®åŠ².&#34;,
    &#34;æˆ‘å–œæ¬¢å¤©çŒ«ï¼Œå¤©çŒ«è´§ç‰©éƒ½å¾ˆå¥½&#34;,
]
raw_inputs1 = [
    &#34;æ‹¼å¤šå¤šä¹°äº†ä¸€ä»¶æ‰è‰²è¡£æœ.&#34;,
    &#34;æˆ‘åœ¨å¤©çŒ«ä¹°çš„è¡£æœé¢œè‰²è¿˜è¡Œ&#34;,
]
#å…è®¸ä¼ å…¥ä¸¤ä¸ªæ•°ç»„ï¼Œç›¸åŒç´¢å¼•ä¼šè‡ªåŠ¨é€šè¿‡[SEP]æ‹¼æ¥ã€‚
inputs = tokenizer(raw_inputs,raw_inputs1, padding=True, truncation=True, return_tensors=&#34;pt&#34;)
print(tokenizer.decode(inputs.input_ids[0]))
print(tokenizer.decode(inputs.input_ids[1]))</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>[CLS] æ‹¼ å¤š å¤š å¾— è´§ ç‰© çœŸ æ˜¯ å·® åŠ². [SEP] æ‹¼ å¤š å¤š ä¹° äº† ä¸€ ä»¶ æ‰ è‰² è¡£ æœ. [SEP] [PAD] [PAD]
[CLS] æˆ‘ å–œ æ¬¢ å¤© çŒ« ï¼Œ å¤© çŒ« è´§ ç‰© éƒ½ å¾ˆ å¥½ [SEP] æˆ‘ åœ¨ å¤© çŒ« ä¹° çš„ è¡£ æœ é¢œ è‰² è¿˜ è¡Œ [SEP]</code></pre></div>
<p>é¢„æµ‹çš„æ•´ä¸ªè¿‡ç¨‹</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#æ¼”ç¤ºé¢„æµ‹çš„æ•´ä¸ªè¿‡ç¨‹ã€‚
import torch
from transformers import AutoModelForSequenceClassification
#æƒ…æ„Ÿåˆ†æä»»åŠ¡
checkpoint = &#34;bert-base-chinese&#34;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
print(type(model))
raw_inputs = [
    &#34;æ‹¼å¤šå¤šå¾—è´§ç‰©çœŸæ˜¯å·®åŠ².&#34;,
    &#34;æˆ‘å–œæ¬¢å¤©çŒ«ï¼Œå¤©çŒ«è´§ç‰©éƒ½å¾ˆå¥½&#34;,
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=&#34;pt&#34;)
outputs = model(**inputs)
#å°†åˆ†è¯çš„è¯åç¼–ç å‡ºæ¥
print(tokenizer.decode(inputs.input_ids[0]),tokenizer.decode(inputs.input_ids[1]))
#&#34;Logits&#34; æ˜¯æŒ‡æ¨¡å‹åœ¨åˆ†ç±»é—®é¢˜ä¸­è¾“å‡ºçš„æœªç»è¿‡ softmax æˆ– sigmoid å‡½æ•°å¤„ç†çš„åŸå§‹é¢„æµ‹å€¼ã€‚
print(&#34;åˆ†ç±»è¾“å‡ºå½¢çŠ¶:&#34;,outputs.logits.shape)
print(&#34;åˆ†ç±»è¾“å‡º:&#34;,outputs.logits)
#ç»è¿‡softmaxå°±æ˜¯é¢„æµ‹çš„ç»“æœäº†
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
#é¢„æµ‹çš„æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªå¥å­ï¼Œç¬¬ä¸€åˆ—è¡¨ç¤ºç§¯æçš„æ¦‚ç‡ï¼Œç¬¬äºŒåˆ—è¡¨ç¤ºä¸ç§¯æçš„æ¦‚ç‡
print(&#34;é¢„æµ‹ç»“æœ:&#34;,predictions)
#æœ‰ä¸¤ç§åˆ†ç±»0è¡¨ç¤ºç§¯æï¼Œ1è¡¨ç¤ºä¸ç§¯æ
print(&#34;labelå’Œç´¢å¼•:&#34;,print(model.config.id2label))</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>[CLS] æ‹¼ å¤š å¤š å¾— è´§ ç‰© çœŸ æ˜¯ å·® åŠ². [SEP] [PAD] [PAD] [CLS] æˆ‘ å–œ æ¬¢ å¤© çŒ« ï¼Œ å¤© çŒ« è´§ ç‰© éƒ½ å¾ˆ å¥½ [SEP]
åˆ†ç±»è¾“å‡ºå½¢çŠ¶: torch.Size([2, 2])
åˆ†ç±»è¾“å‡º: tensor([[0.4789, 1.0043],
        [0.2907, 0.7432]], grad_fn=&lt;AddmmBackward0&gt;)
é¢„æµ‹ç»“æœ: tensor([[0.3716, 0.6284],
        [0.3888, 0.6112]], grad_fn=&lt;SoftmaxBackward0&gt;)
{0: &#39;LABEL_0&#39;, 1: &#39;LABEL_1&#39;}</code></pre></div>
<h1 id="bertæ¨¡å‹å¾®è°ƒ">BERTæ¨¡å‹å¾®è°ƒ</h1>
<h2 id="åŠ è½½æ•°æ®é›†">åŠ è½½æ•°æ®é›†</h2>
<p>æˆ‘ä»¬ä»¥åŒä¹‰å¥åˆ¤æ–­ä»»åŠ¡ä¸ºä¾‹ï¼ˆæ¯æ¬¡è¾“å…¥ä¸¤ä¸ªå¥å­ï¼Œåˆ¤æ–­å®ƒä»¬æ˜¯å¦ä¸ºåŒä¹‰å¥ï¼‰ï¼Œå¸¦å¤§å®¶æ„å»ºæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ª Transformers æ¨¡å‹ã€‚æˆ‘ä»¬é€‰æ‹©èš‚èšé‡‘èè¯­ä¹‰ç›¸ä¼¼åº¦æ•°æ®é›† <a href="https://storage.googleapis.com/cluebenchmark/tasks/afqmc_public.zip" rel="external" target="_blank">AFQMC</a> ä½œä¸ºè¯­æ–™ï¼Œå®ƒæä¾›äº†å®˜æ–¹çš„æ•°æ®åˆ’åˆ†ï¼Œè®­ç»ƒé›†ï¼ˆtrain.jsonï¼‰ / éªŒè¯é›†ï¼ˆdev.jsonï¼‰ / æµ‹è¯•é›†(test.json)åˆ†åˆ«åŒ…å« 34334 / 4316 / 3861 ä¸ªå¥å­å¯¹ï¼Œæ ‡ç­¾ 0 è¡¨ç¤ºéåŒä¹‰å¥ï¼Œ1 è¡¨ç¤ºåŒä¹‰å¥ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>{&#34;sentence1&#34;: &#34;è¿˜æ¬¾è¿˜æ¸…äº†ï¼Œä¸ºä»€ä¹ˆèŠ±å‘—è´¦å•æ˜¾ç¤ºè¿˜è¦è¿˜æ¬¾&#34;, &#34;sentence2&#34;: &#34;èŠ±å‘—å…¨é¢è¿˜æ¸…æ€ä¹ˆæ˜¾ç¤ºæ²¡æœ‰è¿˜æ¬¾&#34;, &#34;label&#34;: &#34;1&#34;}</code></pre></div>
<blockquote>
<p>è®­ç»ƒé›†ç”¨äºè®­ç»ƒæ¨¡å‹ï¼ŒéªŒè¯é›†ç”¨äºæ¯æ¬¡epochåè®­ç»ƒé›†çš„æ­£ç¡®ç‡ï¼Œæµ‹è¯•é›†ç”¨äºéªŒè¯æœ€åç”Ÿæˆæ¨¡å‹çš„å‡†ç¡®ç‡ã€‚</p></blockquote>
<h3 id="dataset">Dataset</h3>
<p>Pytorch é€šè¿‡ Dataset ç±»å’Œ DataLoader ç±»å¤„ç†æ•°æ®é›†å’ŒåŠ è½½æ ·æœ¬ã€‚åŒæ ·åœ°ï¼Œè¿™é‡Œæˆ‘ä»¬é¦–å…ˆç»§æ‰¿ Dataset ç±»æ„é€ è‡ªå®šä¹‰æ•°æ®é›†ï¼Œä»¥ç»„ç»‡æ ·æœ¬å’Œæ ‡ç­¾ã€‚AFQMC æ ·æœ¬ä»¥ json æ ¼å¼å­˜å‚¨ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨ json åº“æŒ‰è¡Œè¯»å–æ ·æœ¬ï¼Œå¹¶ä¸”ä»¥è¡Œå·ä½œä¸ºç´¢å¼•æ„å»ºæ•°æ®é›†ã€‚</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>class MyDataSet(Dataset):
    def __init__(self,filePath):
        self.data={}
        current_directory = os.getcwd()
        with open(current_directory+&#34;/dataset/&#34;+filePath,&#34;rt&#34;, encoding=&#34;utf-8&#34;) as f:
            for idx,line in enumerate(f):
                self.data[idx]=json.loads(line.strip())
    def __getitem__(self, item):
        return self.data[item]

    def __len__(self):
        return len(self.data)
        
train_data=MyDataSet(&#34;train.json&#34;)
dev_data=MyDataSet(&#34;dev.json&#34;)
print(dev_data[1])</code></pre></div>
<p>è¾“å‡º:</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>{&#39;id&#39;: 1, &#39;sentence1&#39;: &#39;ç½‘å•†è´·æ€ä¹ˆè½¬å˜æˆå€Ÿå‘—&#39;, &#39;sentence2&#39;: &#39;å¦‚ä½•å°†ç½‘å•†è´·åˆ‡æ¢ä¸ºå€Ÿå‘—&#39;}</code></pre></div>
<p>å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬ç¼–å†™çš„ AFQMC ç±»æˆåŠŸè¯»å–äº†æ•°æ®é›†ï¼Œæ¯ä¸€ä¸ªæ ·æœ¬éƒ½ä»¥å­—å…¸å½¢å¼ä¿å­˜ï¼Œåˆ†åˆ«ä»¥ sentence1ã€sentence2 å’Œ label ä¸ºé”®å­˜å‚¨å¥å­å¯¹å’Œæ ‡ç­¾ã€‚</p>
<p>å¦‚æœæ•°æ®é›†éå¸¸å·¨å¤§ï¼Œéš¾ä»¥ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç»§æ‰¿ IterableDataset ç±»æ„å»ºè¿­ä»£å‹æ•°æ®é›†ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>class MyDataSetIter(IterableDataset):
    def __init__(self,filePath):
        self.filePath=filePath
    def __iter__(self):
        current_directory = os.getcwd()
        with open(current_directory+&#34;/dataset/&#34;+self.filePath,&#34;rt&#34;, encoding=&#34;utf-8&#34;) as f:
            for _,line in enumerate(f):
                data=json.loads(line.strip())
                yield data
print(next(iter(MyDataSetIter(&#34;dev.json&#34;))))       </code></pre></div>
<p>è¾“å‡ºï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>{&#39;sentence1&#39;: &#39;åŒåä¸€èŠ±å‘—æé¢åœ¨å“ª&#39;, &#39;sentence2&#39;: &#39;é‡Œå¯ä»¥æèŠ±å‘—é¢åº¦&#39;, &#39;label&#39;: &#39;0&#39;}</code></pre></div>
<h3 id="dataloader">DataLoader</h3>
<p>æ¥ä¸‹æ¥å°±éœ€è¦é€šè¿‡ DataLoader åº“æŒ‰æ‰¹ (batch) åŠ è½½æ•°æ®ï¼Œå¹¶ä¸”å°†æ ·æœ¬è½¬æ¢æˆæ¨¡å‹å¯ä»¥æ¥å—çš„è¾“å…¥æ ¼å¼ã€‚å¯¹äº NLP ä»»åŠ¡ï¼Œè¿™ä¸ªç¯èŠ‚å°±æ˜¯å°†æ¯ä¸ª batch ä¸­çš„æ–‡æœ¬æŒ‰ç…§é¢„è®­ç»ƒæ¨¡å‹çš„æ ¼å¼è¿›è¡Œç¼–ç ï¼ˆåŒ…æ‹¬ Paddingã€æˆªæ–­ç­‰æ“ä½œï¼‰ã€‚</p>
<p>æˆ‘ä»¬é€šè¿‡æ‰‹å·¥ç¼–å†™ DataLoader çš„æ‰¹å¤„ç†å‡½æ•° collate_fn æ¥å®ç°ã€‚é¦–å…ˆåŠ è½½åˆ†è¯å™¨ï¼Œç„¶åå¯¹æ¯ä¸ª batch ä¸­çš„æ‰€æœ‰å¥å­å¯¹è¿›è¡Œç¼–ç ï¼ŒåŒæ—¶æŠŠæ ‡ç­¾è½¬æ¢ä¸ºå¼ é‡æ ¼å¼ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#DataLoaderå¤„ç†æ•°æ®ä¸ºseq1 [SEP] seq2
from transformers import AutoTokenizer
import torch
from torch.utils.data import DataLoader
checkpoint = &#34;bert-base-chinese&#34;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
def collote_fn(batch_samples):
    batch_sentence_1, batch_sentence_2 = [], []
    batch_label = []
    for sample in batch_samples:
        batch_sentence_1.append(sample[&#39;sentence1&#39;])
        batch_sentence_2.append(sample[&#39;sentence2&#39;])
        batch_label.append(int(sample[&#39;label&#39;]))
    X = tokenizer(
        batch_sentence_1,
        batch_sentence_2,
        padding=True,
        truncation=True,
        return_tensors=&#34;pt&#34;
    )
    y = torch.tensor(batch_label)
    return X, y
train_loader=DataLoader(train_data,batch_size=4,shuffle=False,collate_fn=collote_fn)
X,y=next(iter(train_loader))
print(&#34;labelçš„ç»´åº¦&#34;,y.shape)
print(&#34;s1,s2åˆå¹¶çš„ç»´åº¦&#34;,X.input_ids.shape)
for idx,d in enumerate(X.input_ids):
    print(&#34;ç¬¬ä¸€æ‰¹æ¬¡4ä¸ªå…ƒç´ ä¸­çš„ç¬¬{}ä¸ªï¼š{},label={}&#34;.format(idx,tokenizer.decode(X.input_ids[idx]),y[idx]))</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>labelçš„ç»´åº¦ torch.Size([4])
s1,s2åˆå¹¶çš„ç»´åº¦ torch.Size([4, 30])
ç¬¬ä¸€æ‰¹æ¬¡4ä¸ªå…ƒç´ ä¸­çš„ç¬¬0ä¸ªï¼š[CLS] èš‚ èš å€Ÿ å‘— ç­‰ é¢ è¿˜ æ¬¾ å¯ ä»¥ æ¢ æˆ å…ˆ æ¯ å æœ¬ å— [SEP] å€Ÿ å‘— æœ‰ å…ˆ æ¯ åˆ° æœŸ è¿˜ æœ¬ å— [SEP],label=0
ç¬¬ä¸€æ‰¹æ¬¡4ä¸ªå…ƒç´ ä¸­çš„ç¬¬1ä¸ªï¼š[CLS] èš‚ èš èŠ± å‘— è¯´ æˆ‘ è¿ çº¦ ä¸€ æ¬¡ [SEP] èš‚ èš èŠ± å‘— è¿ çº¦ è¡Œ ä¸º æ˜¯ ä»€ ä¹ˆ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD],label=0
ç¬¬ä¸€æ‰¹æ¬¡4ä¸ªå…ƒç´ ä¸­çš„ç¬¬2ä¸ªï¼š[CLS] å¸® æˆ‘ çœ‹ ä¸€ ä¸‹ æœ¬ æœˆ èŠ± å‘— è´¦ å• æœ‰ æ²¡ æœ‰ ç»“ æ¸… [SEP] ä¸‹ æœˆ èŠ± å‘— è´¦ å• [SEP] [PAD] [PAD] [PAD] [PAD] [PAD],label=0
ç¬¬ä¸€æ‰¹æ¬¡4ä¸ªå…ƒç´ ä¸­çš„ç¬¬3ä¸ªï¼š[CLS] èš‚ èš å€Ÿ å‘— å¤š é•¿ æ—¶ é—´ ç»¼ åˆ è¯„ ä¼° ä¸€ æ¬¡ [SEP] å€Ÿ å‘— å¾— è¯„ ä¼° å¤š ä¹… [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD],label=0</code></pre></div>
<p>å¯ä»¥çœ‹åˆ°ï¼ŒDataLoader æŒ‰ç…§æˆ‘ä»¬è®¾ç½®çš„ batch size æ¯æ¬¡å¯¹ 4 ä¸ªæ ·æœ¬è¿›è¡Œç¼–ç ï¼Œå¹¶ä¸”é€šè¿‡è®¾ç½® padding=True å’Œ truncation=True æ¥è‡ªåŠ¨å¯¹æ¯ä¸ª batch ä¸­çš„æ ·æœ¬è¿›è¡Œè¡¥å…¨å’Œæˆªæ–­ã€‚è¿™é‡Œæˆ‘ä»¬é€‰æ‹© BERT æ¨¡å‹ä½œä¸º checkpointï¼Œæ‰€ä»¥æ¯ä¸ªæ ·æœ¬éƒ½è¢«å¤„ç†æˆäº†â€œäº†â€œ[CLS] sen1 [SEP] sen2 [SEP]â€çš„å½¢å¼ã€‚</p>
<blockquote>
<p>è¿™ç§åªåœ¨ä¸€ä¸ª batch å†…è¿›è¡Œè¡¥å…¨çš„æ“ä½œè¢«ç§°ä¸ºåŠ¨æ€è¡¥å…¨ (Dynamic padding)ï¼ŒHugging Face ä¹Ÿæä¾›äº† <code>DataCollatorWithPadding</code> ç±»æ¥è¿›è¡Œï¼Œå¦‚æœæ„Ÿå…´è¶£å¯ä»¥è‡ªè¡Œ<a href="https://huggingface.co/course/chapter3/2?fw=pt#dynamic-padding" rel="external" target="_blank">äº†è§£</a>ã€‚</p></blockquote>
<h2 id="è®­ç»ƒæ¨¡å‹">è®­ç»ƒæ¨¡å‹</h2>
<h3 id="æ„å»ºæ¨¡å‹">æ„å»ºæ¨¡å‹</h3>
<p>å¸¸è§çš„å†™æ³•æ˜¯ç»§æ‰¿ Transformers åº“ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹æ¥åˆ›å»ºè‡ªå·±çš„æ¨¡å‹ã€‚ä¾‹å¦‚è¿™é‡Œæˆ‘ä»¬å¯ä»¥ç»§æ‰¿ BERT æ¨¡å‹ï¼ˆBertPreTrainedModel ç±»ï¼‰æ¥åˆ›å»ºä¸€ä¸ªä¸ä¸Šé¢æ¨¡å‹ç»“æ„å®Œå…¨ç›¸åŒçš„åˆ†ç±»å™¨ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#æ„å»ºæ¨¡å‹
from transformers import BertPreTrainedModel,BertModel,AutoConfig
from torch import nn
class BertForPartwiseCLs(BertPreTrainedModel):
    &#34;&#34;&#34;
    å®šä¹‰æ¨¡å‹ç»§æ‰¿è‡ªBertPreTrainedModel
    &#34;&#34;&#34;
    def __init__(self,config):
        &#34;&#34;&#34;
        ä¼ å…¥configï¼ŒåŸå§‹é•œåƒçš„config
        &#34;&#34;&#34;
        super().__init__(config)
        #å®šä¹‰BertModel
        self.model=BertModel(config, add_pooling_layer=False)
        #ä¸¢å¼ƒ10%
        self.dropout=nn.Dropout(config.hidden_dropout_prob)
        #å…¨è¿æ¥ä¸º2åˆ†ç±»
        self.classifier=nn.Linear(768,2)
        #åˆå§‹åŒ–æƒé‡å‚æ•°
        self.post_init()
    def forward(self,input):
        #æ‰§è¡Œæ¨¡å‹äº§ç”Ÿä¸€ä¸ª(æ‰¹æ¬¡ï¼Œè¯å…ƒ,éšè—ç¥ç»å…ƒ)çš„è¾“å‡º
        bert_output=self.model(**input)
        #è¾“å‡ºçš„æ•°æ®æœ‰å¤šä¸ªè¯å…ƒï¼Œå–ç¬¬ä¸€ä¸ª[CLS]è¯å…ƒï¼Œå› ä¸ºæ¯ä¸ªè¯å…ƒé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶éƒ½åŒ…å«äº†å’Œå…¶ä»–è¯çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæ‰€ä»¥åªéœ€è¦ä¸€ä¸ªå³å¯
        #è¿™é‡Œå¥å­çš„ç»´åº¦ç¼–ç¨‹äº†[æ‰¹æ¬¡,1,768]
        vector_data=bert_output.last_hidden_state[:,0,:]
        vector_data=self.dropout(vector_data)
        logits=self.classifier(vector_data)
        return logits
checkpoint = &#34;bert-base-chinese&#34;
config=AutoConfig.from_pretrained(checkpoint)
model=BertForPartwiseCLs.from_pretrained(checkpoint,config=config)
print(model)
X,y=next(iter(train_loader))
print(model(X).shape)</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>D:\python\evn311\Lib\site-packages\huggingface_hub\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of BertForPartwiseCLs were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: [&#39;bert.classifier.bias&#39;, &#39;bert.classifier.weight&#39;, &#39;bert.model.embeddings.LayerNorm.bias&#39;, &#39;bert.model.embeddings.LayerNorm.weight&#39;, &#39;bert.model.embeddings.position_embeddings.weight&#39;, &#39;bert.model.embeddings.token_type_embeddings.weight&#39;, &#39;bert.model.embeddings.word_embeddings.weight&#39;, &#39;bert.model.encoder.layer.0.attention.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.0.attention.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.0.attention.output.dense.bias&#39;, &#39;bert.model.encoder.layer.0.attention.output.dense.weight&#39;, &#39;bert.model.encoder.layer.0.attention.self.key.bias&#39;, &#39;bert.model.encoder.layer.0.attention.self.key.weight&#39;, &#39;bert.model.encoder.layer.0.attention.self.query.bias&#39;, &#39;bert.model.encoder.layer.0.attention.self.query.weight&#39;, &#39;bert.model.encoder.layer.0.attention.self.value.bias&#39;, &#39;bert.model.encoder.layer.0.attention.self.value.weight&#39;, &#39;bert.model.encoder.layer.0.intermediate.dense.bias&#39;, &#39;bert.model.encoder.layer.0.intermediate.dense.weight&#39;, &#39;bert.model.encoder.layer.0.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.0.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.0.output.dense.bias&#39;, &#39;bert.model.encoder.layer.0.output.dense.weight&#39;, &#39;bert.model.encoder.layer.1.attention.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.1.attention.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.1.attention.output.dense.bias&#39;, &#39;bert.model.encoder.layer.1.attention.output.dense.weight&#39;, &#39;bert.model.encoder.layer.1.attention.self.key.bias&#39;, &#39;bert.model.encoder.layer.1.attention.self.key.weight&#39;, &#39;bert.model.encoder.layer.1.attention.self.query.bias&#39;, &#39;bert.model.encoder.layer.1.attention.self.query.weight&#39;, &#39;bert.model.encoder.layer.1.attention.self.value.bias&#39;, &#39;bert.model.encoder.layer.1.attention.self.value.weight&#39;, &#39;bert.model.encoder.layer.1.intermediate.dense.bias&#39;, &#39;bert.model.encoder.layer.1.intermediate.dense.weight&#39;, &#39;bert.model.encoder.layer.1.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.1.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.1.output.dense.bias&#39;, &#39;bert.model.encoder.layer.1.output.dense.weight&#39;, &#39;bert.model.encoder.layer.10.attention.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.10.attention.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.10.attention.output.dense.bias&#39;, &#39;bert.model.encoder.layer.10.attention.output.dense.weight&#39;, &#39;bert.model.encoder.layer.10.attention.self.key.bias&#39;, &#39;bert.model.encoder.layer.10.attention.self.key.weight&#39;, &#39;bert.model.encoder.layer.10.attention.self.query.bias&#39;, &#39;bert.model.encoder.layer.10.attention.self.query.weight&#39;, &#39;bert.model.encoder.layer.10.attention.self.value.bias&#39;, &#39;bert.model.encoder.layer.10.attention.self.value.weight&#39;, &#39;bert.model.encoder.layer.10.intermediate.dense.bias&#39;, &#39;bert.model.encoder.layer.10.intermediate.dense.weight&#39;, &#39;bert.model.encoder.layer.10.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.10.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.10.output.dense.bias&#39;, &#39;bert.model.encoder.layer.10.output.dense.weight&#39;, &#39;bert.model.encoder.layer.11.attention.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.11.attention.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.11.attention.output.dense.bias&#39;, &#39;bert.model.encoder.layer.11.attention.output.dense.weight&#39;, &#39;bert.model.encoder.layer.11.attention.self.key.bias&#39;, &#39;bert.model.encoder.layer.11.attention.self.key.weight&#39;, &#39;bert.model.encoder.layer.11.attention.self.query.bias&#39;, &#39;bert.model.encoder.layer.11.attention.self.query.weight&#39;, &#39;bert.model.encoder.layer.11.attention.self.value.bias&#39;, &#39;bert.model.encoder.layer.11.attention.self.value.weight&#39;, &#39;bert.model.encoder.layer.11.intermediate.dense.bias&#39;, &#39;bert.model.encoder.layer.11.intermediate.dense.weight&#39;, &#39;bert.model.encoder.layer.11.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.11.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.11.output.dense.bias&#39;, &#39;bert.model.encoder.layer.11.output.dense.weight&#39;, &#39;bert.model.encoder.layer.2.attention.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.2.attention.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.2.attention.output.dense.bias&#39;, &#39;bert.model.encoder.layer.2.attention.output.dense.weight&#39;, &#39;bert.model.encoder.layer.2.attention.self.key.bias&#39;, &#39;bert.model.encoder.layer.2.attention.self.key.weight&#39;, &#39;bert.model.encoder.layer.2.attention.self.query.bias&#39;, &#39;bert.model.encoder.layer.2.attention.self.query.weight&#39;, &#39;bert.model.encoder.layer.2.attention.self.value.bias&#39;, &#39;bert.model.encoder.layer.2.attention.self.value.weight&#39;, &#39;bert.model.encoder.layer.2.intermediate.dense.bias&#39;, &#39;bert.model.encoder.layer.2.intermediate.dense.weight&#39;, &#39;bert.model.encoder.layer.2.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.2.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.2.output.dense.bias&#39;, &#39;bert.model.encoder.layer.2.output.dense.weight&#39;, &#39;bert.model.encoder.layer.3.attention.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.3.attention.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.3.attention.output.dense.bias&#39;, &#39;bert.model.encoder.layer.3.attention.output.dense.weight&#39;, &#39;bert.model.encoder.layer.3.attention.self.key.bias&#39;, &#39;bert.model.encoder.layer.3.attention.self.key.weight&#39;, &#39;bert.model.encoder.layer.3.attention.self.query.bias&#39;, &#39;bert.model.encoder.layer.3.attention.self.query.weight&#39;, &#39;bert.model.encoder.layer.3.attention.self.value.bias&#39;, &#39;bert.model.encoder.layer.3.attention.self.value.weight&#39;, &#39;bert.model.encoder.layer.3.intermediate.dense.bias&#39;, &#39;bert.model.encoder.layer.3.intermediate.dense.weight&#39;, &#39;bert.model.encoder.layer.3.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.3.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.3.output.dense.bias&#39;, &#39;bert.model.encoder.layer.3.output.dense.weight&#39;, &#39;bert.model.encoder.layer.4.attention.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.4.attention.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.4.attention.output.dense.bias&#39;, &#39;bert.model.encoder.layer.4.attention.output.dense.weight&#39;, &#39;bert.model.encoder.layer.4.attention.self.key.bias&#39;, &#39;bert.model.encoder.layer.4.attention.self.key.weight&#39;, &#39;bert.model.encoder.layer.4.attention.self.query.bias&#39;, &#39;bert.model.encoder.layer.4.attention.self.query.weight&#39;, &#39;bert.model.encoder.layer.4.attention.self.value.bias&#39;, &#39;bert.model.encoder.layer.4.attention.self.value.weight&#39;, &#39;bert.model.encoder.layer.4.intermediate.dense.bias&#39;, &#39;bert.model.encoder.layer.4.intermediate.dense.weight&#39;, &#39;bert.model.encoder.layer.4.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.4.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.4.output.dense.bias&#39;, &#39;bert.model.encoder.layer.4.output.dense.weight&#39;, &#39;bert.model.encoder.layer.5.attention.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.5.attention.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.5.attention.output.dense.bias&#39;, &#39;bert.model.encoder.layer.5.attention.output.dense.weight&#39;, &#39;bert.model.encoder.layer.5.attention.self.key.bias&#39;, &#39;bert.model.encoder.layer.5.attention.self.key.weight&#39;, &#39;bert.model.encoder.layer.5.attention.self.query.bias&#39;, &#39;bert.model.encoder.layer.5.attention.self.query.weight&#39;, &#39;bert.model.encoder.layer.5.attention.self.value.bias&#39;, &#39;bert.model.encoder.layer.5.attention.self.value.weight&#39;, &#39;bert.model.encoder.layer.5.intermediate.dense.bias&#39;, &#39;bert.model.encoder.layer.5.intermediate.dense.weight&#39;, &#39;bert.model.encoder.layer.5.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.5.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.5.output.dense.bias&#39;, &#39;bert.model.encoder.layer.5.output.dense.weight&#39;, &#39;bert.model.encoder.layer.6.attention.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.6.attention.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.6.attention.output.dense.bias&#39;, &#39;bert.model.encoder.layer.6.attention.output.dense.weight&#39;, &#39;bert.model.encoder.layer.6.attention.self.key.bias&#39;, &#39;bert.model.encoder.layer.6.attention.self.key.weight&#39;, &#39;bert.model.encoder.layer.6.attention.self.query.bias&#39;, &#39;bert.model.encoder.layer.6.attention.self.query.weight&#39;, &#39;bert.model.encoder.layer.6.attention.self.value.bias&#39;, &#39;bert.model.encoder.layer.6.attention.self.value.weight&#39;, &#39;bert.model.encoder.layer.6.intermediate.dense.bias&#39;, &#39;bert.model.encoder.layer.6.intermediate.dense.weight&#39;, &#39;bert.model.encoder.layer.6.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.6.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.6.output.dense.bias&#39;, &#39;bert.model.encoder.layer.6.output.dense.weight&#39;, &#39;bert.model.encoder.layer.7.attention.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.7.attention.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.7.attention.output.dense.bias&#39;, &#39;bert.model.encoder.layer.7.attention.output.dense.weight&#39;, &#39;bert.model.encoder.layer.7.attention.self.key.bias&#39;, &#39;bert.model.encoder.layer.7.attention.self.key.weight&#39;, &#39;bert.model.encoder.layer.7.attention.self.query.bias&#39;, &#39;bert.model.encoder.layer.7.attention.self.query.weight&#39;, &#39;bert.model.encoder.layer.7.attention.self.value.bias&#39;, &#39;bert.model.encoder.layer.7.attention.self.value.weight&#39;, &#39;bert.model.encoder.layer.7.intermediate.dense.bias&#39;, &#39;bert.model.encoder.layer.7.intermediate.dense.weight&#39;, &#39;bert.model.encoder.layer.7.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.7.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.7.output.dense.bias&#39;, &#39;bert.model.encoder.layer.7.output.dense.weight&#39;, &#39;bert.model.encoder.layer.8.attention.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.8.attention.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.8.attention.output.dense.bias&#39;, &#39;bert.model.encoder.layer.8.attention.output.dense.weight&#39;, &#39;bert.model.encoder.layer.8.attention.self.key.bias&#39;, &#39;bert.model.encoder.layer.8.attention.self.key.weight&#39;, &#39;bert.model.encoder.layer.8.attention.self.query.bias&#39;, &#39;bert.model.encoder.layer.8.attention.self.query.weight&#39;, &#39;bert.model.encoder.layer.8.attention.self.value.bias&#39;, &#39;bert.model.encoder.layer.8.attention.self.value.weight&#39;, &#39;bert.model.encoder.layer.8.intermediate.dense.bias&#39;, &#39;bert.model.encoder.layer.8.intermediate.dense.weight&#39;, &#39;bert.model.encoder.layer.8.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.8.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.8.output.dense.bias&#39;, &#39;bert.model.encoder.layer.8.output.dense.weight&#39;, &#39;bert.model.encoder.layer.9.attention.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.9.attention.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.9.attention.output.dense.bias&#39;, &#39;bert.model.encoder.layer.9.attention.output.dense.weight&#39;, &#39;bert.model.encoder.layer.9.attention.self.key.bias&#39;, &#39;bert.model.encoder.layer.9.attention.self.key.weight&#39;, &#39;bert.model.encoder.layer.9.attention.self.query.bias&#39;, &#39;bert.model.encoder.layer.9.attention.self.query.weight&#39;, &#39;bert.model.encoder.layer.9.attention.self.value.bias&#39;, &#39;bert.model.encoder.layer.9.attention.self.value.weight&#39;, &#39;bert.model.encoder.layer.9.intermediate.dense.bias&#39;, &#39;bert.model.encoder.layer.9.intermediate.dense.weight&#39;, &#39;bert.model.encoder.layer.9.output.LayerNorm.bias&#39;, &#39;bert.model.encoder.layer.9.output.LayerNorm.weight&#39;, &#39;bert.model.encoder.layer.9.output.dense.bias&#39;, &#39;bert.model.encoder.layer.9.output.dense.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

BertForPartwiseCLs(
  (model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(21128, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)
torch.Size([4, 2])</code></pre></div>
<p>å¯ä»¥çœ‹åˆ°æ¨¡å‹è¾“å‡ºäº†ä¸€ä¸ª 4Ã—2 çš„å¼ é‡ï¼Œç¬¦åˆæˆ‘ä»¬çš„é¢„æœŸï¼ˆæ¯ä¸ªæ ·æœ¬è¾“å‡º 2 ç»´çš„ logits å€¼åˆ†åˆ«è¡¨ç¤ºä¸¤ä¸ªç±»åˆ«çš„é¢„æµ‹åˆ†æ•°ï¼Œbatch å†…å…± 4 ä¸ªæ ·æœ¬ï¼‰ã€‚</p>
<h3 id="tqdmä½¿ç”¨">tqdmä½¿ç”¨</h3>
<p>tqdmæ˜¯ä¸€ä¸ªPythonåº“,ç”¨äºåœ¨ç»ˆç«¯ä¸­æ˜¾ç¤ºè¿›åº¦æ¡ã€‚å®ƒå¹¿æ³›åº”ç”¨äºå„ç§æ•°æ®å¤„ç†ä»»åŠ¡ä¸­,å¦‚å¾ªç¯ã€è¿­ä»£å™¨ã€pandasæ•°æ®å¸§ç­‰ã€‚ä»¥ä¸‹æ˜¯å¯¹tqdmçš„ç®€è¦ä»‹ç»:</p>
<ul>
<li>ç®€å•æ˜“ç”¨: tqdmæä¾›äº†ç®€å•ç›´è§‚çš„API,å¯ä»¥å¿«é€Ÿé›†æˆåˆ°ä»£ç ä¸­,åªéœ€è¦å‡ è¡Œä»£ç å³å¯å®ç°è¿›åº¦æ¡æ˜¾ç¤ºã€‚</li>
<li>ä¸°å¯Œçš„åŠŸèƒ½: tqdmä¸ä»…å¯ä»¥æ˜¾ç¤ºè¿›åº¦æ¡,è¿˜å¯ä»¥æ˜¾ç¤ºé¢„ä¼°çš„å‰©ä½™æ—¶é—´ã€å®Œæˆç™¾åˆ†æ¯”ã€å·²å¤„ç†çš„æ•°æ®é‡ç­‰ä¿¡æ¯ã€‚</li>
<li>è‡ªåŠ¨æ£€æµ‹ç¯å¢ƒ: tqdmå¯ä»¥è‡ªåŠ¨æ£€æµ‹è¿è¡Œç¯å¢ƒ,åœ¨æ”¯æŒANSIè½¬ä¹‰ç çš„ç»ˆç«¯ä¸­ä½¿ç”¨åŠ¨æ€è¿›åº¦æ¡,åœ¨ä¸æ”¯æŒçš„ç¯å¢ƒä¸­ä½¿ç”¨é™æ€è¿›åº¦æ¡ã€‚</li>
<li>æ”¯æŒå„ç§è¿­ä»£å™¨: tqdmæ”¯æŒå„ç§Pythonå†…ç½®è¿­ä»£å™¨,å¦‚listã€rangeã€enumerateç­‰,ä¹Ÿæ”¯æŒè‡ªå®šä¹‰è¿­ä»£å™¨ã€‚</li>
<li>å¯å®šåˆ¶æ€§å¼º: tqdmæä¾›äº†ä¸°å¯Œçš„å‚æ•°ä¾›ç”¨æˆ·è‡ªå®šä¹‰è¿›åº¦æ¡çš„æ ·å¼å’Œè¡Œä¸º,å¦‚é¢œè‰²ã€å®½åº¦ã€åˆ·æ–°é—´éš”ç­‰ã€‚</li>
</ul>
<p>ä»£ç </p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#tqdmè¿›åº¦æ¡ä½¿ç”¨
from tqdm.auto import tqdm
import time

# åˆ›å»ºä¸€ä¸ªè¿­ä»£å¯¹è±¡ï¼Œæ¯”å¦‚ä¸€ä¸ªåˆ—è¡¨
items = range(10)

# ä½¿ç”¨tqdmæ¥è¿­ä»£è¿™ä¸ªå¯¹è±¡ï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦æ¡
for item in tqdm(items, desc=&#39;Processing&#39;):
    # åœ¨è¿™é‡Œæ‰§è¡Œä½ çš„ä»»åŠ¡
    time.sleep(0.1)  # æ¨¡æ‹Ÿä¸€äº›é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡
# range(10) å…¶å®å°±æ˜¯0-9
print([i for i in range(10)])
#åˆ›å»ºä¸€ä¸ªtqdmå¯¹è±¡ï¼Œä¼ å…¥å¾—å¿…é¡»æ˜¯rangeå¯¹è±¡ï¼Œrange(10) å…¶å®å°±æ˜¯0-9
print(range(10),len(range(10)))
tdm=tqdm(range(10), desc=&#39;Processing&#39;)
for item in range(10):
     time.sleep(1)  # æ¨¡æ‹Ÿä¸€äº›é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡
     #æ›´æ–°ä¸€æ¬¡,å…¶å®å°±æ˜¯è¿›åº¦æ¡åŠ ä¸Šï¼š 1/len(range(10))
     tdm.update(1)</code></pre></div>
<p>æ•ˆæœ
<a href="#R-image-46ebca315be8f636bd9fcc009e49fc74" class="lightbox-link"><img alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/tools_libraries/transformers/transformers_actions_02.md.images/b1881dfe947372cd773bd9c2f1ad131b.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-46ebca315be8f636bd9fcc009e49fc74"><img alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/tools_libraries/transformers/transformers_actions_02.md.images/b1881dfe947372cd773bd9c2f1ad131b.png"></a></p>
<h3 id="è®­ç»ƒæ¨¡å‹-1">è®­ç»ƒæ¨¡å‹</h3>
<p>åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å°†æ¯ä¸€è½® Epoch åˆ†ä¸ºè®­ç»ƒå¾ªç¯å’ŒéªŒè¯/æµ‹è¯•å¾ªç¯ã€‚åœ¨è®­ç»ƒå¾ªç¯ä¸­è®¡ç®—æŸå¤±ã€ä¼˜åŒ–æ¨¡å‹çš„å‚æ•°ï¼Œåœ¨éªŒè¯/æµ‹è¯•å¾ªç¯ä¸­è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸ Pytorch ç±»ä¼¼ï¼ŒTransformers åº“åŒæ ·å®ç°äº†å¾ˆå¤šçš„ä¼˜åŒ–å™¨ï¼Œå¹¶ä¸”ç›¸æ¯” Pytorch å›ºå®šå­¦ä¹ ç‡ï¼ŒTransformers åº“çš„ä¼˜åŒ–å™¨ä¼šéšç€è®­ç»ƒè¿‡ç¨‹é€æ­¥å‡å°å­¦ä¹ ç‡ï¼ˆé€šå¸¸ä¼šäº§ç”Ÿæ›´å¥½çš„æ•ˆæœï¼‰ã€‚ä¾‹å¦‚æˆ‘ä»¬å‰é¢ä½¿ç”¨è¿‡çš„ AdamW ä¼˜åŒ–å™¨
å®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¯ä¸ä½¿ç”¨colabæ¥è¿›è¡Œè®­ç»ƒã€‚</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#è®­ç»ƒæ¨¡å‹å’ŒéªŒè¯æµ‹è¯•
#å®šä¹‰æŸå¤±å‡½æ•°
from torch.nn import CrossEntropyLoss
from transformers import get_scheduler
#å®šä¹‰ä¼˜åŒ–å‡½æ•°,from torch.optim import AdamW
from transformers import AdamW
from tqdm.auto import tqdm
#å®šä¹‰epochè®­ç»ƒæ¬¡æ•°
epochs = 3
#é»˜è®¤å­¦ä¹ ç‡
learning_rate = 1e-5
# batchsize
batch_size=4
#AdamWæ˜¯Adamä¼˜åŒ–å™¨çš„ä¸€ç§å˜ä½“ï¼Œå®ƒåœ¨Adamçš„åŸºç¡€ä¸Šè¿›è¡Œäº†ä¸€äº›æ”¹è¿›ï¼Œæ—¨åœ¨è§£å†³Adamä¼˜åŒ–å™¨å¯èƒ½å¼•å…¥çš„æƒé‡è¡°å‡é—®é¢˜ã€‚
optimizer=AdamW(model.parameters(),lr=1e-5)
#å®šä¹‰äº¤å‰ç†µæŸå¤±å‡½æ•°
loss_fn=CrossEntropyLoss()
#é‡æ–°åˆå§‹åŒ–æ•°æ®é›†
train_loader=DataLoader(train_data,batch_size=batch_size,shuffle=False,collate_fn=collote_fn)
dev_loader=DataLoader(MyDataSet(&#34;dev.json&#34;),batch_size=batch_size,shuffle=False,collate_fn=collote_fn)
#æ€»æ­¥æ•°=epoch*æ‰¹æ¬¡æ•°(æ€»è®°å½•æ•°train_data/ä¸€æ‰¹æ¬¡å¤šå°‘æ¡æ•°æ®batch_size)
num_training_steps = epochs * len(train_loader)
#é»˜è®¤æƒ…å†µä¸‹ï¼Œä¼˜åŒ–å™¨ä¼šçº¿æ€§è¡°å‡å­¦ä¹ ç‡ï¼Œå¯¹äºä¸Šé¢çš„ä¾‹å­ï¼Œå­¦ä¹ ç‡ä¼šçº¿æ€§åœ°ä»le-5 é™åˆ°0
#ã€‚ä¸ºäº†æ­£ç¡®åœ°å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æ€»çš„è®­ç»ƒæ­¥æ•° (step)ï¼Œå®ƒç­‰äºè®­ç»ƒè½®æ•° (Epoch number) ä¹˜ä»¥æ¯ä¸€è½®ä¸­çš„æ­¥æ•°ï¼ˆä¹Ÿå°±æ˜¯è®­ç»ƒ dataloader çš„å¤§å°ï¼‰
lr_scheduler = get_scheduler(
    &#34;linear&#34;,
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
#åˆå§‹åŒ–æ¨¡å‹
device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
checkpoint = &#34;bert-base-chinese&#34;
config=AutoConfig.from_pretrained(checkpoint)
model=BertForPartwiseCLs.from_pretrained(checkpoint,config=config).to(device)
#å®šä¹‰æ€»æŸå¤±
total_loss=0
#å®Œæˆæ€»batch
complete_batch_count=0
#æœ€å¥½çš„æ­£ç¡®ç‡
best_acc = 0.
current_directory = os.getcwd()
for step in range(epochs):
    #è¿›å…¥è®­ç»ƒæ¨¡å¼
    model.train()
    print(f&#34;Epoch {step+1}/{epochs}\n-------------------------------&#34;)
    progress_bar=tqdm(range(len(train_loader)))
    for batch,(X,y) in enumerate(train_loader):
        X,y=X.to(device),y.to(device)
        #è·å–é¢„æµ‹ç»“æœ
        pred=model(X)
        #è®¡ç®—æŸå¤±å‡½æ•°
        loss=loss_fn(pred,y)
        #æ¸…ç©ºæ¢¯åº¦
        optimizer.zero_grad()
        #å‰å‘ä¼ æ’­
        loss.backward();
        #æ›´æ–°æ¨¡å‹å‚æ•°
        optimizer.step();
        #å­¦ä¹ ç‡çº¿æ€§ä¸‹é™,å¿…é¡»æ˜¯æ›´æ–°æ¨¡å‹å‚æ•°ä¹‹åï¼Œå‡½æ•°æ ¹æ®è®¾å®šçš„è§„åˆ™æ¥è°ƒæ•´å­¦ä¹ ç‡ã€‚è¿™ä¸ªè°ƒæ•´éœ€è¦åŸºäºå½“å‰çš„æ¨¡å‹çŠ¶æ€,åŒ…æ‹¬å‚æ•°ã€æŸå¤±å‡½æ•°å€¼ç­‰,æ‰€ä»¥è¦æ”¾åœ¨optimizer.step()ä¹‹åã€‚
        lr_scheduler.step()
        total_loss+=loss.item()
        complete_batch_count+=1
        avg_loss=total_loss/complete_batch_count
        progress_bar.set_description(&#34;loss:{}&#34;.format(avg_loss))
        progress_bar.update(1)
    #ä½¿ç”¨éªŒè¯é›†éªŒè¯æ¨¡å‹æ­£ç¡®æ€§ã€‚
    #è¿›å…¥é¢„æµ‹æ¨¡å¼,å½“å‰è¿™ä¸€æ¬¡epochè®­ç»ƒæ•°æ®çš„æ­£ç¡®ç‡
    model.eval()
    correct=0
    #åŠ è½½éªŒè¯é›†çš„æ•°æ®
    for batch,(X,y) in enumerate(dev_loader):
        #è·å–é¢„æµ‹ç»“æœ
        pred=model(X)
        #å› ä¸ºæ˜¯[[0.9,0.1],[0.3,0.4]]æ‰€ä»¥å–dim=1ç»´åº¦ä¸Šæœ€å¤§å€¼çš„ç´¢å¼•ï¼Œæ¦‚ç‡å¤§çš„ç´¢å¼•å°±æ˜¯é¢„æµ‹çš„ç±»åˆ«ï¼Œå¦‚æœå’Œlabelå€¼yç›¸ç­‰å°±åŠ èµ·æ¥ï¼Œç®—ä¸ªæ•°
        correct += (pred.argmax(dim=1) == y).type(torch.float).sum().item()
    #æ­£ç¡®/æ€»æ•°å°±æ˜¯äº‰å–ç‡
    valid_acc=correct/len(dev_loader.dataset)
    print(f&#34;{step+1} Accuracy: {(100*valid_acc):&gt;0.1f}%\n&#34;)
    if valid_acc &gt; best_acc:
        best_acc = valid_acc
        print(&#39;saving new weights...\n&#39;)
        torch.save(model.state_dict(), current_directory+f&#39;/epoch_{step+1}_valid_acc_{(100*valid_acc):0.1f}_model_weights.bin&#39;)
print(&#34;Done!&#34;)</code></pre></div>
<h3 id="æ¨¡å‹é¢„æµ‹">æ¨¡å‹é¢„æµ‹</h3>
<p>æœ€åï¼Œæˆ‘ä»¬åŠ è½½éªŒè¯é›†ä¸Šæœ€ä¼˜çš„æ¨¡å‹æƒé‡ï¼Œæ±‡æŠ¥å…¶åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ã€‚ç”±äº AFQMC å…¬å¸ƒçš„æµ‹è¯•é›†ä¸Šå¹¶æ²¡æœ‰æ ‡ç­¾ï¼Œæ— æ³•è¯„ä¼°æ€§èƒ½ï¼Œè¿™é‡Œæˆ‘ä»¬æš‚ä¸”ç”¨éªŒè¯é›†ä»£æ›¿è¿›è¡Œæ¼”ç¤ºï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>current_directory = os.getcwd()
model.load_state_dict(torch.load(current_directory+&#39;/model_weights.bin&#39;))
model.eval()
test_loader=DataLoader(test_data,batch_size=4,shuffle=False,collate_fn=collote_fn)
X,y=next(iter(test_loader))
X, y = X.to(device), y.to(device)
pred = model(X)
print(pred.argmax(1) == y)</code></pre></div>
<p>æ–‡ç« éƒ¨åˆ†æ–‡å­—å¼•ç”¨ï¼šhttps://transformers.run/c2/2021-12-17-transformers-note-4/</p>

  <footer class="footline">
              <i class='fa-fw fas fa-calendar'></i> Sep 18, 2025
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          <a id="R-logo" class="R-default" href="/docs/index.html">
            <div class="logo-title">liaomin416100569åšå®¢</div>
          </a>
        </div>
        <search><form action="/docs/search/index.html" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-homelinks">
          <ul class="space collapsible-menu">
            <li class="" data-nav-id="/docs/index.html"><a class="padding" href="/docs/index.html"><i class="fa-fw fas fa-home"></i> Home</a></li>
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-headercontrols">
          <ul class="">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div class="R-sidebarmenu R-shortcutmenu-main">
          <ul class="enlarge morespace collapsible-menu">
            <li class="parent " data-nav-id="/docs/programming/index.html"><a class="padding" href="/docs/programming/index.html">ç¼–ç¨‹å¼€å‘</a><ul id="R-subsections-e3fc01b477dbaf64a8f5013a3dab5c5b" class="collapsible-menu">
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/tools_libraries/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/index.html">å·¥å…·åº“</a><ul id="R-subsections-e43804740042696aa314af8cc1e28fa9" class="collapsible-menu">
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/tools_libraries/transformers/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/transformers/index.html">transformers</a><ul id="R-subsections-c93b786975796f9b9f81f28585ce698d" class="collapsible-menu">
            <li class="" data-nav-id="/docs/programming/ai/tools_libraries/transformers/transformers_actions_01/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/transformers/transformers_actions_01/index.html">Transformerså®æˆ˜01-å¼€ç®±å³ç”¨çš„ pipelines</a></li>
            <li class="active " data-nav-id="/docs/programming/ai/tools_libraries/transformers/transformers_actions_02-copy/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/transformers/transformers_actions_02-copy/index.html">Transformerså®æˆ˜02-BERTé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ</a></li>
            <li class="" data-nav-id="/docs/programming/ai/tools_libraries/transformers/transformers_actions_02/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/transformers/transformers_actions_02/index.html">Transformerså®æˆ˜02-BERTé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ</a></li></ul></li></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/languages/index.html"><a class="padding" href="/docs/programming/languages/index.html">ç¼–ç¨‹è¯­è¨€</a><ul id="R-subsections-1bbde7fb0c312ba940b425df5a4caf67" class="collapsible-menu"></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/plugins/index.html"><a class="padding" href="/docs/programming/plugins/index.html">æ’ä»¶å¼€å‘</a><ul id="R-subsections-de66f54cff99288ca68bfcb5bb0439ae" class="collapsible-menu"></ul></li></ul></li>
          </ul>
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-shortcuts">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div id="R-footer-margin"></div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-footercontrols">
          <ul class="">
          </ul>
        </div>
<div id="R-footer"><p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p></div>
      </div>
    </aside>
    <script src="/docs/js/clipboard/clipboard.min.js?1758263149" defer></script>
    <script src="/docs/js/perfect-scrollbar/perfect-scrollbar.min.js?1758263149" defer></script>
    <script src="/docs/js/theme.js?1758263149" defer></script>
  </body>
</html>
