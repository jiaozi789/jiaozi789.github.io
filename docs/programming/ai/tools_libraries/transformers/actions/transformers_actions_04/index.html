<!DOCTYPE html>
<html lang="en-us" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="html">
  <head><script src="/docs/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=docs/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.150.0">
    <meta name="generator" content="Relearn 8.0.1+b23cf6629eada0c2802f34ae4012e04343497862">
    <meta name="description" content="ç®€ä»‹ GPT-2ï¼ˆGenerative Pre-trained Transformer 2ï¼‰æ˜¯ç”±OpenAIå¼€å‘çš„ä¸€ç§åŸºäºTransformeræ¶æ„çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯å…³äºGPT-2çš„ä¸€äº›å…³é”®ç‰¹ç‚¹å’Œä¿¡æ¯ï¼š
Transformeræ¶æ„ï¼šGPT-2åŸºäºTransformeræ¨¡å‹æ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰è¾“å…¥åºåˆ—ä¸­è¯è¯­ä¹‹é—´ä¾èµ–å…³ç³»çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
é¢„è®­ç»ƒï¼šGPT-2æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ„å‘³ç€å®ƒåœ¨å¤§è§„æ¨¡çš„æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ æ–‡æœ¬æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§å’Œè¯­è¨€æ¨¡å¼ã€‚
æ— ç›‘ç£å­¦ä¹ ï¼šåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒGPT-2é‡‡ç”¨äº†æ— ç›‘ç£å­¦ä¹ çš„æ–¹å¼ï¼Œå³æ¨¡å‹ä»…ä»…é€šè¿‡æ–‡æœ¬æ•°æ®æœ¬èº«æ¥å­¦ä¹ ï¼Œè€Œæ²¡æœ‰ä½¿ç”¨äººå·¥æ ‡æ³¨çš„æ ‡ç­¾æˆ–ç›‘ç£ä¿¡å·ã€‚
ç”Ÿæˆå¼ä»»åŠ¡ï¼šGPT-2è¢«è®¾è®¡ç”¨äºç”Ÿæˆå¼ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆå’Œæ‘˜è¦ç”Ÿæˆç­‰ã€‚å®ƒå¯ä»¥æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ï¼Œå¹¶ä¸”åœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚
å¤šå±‚æ¬¡æ¶æ„ï¼šGPT-2å…·æœ‰å¤šå±‚çš„Transformerç¼–ç å™¨ï¼Œå…¶ä¸­åŒ…å«æ•°ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•è·å¤æ‚çš„è¯­è¨€ç»“æ„å’Œè¯­ä¹‰å…³ç³»ã€‚
å¤§å°å˜ç§ï¼šGPT-2æœ‰å¤šä¸ªå¤§å°çš„å˜ç§ï¼Œä»117Måˆ°1.5Bä¸ªå‚æ•°ä¸ç­‰ï¼Œæ¯ä¸ªå˜ç§éƒ½å…·æœ‰ä¸åŒçš„æ€§èƒ½å’Œèµ„æºè¦æ±‚ã€‚æ›´å¤§çš„æ¨¡å‹å¾€å¾€åœ¨ç”Ÿæˆæ›´åŠ å‡†ç¡®å’Œæµç•…çš„æ–‡æœ¬æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚
å¼€æ”¾è®¸å¯ï¼šGPT-2æ˜¯åœ¨OpenAIçš„ç ”ç©¶ä¸‹å¼€å‘çš„ï¼Œå…¶æ¨¡å‹å’Œç›¸å…³èµ„æºä»¥å¼€æ”¾è®¸å¯çš„å½¢å¼å‘å¸ƒï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥è‡ªç”±åœ°ä½¿ç”¨å’Œæ„å»ºåŸºäºGPT-2çš„åº”ç”¨ã€‚
æ€»çš„æ¥è¯´ï¼ŒGPT-2æ˜¯ä¸€ç§å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¯ç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆã€ç†è§£ã€ç¿»è¯‘ç­‰å„ç§NLPä»»åŠ¡ã€‚
æ¡ˆä¾‹ è¯¥æ¡ˆä¾‹æ¥æºhuggingfaceå­¦ä¹ ä¸­å¿ƒnlp-courseï¼ŒTraining a causal language model from scratch æ–‡ç« 
æè¿° æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç¼©å‡ç‰ˆçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼šæˆ‘ä»¬å°†ä¸“æ³¨äºä¸€è¡Œè¡¥å…¨ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å‡½æ•°æˆ–ç±»ï¼Œä½¿ç”¨Pythonä»£ç çš„ä¸€ä¸ªå­é›†ã€‚åœ¨Pythonä¸­å¤„ç†æ•°æ®æ—¶ï¼Œæ‚¨ä¼šé¢‘ç¹æ¥è§¦åˆ°Pythonæ•°æ®ç§‘å­¦æ ˆï¼ŒåŒ…æ‹¬matplotlibã€seabornã€pandaså’Œscikit-learnåº“ã€‚åœ¨ä½¿ç”¨è¿™äº›æ¡†æ¶æ—¶ï¼Œç»å¸¸éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„å‘½ä»¤ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹æ¥ä¸ºæˆ‘ä»¬å®Œæˆè¿™äº›è°ƒç”¨ï¼Œé‚£å°†æ˜¯å¾ˆå¥½çš„ã€‚
æ”¶é›†æ•°æ® æˆ‘ä»¬ä½¿ç”¨huggingfaceæ”¶é›†å¾—contentåŒ…å«ï¼šâ€œpandasâ€, â€œsklearnâ€, â€œmatplotlibâ€, â€œseabornâ€ è¿™äº›å…³é”®å­—pythonä»£ç  è¿™ä¸ªæ•°æ®é›†æ˜¯ä»githubå…¬å…±ä»“åº“çˆ¬å–ï¼Œæ¯”å¦‚ from datasets import load_dataset, DatasetDictds_train = load_dataset(&#34;huggingface-course/codeparrot-ds-train&#34;, split=&#34;train&#34;)ds_valid = load_dataset(&#34;huggingface-course/codeparrot-ds-valid&#34;, split=&#34;validation&#34;)raw_datasets = DatasetDict({&#34;train&#34;: ds_train, # .shuffle().select(range(50000)),&#34;valid&#34;: ds_valid, # .shuffle().select(range(500))}) è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ•°æ®é›†ä¸­çš„ä¾‹å­ã€‚æˆ‘ä»¬åªéœ€æ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„å‰200ä¸ªå­—ç¬¦ï¼š">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Transformerså®æˆ˜04-å¾®è°ƒgpt-2ç”Ÿæˆpythonä»£ç ã€‚ :: liaomin416100569åšå®¢">
    <meta name="twitter:description" content="ç®€ä»‹ GPT-2ï¼ˆGenerative Pre-trained Transformer 2ï¼‰æ˜¯ç”±OpenAIå¼€å‘çš„ä¸€ç§åŸºäºTransformeræ¶æ„çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯å…³äºGPT-2çš„ä¸€äº›å…³é”®ç‰¹ç‚¹å’Œä¿¡æ¯ï¼š
Transformeræ¶æ„ï¼šGPT-2åŸºäºTransformeræ¨¡å‹æ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰è¾“å…¥åºåˆ—ä¸­è¯è¯­ä¹‹é—´ä¾èµ–å…³ç³»çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
é¢„è®­ç»ƒï¼šGPT-2æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ„å‘³ç€å®ƒåœ¨å¤§è§„æ¨¡çš„æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ æ–‡æœ¬æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§å’Œè¯­è¨€æ¨¡å¼ã€‚
æ— ç›‘ç£å­¦ä¹ ï¼šåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒGPT-2é‡‡ç”¨äº†æ— ç›‘ç£å­¦ä¹ çš„æ–¹å¼ï¼Œå³æ¨¡å‹ä»…ä»…é€šè¿‡æ–‡æœ¬æ•°æ®æœ¬èº«æ¥å­¦ä¹ ï¼Œè€Œæ²¡æœ‰ä½¿ç”¨äººå·¥æ ‡æ³¨çš„æ ‡ç­¾æˆ–ç›‘ç£ä¿¡å·ã€‚
ç”Ÿæˆå¼ä»»åŠ¡ï¼šGPT-2è¢«è®¾è®¡ç”¨äºç”Ÿæˆå¼ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆå’Œæ‘˜è¦ç”Ÿæˆç­‰ã€‚å®ƒå¯ä»¥æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ï¼Œå¹¶ä¸”åœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚
å¤šå±‚æ¬¡æ¶æ„ï¼šGPT-2å…·æœ‰å¤šå±‚çš„Transformerç¼–ç å™¨ï¼Œå…¶ä¸­åŒ…å«æ•°ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•è·å¤æ‚çš„è¯­è¨€ç»“æ„å’Œè¯­ä¹‰å…³ç³»ã€‚
å¤§å°å˜ç§ï¼šGPT-2æœ‰å¤šä¸ªå¤§å°çš„å˜ç§ï¼Œä»117Måˆ°1.5Bä¸ªå‚æ•°ä¸ç­‰ï¼Œæ¯ä¸ªå˜ç§éƒ½å…·æœ‰ä¸åŒçš„æ€§èƒ½å’Œèµ„æºè¦æ±‚ã€‚æ›´å¤§çš„æ¨¡å‹å¾€å¾€åœ¨ç”Ÿæˆæ›´åŠ å‡†ç¡®å’Œæµç•…çš„æ–‡æœ¬æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚
å¼€æ”¾è®¸å¯ï¼šGPT-2æ˜¯åœ¨OpenAIçš„ç ”ç©¶ä¸‹å¼€å‘çš„ï¼Œå…¶æ¨¡å‹å’Œç›¸å…³èµ„æºä»¥å¼€æ”¾è®¸å¯çš„å½¢å¼å‘å¸ƒï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥è‡ªç”±åœ°ä½¿ç”¨å’Œæ„å»ºåŸºäºGPT-2çš„åº”ç”¨ã€‚
æ€»çš„æ¥è¯´ï¼ŒGPT-2æ˜¯ä¸€ç§å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¯ç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆã€ç†è§£ã€ç¿»è¯‘ç­‰å„ç§NLPä»»åŠ¡ã€‚
æ¡ˆä¾‹ è¯¥æ¡ˆä¾‹æ¥æºhuggingfaceå­¦ä¹ ä¸­å¿ƒnlp-courseï¼ŒTraining a causal language model from scratch æ–‡ç« 
æè¿° æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç¼©å‡ç‰ˆçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼šæˆ‘ä»¬å°†ä¸“æ³¨äºä¸€è¡Œè¡¥å…¨ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å‡½æ•°æˆ–ç±»ï¼Œä½¿ç”¨Pythonä»£ç çš„ä¸€ä¸ªå­é›†ã€‚åœ¨Pythonä¸­å¤„ç†æ•°æ®æ—¶ï¼Œæ‚¨ä¼šé¢‘ç¹æ¥è§¦åˆ°Pythonæ•°æ®ç§‘å­¦æ ˆï¼ŒåŒ…æ‹¬matplotlibã€seabornã€pandaså’Œscikit-learnåº“ã€‚åœ¨ä½¿ç”¨è¿™äº›æ¡†æ¶æ—¶ï¼Œç»å¸¸éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„å‘½ä»¤ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹æ¥ä¸ºæˆ‘ä»¬å®Œæˆè¿™äº›è°ƒç”¨ï¼Œé‚£å°†æ˜¯å¾ˆå¥½çš„ã€‚
æ”¶é›†æ•°æ® æˆ‘ä»¬ä½¿ç”¨huggingfaceæ”¶é›†å¾—contentåŒ…å«ï¼šâ€œpandasâ€, â€œsklearnâ€, â€œmatplotlibâ€, â€œseabornâ€ è¿™äº›å…³é”®å­—pythonä»£ç  è¿™ä¸ªæ•°æ®é›†æ˜¯ä»githubå…¬å…±ä»“åº“çˆ¬å–ï¼Œæ¯”å¦‚ from datasets import load_dataset, DatasetDictds_train = load_dataset(&#34;huggingface-course/codeparrot-ds-train&#34;, split=&#34;train&#34;)ds_valid = load_dataset(&#34;huggingface-course/codeparrot-ds-valid&#34;, split=&#34;validation&#34;)raw_datasets = DatasetDict({&#34;train&#34;: ds_train, # .shuffle().select(range(50000)),&#34;valid&#34;: ds_valid, # .shuffle().select(range(500))}) è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ•°æ®é›†ä¸­çš„ä¾‹å­ã€‚æˆ‘ä»¬åªéœ€æ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„å‰200ä¸ªå­—ç¬¦ï¼š">
    <meta property="og:url" content="http://localhost:1313/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_04/index.html">
    <meta property="og:site_name" content="liaomin416100569åšå®¢">
    <meta property="og:title" content="Transformerså®æˆ˜04-å¾®è°ƒgpt-2ç”Ÿæˆpythonä»£ç ã€‚ :: liaomin416100569åšå®¢">
    <meta property="og:description" content="ç®€ä»‹ GPT-2ï¼ˆGenerative Pre-trained Transformer 2ï¼‰æ˜¯ç”±OpenAIå¼€å‘çš„ä¸€ç§åŸºäºTransformeræ¶æ„çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯å…³äºGPT-2çš„ä¸€äº›å…³é”®ç‰¹ç‚¹å’Œä¿¡æ¯ï¼š
Transformeræ¶æ„ï¼šGPT-2åŸºäºTransformeræ¨¡å‹æ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰è¾“å…¥åºåˆ—ä¸­è¯è¯­ä¹‹é—´ä¾èµ–å…³ç³»çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
é¢„è®­ç»ƒï¼šGPT-2æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ„å‘³ç€å®ƒåœ¨å¤§è§„æ¨¡çš„æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ æ–‡æœ¬æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§å’Œè¯­è¨€æ¨¡å¼ã€‚
æ— ç›‘ç£å­¦ä¹ ï¼šåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒGPT-2é‡‡ç”¨äº†æ— ç›‘ç£å­¦ä¹ çš„æ–¹å¼ï¼Œå³æ¨¡å‹ä»…ä»…é€šè¿‡æ–‡æœ¬æ•°æ®æœ¬èº«æ¥å­¦ä¹ ï¼Œè€Œæ²¡æœ‰ä½¿ç”¨äººå·¥æ ‡æ³¨çš„æ ‡ç­¾æˆ–ç›‘ç£ä¿¡å·ã€‚
ç”Ÿæˆå¼ä»»åŠ¡ï¼šGPT-2è¢«è®¾è®¡ç”¨äºç”Ÿæˆå¼ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆå’Œæ‘˜è¦ç”Ÿæˆç­‰ã€‚å®ƒå¯ä»¥æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ï¼Œå¹¶ä¸”åœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚
å¤šå±‚æ¬¡æ¶æ„ï¼šGPT-2å…·æœ‰å¤šå±‚çš„Transformerç¼–ç å™¨ï¼Œå…¶ä¸­åŒ…å«æ•°ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•è·å¤æ‚çš„è¯­è¨€ç»“æ„å’Œè¯­ä¹‰å…³ç³»ã€‚
å¤§å°å˜ç§ï¼šGPT-2æœ‰å¤šä¸ªå¤§å°çš„å˜ç§ï¼Œä»117Måˆ°1.5Bä¸ªå‚æ•°ä¸ç­‰ï¼Œæ¯ä¸ªå˜ç§éƒ½å…·æœ‰ä¸åŒçš„æ€§èƒ½å’Œèµ„æºè¦æ±‚ã€‚æ›´å¤§çš„æ¨¡å‹å¾€å¾€åœ¨ç”Ÿæˆæ›´åŠ å‡†ç¡®å’Œæµç•…çš„æ–‡æœ¬æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚
å¼€æ”¾è®¸å¯ï¼šGPT-2æ˜¯åœ¨OpenAIçš„ç ”ç©¶ä¸‹å¼€å‘çš„ï¼Œå…¶æ¨¡å‹å’Œç›¸å…³èµ„æºä»¥å¼€æ”¾è®¸å¯çš„å½¢å¼å‘å¸ƒï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥è‡ªç”±åœ°ä½¿ç”¨å’Œæ„å»ºåŸºäºGPT-2çš„åº”ç”¨ã€‚
æ€»çš„æ¥è¯´ï¼ŒGPT-2æ˜¯ä¸€ç§å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¯ç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆã€ç†è§£ã€ç¿»è¯‘ç­‰å„ç§NLPä»»åŠ¡ã€‚
æ¡ˆä¾‹ è¯¥æ¡ˆä¾‹æ¥æºhuggingfaceå­¦ä¹ ä¸­å¿ƒnlp-courseï¼ŒTraining a causal language model from scratch æ–‡ç« 
æè¿° æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç¼©å‡ç‰ˆçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼šæˆ‘ä»¬å°†ä¸“æ³¨äºä¸€è¡Œè¡¥å…¨ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å‡½æ•°æˆ–ç±»ï¼Œä½¿ç”¨Pythonä»£ç çš„ä¸€ä¸ªå­é›†ã€‚åœ¨Pythonä¸­å¤„ç†æ•°æ®æ—¶ï¼Œæ‚¨ä¼šé¢‘ç¹æ¥è§¦åˆ°Pythonæ•°æ®ç§‘å­¦æ ˆï¼ŒåŒ…æ‹¬matplotlibã€seabornã€pandaså’Œscikit-learnåº“ã€‚åœ¨ä½¿ç”¨è¿™äº›æ¡†æ¶æ—¶ï¼Œç»å¸¸éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„å‘½ä»¤ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹æ¥ä¸ºæˆ‘ä»¬å®Œæˆè¿™äº›è°ƒç”¨ï¼Œé‚£å°†æ˜¯å¾ˆå¥½çš„ã€‚
æ”¶é›†æ•°æ® æˆ‘ä»¬ä½¿ç”¨huggingfaceæ”¶é›†å¾—contentåŒ…å«ï¼šâ€œpandasâ€, â€œsklearnâ€, â€œmatplotlibâ€, â€œseabornâ€ è¿™äº›å…³é”®å­—pythonä»£ç  è¿™ä¸ªæ•°æ®é›†æ˜¯ä»githubå…¬å…±ä»“åº“çˆ¬å–ï¼Œæ¯”å¦‚ from datasets import load_dataset, DatasetDictds_train = load_dataset(&#34;huggingface-course/codeparrot-ds-train&#34;, split=&#34;train&#34;)ds_valid = load_dataset(&#34;huggingface-course/codeparrot-ds-valid&#34;, split=&#34;validation&#34;)raw_datasets = DatasetDict({&#34;train&#34;: ds_train, # .shuffle().select(range(50000)),&#34;valid&#34;: ds_valid, # .shuffle().select(range(500))}) è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ•°æ®é›†ä¸­çš„ä¾‹å­ã€‚æˆ‘ä»¬åªéœ€æ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„å‰200ä¸ªå­—ç¬¦ï¼š">
    <meta property="og:locale" content="en_us">
    <meta property="og:type" content="article">
    <meta property="article:section" content="ç¼–ç¨‹å¼€å‘">
    <meta property="article:published_time" content="2025-09-18T16:55:17+08:00">
    <meta property="article:modified_time" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="name" content="Transformerså®æˆ˜04-å¾®è°ƒgpt-2ç”Ÿæˆpythonä»£ç ã€‚ :: liaomin416100569åšå®¢">
    <meta itemprop="description" content="ç®€ä»‹ GPT-2ï¼ˆGenerative Pre-trained Transformer 2ï¼‰æ˜¯ç”±OpenAIå¼€å‘çš„ä¸€ç§åŸºäºTransformeræ¶æ„çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯å…³äºGPT-2çš„ä¸€äº›å…³é”®ç‰¹ç‚¹å’Œä¿¡æ¯ï¼š
Transformeræ¶æ„ï¼šGPT-2åŸºäºTransformeræ¨¡å‹æ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰è¾“å…¥åºåˆ—ä¸­è¯è¯­ä¹‹é—´ä¾èµ–å…³ç³»çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
é¢„è®­ç»ƒï¼šGPT-2æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ„å‘³ç€å®ƒåœ¨å¤§è§„æ¨¡çš„æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ æ–‡æœ¬æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§å’Œè¯­è¨€æ¨¡å¼ã€‚
æ— ç›‘ç£å­¦ä¹ ï¼šåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒGPT-2é‡‡ç”¨äº†æ— ç›‘ç£å­¦ä¹ çš„æ–¹å¼ï¼Œå³æ¨¡å‹ä»…ä»…é€šè¿‡æ–‡æœ¬æ•°æ®æœ¬èº«æ¥å­¦ä¹ ï¼Œè€Œæ²¡æœ‰ä½¿ç”¨äººå·¥æ ‡æ³¨çš„æ ‡ç­¾æˆ–ç›‘ç£ä¿¡å·ã€‚
ç”Ÿæˆå¼ä»»åŠ¡ï¼šGPT-2è¢«è®¾è®¡ç”¨äºç”Ÿæˆå¼ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆå’Œæ‘˜è¦ç”Ÿæˆç­‰ã€‚å®ƒå¯ä»¥æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ï¼Œå¹¶ä¸”åœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚
å¤šå±‚æ¬¡æ¶æ„ï¼šGPT-2å…·æœ‰å¤šå±‚çš„Transformerç¼–ç å™¨ï¼Œå…¶ä¸­åŒ…å«æ•°ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•è·å¤æ‚çš„è¯­è¨€ç»“æ„å’Œè¯­ä¹‰å…³ç³»ã€‚
å¤§å°å˜ç§ï¼šGPT-2æœ‰å¤šä¸ªå¤§å°çš„å˜ç§ï¼Œä»117Måˆ°1.5Bä¸ªå‚æ•°ä¸ç­‰ï¼Œæ¯ä¸ªå˜ç§éƒ½å…·æœ‰ä¸åŒçš„æ€§èƒ½å’Œèµ„æºè¦æ±‚ã€‚æ›´å¤§çš„æ¨¡å‹å¾€å¾€åœ¨ç”Ÿæˆæ›´åŠ å‡†ç¡®å’Œæµç•…çš„æ–‡æœ¬æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚
å¼€æ”¾è®¸å¯ï¼šGPT-2æ˜¯åœ¨OpenAIçš„ç ”ç©¶ä¸‹å¼€å‘çš„ï¼Œå…¶æ¨¡å‹å’Œç›¸å…³èµ„æºä»¥å¼€æ”¾è®¸å¯çš„å½¢å¼å‘å¸ƒï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥è‡ªç”±åœ°ä½¿ç”¨å’Œæ„å»ºåŸºäºGPT-2çš„åº”ç”¨ã€‚
æ€»çš„æ¥è¯´ï¼ŒGPT-2æ˜¯ä¸€ç§å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¯ç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆã€ç†è§£ã€ç¿»è¯‘ç­‰å„ç§NLPä»»åŠ¡ã€‚
æ¡ˆä¾‹ è¯¥æ¡ˆä¾‹æ¥æºhuggingfaceå­¦ä¹ ä¸­å¿ƒnlp-courseï¼ŒTraining a causal language model from scratch æ–‡ç« 
æè¿° æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç¼©å‡ç‰ˆçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼šæˆ‘ä»¬å°†ä¸“æ³¨äºä¸€è¡Œè¡¥å…¨ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å‡½æ•°æˆ–ç±»ï¼Œä½¿ç”¨Pythonä»£ç çš„ä¸€ä¸ªå­é›†ã€‚åœ¨Pythonä¸­å¤„ç†æ•°æ®æ—¶ï¼Œæ‚¨ä¼šé¢‘ç¹æ¥è§¦åˆ°Pythonæ•°æ®ç§‘å­¦æ ˆï¼ŒåŒ…æ‹¬matplotlibã€seabornã€pandaså’Œscikit-learnåº“ã€‚åœ¨ä½¿ç”¨è¿™äº›æ¡†æ¶æ—¶ï¼Œç»å¸¸éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„å‘½ä»¤ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹æ¥ä¸ºæˆ‘ä»¬å®Œæˆè¿™äº›è°ƒç”¨ï¼Œé‚£å°†æ˜¯å¾ˆå¥½çš„ã€‚
æ”¶é›†æ•°æ® æˆ‘ä»¬ä½¿ç”¨huggingfaceæ”¶é›†å¾—contentåŒ…å«ï¼šâ€œpandasâ€, â€œsklearnâ€, â€œmatplotlibâ€, â€œseabornâ€ è¿™äº›å…³é”®å­—pythonä»£ç  è¿™ä¸ªæ•°æ®é›†æ˜¯ä»githubå…¬å…±ä»“åº“çˆ¬å–ï¼Œæ¯”å¦‚ from datasets import load_dataset, DatasetDictds_train = load_dataset(&#34;huggingface-course/codeparrot-ds-train&#34;, split=&#34;train&#34;)ds_valid = load_dataset(&#34;huggingface-course/codeparrot-ds-valid&#34;, split=&#34;validation&#34;)raw_datasets = DatasetDict({&#34;train&#34;: ds_train, # .shuffle().select(range(50000)),&#34;valid&#34;: ds_valid, # .shuffle().select(range(500))}) è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ•°æ®é›†ä¸­çš„ä¾‹å­ã€‚æˆ‘ä»¬åªéœ€æ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„å‰200ä¸ªå­—ç¬¦ï¼š">
    <meta itemprop="datePublished" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="dateModified" content="2025-09-18T16:55:17+08:00">
    <meta itemprop="wordCount" content="2543">
    <title>Transformerså®æˆ˜04-å¾®è°ƒgpt-2ç”Ÿæˆpythonä»£ç ã€‚ :: liaomin416100569åšå®¢</title>
    <link href="/docs/css/auto-complete/auto-complete.min.css?1758266232" rel="stylesheet">
    <script src="/docs/js/auto-complete/auto-complete.min.js?1758266232" defer></script>
    <script src="/docs/js/search-lunr.js?1758266232" defer></script>
    <script src="/docs/js/search.js?1758266232" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/docs/searchindex.en.js?1758266232";
    </script>
    <script src="/docs/js/lunr/lunr.min.js?1758266232" defer></script>
    <script src="/docs/js/lunr/lunr.stemmer.support.min.js?1758266232" defer></script>
    <script src="/docs/js/lunr/lunr.multi.min.js?1758266232" defer></script>
    <script src="/docs/js/lunr/lunr.en.min.js?1758266232" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['en'];
    </script>
    <link href="/docs/fonts/fontawesome/css/fontawesome-all.min.css?1758266232" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/docs/fonts/fontawesome/css/fontawesome-all.min.css?1758266232" rel="stylesheet"></noscript>
    <link href="/docs/css/perfect-scrollbar/perfect-scrollbar.min.css?1758266232" rel="stylesheet">
    <link href="/docs/css/theme.css?1758266232" rel="stylesheet">
    <link href="/docs/css/format-html.css?1758266232" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = ``;
      window.relearn.path='\/programming\/ai\/tools_libraries\/transformers\/actions\/transformers_actions_04\/index.html';
      window.relearn.relBasePath='..\/..\/..\/..\/..\/..';
      window.relearn.relBaseUri='..\/..\/..\/..\/..\/..\/..';
      window.relearn.absBaseUri='http:\/\/localhost:1313\/docs';
      window.relearn.disableAnchorCopy=false;
      window.relearn.disableAnchorScrolling=false;
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      // variant stuff
      window.relearn.themevariants = [ 'auto' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script>
    <link href="/docs/css/custom.css?1758266232" rel="stylesheet">
  </head>
  <body class="mobile-support html" data-url="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_04/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
<nav class="TableOfContents">
  <ul>
    <li><a href="#ç®€ä»‹">ç®€ä»‹</a></li>
    <li><a href="#æ¡ˆä¾‹">æ¡ˆä¾‹</a>
      <ul>
        <li><a href="#æè¿°">æè¿°</a></li>
        <li><a href="#æ”¶é›†æ•°æ®">æ”¶é›†æ•°æ®</a></li>
        <li><a href="#æ•°æ®é›†å¤„ç†">æ•°æ®é›†å¤„ç†</a>
          <ul>
            <li><a href="#å›é¡¾é¢„å¤„ç†">å›é¡¾é¢„å¤„ç†</a>
              <ul>
                <li><a href="#input_idså’Œattention_mask">input_idså’Œattention_maskï¼š</a></li>
                <li><a href="#special-token">special token</a></li>
                <li><a href="#chunk">chunk</a></li>
                <li><a href="#datacollator">datacollator</a></li>
                <li><a href="#map">map</a></li>
              </ul>
            </li>
            <li><a href="#é¢„å¤„ç†">é¢„å¤„ç†</a></li>
          </ul>
        </li>
        <li><a href="#åˆå§‹åŒ–æ¨¡å‹">åˆå§‹åŒ–æ¨¡å‹</a>
          <ul>
            <li><a href="#å›é¡¾æ¨¡å‹">å›é¡¾æ¨¡å‹</a>
              <ul>
                <li><a href="#å‚æ•°è®¡ç®—">å‚æ•°è®¡ç®—</a></li>
              </ul>
            </li>
            <li><a href="#åˆå§‹åŒ–">åˆå§‹åŒ–</a></li>
          </ul>
        </li>
        <li><a href="#å®Œæ•´ä»£ç ">å®Œæ•´ä»£ç </a></li>
        <li><a href="#æµ‹è¯•">æµ‹è¯•</a></li>
      </ul>
    </li>
  </ul>
</nav>
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/index.html"><span itemprop="name">liaomin416100569åšå®¢</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/index.html"><span itemprop="name">ç¼–ç¨‹å¼€å‘</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/index.html"><span itemprop="name">äººå·¥æ™ºèƒ½</span></a><meta itemprop="position" content="3">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/tools_libraries/index.html"><span itemprop="name">å·¥å…·åº“</span></a><meta itemprop="position" content="4">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/tools_libraries/transformers/index.html"><span itemprop="name">transformers</span></a><meta itemprop="position" content="5">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/docs/programming/ai/tools_libraries/transformers/actions/index.html"><span itemprop="name">transformerså®æˆ˜</span></a><meta itemprop="position" content="6">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">Transformerså®æˆ˜04-å¾®è°ƒgpt-2ç”Ÿæˆpythonä»£ç ã€‚</span><meta itemprop="position" content="7"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_03/index.html" title="Transformerså®æˆ˜03-PEFTåº“ä½¿ç”¨LORAæ–¹æ³•å¾®è°ƒVITå›¾åƒåˆ†ç±»ã€‚ (ğŸ¡)"><i class="fa-fw fas fa-chevron-left"></i></a>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show"><a class="topbar-control" href="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_05/index.html" title="Transformerså®æˆ˜05-æ¨¡å‹é‡åŒ– (ğŸ¡’)"><i class="fa-fw fas fa-chevron-right"></i></a>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show"><button class="topbar-control" onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable programming" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
  </header>

<h1 id="transformerså®æˆ˜04-å¾®è°ƒgpt-2ç”Ÿæˆpythonä»£ç ">Transformerså®æˆ˜04-å¾®è°ƒgpt-2ç”Ÿæˆpythonä»£ç ã€‚</h1>

<h1 id="ç®€ä»‹">ç®€ä»‹</h1>
<p>GPT-2ï¼ˆGenerative Pre-trained Transformer 2ï¼‰æ˜¯ç”±OpenAIå¼€å‘çš„ä¸€ç§åŸºäºTransformeræ¶æ„çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯å…³äºGPT-2çš„ä¸€äº›å…³é”®ç‰¹ç‚¹å’Œä¿¡æ¯ï¼š</p>
<ol>
<li>
<p><strong>Transformeræ¶æ„</strong>ï¼šGPT-2åŸºäºTransformeræ¨¡å‹æ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰è¾“å…¥åºåˆ—ä¸­è¯è¯­ä¹‹é—´ä¾èµ–å…³ç³»çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</p>
</li>
<li>
<p><strong>é¢„è®­ç»ƒ</strong>ï¼šGPT-2æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ„å‘³ç€å®ƒåœ¨å¤§è§„æ¨¡çš„æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ æ–‡æœ¬æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§å’Œè¯­è¨€æ¨¡å¼ã€‚</p>
</li>
<li>
<p><strong>æ— ç›‘ç£å­¦ä¹ </strong>ï¼šåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒGPT-2é‡‡ç”¨äº†æ— ç›‘ç£å­¦ä¹ çš„æ–¹å¼ï¼Œå³æ¨¡å‹ä»…ä»…é€šè¿‡æ–‡æœ¬æ•°æ®æœ¬èº«æ¥å­¦ä¹ ï¼Œè€Œæ²¡æœ‰ä½¿ç”¨äººå·¥æ ‡æ³¨çš„æ ‡ç­¾æˆ–ç›‘ç£ä¿¡å·ã€‚</p>
</li>
<li>
<p><strong>ç”Ÿæˆå¼ä»»åŠ¡</strong>ï¼šGPT-2è¢«è®¾è®¡ç”¨äºç”Ÿæˆå¼ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç”Ÿæˆå’Œæ‘˜è¦ç”Ÿæˆç­‰ã€‚å®ƒå¯ä»¥æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ï¼Œå¹¶ä¸”åœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
</li>
<li>
<p><strong>å¤šå±‚æ¬¡æ¶æ„</strong>ï¼šGPT-2å…·æœ‰å¤šå±‚çš„Transformerç¼–ç å™¨ï¼Œå…¶ä¸­åŒ…å«æ•°ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•è·å¤æ‚çš„è¯­è¨€ç»“æ„å’Œè¯­ä¹‰å…³ç³»ã€‚</p>
</li>
<li>
<p><strong>å¤§å°å˜ç§</strong>ï¼šGPT-2æœ‰å¤šä¸ªå¤§å°çš„å˜ç§ï¼Œä»117Måˆ°1.5Bä¸ªå‚æ•°ä¸ç­‰ï¼Œæ¯ä¸ªå˜ç§éƒ½å…·æœ‰ä¸åŒçš„æ€§èƒ½å’Œèµ„æºè¦æ±‚ã€‚æ›´å¤§çš„æ¨¡å‹å¾€å¾€åœ¨ç”Ÿæˆæ›´åŠ å‡†ç¡®å’Œæµç•…çš„æ–‡æœ¬æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚</p>
</li>
<li>
<p><strong>å¼€æ”¾è®¸å¯</strong>ï¼šGPT-2æ˜¯åœ¨OpenAIçš„ç ”ç©¶ä¸‹å¼€å‘çš„ï¼Œå…¶æ¨¡å‹å’Œç›¸å…³èµ„æºä»¥å¼€æ”¾è®¸å¯çš„å½¢å¼å‘å¸ƒï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥è‡ªç”±åœ°ä½¿ç”¨å’Œæ„å»ºåŸºäºGPT-2çš„åº”ç”¨ã€‚</p>
</li>
</ol>
<p>æ€»çš„æ¥è¯´ï¼ŒGPT-2æ˜¯ä¸€ç§å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¯ç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆã€ç†è§£ã€ç¿»è¯‘ç­‰å„ç§NLPä»»åŠ¡ã€‚</p>
<h1 id="æ¡ˆä¾‹">æ¡ˆä¾‹</h1>
<p>è¯¥æ¡ˆä¾‹æ¥æºhuggingfaceå­¦ä¹ ä¸­å¿ƒ<a href="https://huggingface.co/learn" rel="external" target="_blank">nlp-course</a>ï¼Œ<a href="https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt" rel="external" target="_blank">Training a causal language model from scratch</a>
æ–‡ç« </p>
<h2 id="æè¿°">æè¿°</h2>
<p>æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç¼©å‡ç‰ˆçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼šæˆ‘ä»¬å°†ä¸“æ³¨äºä¸€è¡Œè¡¥å…¨ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å‡½æ•°æˆ–ç±»ï¼Œä½¿ç”¨Pythonä»£ç çš„ä¸€ä¸ªå­é›†ã€‚åœ¨Pythonä¸­å¤„ç†æ•°æ®æ—¶ï¼Œæ‚¨ä¼šé¢‘ç¹æ¥è§¦åˆ°Pythonæ•°æ®ç§‘å­¦æ ˆï¼ŒåŒ…æ‹¬matplotlibã€seabornã€pandaså’Œscikit-learnåº“ã€‚åœ¨ä½¿ç”¨è¿™äº›æ¡†æ¶æ—¶ï¼Œç»å¸¸éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„å‘½ä»¤ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹æ¥ä¸ºæˆ‘ä»¬å®Œæˆè¿™äº›è°ƒç”¨ï¼Œé‚£å°†æ˜¯å¾ˆå¥½çš„ã€‚</p>
<h2 id="æ”¶é›†æ•°æ®">æ”¶é›†æ•°æ®</h2>
<p>æˆ‘ä»¬ä½¿ç”¨huggingfaceæ”¶é›†å¾—contentåŒ…å«ï¼š&ldquo;pandas&rdquo;, &ldquo;sklearn&rdquo;, &ldquo;matplotlib&rdquo;, &ldquo;seaborn&rdquo; è¿™äº›å…³é”®å­—pythonä»£ç 
<a href="#R-image-e36bf76c56fe5625e9e4494277912a63" class="lightbox-link"><img alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/tools_libraries/transformers/actions/transformers_actions_04.md.images/5289affa0aab4eec9ffcb5c434146b7d.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-e36bf76c56fe5625e9e4494277912a63"><img alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/tools_libraries/transformers/actions/transformers_actions_04.md.images/5289affa0aab4eec9ffcb5c434146b7d.png"></a>
è¿™ä¸ªæ•°æ®é›†æ˜¯ä»githubå…¬å…±ä»“åº“çˆ¬å–ï¼Œæ¯”å¦‚
<a href="#R-image-be66cca668eedb46880e798cb773a902" class="lightbox-link"><img alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/tools_libraries/transformers/actions/transformers_actions_04.md.images/3bd924ccf8e941efa901ec62731eb0f4.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-be66cca668eedb46880e798cb773a902"><img alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/tools_libraries/transformers/actions/transformers_actions_04.md.images/3bd924ccf8e941efa901ec62731eb0f4.png"></a></p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>from datasets import load_dataset, DatasetDict

ds_train = load_dataset(&#34;huggingface-course/codeparrot-ds-train&#34;, split=&#34;train&#34;)
ds_valid = load_dataset(&#34;huggingface-course/codeparrot-ds-valid&#34;, split=&#34;validation&#34;)

raw_datasets = DatasetDict(
    {
        &#34;train&#34;: ds_train,  # .shuffle().select(range(50000)),
        &#34;valid&#34;: ds_valid,  # .shuffle().select(range(500))
    }
)</code></pre></div>
<p>è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ•°æ®é›†ä¸­çš„ä¾‹å­ã€‚æˆ‘ä»¬åªéœ€æ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„å‰200ä¸ªå­—ç¬¦ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>for key in raw_datasets[&#34;train&#34;][0]:
    print(f&#34;{key.upper()}: {raw_datasets[&#39;train&#39;][0][key][:200]}&#34;)</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>&#39;REPO_NAME: kmike/scikit-learn&#39;
&#39;PATH: sklearn/utils/__init__.py&#39;
&#39;COPIES: 3&#39;
&#39;SIZE: 10094&#39;
&#39;&#39;&#39;CONTENT: &#34;&#34;&#34;
The :mod:`sklearn.utils` module includes various utilites.
&#34;&#34;&#34;

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause&#39;&#39;&#39;</code></pre></div>
<h2 id="æ•°æ®é›†å¤„ç†">æ•°æ®é›†å¤„ç†</h2>
<p>é¦–å…ˆè¦å¯¹æ•°æ®è¿›è¡Œæ ‡è®°åŒ–ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½ç”¨å®ƒè¿›è¡Œè®­ç»ƒã€‚ç”±äºæˆ‘ä»¬çš„ç›®æ ‡ä¸»è¦æ˜¯è‡ªåŠ¨è¡¥å…¨çŸ­å‡½æ•°è°ƒç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä¿æŒä¸Šä¸‹æ–‡å¤§å°ç›¸å¯¹è¾ƒå°ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯æˆ‘ä»¬å¯ä»¥æ›´å¿«åœ°è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸”éœ€è¦çš„å†…å­˜é‡æ˜æ˜¾è¾ƒå°‘ã€‚å¦‚æœä½ çš„åº”ç”¨ç¨‹åºéœ€è¦æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœä½ å¸Œæœ›æ¨¡å‹èƒ½å¤ŸåŸºäºåŒ…å«å‡½æ•°å®šä¹‰çš„æ–‡ä»¶ç¼–å†™å•å…ƒæµ‹è¯•ï¼‰ï¼Œè¯·ç¡®ä¿å¢åŠ è¯¥æ•°å­—ï¼Œä½†ä¹Ÿè¦è®°ä½è¿™ä¼šå¢åŠ GPUçš„å†…å­˜å ç”¨ã€‚ç›®å‰ï¼Œè®©æˆ‘ä»¬å°†ä¸Šä¸‹æ–‡å¤§å°å›ºå®šä¸º128ä¸ªæ ‡è®°ï¼Œè€Œä¸æ˜¯ GPT-2 æˆ– GPT-3 ä¸­åˆ†åˆ«ä½¿ç”¨çš„ 1,024 æˆ– 2,048ã€‚</p>
<h3 id="å›é¡¾é¢„å¤„ç†">å›é¡¾é¢„å¤„ç†</h3>
<h4 id="input_idså’Œattention_mask">input_idså’Œattention_maskï¼š</h4>
<p>input_idsæ˜¯tokenizerå¤„ç†åå¾—åˆ°çš„è¾“å…¥ç‰¹å¾ï¼Œå®ƒå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æ•°å­—åºåˆ—ã€‚æ¯ä¸ªå•è¯æˆ–è€…æ ‡è®°ï¼ˆtokenï¼‰éƒ½ä¼šè¢«æ˜ å°„æˆå¯¹åº”çš„å”¯ä¸€æ•´æ•°ã€‚è¿™äº›æ•´æ•°åºåˆ—å°±æ˜¯æ¨¡å‹çš„å®é™…è¾“å…¥ã€‚
<strong>ç¤ºä¾‹</strong>ï¼šå‡è®¾åŸå§‹æ–‡æœ¬ç»è¿‡tokenizerå¤„ç†åï¼Œç”Ÿæˆçš„<code>input_ids</code>å¯èƒ½æ˜¯ä¸€ä¸ªæ•´æ•°åºåˆ—ï¼Œå¦‚<code>[101, 2023, 2003, 1037, 2814, 2242, 102]</code>ï¼Œæ¯ä¸ªæ•´æ•°å¯¹åº”ä¸€ä¸ªtokenã€‚</p>
<p>attention_maskç”¨äºå‘Šè¯‰æ¨¡å‹å“ªäº›éƒ¨åˆ†æ˜¯çœŸå®çš„è¾“å…¥ï¼Œå“ªäº›éƒ¨åˆ†æ˜¯å¡«å……ï¼ˆpaddingï¼‰çš„ï¼Œä»¥ä¾¿æ¨¡å‹åœ¨è®¡ç®—æ—¶èƒ½å¤Ÿæ­£ç¡®å¤„ç†ã€‚
å¯¹äºè¾“å…¥ä¸­çš„çœŸå®tokenï¼Œå¯¹åº”ä½ç½®çš„attention_maskå€¼ä¸º1ï¼›å¯¹äºå¡«å……çš„ä½ç½®ï¼Œattention_maskå€¼ä¸º0ã€‚
ç¤ºä¾‹ï¼šå¦‚æœinput_idsæ˜¯[101, 2023, 2003, 1037, 2814, 2242, 102]ï¼Œé‚£ä¹ˆå¯¹åº”çš„attention_maskå¯èƒ½æ˜¯[1, 1, 1, 1, 1, 1, 1]ï¼Œè¡¨ç¤ºæ‰€æœ‰ä½ç½®éƒ½æ˜¯çœŸå®çš„è¾“å…¥ï¼Œå¦‚æœæŸä¸ªå¥å­è¯å…ƒæ¯”ä»–å°ï¼Œå¯èƒ½å°±éœ€è¦å¡«å……ã€‚</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#è¿™é‡Œæ¼”ç¤ºåˆ†è¯å™¨
from transformers import AutoModel, BertTokenizer
model_name=&#34;bert-base-chinese&#34; #bert-base-uncased
model=AutoModel.from_pretrained(model_name)
tokenizer=BertTokenizer.from_pretrained(model_name)
print(type(model),type(tokenizer))
sequence = [&#34;æˆ‘å‡ºç”Ÿåœ¨æ¹–å—å²³é˜³,æˆ‘çš„å®¶åœ¨æ·±åœ³.&#34;,&#34;æˆ‘å¾—å„¿å­æ˜¯å°è°¦è°¦&#34;]
#è¾“å‡ºä¸­åŒ…å«ä¸¤ä¸ªé”® input_ids å’Œ attention_maskï¼Œå…¶ä¸­ input_ids å¯¹åº”åˆ†è¯ä¹‹åçš„ tokens æ˜ å°„åˆ°çš„æ•°å­—ç¼–å·åˆ—è¡¨ï¼Œè€Œ attention_mask åˆ™æ˜¯ç”¨æ¥æ ‡è®°å“ªäº› tokens #æ˜¯è¢«å¡«å……çš„ï¼ˆè¿™é‡Œâ€œ1â€è¡¨ç¤ºæ˜¯åŸæ–‡ï¼Œâ€œ0â€è¡¨ç¤ºæ˜¯å¡«å……å­—ç¬¦ï¼‰ã€‚
print(tokenizer(sequence, padding=True, truncation=True, return_tensors=&#34;pt&#34;,pair=True))

# è·å–å¡«å……tokençš„id
pad_token_id = tokenizer.pad_token_id
# è·å–å¡«å……tokençš„å­—ç¬¦ä¸²è¡¨ç¤º
pad_token = tokenizer.convert_ids_to_tokens(pad_token_id)
print(f&#34;å®é™…å¡«å……æ˜¯id,padid={pad_token_id},padtoken={pad_token}&#34;)
#è·å–è¯æ±‡è¡¨å¤§å°
vocab = tokenizer.get_vocab()
vocab_size = len(vocab)
print(&#34;è¯æ±‡è¡¨å¤§å°:&#34;, vocab_size,len(tokenizer))
# æ‰“å°è¯æ±‡è¡¨å†…å®¹ï¼ˆå¯é€‰ï¼‰
print(&#34;è¯æ±‡è¡¨å†…å®¹:&#34;, vocab)
#å°†è¾“å…¥åˆ‡åˆ†ä¸ºè¯è¯­ã€å­è¯æˆ–è€…ç¬¦å·ï¼ˆä¾‹å¦‚æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œç»Ÿç§°ä¸º tokensï¼›
print(tokenizer.tokenize(sequence[0]),len(tokenizer.tokenize(sequence[0])))
#æˆ‘ä»¬é€šè¿‡ convert_tokens_to_ids() å°†åˆ‡åˆ†å‡ºçš„ tokens è½¬æ¢ä¸ºå¯¹åº”çš„ token IDsï¼š
print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence[0])))
#å¯ä»¥é€šè¿‡ encode() å‡½æ•°å°†è¿™ä¸¤ä¸ªæ­¥éª¤åˆå¹¶ï¼Œå¹¶ä¸” encode() ä¼šè‡ªåŠ¨æ·»åŠ æ¨¡å‹éœ€è¦çš„ç‰¹æ®Š tokenï¼Œä¾‹å¦‚ BERT åˆ†è¯å™¨ä¼šåˆ†åˆ«åœ¨åºåˆ—çš„é¦–å°¾æ·»åŠ [CLS] å’Œ [SEP]
print(tokenizer.encode(sequence[0]))
#è§£ç è¿˜åŸæ–‡å­—ï¼Œå¯ä»¥çœ‹åˆ°encodeå‰ååŠ äº†[CLS] å’Œ [SEP]
print(tokenizer.decode(tokenizer.encode(sequence[1])))</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>&lt;class &#39;transformers.models.bert.modeling_bert.BertModel&#39;&gt; &lt;class &#39;transformers.models.bert.tokenization_bert.BertTokenizer&#39;&gt;
{&#39;input_ids&#39;: tensor([[ 101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345,  117, 2769, 4638,
         2157, 1762, 3918, 1766,  119,  102],
        [ 101, 2769, 2533, 1036, 2094, 3221, 2207, 6472, 6472,  102,    0,    0,
            0,    0,    0,    0,    0,    0]]), &#39;token_type_ids&#39;: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}
å®é™…å¡«å……æ˜¯id,padid=0,padtoken=[PAD]
è¯æ±‡è¡¨å¤§å°: 21128 21128
è¯æ±‡è¡¨å†…å®¹: {&#39;[PAD]&#39;: 0, &#39;[unused1]&#39;: 1, &#39;[unused2]&#39;: 2, &#39;[unused3]&#39;: 3, &#39;[unused4]&#39;: 4, &#39;[unused5]&#39;: 5, &#39;[unused6]&#39;: 6, &#39;[unused7]&#39;: 7, &#39;[unused8]&#39;: 8, &#39;[unused9]&#39;: 9, &#39;[unused10]&#39;: 10, &#39;[unused11]&#39;: 11,ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚
[&#39;æˆ‘&#39;, &#39;å‡º&#39;, &#39;ç”Ÿ&#39;, &#39;åœ¨&#39;, &#39;æ¹–&#39;, &#39;å—&#39;, &#39;å²³&#39;, &#39;é˜³&#39;, &#39;,&#39;, &#39;æˆ‘&#39;, &#39;çš„&#39;, &#39;å®¶&#39;, &#39;åœ¨&#39;, &#39;æ·±&#39;, &#39;åœ³&#39;, &#39;.&#39;] 16
[2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638, 2157, 1762, 3918, 1766, 119]
[101, 2769, 1139, 4495, 1762, 3959, 1298, 2277, 7345, 117, 2769, 4638, 2157, 1762, 3918, 1766, 119, 102]
[CLS] æˆ‘ å¾— å„¿ å­ æ˜¯ å° è°¦ è°¦ [SEP]</code></pre></div>
<h4 id="special-token">special token</h4>
<p>Tokenizer çš„ç‰¹æ®Šæ ‡è®°ï¼ˆspecial tokensï¼‰æ˜¯åœ¨å¤„ç†æ–‡æœ¬æ•°æ®æ—¶ç»å¸¸ç”¨åˆ°çš„ä¸€äº›ç‰¹æ®Šç¬¦å·æˆ–è€…å­—ç¬¦ä¸²ï¼Œå®ƒä»¬åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­èµ·ç€é‡è¦çš„ä½œç”¨ã€‚è¿™äº›ç‰¹æ®Šæ ‡è®°é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹å‡ ç±»ï¼š</p>
<ol>
<li>
<p><strong>Padding token (<code>[PAD]</code>)</strong> pad_tokenï¼š<br>
åœ¨è¿›è¡Œæ‰¹é‡å¤„ç†æ—¶ï¼Œåºåˆ—é•¿åº¦ä¸ä¸€è‡´æ˜¯å¾ˆå¸¸è§çš„æƒ…å†µã€‚ä¸ºäº†ä¿è¯è¾“å…¥æ•°æ®çš„ç»Ÿä¸€æ€§ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šä½¿ç”¨ <code>[PAD]</code> æ ‡è®°æ¥å¡«å……è¾ƒçŸ­çš„åºåˆ—ï¼Œä½¿å…¶ä¸å…¶ä»–åºåˆ—çš„é•¿åº¦ç›¸åŒã€‚</p>
</li>
<li>
<p><strong>Start of sequence token (<code>[CLS]</code>)</strong>  bos_tokenï¼š<br>
åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ï¼‰ä¸­ï¼Œéœ€è¦åœ¨è¾“å…¥åºåˆ—çš„å¼€å¤´æ·»åŠ ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Œä¾‹å¦‚ <code>[CLS]</code>ï¼Œç”¨äºæ¨¡å‹ç†è§£è¿™æ˜¯ä¸€ä¸ªåºåˆ—çš„èµ·å§‹ç‚¹ï¼Œgpt2çš„å¼€å§‹tokenæ˜¯ï¼š&lt;|endoftext|&gt;ã€‚</p>
</li>
<li>
<p><strong>End of sequence token (<code>[SEP]</code>)</strong> eos_tokenï¼š<br>
ç±»ä¼¼åœ°ï¼Œ<code>[SEP]</code> æ ‡è®°é€šå¸¸ç”¨äºè¡¨ç¤ºåºåˆ—çš„ç»“æŸï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šä¸ªå¥å­æˆ–æ–‡æœ¬å¯¹æ—¶ï¼Œå¯ä»¥ç”¨ <code>[SEP]</code> åˆ†éš”å®ƒä»¬ã€‚</p>
</li>
<li>
<p><strong>Mask token (<code>[MASK]</code>)</strong> mask_tokenï¼š<br>
åœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­ï¼Œä¸ºäº†è¿›è¡Œè¯­è¨€æ¨¡å‹çš„æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMasked Language Modelingï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å°†ä¸€äº›å•è¯æˆ–å­è¯éšæœºåœ°ç”¨ <code>[MASK]</code> æ ‡è®°æ›¿æ¢æ‰ï¼Œè®©æ¨¡å‹é¢„æµ‹è¢«æ©ç çš„éƒ¨åˆ†ã€‚</p>
</li>
<li>
<p><strong>unk_token</strong> æ˜¯ tokenizer ä¸­çš„ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Œé€šå¸¸ç”¨æ¥è¡¨ç¤ºæœªç™»å½•è¯ï¼ˆUnknown Tokenï¼‰ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œæœªç™»å½•è¯æŒ‡çš„æ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„è¯æ±‡æˆ–è€…å­è¯ã€‚å½“æ¨¡å‹åœ¨å¤„ç†è¾“å…¥æ–‡æœ¬æ—¶é‡åˆ°æœªç™»å½•è¯ï¼Œå®ƒä¼šç”¨ unk_token æ¥æ›¿ä»£è¿™äº›è¯ï¼Œä»¥ä¾¿ç»§ç»­è¿›è¡Œå¤„ç†æˆ–é¢„æµ‹ã€‚</p>
</li>
<li>
<p><strong>sep_token</strong> æ˜¯ tokenizer ä¸­çš„å¦ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Œé€šå¸¸ç”¨æ¥è¡¨ç¤ºåºåˆ—çš„åˆ†éš”ç¬¦ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­ï¼Œsep_token ä¸»è¦ç”¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
æŸäº›é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆå¦‚ BERTï¼‰è¦æ±‚è¾“å…¥æ•°æ®æŒ‰ç…§ç‰¹å®šæ ¼å¼ç»„ç»‡ï¼ŒåŒ…æ‹¬ä½¿ç”¨ sep_token æ¥åˆ†éš”è¾“å…¥çš„å„ä¸ªéƒ¨åˆ†ã€‚ä¾‹å¦‚ï¼Œåœ¨æ–‡æœ¬å¯¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¯ä»¥ç”¨ [SEP] æ ‡è®°åˆ†éš”ä¸¤ä¸ªå¥å­ï¼š
<code>[CLS] Sentence A [SEP] Sentence B [SEP]</code></p>
</li>
<li>
<p><strong>cls_token</strong> æ˜¯ tokenizer ä¸­çš„å¦ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Œé€šå¸¸ç”¨æ¥è¡¨ç¤ºåºåˆ—çš„å¼€å¤´æˆ–è€…åˆ†ç±»ä»»åŠ¡ä¸­çš„ç‰¹æ®Šæ ‡è®°ã€‚</p>
</li>
</ol>
<p>è¿™äº›ç‰¹æ®Šæ ‡è®°åœ¨ä¸åŒçš„ä»»åŠ¡å’Œæ¨¡å‹ä¸­å…·æœ‰ä¸åŒçš„ç”¨é€”ï¼Œä½†å®ƒä»¬çš„å…±åŒä½œç”¨æ˜¯å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å¤„ç†æ–‡æœ¬æ•°æ®ï¼Œå¤„ç†è¾“å…¥åºåˆ—çš„é•¿åº¦å˜åŒ–ï¼Œä»¥åŠåœ¨ç‰¹å®šä»»åŠ¡ä¸­å¼•å¯¼æ¨¡å‹å­¦ä¹ å’Œé¢„æµ‹ã€‚é€šè¿‡é€‚å½“ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºæ¨¡å‹å¯¹è¯­è¨€æ•°æ®çš„ç†è§£å’Œå¤„ç†èƒ½åŠ›ã€‚</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#ç‰¹æ®Štoken
from transformers import GPT2Tokenizer,AutoTokenizer

# åˆå§‹åŒ– GPT-2 åˆ†è¯å™¨
tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)
tokenizer1 = AutoTokenizer.from_pretrained(&#39;bert-base-chinese&#39;)
# æ‰“å°æ‰€æœ‰ç‰¹æ®Šæ ‡è®°
print(&#34;gpt2ç‰¹æ®Šæ ‡è®°:&#34;)
for token_name, token_value in tokenizer.special_tokens_map.items():
    print(f&#34;{token_name}: {token_value}&#34;)
print(&#34;bert-base-chineseç‰¹æ®Šæ ‡è®°:&#34;)
for token_name, token_value in tokenizer1.special_tokens_map.items():
    print(f&#34;{token_name}: {token_value}&#34;)</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>gpt2ç‰¹æ®Šæ ‡è®°:
bos_token: &lt;|endoftext|&gt;
eos_token: &lt;|endoftext|&gt;
unk_token: &lt;|endoftext|&gt;
--------------------------
bert-base-chineseç‰¹æ®Šæ ‡è®°:
unk_token: [UNK]
sep_token: [SEP]
pad_token: [PAD]
cls_token: [CLS]
mask_token: [MASK]</code></pre></div>
<h4 id="chunk">chunk</h4>
<p>å½“ä½ æœ‰å¤šä¸ªå¥å­æˆ–æ–‡æœ¬æ®µè½éœ€è¦å¤„ç†æ—¶ï¼Œä½ å¯ä»¥å°†å®ƒä»¬åˆ’åˆ†æˆå›ºå®šé•¿åº¦çš„å°å—ï¼ˆchunksï¼‰ï¼Œä»¥ä¾¿è¾“å…¥åˆ°æ¨¡å‹ä¸­è¿›è¡Œå¤„ç†ã€‚è¿™ä¸ªè¿‡ç¨‹é€šå¸¸ç”¨äºå¤„ç†è¾ƒé•¿çš„æ–‡æœ¬ï¼Œä»¥ç¡®ä¿æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†è¾“å…¥æ•°æ®ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨Transformerç­‰æ¨¡å‹æ—¶ï¼Œå…¶è¾“å…¥é•¿åº¦é€šå¸¸æ˜¯æœ‰é™åˆ¶çš„ã€‚</p>
<p>chunkçš„é€»è¾‘æ˜¯ï¼Œè¾“å…¥æ•°æ®çš„æ¯ä¸€è¡Œå¥å­ï¼Œè¶…è¿‡max_length éƒ½ä¼šè¢«æˆªæ–­ï¼Œå½“å‰å¥å­è¢«æ‹†åˆ†æˆçš„chuckçš„ä¸ªæ•°ä¸ºï¼šlen(å¥å­)%max_length +1ï¼Œå½“å‰æœ‰äº›æ¨¡å‹ä¼šæ·»åŠ ä¸€äº›å¼€å§‹å’Œåˆ†å‰²å­—ç¬¦ æ¯”å¦‚[CLS][SEQ]ç­‰ä¹Ÿè¦ç®—å…¥é•¿åº¦ã€‚</p>
<blockquote>
<p>æ³¨æ„tokenizeræ‹†åˆ†å°å—çš„å¼€å¯ç”± truncation=True,å†³å®šï¼Œå¦‚æœæ˜¯False max_lengthç­‰å°±æ— æ•ˆäº†ã€‚</p></blockquote>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>#truckçš„é—®é¢˜ã€‚
content = [&#34;This is the first sentence. This is the second sentence.&#34;,&#34;i am a stupid man&#34;]
from transformers import AutoTokenizer

# é€‰æ‹©ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å’Œå¯¹åº”çš„tokenizer
model_name = &#34;bert-base-uncased&#34;
tokenizer = AutoTokenizer.from_pretrained(model_name)

# æœ€å¤§çš„å­—ç¬¦é•¿åº¦ï¼Œå› ä¸ºå­—ç¬¦çš„æœ€å‰é¢ä¼šåŠ ä¸€ä¸ª[CLS],æœ€åä¼šè¡¥ä¸€ä¸ª[SEP]ï¼Œæ¯ä¸€ä¸ªå¥å­éƒ½ä¼šè¢«æ‹†åˆ†ä¸€æ¬¡ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªtruckè¡Œåªèƒ½10ä¸ªå­—ç¬¦ï¼Œcontentã€0ã€‘å› ä¸ºè¶…è¿‡10ä¸ªå­—ç¬¦ï¼Œæ‰€ä»¥è¢«åˆ‡å‰²æˆ2ä¸ªtruckã€‚
# è¾“å‡ºçš„trucklengthæ˜¯[10,6]ï¼Œç¬¬äºŒä¸ªå¥å­ä¸æ»¡10ä¸ªåªæœ‰7ä¸ªï¼Œæœ€ålength=[10, 6, 7]
max_length = 10

# è¿›è¡Œtokenizationï¼Œå¹¶è¿”å›ç»“æœ
outputs = tokenizer(
    content,
    truncation=True,
    max_length=max_length,
    return_overflowing_tokens=True,
    return_length=True,
)
# è¾“å‡ºç»“æœ
print(outputs)
print(tokenizer.decode(outputs[&#39;input_ids&#39;][0]))
print(tokenizer.decode(outputs[&#39;input_ids&#39;][1]))</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>{&#39;input_ids&#39;: [[101, 2023, 2003, 1996, 2034, 6251, 1012, 2023, 2003, 102], [101, 1996, 2117, 6251, 1012, 102], [101, 1045, 2572, 1037, 5236, 2158, 102]], &#39;token_type_ids&#39;: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], &#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]], &#39;length&#39;: [10, 6, 7], &#39;overflow_to_sample_mapping&#39;: [0, 0, 1]}
[CLS] this is the first sentence. this is [SEP]
[CLS] the second sentence. [SEP]</code></pre></div>
<p>æ³¨æ„overflow_to_sample_mappingä¸­æ˜¯æ ‡è¯†æ¯ä¸ªå°chuckå±äºä¹‹å‰å“ªä¸ªå¥å­ç´¢å¼•ï¼Œç¬¬1-2ä¸ªchuckæ˜¯å±äºç¬¬0ä¸ªç´¢å¼•ä¹Ÿå°±æ˜¯ç¬¬ä¸€ä¸ªå¥å­ï¼Œ3ä¸ªç¬¬äºŒä¸ªå¥å­ã€‚</p>
<p>å¦‚æœåŠ äº† padding=True,æ‰€æœ‰çš„å­å¥éƒ½ä¼šè‡ªåŠ¨è¡¥ä¸Špadding_idï¼Œæœ€ç»ˆlengthéƒ½ä¼šæ˜¯10ï¼Œç»“æœå°±å˜æˆ</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>{&#39;input_ids&#39;: [[101, 2023, 2003, 1996, 2034, 6251, 1012, 2023, 2003, 102], [101, 1996, 2117, 6251, 1012, 102, 0, 0, 0, 0], [101, 1045, 2572, 1037, 5236, 2158, 102, 0, 0, 0]], &#39;token_type_ids&#39;: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], &#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], &#39;length&#39;: [10, 10, 10], &#39;overflow_to_sample_mapping&#39;: [0, 0, 1]}
[CLS] this is the first sentence. this is [SEP]
[CLS] the second sentence. [SEP] [PAD] [PAD] [PAD] [PAD]</code></pre></div>
<p>å…¶ä»–æ›´è¯¦ç»†çš„é¢„å¤„ç†å‚è€ƒï¼šhttps://github.com/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb</p>
<h4 id="datacollator">datacollator</h4>
<ul>
<li>DataCollatorForLanguageModeling çš„ä¸»è¦åŠŸèƒ½æ˜¯ä¸ºæ©ç è¯­è¨€æ¨¡å‹ï¼ˆMasked Language Modelingï¼ŒMLMï¼‰ä»»åŠ¡å‡†å¤‡æ•°æ®ã€‚å®ƒçš„ä¸»è¦ä½œç”¨æ˜¯éšæœºåœ°æ©ç›–è¾“å…¥ä¸­çš„ä¸€äº›æ ‡è®°ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„æ ‡ç­¾ï¼Œä»¥ä¾¿æ¨¡å‹åœ¨è®­ç»ƒæ—¶èƒ½å¤Ÿé¢„æµ‹è¿™äº›è¢«æ©ç›–çš„æ ‡è®°ã€‚</li>
<li>DataCollatorWithPaddingï¼šå¯¹è¾“å…¥è¿›è¡Œå¡«å……ï¼Œä½¿å¾—è¾“å…¥å¼ é‡å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚</li>
</ul>
<p>æ›´å¤šç›¸å…³ç±»çš„å®ç°ï¼Œè¯·<a href="https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling" rel="external" target="_blank">å‚è€ƒ</a>å®˜æ–¹api</p>
<p>ä»¥ä¸‹æ˜¯ä¸€ä¸ªä¾‹å­</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>import torch
from transformers import BertTokenizer, DataCollatorForLanguageModeling
# åˆå§‹åŒ–BERTåˆ†è¯å™¨
tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
# å®šä¹‰ç¤ºä¾‹æ–‡æœ¬
texts = [&#34;Hello, how are you?&#34;, &#34;I am fine, thank you.&#34;]
# å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
inputs = tokenizer(texts, return_tensors=&#39;pt&#39;, padding=True, truncation=True)
# æ‰“å°ç¼–ç åçš„è¾“å…¥
print(&#34;Encoded inputs:&#34;, inputs)
# å°†è¾“å…¥è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œä»¥é€‚åº”DataCollatorForLanguageModelingçš„è¾“å…¥æ ¼å¼,ä»–çš„æ ¼å¼è¦æ±‚æœ‰å¤šå°‘ä¸ªå¥å­å°±å¤šå°‘è¡Œ[{&#39;input_ids&#39;:,&#39;attention_mask&#39;:},{&#39;input_ids&#39;:,&#39;attention_mask&#39;:}]
# tokenizer encodeçš„æ ¼å¼æ˜¯å­—å…¸ {&#39;input_ids&#39;: [[],[]]æ˜¯åœ¨äºŒç»´æ•°ç»„ä½“ç°ï¼Œæ‰€ä»¥å¼ºåˆ¶è½¬ä¸€ä¸‹
batch = [{key: val[i] for key, val in inputs.items()} for i in range(len(texts))]
print(&#34;collatoréœ€è¦æ ¼å¼&#34;,batch)
# åˆå§‹åŒ–æ•°æ®æ•´ç†å™¨ï¼ŒæŒ‡å®šè¿›è¡Œæ©ç è¯­è¨€æ¨¡å‹ä»»åŠ¡
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

# å¯¹è¾“å…¥æ•°æ®è¿›è¡Œæ•´ç†
collated_inputs = data_collator(batch)

# æ‰“å°æ•´ç†åçš„è¾“å…¥,è¿™é‡Œå› ä¸ºmlm=Trueæ˜¯è‡ªåŠ¨æ©ç›–ï¼Œæœ‰15%çš„æ•°æ®è¢«æ©ç›–ï¼Œè¢«æ©ç›–çš„æ•°æ®åœ¨input_idsè¢«æ›¿æ¢æˆ103ï¼Œç„¶ååœ¨ç”Ÿæˆçš„labelsä¸Šï¼Œæ²¡æœ‰è¢«æ©ç›–çš„æ•°æ®éƒ½å˜æˆ-100ï¼Œè¢«æ©ç›–çš„æ•°æ®æ›¿æ¢ä¸ºä¹‹å‰çš„æ•°æ®
# labelsæ˜¯æœ€åçš„æ ‡ç­¾ï¼Œé€šè¿‡è®­ç»ƒåå‘å°±èƒ½å¾ˆå¥½çš„ä¼˜åŒ–æ¨¡å‹ï¼Œè¿™å°±æ˜¯maskedæ¨¡å‹æ•°æ®å¤„ç†
print(&#34;Collated inputs:&#34;, collated_inputs)
data_collator1 = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
collated_inputs = data_collator1(batch)
#mlm=Falseï¼Œä¸ä¼šäº§ç”Ÿé®ç›–ï¼Œæ‰€æœ‰çš„è¾“å…¥ç”Ÿæˆçš„æ˜¯è¾“å‡ºç›¸åŒçš„labelsï¼Œå¦‚æœæ˜¯paddingå­—ç¬¦ï¼Œlabelsæ˜¯-100
print(&#34;Collated inputs:&#34;, collated_inputs)</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>Encoded inputs: {&#39;input_ids&#39;: tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102,    0],
        [ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012,  102]]), &#39;token_type_ids&#39;: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}
collatoréœ€è¦æ ¼å¼ [{&#39;input_ids&#39;: tensor([ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102,    0]), &#39;token_type_ids&#39;: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]), &#39;attention_mask&#39;: tensor([1, 1, 1, 1, 1, 1, 1, 1, 0])}, {&#39;input_ids&#39;: tensor([ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012,  102]), &#39;token_type_ids&#39;: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]), &#39;attention_mask&#39;: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])}]
Collated inputs: {&#39;input_ids&#39;: tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102,    0],
        [ 101, 1045, 2572, 2986,  103, 4067,  103, 1012,  102]]), &#39;token_type_ids&#39;: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1]]), &#39;labels&#39;: tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, 1010, -100, 2017, -100, -100]])}
Collated inputs: {&#39;input_ids&#39;: tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102,    0],
        [ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012,  102]]), &#39;token_type_ids&#39;: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1]]), &#39;labels&#39;: tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102, -100],
        [ 101, 1045, 2572, 2986, 1010, 4067, 2017, 1012,  102]])}</code></pre></div>
<h4 id="map">map</h4>
<p>åœ¨ä½¿ç”¨ transformers åº“æ—¶ï¼Œdatasets ä¸­çš„ map æ–¹æ³•æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„å·¥å…·ï¼Œç”¨äºå¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ã€ç‰¹å¾æå–ã€æ•°æ®å¢å¼ºç­‰æ“ä½œã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ map æ–¹æ³•å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ä¾¿äºå°†å…¶ç”¨äºè®­ç»ƒä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ¨¡å‹ã€‚
è¯¦ç»†å¤„ç†å‚è€ƒï¼š<a href="https://huggingface.co/docs/datasets/use_dataset" rel="external" target="_blank">https://huggingface.co/docs/datasets/use_dataset</a>
<code>map</code> å‡½æ•°æ˜¯ <code>datasets</code> åº“ä¸­ä¸€ä¸ªéå¸¸å¼ºå¤§çš„å·¥å…·ï¼Œå®ƒå…è®¸ä½ å¯¹æ•°æ®é›†çš„æ¯ä¸ªæ ·æœ¬æˆ–æ‰¹æ¬¡è¿›è¡Œæ“ä½œå’Œå˜æ¢ã€‚ä»¥ä¸‹æ˜¯ <code>map</code> å‡½æ•°çš„å‡ ä¸ªå…³é”®å‚æ•°åŠå…¶è§£é‡Šï¼š</p>
<ol>
<li><strong><code>function</code></strong></li>
</ol>
<p>è¿™æ˜¯ä¸€ä¸ªç”¨æˆ·å®šä¹‰çš„å‡½æ•°ï¼Œå®ƒå°†åº”ç”¨äºæ•°æ®é›†çš„æ¯ä¸ªæ ·æœ¬æˆ–æ‰¹æ¬¡ã€‚å‡½æ•°å¯ä»¥æ¥å—ä¸€ä¸ªæ ·æœ¬æˆ–ä¸€ç»„æ ·æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªæˆ–å¤šä¸ªæ–°çš„å­—æ®µã€‚</p>
<p><code>def preprocess_function(examples):     # ä½ çš„é¢„å¤„ç†é€»è¾‘     return examples</code></p>
<ol start="2">
<li><strong><code>batched</code></strong></li>
</ol>
<ul>
<li>ç±»å‹ï¼š<code>bool</code></li>
<li>é»˜è®¤å€¼ï¼š<code>False</code></li>
<li>è§£é‡Šï¼šå¦‚æœè®¾ç½®ä¸º <code>True</code>ï¼Œ<code>function</code> å°†ä¼šæ‰¹é‡åº”ç”¨åˆ°æ•°æ®é›†ä¸­ã€‚è¿™æ„å‘³ç€ <code>function</code> å°†æ¥æ”¶ä¸€ä¸ªåŒ…å«å¤šä¸ªæ ·æœ¬çš„å­—å…¸ä½œä¸ºè¾“å…¥ã€‚</li>
</ul>
<p><code>dataset.map(preprocess_function, batched=True)</code></p>
<ol start="3">
<li><strong><code>batch_size</code></strong></li>
</ol>
<ul>
<li>ç±»å‹ï¼š<code>int</code></li>
<li>é»˜è®¤å€¼ï¼š<code>1000</code></li>
<li>è§£é‡Šï¼šæŒ‡å®šæ‰¹é‡å¤„ç†æ—¶çš„æ‰¹æ¬¡å¤§å°ã€‚ä»…å½“ <code>batched=True</code> æ—¶æœ‰æ•ˆã€‚</li>
</ul>
<p><code>dataset.map(preprocess_function, batched=True, batch_size=32)</code></p>
<ol start="4">
<li><strong><code>remove_columns</code></strong></li>
</ol>
<ul>
<li>ç±»å‹ï¼š<code>list</code> or <code>str</code></li>
<li>é»˜è®¤å€¼ï¼š<code>None</code></li>
<li>è§£é‡Šï¼šæŒ‡å®šè¦ä»æ•°æ®é›†ä¸­ç§»é™¤çš„åˆ—ã€‚è¿™å¯¹äºæ¸…ç†ä¸éœ€è¦çš„å­—æ®µéå¸¸æœ‰ç”¨ã€‚</li>
</ul>
<p><code>dataset.map(preprocess_function, remove_columns=[&quot;column_name&quot;])</code></p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code># å¯¼å…¥å¿…è¦çš„åº“
from datasets import Dataset

# åˆ›å»ºä¸€ä¸ªç®€å•çš„æ•°æ®é›†
data = {
    &#39;text&#39;: [
        &#34;This is the first sentence.&#34;,
        &#34;Here&#39;s the second sentence.&#34;,
        &#34;And this is the third one.&#34;
    ],
    &#39;label&#39;: [1, 0, 1]
}

# è½¬æ¢ä¸º Dataset å¯¹è±¡
dataset = Dataset.from_dict(data)

# æ‰“å°åŸå§‹æ•°æ®é›†
print(&#34;åŸå§‹æ•°æ®é›†ï¼š&#34;)
print(dataset)

# å¯¼å…¥å¿…è¦çš„åº“
from transformers import AutoTokenizer

# åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)

# å®šä¹‰é¢„å¤„ç†å‡½æ•°
def preprocess_function(examples):
    print(&#34;ä¼ å…¥æ•°æ®é›†&#34;,examples)
    # ä½¿ç”¨åˆ†è¯å™¨å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
    encoded_tokenizer = tokenizer(examples[&#39;text&#39;], truncation=True, padding=&#39;max_length&#39;, max_length=8)
    print(&#34;åˆ†è¯æ•°æ®é›†&#34;,encoded_tokenizer)
    #è¿”å›çš„å­—å…¸æ•°æ®ä¼šè¢«ç´¯åŠ åˆ°åŸå§‹æ•°æ®é›†ä¸Šã€‚
    return encoded_tokenizer

# ä½¿ç”¨ map æ–¹æ³•åº”ç”¨é¢„å¤„ç†å‡½æ•°
encoded_dataset = dataset.map(preprocess_function, batched=True,batch_size=2)

# æ‰“å°é¢„å¤„ç†åçš„æ•°æ®é›†
print(&#34;\né¢„å¤„ç†åçš„æ•°æ®é›†ç»“æ„ï¼š&#34;,encoded_dataset)
print(&#34;\né¢„å¤„ç†åçš„æ•°æ®é›†ï¼š&#34;,encoded_dataset[0:3])

# ä½¿ç”¨ map æ–¹æ³•åº”ç”¨é¢„å¤„ç†å‡½æ•°,remove_columnsè¡¨ç¤ºåˆ é™¤æŸäº›åˆ—æ˜¯ä¸ªæ•°ç»„ã€‚
encoded_dataset = dataset.map(preprocess_function, batched=True,batch_size=2,remove_columns=dataset.features)

# æ‰“å°é¢„å¤„ç†åçš„æ•°æ®é›†
print(&#34;\né¢„å¤„ç†åçš„æ•°æ®é›†ï¼š&#34;,encoded_dataset[0:3])</code></pre></div>
<p>è¾“å‡ºï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>åŸå§‹æ•°æ®é›†ï¼š
Dataset({
    features: [&#39;text&#39;, &#39;label&#39;],
    num_rows: 3
})
Map:â€‡100%
â€‡3/3â€‡[00:00&lt;00:00,â€‡138.43â€‡examples/s]
ä¼ å…¥æ•°æ®é›† {&#39;text&#39;: [&#39;This is the first sentence.&#39;, &#34;Here&#39;s the second sentence.&#34;, &#39;And this is the third one.&#39;], &#39;label&#39;: [1, 0, 1]}
åˆ†è¯æ•°æ®é›† {&#39;input_ids&#39;: [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], &#39;token_type_ids&#39;: [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], &#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}

é¢„å¤„ç†åçš„æ•°æ®é›†ç»“æ„ï¼š Dataset({
    features: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;],
    num_rows: 3
})

é¢„å¤„ç†åçš„æ•°æ®é›†ï¼š {&#39;text&#39;: [&#39;This is the first sentence.&#39;, &#34;Here&#39;s the second sentence.&#34;, &#39;And this is the third one.&#39;], &#39;label&#39;: [1, 0, 1], &#39;input_ids&#39;: [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], &#39;token_type_ids&#39;: [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], &#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}
é¢„å¤„ç†åçš„æ•°æ®é›†ï¼š {&#39;input_ids&#39;: [[101, 2023, 2003, 1996, 2034, 6251, 1012, 102], [101, 2182, 1005, 1055, 1996, 2117, 6251, 102], [101, 1998, 2023, 2003, 1996, 2353, 2028, 102]], &#39;token_type_ids&#39;: [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], &#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}</code></pre></div>
<h3 id="é¢„å¤„ç†">é¢„å¤„ç†</h3>
<p>å¤§å¤šæ•°æ–‡æ¡£çš„æ ‡è®°æ•°è¿œè¶…è¿‡ 128 ä¸ªï¼Œå› æ­¤ç®€å•åœ°å°†è¾“å…¥æˆªæ–­åˆ°æœ€å¤§é•¿åº¦ä¼šæ¶ˆé™¤æˆ‘ä»¬æ•°æ®é›†çš„å¾ˆå¤§ä¸€éƒ¨åˆ†ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ return_overflowing_tokens é€‰é¡¹æ¥å¯¹æ•´ä¸ªè¾“å…¥è¿›è¡Œæ ‡è®°ï¼Œå¹¶å°†å…¶æ‹†åˆ†ä¸ºå‡ ä¸ªå—ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ return_length é€‰é¡¹è‡ªåŠ¨è¿”å›æ¯ä¸ªåˆ›å»ºå—çš„é•¿åº¦ã€‚é€šå¸¸ï¼Œæœ€åä¸€ä¸ªå—ä¼šå°äºä¸Šä¸‹æ–‡å¤§å°ï¼Œæˆ‘ä»¬å°†å»æ‰è¿™äº›éƒ¨åˆ†ä»¥é¿å…å¡«å……é—®é¢˜ï¼›å®é™…ä¸Šæˆ‘ä»¬ä¸éœ€è¦å®ƒä»¬ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰å¾ˆå¤šæ•°æ®ã€‚
<a href="#R-image-c38e83611408c2205abaa159bb4121d9" class="lightbox-link"><img alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" class="lazy lightbox figure-image" loading="lazy" src="/docs/images/content/programming/ai/tools_libraries/transformers/actions/transformers_actions_04.md.images/3f5c55e1611842b7a9e191ecc1305279.png" style=" height: auto; width: auto;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-c38e83611408c2205abaa159bb4121d9"><img alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°" class="lazy lightbox lightbox-image" loading="lazy" src="/docs/images/content/programming/ai/tools_libraries/transformers/actions/transformers_actions_04.md.images/3f5c55e1611842b7a9e191ecc1305279.png"></a>
è®©æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹å‰ä¸¤ä¸ªä¾‹å­æ¥çœ‹çœ‹è¿™åˆ°åº•æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>from transformers import AutoTokenizer

context_length = 128
#è¿™ä¸ªåˆ†è¯å™¨ä¸“é—¨ä¸ºä»£ç æœç´¢å’Œç†è§£ä»»åŠ¡è®¾è®¡ã€‚å®ƒä¸»è¦ç”¨äºå¤„ç†ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ Pythonã€JavaScriptã€Java ç­‰ï¼‰çš„æºä»£ç ã€‚
tokenizer = AutoTokenizer.from_pretrained(&#34;huggingface-course/code-search-net-tokenizer&#34;)

outputs = tokenizer(
    #è·å–0ï¼Œ1è¿™ä¸¤ä¸ªæ•°æ®é›†çš„è„šæœ¬å†…å®¹
    raw_datasets[&#34;train&#34;][:2][&#34;content&#34;],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f&#34;Input IDs length: {len(outputs[&#39;input_ids&#39;])}&#34;)
print(f&#34;Input chunk lengths: {(outputs[&#39;length&#39;])}&#34;)
print(f&#34;Chunk mapping: {outputs[&#39;overflow_to_sample_mapping&#39;]}&#34;)</code></pre></div>
<blockquote>
<p><code>huggingface-course/code-search-net-tokenizer</code></p>
<ul>
<li><strong>è®¾è®¡ç›®æ ‡</strong>ï¼šè¿™ä¸ªåˆ†è¯å™¨ä¸“é—¨ä¸ºä»£ç æœç´¢å’Œç†è§£ä»»åŠ¡è®¾è®¡ã€‚å®ƒä¸»è¦ç”¨äºå¤„ç†ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ &gt;Pythonã€JavaScriptã€Java ç­‰ï¼‰çš„æºä»£ç ã€‚</li>
<li><strong>è®­ç»ƒæ•°æ®</strong>ï¼šè¯¥åˆ†è¯å™¨ä½¿ç”¨ CodeSearchNet æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ•°æ®é›†ä¸­åŒ…å«äº†å¤§é‡çš„ä»£ç ç¤ºä¾‹&gt;å’Œæ³¨é‡Šã€‚</li>
<li><strong>åº”ç”¨é¢†åŸŸ</strong>ï¼šé€‚ç”¨äºä»£ç æœç´¢ã€ä»£ç è¡¥å…¨ã€ä»£ç ç”Ÿæˆå’Œå…¶ä»–ä¸ä»£ç ç›¸å…³çš„ä»»åŠ¡ã€‚</li>
<li><strong>è¯æ±‡è¡¨</strong>ï¼šè¯æ±‡è¡¨ä¸­åŒ…å«äº†å¤§é‡çš„ç¼–ç¨‹è¯­è¨€ç‰¹å®šçš„æ ‡è®°ï¼ˆå¦‚å…³é”®å­—ã€æ“ä½œç¬¦ã€å˜é‡åç­‰ï¼‰ï¼Œä»¥åŠ&gt;å¸¸è§çš„ç¼–ç¨‹è¯­è¨€è¯­æ³•å’Œç»“æ„ã€‚
<!-- raw HTML omitted -->æ³¨æ„ï¼šåˆ†è¯å™¨æ¨¡å‹çš„ä½œç”¨æ˜¯å°†å•è¯è½¬æ¢ä¸ºä¸€ä¸ªä¸ªçš„æ•°å­—ï¼Œè®­ç»ƒæ—¶ä½¿ç”¨çš„æ•°å­—è®¡ç®—æ•°å­—ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»ï¼Œæœ€åæ¨ç®—å¯¹åº”çš„æ•°å­—åï¼Œåå‘é€šè¿‡è¯å…¸è§£ææˆæ–‡å­—ï¼Œæ‰€ä»¥å¦‚æœéœ€è¦è®­ç»ƒä¸­æ–‡ï¼Œä½ åªéœ€è¦æœ‰ä¸€ä¸ªä¸­æ–‡åˆ†è¯æ¨¡å‹å³å¯ï¼Œè®­ç»ƒåªå’Œæ•°å­—ç›¸å…³ã€‚<!-- raw HTML omitted --></li>
</ul></blockquote>
<p>è¾“å‡ºï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</code></pre></div>
<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä»è¿™ä¸¤ä¸ªä¾‹å­ä¸­æ€»å…±å¾—åˆ°äº† 34 ä¸ªç‰‡æ®µã€‚æŸ¥çœ‹ç‰‡æ®µé•¿åº¦ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªæ–‡æ¡£æœ«å°¾çš„ç‰‡æ®µéƒ½å°‘äº 128 ä¸ªæ ‡è®°ï¼ˆåˆ†åˆ«ä¸º 117 å’Œ 41ï¼‰ã€‚è¿™äº›ä»…å æˆ‘ä»¬æ‹¥æœ‰çš„æ€»ç‰‡æ®µçš„ä¸€å°éƒ¨åˆ†ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°ä¸¢å¼ƒå®ƒä»¬ã€‚ä½¿ç”¨ overflow_to_sample_mapping å­—æ®µï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é‡å»ºå“ªäº›ç‰‡æ®µå±äºå“ªäº›è¾“å…¥æ ·æœ¬ã€‚</p>
<p>é€šè¿‡è¿™ä¸ªæ“ä½œï¼Œæˆ‘ä»¬åˆ©ç”¨äº† ğŸ¤— Datasets ä¸­ Dataset.map() å‡½æ•°çš„ä¸€ä¸ªä¾¿åˆ©åŠŸèƒ½ï¼Œå³å®ƒä¸éœ€è¦ä¸€ä¸€å¯¹åº”çš„æ˜ å°„ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæ¯”è¾“å…¥æ‰¹æ¬¡å¤šæˆ–å°‘çš„å…ƒç´ æ‰¹æ¬¡ã€‚å½“è¿›è¡Œæ•°æ®å¢å¼ºæˆ–æ•°æ®è¿‡æ»¤ç­‰ä¼šæ”¹å˜å…ƒç´ æ•°é‡çš„æ“ä½œæ—¶ï¼Œè¿™éå¸¸æœ‰ç”¨ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå½“å°†æ¯ä¸ªå…ƒç´ æ ‡è®°ä¸ºæŒ‡å®šä¸Šä¸‹æ–‡å¤§å°çš„å—æ—¶ï¼Œæˆ‘ä»¬ä»æ¯ä¸ªæ–‡æ¡£ä¸­åˆ›å»ºäº†è®¸å¤šæ ·æœ¬ã€‚æˆ‘ä»¬åªéœ€è¦ç¡®ä¿åˆ é™¤ç°æœ‰åˆ—ï¼Œå› ä¸ºå®ƒä»¬çš„å¤§å°ä¸ä¸€è‡´ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¿ç•™å®ƒä»¬ï¼Œå¯ä»¥é€‚å½“é‡å¤å¹¶åœ¨ Dataset.map() è°ƒç”¨ä¸­è¿”å›å®ƒä»¬ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>def tokenize(element):
    outputs = tokenizer(
        element[&#34;content&#34;],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    #è·å–å½“å‰input_idså’Œé•¿åº¦ï¼Œæœ«å°¾chuckä¸ç­‰äºcontext_lengthï¼Œå°±ä¸éœ€è¦åŠ å…¥äº†
    for length, input_ids in zip(outputs[&#34;length&#34;], outputs[&#34;input_ids&#34;]):
        if length == context_length:
            input_batch.append(input_ids)
    return {&#34;input_ids&#34;: input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets[&#34;train&#34;].column_names
)
tokenized_datasets</code></pre></div>
<p>è¾“å‡ºï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>DatasetDict({
    train: Dataset({
        features: [&#39;input_ids&#39;],
        num_rows: 16702061
    })
    valid: Dataset({
        features: [&#39;input_ids&#39;],
        num_rows: 93164
    })
})</code></pre></div>
<p>æˆ‘ä»¬ç°åœ¨æœ‰ 1670 ä¸‡ä¸ªä¾‹å­ï¼Œæ¯ä¸ªä¾‹å­æœ‰ 128 ä¸ªæ ‡è®°ï¼Œæ€»å…±å¯¹åº”å¤§çº¦ 21 äº¿ä¸ªæ ‡è®°ã€‚ä¾›å‚è€ƒï¼ŒOpenAI çš„ GPT-3 å’Œ Codex æ¨¡å‹åˆ†åˆ«åœ¨ 300 å’Œ 1000 äº¿ä¸ªæ ‡è®°ä¸Šè®­ç»ƒï¼Œå…¶ä¸­ Codex æ¨¡å‹æ˜¯ä» GPT-3 æ£€æŸ¥ç‚¹åˆå§‹åŒ–çš„ã€‚æˆ‘ä»¬åœ¨è¿™ä¸€éƒ¨åˆ†çš„ç›®æ ‡ä¸æ˜¯ä¸è¿™äº›æ¨¡å‹ç«äº‰ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥ç”Ÿæˆé•¿è€Œè¿è´¯çš„æ–‡æœ¬ï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªç¼©å‡ç‰ˆæœ¬ï¼Œä¸ºæ•°æ®ç§‘å­¦å®¶æä¾›å¿«é€Ÿè‡ªåŠ¨è¡¥å…¨åŠŸèƒ½ã€‚
ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½æ•°æ®é›†ï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬è®¾ç½®æ¨¡å‹ï¼</p>
<h2 id="åˆå§‹åŒ–æ¨¡å‹">åˆå§‹åŒ–æ¨¡å‹</h2>
<h3 id="å›é¡¾æ¨¡å‹">å›é¡¾æ¨¡å‹</h3>
<h4 id="å‚æ•°è®¡ç®—">å‚æ•°è®¡ç®—</h4>
<p>åœ¨ PyTorch ä¸­ï¼Œ<code>t.numel()</code> æ˜¯ä¸€ä¸ªå¼ é‡æ–¹æ³•ï¼Œç”¨äºè¿”å›å¼ é‡ä¸­æ‰€æœ‰å…ƒç´ çš„æ•°é‡ã€‚å®ƒç­‰ä»·äºè®¡ç®—å¼ é‡çš„å¤§å°ï¼ˆshapeï¼‰çš„æ‰€æœ‰ç»´åº¦çš„ä¹˜ç§¯ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå½¢çŠ¶ä¸º (3, 4, 5) çš„å¼ é‡æœ‰ 3 * 4 * 5 = 60 ä¸ªå…ƒç´ ã€‚</p>
<p>åœ¨ä½ æä¾›çš„ä»£ç ä¸­ï¼š</p>
<p><code>model_size = sum(t.numel() for t in model.parameters())</code></p>
<p>è¿™é‡Œ <code>model.parameters()</code> è¿”å›æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°çš„ä¸€ä¸ªç”Ÿæˆå™¨ã€‚é€šè¿‡ <code>t.numel()</code> è®¡ç®—æ¯ä¸ªå‚æ•°å¼ é‡ä¸­çš„å…ƒç´ æ•°é‡ï¼Œç„¶åä½¿ç”¨ <code>sum()</code> å‡½æ•°å°†æ‰€æœ‰è¿™äº›æ•°é‡åŠ èµ·æ¥ï¼Œå¾—åˆ°æ•´ä¸ªæ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°çš„æ€»å…ƒç´ æ•°é‡ï¼Œå³æ¨¡å‹çš„æ€»å¤§å°ã€‚
ç¤ºä¾‹
å‡è®¾æœ‰ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(10, 20)
        self.fc2 = nn.Linear(20, 30)
        
    def forward(self, x):
        x = self.fc1(x)
        x = self.fc2(x)
        return x

model = SimpleModel()</code></pre></div>
<p>è®¡ç®—æ¨¡å‹å¤§å°çš„ä»£ç å¦‚ä¸‹ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>model_size = sum(t.numel() for t in model.parameters())
print(model_size)</code></pre></div>
<p>åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ<code>model.parameters()</code> ä¼šè¿”å› <code>fc1</code> å’Œ <code>fc2</code> çš„å‚æ•°å¼ é‡ã€‚</p>
<ul>
<li><code>fc1</code> çš„æƒé‡å¼ é‡å½¢çŠ¶ä¸º (20, 10)ï¼Œæœ‰ 20 * 10 = 200 ä¸ªå…ƒç´ ã€‚</li>
<li><code>fc1</code> çš„åç½®å¼ é‡å½¢çŠ¶ä¸º (20,)ï¼Œæœ‰ 20 ä¸ªå…ƒç´ ã€‚</li>
<li><code>fc2</code> çš„æƒé‡å¼ é‡å½¢çŠ¶ä¸º (30, 20)ï¼Œæœ‰ 30 * 20 = 600 ä¸ªå…ƒç´ ã€‚</li>
<li><code>fc2</code> çš„åç½®å¼ é‡å½¢çŠ¶ä¸º (30,)ï¼Œæœ‰ 30 ä¸ªå…ƒç´ ã€‚</li>
</ul>
<p>æ€»è®¡æ¨¡å‹ä¸­æœ‰ 200 + 20 + 600 + 30 = 850 ä¸ªå‚æ•°å…ƒç´ ã€‚å› æ­¤ï¼Œ<code>model_size</code> çš„å€¼å°†æ˜¯ 850ã€‚</p>
<h3 id="åˆå§‹åŒ–">åˆå§‹åŒ–</h3>
<p>æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥æ˜¯åˆå§‹åŒ–ä¸€ä¸ªGPT-2æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä¸ºæˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨ä¸å°å‹GPT-2æ¨¡å‹ç›¸åŒçš„é…ç½®ï¼Œå› æ­¤æˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒçš„é…ç½®ï¼Œç¡®ä¿æ ‡è®°å™¨å¤§å°ä¸æ¨¡å‹è¯æ±‡å¤§å°åŒ¹é…ï¼Œå¹¶ä¼ é€’boså’Œeosï¼ˆåºåˆ—å¼€å§‹å’Œç»“æŸï¼‰ä»¤ç‰ŒIDï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    &#34;gpt2&#34;,
    vocab_size=len(tokenizer), #è·å–è¯æ±‡è¡¨å¤§å°
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)</code></pre></div>
<blockquote>
<p>å› ä¸ºä½¿ç”¨äº†ä¸åŒçš„åˆ†è¯å™¨ï¼Œæ‰€ä»¥é‡æ–°åŠ è½½é…ç½®</p></blockquote>
<p>é€šè¿‡è¯¥é…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½ä¸€ä¸ªæ–°æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸ä½¿ç”¨ from_pretrained() å‡½æ•°ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨è‡ªå·±åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f&#34;GPT-2 size: {model_size/1000**2:.1f}M parameters&#34;)</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>GPT-2 size: 124.2M parameters</code></pre></div>
<p>æˆ‘ä»¬çš„æ¨¡å‹æœ‰ 124M ä¸ªå‚æ•°éœ€è¦è°ƒä¼˜ã€‚åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®ä¸€ä¸ªæ•°æ®æ•´ç†å™¨ï¼Œæ¥å¤„ç†åˆ›å»ºæ‰¹æ¬¡çš„å·¥ä½œã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ DataCollatorForLanguageModeling æ•´ç†å™¨ï¼Œå®ƒæ˜¯ä¸“é—¨ä¸ºè¯­è¨€å»ºæ¨¡è®¾è®¡çš„ï¼ˆæ­£å¦‚å…¶åç§°å¾®å¦™åœ°æš—ç¤ºçš„é‚£æ ·ï¼‰ã€‚é™¤äº†å †å å’Œå¡«å……æ‰¹æ¬¡å¤–ï¼Œå®ƒè¿˜è´Ÿè´£åˆ›å»ºè¯­è¨€æ¨¡å‹æ ‡ç­¾â€”â€”åœ¨å› æœè¯­è¨€å»ºæ¨¡ä¸­ï¼Œè¾“å…¥ä¹Ÿä½œä¸ºæ ‡ç­¾ï¼ˆä»…åç§»ä¸€ä¸ªå…ƒç´ ï¼‰ï¼Œè¿™ä¸ªæ•°æ®æ•´ç†å™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶åˆ›å»ºå®ƒä»¬ï¼Œå› æ­¤æˆ‘ä»¬ä¸éœ€è¦é‡å¤ input_idsã€‚
è¯·æ³¨æ„ï¼ŒDataCollatorForLanguageModeling æ”¯æŒæ©ç è¯­è¨€å»ºæ¨¡ (MLM) å’Œå› æœè¯­è¨€å»ºæ¨¡ (CLM)ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒä¸º MLM å‡†å¤‡æ•°æ®ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½®å‚æ•° mlm=False åˆ‡æ¢åˆ° CLMï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)</code></pre></div>
<p>è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªä¾‹å­ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>out = data_collator([tokenized_datasets[&#34;train&#34;][i] for i in range(5)])
for key in out:
    print(f&#34;{key} shape: {out[key].shape}&#34;)</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])</code></pre></div>
<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¤ºä¾‹å·²ç»è¢«å †å ï¼Œæ‰€æœ‰å¼ é‡å½¢çŠ¶ç›¸åŒã€‚</p>
<p>å‰©ä¸‹çš„å°±æ˜¯é…ç½®è®­ç»ƒå‚æ•°å¹¶å¯åŠ¨è®­ç»ƒå™¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦ï¼Œå¹¶è¿›è¡Œä¸€äº›é¢„çƒ­ï¼Œå®é™…æ‰¹é‡å¤§å°ä¸º256ï¼ˆper_device_train_batch_size * gradient_accumulation_stepsï¼‰ã€‚å½“å•ä¸ªæ‰¹æ¬¡æ— æ³•é€‚åº”å†…å­˜æ—¶ï¼Œä¼šä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå®ƒé€šè¿‡å¤šæ¬¡å‰å‘/åå‘ä¼ é€’é€æ­¥ç´¯ç§¯æ¢¯åº¦ã€‚å½“æˆ‘ä»¬ä½¿ç”¨ğŸ¤— Accelerate åˆ›å»ºè®­ç»ƒå¾ªç¯æ—¶ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è¿™ä¸€ç‚¹çš„å®é™…åº”ç”¨ã€‚</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir=&#34;codeparrot-ds&#34;,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy=&#34;steps&#34;,
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type=&#34;cosine&#34;,
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets[&#34;train&#34;],
    eval_dataset=tokenized_datasets[&#34;valid&#34;],
)</code></pre></div>
<p>ç°åœ¨æˆ‘ä»¬å¯ä»¥å¯åŠ¨è®­ç»ƒå™¨å¹¶ç­‰å¾…è®­ç»ƒå®Œæˆã€‚æ ¹æ®æ‚¨æ˜¯åœ¨å®Œæ•´çš„è®­ç»ƒé›†ä¸Šè¿è¡Œè¿˜æ˜¯åœ¨å­é›†ä¸Šè¿è¡Œï¼Œè¿™å°†åˆ†åˆ«éœ€è¦ 20 å°æ—¶æˆ– 2 å°æ—¶ï¼Œæ‰€ä»¥å‡†å¤‡å‡ æ¯å’–å•¡å’Œä¸€æœ¬å¥½ä¹¦æ¥é˜…è¯»å§ï¼</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>trainer.train()</code></pre></div>
<h2 id="å®Œæ•´ä»£ç ">å®Œæ•´ä»£ç </h2>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>from datasets import load_dataset, DatasetDict

ds_train = load_dataset(&#34;huggingface-course/codeparrot-ds-train&#34;, split=&#34;train&#34;)
ds_valid = load_dataset(&#34;huggingface-course/codeparrot-ds-valid&#34;, split=&#34;validation&#34;)

raw_datasets = DatasetDict(
    {
        &#34;train&#34;: ds_train,  # .shuffle().select(range(50000)),
        &#34;valid&#34;: ds_valid,  # .shuffle().select(range(500))
    }
)
from transformers import AutoTokenizer

context_length = 128
#è¿™ä¸ªåˆ†è¯å™¨æ¨¡å‹ä¸“é—¨ä¸ºä»£ç æœç´¢å’Œç†è§£ä»»åŠ¡è®¾è®¡ã€‚å®ƒä¸»è¦ç”¨äºå¤„ç†ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ Pythonã€JavaScriptã€Java ç­‰ï¼‰çš„æºä»£ç ï¼Œåˆ†è¯å™¨çš„ç›®çš„æ˜¯å°†å¯¹åº”è¯å…ƒè½¬æ¢ä¸ºæ•°å­—ï¼Œè®©æ¨¡å‹é€šè¿‡è®¡ç®—æ¥ç†è§£æ•°å­—å’Œæ•°å­—ä¹‹é—´çš„å…³ç³»ï¼Œé€‰æ‹©æ¨¡å‹çš„åˆ†è¯å™¨éå¸¸é‡è¦ã€‚
tokenizer = AutoTokenizer.from_pretrained(&#34;huggingface-course/code-search-net-tokenizer&#34;)

def tokenize(element):
    outputs = tokenizer(
        element[&#34;content&#34;],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    #è·å–å½“å‰input_idså’Œé•¿åº¦ï¼Œæœ«å°¾chuckä¸ç­‰äºcontext_lengthï¼Œå°±ä¸éœ€è¦åŠ å…¥äº†
    for length, input_ids in zip(outputs[&#34;length&#34;], outputs[&#34;input_ids&#34;]):
        if length == context_length:
            input_batch.append(input_ids)
    return {&#34;input_ids&#34;: input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets[&#34;train&#34;].column_names
)
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    &#34;gpt2&#34;,
    vocab_size=len(tokenizer), #è·å–è¯æ±‡è¡¨å¤§å°
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
model = GPT2LMHeadModel(config)
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir=&#34;/kaggle/working&#34;,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy=&#34;steps&#34;,
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type=&#34;cosine&#34;,
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    report_to=&#34;none&#34;,
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets[&#34;train&#34;],
    eval_dataset=tokenized_datasets[&#34;valid&#34;],
)
trainer.train()</code></pre></div>
<h2 id="æµ‹è¯•">æµ‹è¯•</h2>
<blockquote>
<p>ç”±äºä½¿ç”¨kaggleçš„gpuæ— æ³•åœ¨12å°æ—¶è®­ç»ƒå®Œæˆï¼Œæ‰€ä»¥è¿™é‡Œåªèƒ½ç”¨å®˜æ–¹å·²ç»è®­ç»ƒå¥½çš„é•œåƒæµ‹è¯•äº†ã€‚</p></blockquote>
<p>ç°åœ¨æ˜¯è§è¯ç»“æœçš„æ—¶åˆ»ï¼šè®©æˆ‘ä»¬çœ‹çœ‹è®­ç»ƒå¥½çš„æ¨¡å‹å®é™…è¡¨ç°å¦‚ä½•ï¼æˆ‘ä»¬å¯ä»¥åœ¨æ—¥å¿—ä¸­çœ‹åˆ°æŸå¤±å€¼ä¸€ç›´åœ¨ç¨³å®šä¸‹é™ï¼Œä½†ä¸ºäº†çœŸæ­£æµ‹è¯•æ¨¡å‹çš„æ•ˆæœï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å®ƒåœ¨ä¸€äº›æç¤ºä¿¡æ¯ä¸Šçš„è¡¨ç°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å°è£…åˆ°ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆç®¡é“ä¸­ï¼Œå¹¶å¦‚æœæ¡ä»¶å…è®¸çš„è¯ï¼Œå°†å…¶éƒ¨ç½²åˆ° GPU ä¸Šä»¥å®ç°å¿«é€Ÿç”Ÿæˆï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>import torch
from transformers import pipeline

device = torch.device(&#34;cuda&#34;) if torch.cuda.is_available() else torch.device(&#34;cpu&#34;)
pipe = pipeline(
    &#34;text-generation&#34;, model=&#34;huggingface-course/codeparrot-ds&#34;, device=device
)</code></pre></div>
<p>è®©æˆ‘ä»¬ä»åˆ›å»ºæ•£ç‚¹å›¾çš„ç®€å•ä»»åŠ¡å¼€å§‹ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>txt = &#34;&#34;&#34;\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
&#34;&#34;&#34;
print(pipe(txt, num_return_sequences=1)[0][&#34;generated_text&#34;])</code></pre></div>
<p>è¾“å‡ºï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code># create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter</code></pre></div>
<p>ç»“æœçœ‹èµ·æ¥æ˜¯æ­£ç¡®çš„ã€‚å¯¹äº pandas çš„æ“ä½œæ˜¯å¦ä¹Ÿé€‚ç”¨å‘¢ï¼Ÿæˆ‘ä»¬æ¥çœ‹çœ‹èƒ½å¦ä»ä¸¤ä¸ªæ•°ç»„åˆ›å»ºä¸€ä¸ª DataFrameï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>txt = &#34;&#34;&#34;\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
&#34;&#34;&#34;
print(pipe(txt, num_return_sequences=1)[0][&#34;generated_text&#34;])</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code># create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({&#39;x&#39;: x, &#39;y&#39;: y})
df.insert(0,&#39;x&#39;, x)
for</code></pre></div>
<p>å¥½çš„ï¼Œè¿™æ˜¯æ­£ç¡®çš„ç­”æ¡ˆâ€”â€”å°½ç®¡éšååˆæ’å…¥äº†åˆ— xã€‚ç”±äºç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡æœ‰é™ï¼Œä¸‹é¢çš„ for å¾ªç¯è¢«æˆªæ–­äº†ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹èƒ½å¦åšä¸€äº›æ›´å¤æ‚çš„äº‹æƒ…ï¼Œå¹¶è®©æ¨¡å‹å¸®åŠ©æˆ‘ä»¬ä½¿ç”¨ groupby æ“ä½œï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>txt = &#34;&#34;&#34;\
# dataframe with profession, income and name
df = pd.DataFrame({&#39;profession&#39;: x, &#39;income&#39;:y, &#39;name&#39;: z})

# calculate the mean income per profession
&#34;&#34;&#34;
print(pipe(txt, num_return_sequences=1)[0][&#34;generated_text&#34;])</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code># dataframe with profession, income and name
df = pd.DataFrame({&#39;profession&#39;: x, &#39;income&#39;:y, &#39;name&#39;: z})

# calculate the mean income per profession
profession = df.groupby([&#39;profession&#39;]).mean()

# compute the</code></pre></div>
<p>è¿˜ä¸é”™ï¼›è¿™æ ·åšæ˜¯å¯¹çš„ã€‚æœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦ä¹Ÿèƒ½ç”¨å®ƒæ¥ä¸º scikit-learn è®¾ç½®ä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹ï¼š</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code>txt = &#34;&#34;&#34;
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
&#34;&#34;&#34;
print(pipe(txt, num_return_sequences=1)[0][&#34;generated_text&#34;])</code></pre></div>
<p>è¾“å‡º</p>
<div class="highlight wrap-code" dir="auto"><pre tabindex="0"><code># import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf</code></pre></div>
<p>æŸ¥çœ‹è¿™å‡ ä¸ªä¾‹å­ï¼Œæ¨¡å‹ä¼¼ä¹å­¦åˆ°äº†ä¸€äº› Python æ•°æ®ç§‘å­¦å¥—ä»¶çš„è¯­æ³•ã€‚</p>

  <footer class="footline">
              <i class='fa-fw fas fa-calendar'></i> Sep 18, 2025
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          <a id="R-logo" class="R-default" href="/docs/index.html">
            <div class="logo-title">liaomin416100569åšå®¢</div>
          </a>
        </div>
        <search><form action="/docs/search/index.html" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-homelinks">
          <ul class="space collapsible-menu">
            <li class="" data-nav-id="/docs/index.html"><a class="padding" href="/docs/index.html"><i class="fa-fw fas fa-home"></i> Home</a></li>
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-headercontrols">
          <ul class="">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div class="R-sidebarmenu R-shortcutmenu-main">
          <ul class="enlarge morespace collapsible-menu">
            <li class="parent " data-nav-id="/docs/programming/index.html"><a class="padding" href="/docs/programming/index.html">ç¼–ç¨‹å¼€å‘</a><ul id="R-subsections-e3fc01b477dbaf64a8f5013a3dab5c5b" class="collapsible-menu">
            <li class="alwaysopen " data-nav-id="/docs/programming/languages/index.html"><a class="padding" href="/docs/programming/languages/index.html">ç¼–ç¨‹è¯­è¨€</a><ul id="R-subsections-1bbde7fb0c312ba940b425df5a4caf67" class="collapsible-menu"></ul></li>
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/index.html"><a class="padding" href="/docs/programming/ai/index.html">äººå·¥æ™ºèƒ½</a><ul id="R-subsections-9d06be7bd8c736c09a65fb0b91b71d0e" class="collapsible-menu">
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/tools_libraries/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/index.html">å·¥å…·åº“</a><ul id="R-subsections-e43804740042696aa314af8cc1e28fa9" class="collapsible-menu">
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/tools_libraries/transformers/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/transformers/index.html">transformers</a><ul id="R-subsections-c93b786975796f9b9f81f28585ce698d" class="collapsible-menu">
            <li class="alwaysopen " data-nav-id="/docs/programming/ai/tools_libraries/transformers/basic/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/transformers/basic/index.html">transformersæ¨¡å‹è¯¦è§£</a><ul id="R-subsections-1e672efdff9aa37295341bdd1b243398" class="collapsible-menu"></ul></li>
            <li class="parent alwaysopen " data-nav-id="/docs/programming/ai/tools_libraries/transformers/actions/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/transformers/actions/index.html">transformerså®æˆ˜</a><ul id="R-subsections-7dfd1a2fc9789505d186535459f93268" class="collapsible-menu">
            <li class="" data-nav-id="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_01/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_01/index.html">Transformerså®æˆ˜01-å¼€ç®±å³ç”¨çš„ pipelines</a></li>
            <li class="" data-nav-id="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_02/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_02/index.html">Transformerså®æˆ˜02-BERTé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ</a></li>
            <li class="" data-nav-id="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_03/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_03/index.html">Transformerså®æˆ˜03-PEFTåº“ä½¿ç”¨LORAæ–¹æ³•å¾®è°ƒVITå›¾åƒåˆ†ç±»ã€‚</a></li>
            <li class="active " data-nav-id="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_04/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_04/index.html">Transformerså®æˆ˜04-å¾®è°ƒgpt-2ç”Ÿæˆpythonä»£ç ã€‚</a></li>
            <li class="" data-nav-id="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_05/index.html"><a class="padding" href="/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_05/index.html">Transformerså®æˆ˜05-æ¨¡å‹é‡åŒ–</a></li></ul></li></ul></li></ul></li></ul></li>
            <li class="alwaysopen " data-nav-id="/docs/programming/plugins/index.html"><a class="padding" href="/docs/programming/plugins/index.html">æ’ä»¶å¼€å‘</a><ul id="R-subsections-de66f54cff99288ca68bfcb5bb0439ae" class="collapsible-menu"></ul></li></ul></li>
          </ul>
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-shortcuts">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div id="R-footer-margin"></div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-footercontrols">
          <ul class="">
          </ul>
        </div>
<div id="R-footer"><p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p></div>
      </div>
    </aside>
    <script src="/docs/js/clipboard/clipboard.min.js?1758266232" defer></script>
    <script src="/docs/js/perfect-scrollbar/perfect-scrollbar.min.js?1758266232" defer></script>
    <script src="/docs/js/theme.js?1758266232" defer></script>
  </body>
</html>
