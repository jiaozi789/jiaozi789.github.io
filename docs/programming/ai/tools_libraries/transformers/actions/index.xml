<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>transformers实战 :: liaomin416100569博客</title>
    <link>https://jiaozi789.github.io/docs/programming/ai/tools_libraries/transformers/actions/index.html</link>
    <description></description>
    <generator>Hugo</generator>
    <language>zh</language>
    <lastBuildDate>Thu, 18 Sep 2025 16:55:17 +0800</lastBuildDate>
    <atom:link href="https://jiaozi789.github.io/docs/programming/ai/tools_libraries/transformers/actions/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Transformers实战01-开箱即用的 pipelines</title>
      <link>https://jiaozi789.github.io/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_01/index.html</link>
      <pubDate>Thu, 18 Sep 2025 16:55:17 +0800</pubDate>
      <guid>https://jiaozi789.github.io/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_01/index.html</guid>
      <description>Hugging Face Hugging Face是一个人工智能（AI）公司，它致力于开发和推广自然语言处理（NLP）相关的技术和工具。该公司以其开源项目和社区而闻名，其最知名的项目之一是Transformers库，它提供了一系列预训练的语言模型，包括BERT、GPT和RoBERTa等。这些模型已经在各种NLP任务中取得了显著的成功，并成为了许多研究和工业应用的基础。&#xA;除了提供预训练的模型之外，Hugging Face还开发了一系列工具和平台，使得使用和部署这些模型变得更加简单。其中包括：&#xA;Transformers库：提供了各种预训练的语言模型的接口和工具，使得开发者可以轻松地使用这些模型进行文本分类、命名实体识别、语言生成等任务。&#xA;Datasets库：包含了各种NLP数据集的接口和工具，使得开发者可以方便地使用这些数据集进行模型训练和评估。&#xA;Trainer库：提供了一个训练和微调模型的框架，使得开发者可以方便地使用自己的数据集对预训练模型进行微调，以适应特定的任务和应用场景。&#xA;Model Hub：一个模型分享和发布平台，开发者可以在这里分享自己训练的模型，也可以找到其他人分享的模型，并且可以直接在自己的项目中使用这些模型。&#xA;datasets数据集处理，transformers预训练微调等相关教程请参考官网hugging face官方文档。&#xA;Transformers Transformers 是由 Hugging Face 开发的一个 NLP 包，支持加载目前绝大部分的预训练模型。随着 BERT、GPT 等大规模语言模型的兴起，越来越多的公司和研究者采用 Transformers 库来构建 NLP 应用，官网地址。 它提供了各种预训练的 Transformer 模型，包括 BERT、GPT、RoBERTa、DistilBERT 等。这些模型在多个 NLP 任务上取得了 state-of-the-art 的性能，并且 Transformers 库提供了简单易用的接口，使得使用这些预训练模型变得非常便捷。&#xA;安装 官网安装教程参考：https://huggingface.co/docs/transformers/installation&#xA;您可以通过 pip 安装 Transformers 库。在终端或命令行界面中执行以下命令（我这里使用pytorch，如果需要tensorflow的版本参考官网）：&#xA;pip install &#39;transformers[torch]&#39;&#xA;这将会自动从 PyPI（Python Package Index）下载并安装最新版本的 Transformers 库及其依赖项。&#xA;如果您使用的是 Anaconda 环境，您也可以通过 conda 安装：&#xA;conda install -c huggingface transformers&#xA;这将会从 Anaconda 仓库中下载并安装 Transformers 库及其依赖项。&#xA;安装完成后，您就可以在 Python 环境中使用 Transformers 库了。您可以编写代码来加载预训练模型、执行各种 NLP 任务，或者使用 Transformers 提供的高级 API，如 pipelines 来快速完成任务。</description>
    </item>
    <item>
      <title>Transformers实战02-BERT预训练模型微调</title>
      <link>https://jiaozi789.github.io/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_02/index.html</link>
      <pubDate>Thu, 18 Sep 2025 16:55:17 +0800</pubDate>
      <guid>https://jiaozi789.github.io/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_02/index.html</guid>
      <description>BERT（Bidirectional Encoder Representations from Transformers）是一种基于 Transformer 模型的预训练语言表示方法，由Google研究团队于2018年提出。BERT 通过在大规模文本语料上进行无监督的预训练，学习了通用的语言表示，并且在各种自然语言处理任务中取得了显著的性能提升。&#xA;BERT仅使用了Transformer架构的Encoder部分。BERT自2018年由谷歌发布后，在多种NLP任务中（例如QA、文本生成、情感分析等等）都实现了更好的结果。&#xA;“Word2vec与GloVe都有一个特点，就是它们是上下文无关（context-free）的词嵌入。所以它们没有解决：一个单词在不同上下文中代表不同的含义的问题。例如，对于单词bank，它在不同的上下文中，有银行、河畔这种差别非常大的含义。BERT的出现，解决了这个问题。&#xA;BERT 的主要特点包括：&#xA;双向性：BERT 使用双向 Transformer 模型来处理输入序列，从而能够同时考虑上下文的信息，而不仅仅是单向的上下文信息。这种双向性使得 BERT 能够更好地理解句子中的语义和语境。&#xA;预训练-微调框架：BERT 使用了预训练-微调的方法。首先，在大规模文本语料上进行无监督的预训练，通过 Masked Language Model（MLM）和 Next Sentence Prediction（NSP）任务学习语言表示；然后，在特定的下游任务上微调模型参数，使其适应于特定的任务，如文本分类、命名实体识别等。&#xA;Transformer 模型：BERT 基于 Transformer 模型结构，其中包括多层的编码器，每个编码器由自注意力机制和前馈神经网络组成。这种结构能够有效地捕获输入序列中的长距离依赖关系，有助于提高模型在各种自然语言处理任务中的性能。&#xA;多层表示：BERT 提供了多层的语言表示，使得用户可以根据具体任务选择不同层的表示进行应用。较底层的表示通常更加接近原始输入，而较高层的表示则更加抽象，包含了更多的语义信息。&#xA;开放源代码：BERT 的源代码和预训练模型已经在 GitHub 上开放，使得研究人员和开发者可以基于 BERT 进行进一步的研究和应用开发。&#xA;BERT 通过预训练大规模文本语料上的通用语言表示，以及在各种下游任务上的微调，有效地提高了自然语言处理任务的性能，并且成为了当前领域内最具影响力的预训练模型之一。&#xA;transformer提供了不同领域中常见的机器学习模型类型：&#xA;TEXT MODELS（文本模型）：用于处理和分析文本数据的模型，如自然语言处理（NLP）中的BERT、GPT等。&#xA;VISION MODELS（视觉模型）：用于处理和分析图像数据的模型，如卷积神经网络（CNN）中的ResNet、VGG,Vision Transformer (ViT)等。&#xA;AUDIO MODELS（音频模型）：用于处理和分析音频数据的模型，如声学模型、语音识别模型等。&#xA;VIDEO MODELS（视频模型）：用于处理和分析视频数据的模型，如视频分类、目标检测、行为识别等。&#xA;MULTIMODAL MODELS（多模态模型）：结合多种数据类型（如文本、图像、音频等）进行分析和预测的模型，如OpenAI的CLIP。&#xA;REINFORCEMENT LEARNING MODELS（强化学习模型）：用于解决强化学习问题的模型，如Deep Q-Networks（DQN）、Actor-Critic等。&#xA;TIME SERIES MODELS（时间序列模型）：用于分析和预测时间序列数据的模型，如循环神经网络（RNN）、长短期记忆网络（LSTM）等。&#xA;GRAPH MODELS（图模型）：用于处理和分析图数据的模型，如图神经网络（GNN）、图卷积网络（GCN）等。&#xA;BERT的基本原理 BERT基于的是Transformer模型，并且仅使用Transformer模型的Encoder部分。在Transformer模型中，Encoder的输入是一串序列，输出的是对序列中每个字符的表示。同样，在BERT中，输入的是一串序列，输出的是也是对应序列中每个单词的编码。 以“He got bit by Python”为例，BERT的输入输出如下图所示： 其中输入为序列“He got bit by Python”，输出的是对每个单词的编码$R_{word}$。这样在经过了BERT处理后，即得到了对每个单词包含的上下文表示$R_{word}$。</description>
    </item>
    <item>
      <title>Transformers实战03-PEFT库使用LORA方法微调VIT图像分类。</title>
      <link>https://jiaozi789.github.io/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_03/index.html</link>
      <pubDate>Thu, 18 Sep 2025 16:55:17 +0800</pubDate>
      <guid>https://jiaozi789.github.io/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_03/index.html</guid>
      <description>简介 PEFT PEFT（Parameter-Efficient Fine-Tuning）是一个用于高效地将大型预训练模型适配到各种下游应用的库，而无需对模型的所有参数进行微调，因为这在计算上是非常昂贵的。PEFT 方法只微调少量的（额外的）模型参数——显著降低了计算和存储成本——同时其性能与完全微调的模型相当。这使得在消费者硬件上训练和存储大型语言模型（LLMs）变得更加可行。&#xA;PEFT 集成了 Transformers、Diffusers 和 Accelerate 库，以提供更快、更简单的方法来加载、训练和使用大型模型进行推理。&#xA;LORA方法 一种高效训练大型模型的流行方法是在注意力块中插入较小的可训练矩阵，这些矩阵是微调期间要学习的增量权重矩阵的低秩分解。预训练模型的原始权重矩阵被冻结，仅更新较小的矩阵。这减少了可训练参数的数量，降低了内存使用和训练时间，而这些在大型模型中可能非常昂贵。&#xA;有几种不同的方法可以将权重矩阵表示为低秩分解，但最常见的方法是低秩适应（LoRA原理）。PEFT 库支持几种其他 LoRA 变体，例如低秩Hadamard积（LoHa）、低秩Kronecker积（LoKr）和自适应低秩适应（AdaLoRA）。你可以在适配器指南中了解这些方法的概念工作原理。如果你有兴趣将这些方法应用于其他任务和用例，比如语义分割、标记分类，可以看看我们的笔记本集合！&#xA;Vision Transformer (ViT) Vision Transformer（ViT）模型是由Alexey Dosovitskiy，Lucas Beyer，Alexander Kolesnikov，Dirk Weissenborn，Xiaohua Zhai，Thomas Unterthiner，Mostafa Dehghani，Matthias Minderer，Georg Heigold，Sylvain Gelly，Jakob Uszkoreit，Neil Houlsby在《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》中提出的。这是第一篇成功在ImageNet上训练Transformer编码器并获得非常好结果的论文。&#xA;这篇论文的摘要是：&#xA;虽然Transformer架构已经成为自然语言处理任务的事实标准，但它在计算机视觉中的应用仍然有限。在视觉领域，注意力要么与卷积网络一起应用，要么用来替换卷积网络的某些组件，同时保持其总体结构不变。我们展示了在这种对CNN的依赖不是必要的，纯Transformer直接应用于图像块序列可以在图像分类任务上表现得非常好。当在大量数据上进行预训练并转移到多个中等规模或小型图像识别基准数据集（ImageNet，CIFAR-100，VTAB等）时，Vision Transformer（ViT）与最先进的卷积网络相比取得了出色的结果，同时训练所需的计算资源大大减少。&#xA;具体关于该模型得结构参考：https://huggingface.co/docs/transformers/model_doc/vit&#xA;lora方法实战 图像分类微调 本指南将向你展示如何快速训练一个图像分类模型——使用低秩分解方法——来识别图像中显示的食物类别。 案例来自官网：https://huggingface.co/docs/peft/task_guides/lora_based_methods&#xA;模型选择 google/vit-base-patch16-224-in21k 是一个基于Transformer编码器的模型（类似于BERT），在监督方式下，即ImageNet-21k上以224x224像素的分辨率预训练了大量图像。&#xA;图像被呈现给模型作为固定大小的补丁序列（分辨率为16x16），这些补丁被线性嵌入。在序列的开头还添加了一个[CLS]标记，用于分类任务。在将序列馈送到Transformer编码器的层之前，还会添加绝对位置嵌入。&#xA;需要注意的是，这个模型不提供任何经过微调的头部，因为这些头部被Google研究人员清零了。但是，模型包括预训练的汇聚层，可以用于下游任务（如图像分类）。&#xA;通过预训练模型，它学习了图像的内部表示，然后可以用于提取对下游任务有用的特征：例如，如果您有一个带标签的图像数据集，可以通过在预训练编码器顶部放置一个线性层来训练标准分类器。通常将线性层放置在[CLS]标记的顶部，因为该标记的最后隐藏状态可以被视为整个图像的表示。&#xA;from transformers import ViTImageProcessor, FlaxViTModel&#xD;from PIL import Image&#xD;import requests&#xD;url = &#39;http://images.cocodataset.org/val2017/000000039769.jpg&#39;&#xD;image = Image.open(requests.get(url, stream=True).raw)&#xD;processor = ViTImageProcessor.from_pretrained(&#39;google/vit-base-patch16-224-in21k&#39;)&#xD;model = FlaxViTModel.from_pretrained(&#39;google/vit-base-patch16-224-in21k&#39;)&#xD;inputs = processor(images=image, return_tensors=&#34;np&#34;)&#xD;outputs = model(**inputs)&#xD;last_hidden_states = outputs.last_hidden_state&#xD;print(last_hidden_states.shape) 不包含分类信息，不包含label信息</description>
    </item>
    <item>
      <title>Transformers实战04-微调gpt-2生成python代码。</title>
      <link>https://jiaozi789.github.io/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_04/index.html</link>
      <pubDate>Thu, 18 Sep 2025 16:55:17 +0800</pubDate>
      <guid>https://jiaozi789.github.io/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_04/index.html</guid>
      <description>简介 GPT-2（Generative Pre-trained Transformer 2）是由OpenAI开发的一种基于Transformer架构的自然语言处理模型。以下是关于GPT-2的一些关键特点和信息：&#xA;Transformer架构：GPT-2基于Transformer模型架构，这是一种使用自注意力机制来捕捉输入序列中词语之间依赖关系的深度学习模型。&#xA;预训练：GPT-2是一个预训练的语言模型，意味着它在大规模的文本数据上进行了预训练，以学习文本数据的统计特性和语言模式。&#xA;无监督学习：在预训练过程中，GPT-2采用了无监督学习的方式，即模型仅仅通过文本数据本身来学习，而没有使用人工标注的标签或监督信号。&#xA;生成式任务：GPT-2被设计用于生成式任务，如文本生成、对话生成和摘要生成等。它可以根据给定的上下文生成连贯的文本，并且在语言理解和生成方面表现出色。&#xA;多层次架构：GPT-2具有多层的Transformer编码器，其中包含数百万个参数，使得模型能够捕获复杂的语言结构和语义关系。&#xA;大小变种：GPT-2有多个大小的变种，从117M到1.5B个参数不等，每个变种都具有不同的性能和资源要求。更大的模型往往在生成更加准确和流畅的文本方面表现更好，但同时也需要更多的计算资源。&#xA;开放许可：GPT-2是在OpenAI的研究下开发的，其模型和相关资源以开放许可的形式发布，使得研究人员和开发者可以自由地使用和构建基于GPT-2的应用。&#xA;总的来说，GPT-2是一种强大的语言模型，具有广泛的应用潜力，可用于自然语言生成、理解、翻译等各种NLP任务。&#xA;案例 该案例来源huggingface学习中心nlp-course，Training a causal language model from scratch 文章&#xA;描述 我们将构建一个缩减版的代码生成模型：我们将专注于一行补全，而不是完整的函数或类，使用Python代码的一个子集。在Python中处理数据时，您会频繁接触到Python数据科学栈，包括matplotlib、seaborn、pandas和scikit-learn库。在使用这些框架时，经常需要查找特定的命令，因此如果我们可以使用一个模型来为我们完成这些调用，那将是很好的。&#xA;收集数据 我们使用huggingface收集得content包含：“pandas”, “sklearn”, “matplotlib”, “seaborn” 这些关键字python代码 这个数据集是从github公共仓库爬取，比如 from datasets import load_dataset, DatasetDict&#xD;ds_train = load_dataset(&#34;huggingface-course/codeparrot-ds-train&#34;, split=&#34;train&#34;)&#xD;ds_valid = load_dataset(&#34;huggingface-course/codeparrot-ds-valid&#34;, split=&#34;validation&#34;)&#xD;raw_datasets = DatasetDict(&#xD;{&#xD;&#34;train&#34;: ds_train, # .shuffle().select(range(50000)),&#xD;&#34;valid&#34;: ds_valid, # .shuffle().select(range(500))&#xD;}&#xD;) 让我们看一个数据集中的例子。我们只需显示每个字段的前200个字符：</description>
    </item>
    <item>
      <title>Transformers实战05-模型量化</title>
      <link>https://jiaozi789.github.io/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_05/index.html</link>
      <pubDate>Thu, 18 Sep 2025 16:55:17 +0800</pubDate>
      <guid>https://jiaozi789.github.io/docs/programming/ai/tools_libraries/transformers/actions/transformers_actions_05/index.html</guid>
      <description>简介 模型量化（Model Quantization）是一种优化技术，旨在减少机器学习模型的计算资源需求和存储空间，同时在精度损失最小化的前提下提高推理效率。量化通过将模型权重和激活函数的数值从高精度（如 32 位浮点数）转换为低精度（如 8 位整数），显著减少了模型大小和计算复杂度。&#xA;主要类型 静态量化（Post-Training Quantization, PTQ）&#xA;在模型训练完成后进行量化。 通过分析训练数据的分布，将权重和激活函数映射到低精度表示。 不需要重新训练模型。 适用于对性能影响较小的场景。 动态量化（Dynamic Quantization）&#xA;在推理时动态地将浮点数转换为低精度整数。 在运行过程中对激活函数进行量化。 比静态量化更简单，因为不需要分析训练数据。 对推理速度有显著提升，尤其是对模型输入依赖较少的层（如全连接层）。 量化感知训练（Quantization-Aware Training, QAT）&#xA;在训练过程中模拟量化影响。 模型在训练过程中考虑量化误差，以便在量化后保持更高的精度。 比静态量化和动态量化需要更多的计算资源，但精度损失最小。 适用于对精度要求较高的应用。 这里例子就演示下动态量化，bitsandbytes本身以上三种都支持。&#xA;量化的优点 减小模型大小：通过将权重和激活函数表示从 32 位浮点数转换为 8 位整数，模型大小可以显著减少。 加快推理速度：低精度运算速度更快，可以显著提高推理效率。 降低内存带宽需求：低精度表示占用更少的内存，减少了内存带宽的需求。 量化的缺点 精度损失：由于数值表示的精度降低，模型可能会经历一定程度的精度损失，具体程度取决于模型结构和数据分布。 复杂性增加：在某些情况下，量化过程可能会增加模型部署的复杂性，尤其是需要进行量化感知训练时。 量化过程 以下过程只是一种最简单的思路，方便理解，实际要比这更复杂。&#xA;量化过程 确定值域： 首先，确定要量化的数据的值域范围。例如，假设我们有一组数据的值域为 $[min,max]$。&#xA;确定量化级别： 确定量化的级别或分辨率，这决定了将值域划分成多少个区间。在4位整数的情况下，共有 $2^4=16$ 个可能的值。&#xA;线性映射： 将原始数据映射到4位整数的范围内。通常使用线性映射来实现，计算公式如下： $$\text{quantized_value} = \frac{\text{original_value} - \text{min}}{\text{max} - \text{min}} \times (\text{number of levels} - 1)$$&#xA;这里的 number of levels 是16（对应4位整数的值域范围）。</description>
    </item>
  </channel>
</rss>